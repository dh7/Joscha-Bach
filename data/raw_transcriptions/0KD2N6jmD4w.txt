('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/0KD2N6jmD4w"> </iframe> </center>', " Boom! What's up everyone? Welcome to Simulation. I'm your host, Alan Sakian. Very excited to be talking about conscious machines. We have Dr. Yoshua Bak joining us on the show, hello. Hi. Thanks so much for coming on, really appreciate it. I'm super excited for this episode. This is gonna be a really good one, guys. Dr. Yoshua Bak is VP of Research at AI Foundation and author of Principles of Synthetic Intelligence. He's focused on how our minds work and how to build machines that can perceive, think, and learn. And you can find the links below in the bio to bach.ai, b-a-c-h.ai, as well as his LinkedIn and Twitter profiles. All right, Josche, let's start things off with one of our favorite questions. What are your thoughts on the direction of our world? I think we are right now at the top of our world. We are at the best possible position to be born into. Imagine that you have the choice of being born into a sustainable, sane world that is an agricultural society, 300 million people tops, no internet, no long-range transportation, no interesting educational system, and so on, right? Or you can be in an insane world, a world that has just burned 100 million years worth of tweets in a single century to give everybody plumbing and access to the world's knowledge as an afterthought to a global chat room for porn distribution, basically the internet, right? And this is the thing that we have right now. It's an insane world. It's a world in which we have computers, in which we can travel anywhere. We can talk to 7 and 1⁄2 billion people that are often very smart and interesting. And we are in the first time in history on this planet of any life form, I think, in a situation that we can meaningfully reflect the universe. This is a really amazing time to be alive. It's really the best time to be alive. So in some sense, you could see life on Earth is this entropic puddle that is more or less sustainable for a while. So in some sense, you could see life on Earth is this entropic puddle that is more or less sustainable for a while. I mean, 1 and 1 half billion years, then the atmosphere will be gone. And life on Earth in its current form will cease to exist. And a few billion years later, the sun will engulf the planet. And I don't think that there will be a meaningful export of life from this planet in an intelligent form. But might be. Who knows? Probably not our species, though. We are the first ones that basically made this very short game to jump out of this puddle as high as we can. And we are right now, I think, at the top of this parabola. And from here, of course, it will go down again. So people look down, and they freak out. They realize basically we cantilevered ourselves over an entropic abyss, and there's no land on the other side. When we come down, it will not be gentle. But it was never nice down there to begin with. You don't want to be down there. You want to have a few generations up here if your goal is to understand what's going on in the universe. And if that is not your preference, if you want to have a sustainable world, well, bad luck. You didn't get to make the choice. These decisions were made long ago. I think the last important bifurcation that we had was when we opted in for the Industrial Revolution. And there was not much that we could change that train on that we are currently in. And I'm very much in favor of trying to make a longer game and make it more sustainable, but it's quite likely that we are unable to return to a sustainable state, or that we are able to make the present growth-oriented cancerous world sustainable, right? But yeah, you can look at this, oh my god, I'm so depressed because the world is going to go down from here, or you can see it's never been that high up, right, and we wouldn't be talking if we wouldn't be in this giant titanic that is heading for the iceberg. Okay, excellent synthesis. I do want to know how you know that we are at the top of the parabola and not how we're not just going to continue going up on like a y equals x just keep going up. So typically when you look at the way things happen, right, if you see some kind of growth and if you zoom out a little bit you see that it is usually part of an S-curve that hits a saturation point, right? And if you zoom out even further you see that this is going to come down because everything that comes up tends to come down again at some point. And the question is where you are right now and people tend to extrapolate from the current direction of the curve and not from the logical dynamics of the overall curve. And when it becomes apparent that things are flattening out or things might be going down, people go into denial because they get invested in the present state of affairs which looks like it's going up. We can arguably say that over the last several million years that we've been following an S-curve in terms of amount of people, amount of intelligence, like you said, access to the wealth of knowledge on the internet, all these great things, longevity. But why is it all of a sudden going to go from this up in the S-curve to a down? I think that for the last few million years, if you take this perspective, there was mostly not many humans around, right? After the Mount Toga eruption, there were a few. By population bottleneck, it was, I think, 7,000 people or so. Not that many ancestors that we depend on. There were probably more people there, but those people that were successful and replaced everybody was a small group back then. And then we were, for a long time, about a million people, a million human beings roaming the planet. And then we were for a long time about a million people, a million human beings roaming the planet. And then we invented agricultural societies and probably religions to organize them and so on. And this got us to something like a hundred million people. And at the turn of the times, when we had switched to monotheistic religions, we were at something like 300 million people. And we stayed there for a long time. There was little growth and dips if we had a war or some famine or a big disease and the pandemic. And then basically at the beginning of the Industrial Revolution, we were still at something like 400 million. And only Industrial Revolution got us to billions of people, the modernist societies. And now these modernist societies have mostly collapsed after World War II, and we are living in the aftermath of these societies that have been created basically in lapse, where people designed the social systems that we are in that are optimized for growth, for feeding large number of people, for having mass media to coordinate them, and so on. And these societies didn't invent good mechanisms for error correction, so they're manifestly unstable which freaks people out right now. Okay, so the instability of the massive systems that are letting 8 billion of us play on this planet is one of the reasons why you predict that the S-curve goes down. You could put it another way. Every organism that stays around for a long time is playing a very long game. So basically, the fitness function of the evolution needs to get to the individuals, not just in every generation, but over many generations. For instance, if you are a mouse and you live in a complicated ecosystem, you depend on many, many food species. If you have food scarcity, what happens is the next generation of mice is more sparse and more scrawny. And this happens for a few generations so they don't overgraze on their environment. So the species can stick around the long term if their environment comes back and regrows, right? And we are now in the situation that we have the choice of feeding our children, of having them being alive, or having civilization being alive. We always pick our kids, obviously, right? If you have the choice of having your children die, or having civilization die in the long run, you want your children to have a good life. You want to make them well-fed, you want them to be healthy, to be educated, they should have a good house, they should have access to friends and computer games and so on. And because we are always confronted with this choice and individually we are basically unable to constrain ourselves in such a way that we don't overgraze our resources. We don't seem to have a force that would constrain us. But at the same time we are heading in to a large degree towards the abundance of being able to grow our own things like meat and leather and all these other interesting ways of being able to sustain more effectively on the planet sequester carbon, a fusion for energy. So it's just how is it that we have either kids or long-term civilization? Why can't it be both providing for children and having long-term civilization? I think it might be possible, but it's not going to be all roses if we would be doing this. We probably would have to submit to an organizing principle that is extracting a heavy toll from us. I mean, it's conceivable that we create some AI dictatorship that organizes us in such a way that we live in a few population-controlled habitats where we can outlive our days as a species in relative peace and harmony, basically some kind of people zoo. But if we let people self-organize, we are very much like orcs that overrun the environment and eat everything in their path. And then when we think about how to make this sustainable, we tend to focus on these few ideas. Carbon sequestration is an excellent example. The reason why all this carbon is in the atmosphere, all this carbon dioxide, is because by burning the carbon into carbon dioxide, we got the fuel that built the industrial civilization in the first place. The reason why we have 700 billion people living in relative comfort is because we burned all this carbon and put it there. So basically we are running on the debt and paying back this debt was never in the cards. And a carbon sequestration would mean basically we would need to use a multiple of that energy to, without a business case, to get all this carbon out of the atmosphere again and bury it in the ground in a way in which it doesn't escape. And we are still burning carbon that we get out of the ground. We are still doing this. So the easiest way of carbon sequestration would be to stop burning any carbon at all, and especially not getting new ones out of the ground. And we don't do this. So I think it's a pipe dream. It's wishful thinking. People are very good at wishful thinking. It's an interesting phenomenon to observe. And it's also nothing to be upset about, right? It's just how people are. Like you said, the organizing principles are the things that can keep propagating the S-curve up if we can galvanize together and make these code changes within our social dynamics. OK. If you have something that has unlimited growth, it hits a singularity at some point, right? There is no system that remains stable without symmetries. Okay, all right, this is an excellent intro to the episode. All right, let's do the journey. So, okay, born in Germany, loved computer science and AI, growing up in philosophy as well, and then you ended up doing your PhD in cognitive science, you spent some time in New Zealand as well, so teach us about who you were growing up and how you even got interested in these fields. Well I grew up in some somewhat insane society, communist Eastern Germany. It had burned down its civilizational hive mind, so to speak. Every philosophy was replaced by a very vulgar version of materialist Marxist dialectic. And everything that your teachers told you seemed to be, at least to me, to my little nerd mind, insane and unbelievable. Like, not even the mathematics was correct, I felt as a child. And I grew up as a child of an artist in the forest. So basically a feral in a big cave full of books. And then I ventured out into the world after having already read too many books. I went to school, I was bored for eight years, then I went to a mathematics school. Then I saw what real mathematics looks like, and then I decided that it's possible to make sense of the world, but I would have to do it by myself. But to understand the meaning of a statement, what it actually means, that something is true or not true, in some sense you have to go full circle, right? You have to go down to the foundations. And to make this journey to the foundations, that's in some sense my journey. It's very hard to make this right as an autonomous intellect. Most of the autonomous intellects don't get very far, and I probably don't get very far either. Whenever I have a tremendous insight, I'm typically like 400 years late to the party. But I discovered that these parties become more interesting and obscure. There are smaller parties now that I discover. Yeah. Yeah. Yeah. At the same. Yeah. Yeah. Yeah. Yeah. At the same time, there are the big parties, right? There are large groups of people that share ideas. And these large groups of people that share ideas, they are often not right. Because the principles by which people get to consensus are often not truth-oriented, right? So individual minds often get it right. And groups, if they agree on the thing, that's not likely to be true. Like the scientific consensus on arbitrary topics is largely something that is going to change. Like there was for a long time a consensus about nutrition. And we now know it's basically all wrong. And how is that possible? And I think if we are, for instance, programmers, realize it's impossible for us to even agree about simple things, like which programming used for which kind of project. It's very hard to do this. And if you have a consensus about complicated questions, more complicated than this, you need a force that builds consensus. And this force is probably not truth-oriented, but consensus-oriented. It's a political force. So people, if they are thinking groups, tend to believe things for political reasons, not because they are true. So we have to find this balance. We have to figure out to which degree do we have people proper epistemology. And this question of epistemology is, in some sense, the foundation of our thinking. We should have all the possible beliefs, but shift the confidence according to the evidence on them, right? So we can only believe something to the degree that we have evidence for it, that it's possible that this is supported by something. A belief should not be a verb, it's not related to us, we should not be identified with our beliefs. We should only be directed by truth, we don't get better people if you believe the right thing or worse people if you believe the wrong things. These are just things that we are looking at, models in our mind. Let's jump into this. Okay, yeah, let's do it. It totally actually gave me a bunch of light bulbs when you were talking about who you are, how you became who you are. Yeah, when you were talking, I was like, wow, this makes more sense along the way. You actually ended up moving to Cambridge as well doing some time at MIT Media Lab and Harvard Evolutionary Dynamics before coming out here. Yeah. Well just quick what was the spark that got you to move from Germany to Cambridge to the United States? Well I studied computer science and philosophy. I found that philosophy was not as useful as I hoped and only later I learned that this was because of the state philosophy in academia is in right now. It's mostly a simulation of the real thing. It's very hard to get paid for real philosophy. You typically get paid if you're a philosopher for being part of a particular culture of talking like a philosopher, reading like a philosopher, citing like a philosopher, and publishing like a philosopher. And it's very hard to do actual philosophy on the side, like intellectual progress. You're not incentivized to do that. So more interesting progress, I think, happens in more foundational fields like computer science where people have a formal education. And I studied a little bit of psychology and physics and so on, and then I did my PhD in cognitive science. And I found it hard to fit into the academic system as it was, because I was largely mostly interested in applying methods and not so much in answering the questions that I had. And so I started a few companies and co-founded them. And then I got an opportunity to work at MIT, which I jumped on and so I spent some time in the US. And now I got an offer to join an amazing team in San Francisco called AI Foundation where I can work on some of my ideas on how to model motivation and emotion in more detail. Yeah, and the motivation and emotion of how we perceive the world as well as how machines will perceive the world is a central key to what we're going to be discussing now. So, all right, let's jump into it. We have some awesome visuals to show you guys along the way. Okay, so we find ourselves in the 3D world, in the physical world. What is the relationship between the physical world and our minds? It's an age-old question, right? So we could first of all start out with this idea of what is access to the world. When I was sitting in front of my first computer, a Commodore 64, at about 1983, I looked at the screen and I realized that everything that I can think of can be displayed on a screen, if I really know how to think about it. I can write a program that produces an arbitrary pattern on the screen. Everything that I perceive is some kind of pattern. So in some sense, my access to the world, the universe out there, is to some kind of screen. And that screen might be my retina or my thalamus, and my brain is making sense of this. And to understand the world, I need to understand what a pattern generator looks like that is responsible for producing those patterns. So we can ask ourselves, what kind of computer do we need to produce the patterns that we observe? And it could be a discrete state machine like my Commodore 64 with a finite amount of memory. Or it could be a probabilistic state machine that randomly goes from state to state or has a certain degree of randomness in there. Or it could be some quantum computer where every state is a superposition of ground states. Or it could be one of these versions, but with infinite memory. And number four is basically our discrete state machine. With infinite memory is our old friend, the Turing machine, this mathematical formalization of modern computer. Or we could take the perspective of physics, which mostly thinks that the universe is geometric, which means things move continuously. And there is, at least at the time level, infinite resolution. And it could also be that it's even an a-causal hypercomputer. So there could be closed time-like loops, so information could be sent back in time. All these are, in some sense, mathematical possibilities. But what we have discovered is that there is a difference between mathematics and computation. Mathematics is the domain of all languages in which you can specify things. And computation is the part that can be implemented. And there's something that was discovered earlier in the 20th century. Basically, Hilbert saw some inconsistencies, especially after looking at counter set theory, when it came to infinities, to infinite sets, and the total set. And he discovered we have a problem in semantics of mathematics in the way we define truth, and we define what is true at all. Please, mathematicians, fix our meta-mathematics. And so they went to work, and Gödel came back and said, I have bad news. We cannot really build a computer in mathematics that doesn't break, while running the semantics of mathematics. Oh, we have a, oh, there's a question? Oh, that should be in there. What are his thoughts on psychedelics? We're already getting asked about your thoughts on psychedelics from the audience. Maybe we should just take a quick, what are thoughts? your thoughts on psychedelics from the audience. Maybe we should just take a quick what are thoughts. No. That was by accident. I think that psychedelics relax your priors, which means there are more things that you consider to be true when you are in psychedelics apparently. And as a result, you can make inferences that you otherwise could not make, but which are largely wrong. So basically, psychedelics might help people to escape local minima in the belief space. And it's the same purpose, I think, what dreams have. And I suspect that psychedelics are very similar to lucid dreaming in their functionality. Escaping local minimum beliefs, yeah. Yeah. That's good stuff. Yeah, but sometimes the past that can be wrong. The thing that you enter in does not have to be a better belief. It can be one that is fundamentally more loopy, especially if you fall in love with it and don't escape that minimum again. Yeah, yeah, yeah, yeah, because we can become attached to that idea and then that. Yeah, correct, versus continuing to have an open mind. OK, yes. Keep going. So Turing also worked on this problem, the same one that Gödel worked on. And he discovered that the same problem, right, truth is defined by a proof in mathematics. Something is true if you can prove it. And a proof is the reduction of a statement to axioms. It's basically a form of data compression. You compress the statement to the axioms. And the way you do this is you apply a procedure. And mathematics is defined in a timeless fashion. So you can use infinite many steps to get there. And what Gödel and Turing discovered is that you cannot use infinitely many steps because nobody can do this, right? Nothing can do this. You cannot make the claim that something is true when you cannot actually get there. In order to actually get there, you have finitely many steps, which means you have to redefine truth into something where you actually show the money, where you actually show how to get from your axioms to the proof in a finite number of steps. So in this sense, pi, for instance, is not a value. Pi is a function. You can run this function. It gives you new digits until your local sun burns out, and this is it. a value. Pi is a function. You can run this function, gives you new digits until your local sun burns out, and this is it. But you can never have a computation that depends on knowing the last digit of pi. And it's interesting that a lot of physics behaves as if you could, because physics basically checked out this code library from mathematics without reading the comments, especially the more recent ones that were done in the first half of the 20th century. So basically, the part of mathematics that works is constructive mathematics. It's the thing that you can implement its computation, which means, of course, we can build an a-causal hypercomputer in some sense, a computer that can send information back in time. We just do this by freezing this universe state, then running a copy of the universe into the future, taking that information and merging it into the present state and continuing. So in some sense you can do this. You can not have a continuous universe, but you can have one that is arbitrarily quasi-continuous, that gets as close to continuity as you want. And on the level where we interact, there are always too many things to count, so if you squint it's almost like infinite. So if you look at these computations of how many parts behave in the limit, which means they're almost continuous. But if you zoom in, you realize it's still integers. It's still discrete. There is still only finite amounts of information being moved around in finite amounts of time. So in some sense, we can use computation as a metaphor. This ability of a system to go from state to state is a transition function. It's a deterministic one if it's the same one every time, and a deterministic one if it's a different one every time. We can use this to understand all the pattern generators that can produce observables. But we can also build minds that observe these principles and do anything that's computable, that is, make any kind of model. So basically, Ocommodore 64 is enough if you give it enough memory. It's a very interesting question about quantum computers, of course, because quantum computers have been around the corner for at least half a century. And we haven't gotten quite there yet. And it's an interesting question why that is. Why is it And we haven't gotten quite there yet. And it's an interesting question why that is. Why is it that we haven't gotten quite there yet? Is it because the formalization of quantum mechanics is not completely computable and is only approximately true? Or is it because of something else, because there's too much noise in there? Maybe there is no continuous space underlying the whole thing, and the non-local links create too much noise. Or another way of looking at the whole thing is that the premise of quantum mechanics is that our particle universe is inefficiently implemented on the underlying substrate. If the particle universe is inefficiently implemented, we can harvest the computation of the underlying substrate. Like imagine you live in Minecraft, and you are efficiently implementing the CPU, then your CPU is only polynomial times faster than you are. So you cannot build a computer in Minecraft that is going to be faster than the normal computations of Minecraft, classical Minecraft operations. Even if you use the CPU, you're only going to be faster by some polynomial amount of time. But if Minecraft is inefficiently implemented, if it's super slowly implemented on the CPU, this is Minecraft you wouldn't notice. But if you could somehow harvest the computation that is underneath it, even if it's a classical one, it would be more than polynomial fast with respect to the computers that we can make from Minecraft particles. So this is the interesting question. Basically, is our universe efficiently implemented or inefficiently implemented? But it's still going to be computational. OK, so let's go back a step, and let's think about what's our relationship to the universe. What's the relationship between mind and universe, right? And so the traditional question is. That was an interesting priming for the first question. Oh man, holy cow, there's... it's very evident that you've taken a lot of time to synthesize these fields together into what you're presenting to us now. So basically the idealist position that you'll find in many Eastern religions, for instance, is the one that we live in a dream. The reality that we perceive is not mechanical. There is something else going on. It's a dream in which magic is possible, in which you can have precognition, in which you can perform rituals that change the course of the planets and human destiny, and so on, in which you can perform rituals that change the course of the planets and human destiny and so on, in which astrology works. All these things are only explainable by us not living in a mechanical universe, but in a dream universe that is dreamt by a mind on a higher plane of existence, right? So you can have these symbolic interactions, you make this ritual sacrifice and as a result your life changes. How is that possible? This is something that we can basically only explain by accepting that we live in a dream if we think that is real. We could also say we have a dualist situation where your mind and universe are basically separate computers that interact through an interface. So we have our computational mind that is making sense of the world, and on the other side, we have our computational mind that is making sense of the world and on the other side we have our computational universe. And the typical position that we have right now in our Western world is a modest position. Basically your universe gives you the computations on which you have your mind run. Basically the part of the universe that runs your mind is the brain. And on that brain computer we generate interpretations of what's happening in the outside world. We observe the world through some kinds of observational interface like our retinas and cochleas and so on. And the surface of our body. Okay. And why is this mono? Monism. Because it's only one thing. There is not a mind world and a material world, but it's all the material world. Right? It's all this primary substrate in which we exist in a monist world. So basically the materialist monism says the universe is mechanical and everything that happens in your mind is part of that mechanism. And what I would suggest is a small correction or a small change to this perspective. I think that these positions of materialism, one is materialism and one is idealism, where you only have dreams and where you only have the material world, are actually in some sense complementary. I think that you do exist in a dream. But it's a dream that is being dreamt in the brain of a primate. Right, and this primate lives in the physical universe. So the mind on the higher plane of existence is in physics. Physics is the higher plane of existence. The reason why previous civilizations were largely not interested in describing this higher plane of existence is because they didn't have devices to play with physics. Or because they didn't have social technology by which a philosopher could change the feudalist agricultural society that they were living in. And they were not putting much emphasis on this. If you don't get along with the world as it is, you need to change the way you perceive it. You need to change your motivations. You need to change the way you generate your dream of the world and your place in it. And if you adjust this, you're usually fine. Or you're not, but you're as fine as you can be under the circumstances. So basically, you focus on this inner world, on how you generate your perception of reality and these techniques. And this is something that we largely don't do in the West. Our Western rationality was largely leave the mind alone. And instead, if you don't like something, change reality. So we focus on this outside world and we neglect the way our perception is generated. And a lot of people, even a lot of Western philosophers, are fascinatingly unaware of the fact that we live in a dream. That the same circuits that make streams at night, they make a dream during the day. Only the dream during the day is tuned to predict your sensory patterns. Right? So it's a model that we make. What is a model? When we're in our dream space and when we're in our awake space, we are running, we're using same neural architecture within ourselves for both of those situations. Yet there is a, in the physical world, there is a, there is a, this... It's a machine made from neurons. And there's a, you called it a dream state that we're in, in the body of a primate. Yes. But the monism is that it's all in this... It's all, it's basically only a mechanical universe, and the mechanical universe creates emergent patterns. But VR is real as a character in a novel. You and me are characters in a multimedia novel that is being authored by the brain. Star of my own movie. Yeah. I keep telling you, Alan. Star of our own movies, all of us. Yeah. But a star of our own movies or star of collective a Collective movie there is no way you can get out of the individual dream the collective is part of your dream It's not that other minds tell you what to do you? Interpret a part of the universe of the patterns that you see on your retina as being emanations of other people other minds are your invention now as being emanations of other people. Other minds are your invention. Other minds are my invention? Yes. You can only perceive other minds to the degree that you can perceive my mind. That you construct a model of it. That you think I'm a sentient being that you can interact with. But this interaction takes place entirely in your own mind. Yes. takes place entirely in your own mind. Yes. Okay. Yeah, there's so much to... There's no way to wake up from this dream. Whenever you find yourself immersed in a reality, you are in a trance state where you basically take some representation and think you are inside of that. It's a similar trance to when you see a movie, right? When you see a movie, you suspend your disbelief and you are that cowboy that sees the wild west and you ride along the prairie and then suddenly the light goes on and realize, oh my god this is all not true. I'm in a different reality now, right? And you cannot wake up from that one. At least you cannot wake up from it without losing what makes you human, what makes you make sense of the world. And there's my ability to change the way that I perceive versus changing a physical world. That was one of the points that you made. So we can kind of upgrade our own perception of reality and the way that we engage with the reality that we're embedded in versus trying to necessarily change that physical reality. It's often also very necessary that you become aware of that. For instance, if you have self-hatred, you perceive that the universe is mean to you, that you're experiencing many ways that the universe treats you badly. And you also treat yourself badly, because part of your software does not really buy into you. It thinks this thing that you are is very flawed and should not really be supported. So if you don't like yourself, you're going to punish yourself in some sense, which means your mind is going to create a simulation in which reality treats yourself badly. And this will sometimes lead to bad outcomes, sometimes it might lead to better ones. There is a reason why your mind thinks this is the best setup. But this perception that you have of reality is virtual. This identification that you have with your beliefs and your experiences is something that's generated inside of your mind. It's basically like a groove that is pulled through a record in a record player and the record is the groove of your life and you're being pulled through this and when you dream you can put this groove into something else or when you are in the psychedelic state like our friend here, us. Right, and so this identification with what you feel, what you experience, in some sense it can become your choice, you can become responsible for this. So I can simulate a self-hatred state or I can simulate a self-love state. And so upgrading my perception is upgrading the way that I choose which I simulate, which I collapse into my reality. You could also get to a neutral state in which you realize that all these evaluations that you put on there where you think that you are super lovable or super flawed are actually not helpful or not truthful. And you just are, you're just a physical system. And you're not in the category of things that has the ability to be adequate or inadequate. You just exist and you deal with that. Okay, yes, yes. Let's continue. Good. So how do we make models? A model is basically a set of variables, things that can change. And the relationships between them, the relationships are those things that don't change, the invariances, right? The relation between the different variables is, for instance, the laws of perspective. They don't change in the world in which we exist. That's an invariance. And the laws of perspective dictate if I see your nose and your nose points in a particular direction, which is a free variable, it predicts the position of your face. Your face should point in the same direction as your nose. Otherwise, I'm looking at something that is impossible. So these relationships are, in some sense, they are possible-istic. And the world that we are in is not a probable world, it's a possible world. Even if a tiger comes after you and it is not probable, it should still be possible to represent this, right? And we will always try to it is not probable, it should still be possible to represent this. And we will always try to find an interpretation of our sensory patterns that is possible, where every feature that we perceive is compatible according to the relationship that we've learned to all the others. But in order to get there, we need to learn probability, because there are infinitely many possible states, or almost infinitely many. And so when we see a bunch of patterns after we wake up in the morning, we need to converge on one of these possible states. So we know that your nose being pointed a certain ways where your face is gonna be pointed with almost 100% accuracy, almost. If not, then we have learned that there are prognosis. So we will put our attention on this thing where we have this inconsistencies, and we will try to resolve it. And this means usually we extend the model. And this is largely how we learn. We discover inconsistencies in our models, and then we extend them by adding new relationships or modifying existing ones. And if we have a model that already works, most of the time, then we need a way to get it to converge to what we currently perceive. The patterns that we have in the environment need to be interpreted with a particular kind of model. So we get this model state by following probability. The probability tells you if I see the following patterns, it's likely that the following model should apply and I should interpret the world in this particular way. And these links are responsible, for instance, for optical illusions. They're shortcuts that make us see the world in a particular way even though it might be different. But it helps you to speed up your convergence to a particular state. That's why we have many of these illusions. And then we have the preferences. They give color to our perception, right? It's not just all meaningless, interesting, structured patterns. There is something going on that is relevant to us because it can give us pleasure and pain. And these preference relationships, they tell us how certain things deviate from a target instead of just measuring them absolutely, right? So in some sense, this measurement that is not absolute but relative is an identification where you think something should be like this instead of it's like this. And this is where we could say within the model there are motivations. Yes. And the motivations guide us towards the preferences. Yes. And the preference means you are a system that wants to change these things towards that preference. So when you feel pain, you might have a preference to make that pain end. And in some sense, it's the nature of that pain, right? Or if you feel that you live in a world that is not just, you might feel a pain that directs you on making it more just and you identify with this, you realize I am a system that is directed on making the world more just and it is some imposition on the world that you make and it might be a good one, it might have good outcomes, it might also be that the just world never existed and wouldn't even work, but it's a commitment that your mind makes that you should perceive the world in a particular way, the world should be different than it is. In this sense that the world should be different than it is, that's the purpose of the existence of our mind is to help the organism to regulate for its evolutionary needs. And when we are born into the world, do we have within us already the DNA for the preferences and for the motivations, for what our character decides is the thing that's our North Star? To some degree I think so. I have found that in my own children, after they're born, they had very strong preferences and eventually what became more detailed was not the preferences, but their ideas, their own ideas of what exactly the preferences are and how to realize them. And that largely depends on the environment, but I suspect that the things that make you happy or unhappy are things that are determined innately in the organism. Yeah. And it also makes sense, right, because motivation is not adaptive, it's resistive. Motivation is how you resist physics and society and everything else. It's not to give in, just dissolve into a puddle on the ground. It's where you push against reality. It's where you regulate. So it needs to be innate. It cannot be adaptive. We're running within us the motivations that we want to adapt, change our perception, but also as well in the reality that we live in with what we want to do in that reality, how we want to change that reality towards those motivations, those preferences. But of course we can learn to change this to some degree. We can learn to disidentify from certain things. But it's hard and it typically happens relatively late in our life. Yes, yes, okay. So basically the control of this organism starts with feedback loops and they're implemented in our brainstem, our reptilian side so to speak. So there are lots of feedback loops that regulate your breathing patterns, heart rate, your body movements, and so on. And as soon as it becomes so complicated that it cannot be done automatically, you create models of this in your neocortex. And in between, you have these identifications where you... This is autonomic nervous system, blinking, heart rate, yeah, yeah. Yeah, but you can make all these things, you can submit them to your own control, and you usually do so when the automatic regulation fails. Then you will spawn some cortical process that takes care of regulating your reality. So the most primordial feedback loops are at the most limbic earliest reptilian. I think they're being made available here at the brainstem because they have the potential that they need to be regulated. For instance, if your health becomes fragile, you might need to pay attention to your heart rate and you can learn how to regulate your heart rate. But as long as that's not necessary because your autonomous regulation of the heart rate is perfect, you're unaware of your heart rate. And I suspect this happens for almost all the things. Whenever your feedback loops are sufficient, you don't need to pay attention and you don't need to run lots of particle computations on it. And the attention is necessary to create new mental processes that then become automated. And as soon as they're automatic, you usually don't spend attention on them anymore. Which means you cannot remember them. Then you're no longer conscious of them, right? And then if we want to remember, we need to go through the process of going through that tedious remembering of how to do that. So the limbic system is in some sense implementing a machine of motivation where you have social and cognitive and physiological needs and they all compete. They create different pleasure and pain signals and when we act we don't just act on pleasure and pain, we act on models of them which you could call purposes. The needs don't form a hierarchy, they just co-exist and compete. They don't form a hierarchy? No, needs just co-exist. They might have different strengths, but there are people that might starve because they serve political goals or because their children are more important to them. It's not that you first solve existential safety and then you do the revolution or then you do the art. It might be that you do this all at the same time and you just get drawn to these things in different proportions and depending on the opportunity and the relative strength of the need at a given moment you are more drawn to this or more drawn to that. But when you want to coordinate your life you need to make models of them, you need to understand concepts like survival. Survival itself I don't think it's a need, it's too complicated, right? You need to understand death in order to understand survival. Death is a complicated concept, it's too complicated. You need to understand death in order to understand survival. Death is a complicated concept. It was difficult to understand for my children. I remember when they realized, oh, some organisms stop moving at some point, and all people at some point, they're no longer around, and this will also happen to me. And it's something that happens relatively late. It's not in the first few months. But in the first few months, But in the first few months you already have pain avoidance and some sense of existential safety and so on. You only combine this into a purpose of survival when you are able to form the model. And these purposes, they need to form a hierarchy so they become accountable to each other. And this hierarchy has different solutions in different people. So in some sense the shape of your soul is the hierarchy of your purposes. And the top note is usually not the ego. The ego is just a function that integrates the expected reward over the next 50 years or so. And the really interesting functions are above that, where you serve long-ranging purposes and this allows us to cooperate because we serve shared purposes. Yes. I think that's at the core of this altruistic love is the discovery of shared purpose in others. Yes, so the North Star that we find most meaning in doing every single day usually transcends our own ego towards the shared purpose. Yes, you also realize your ego is just a lot of work. Why do you keep it up? There needs to be an underlying purpose to make that necessary. You need to be relatively stupid to think everything is only important for your ego because you're not thinking very far, because you're not getting much out of the ego ultimately. It's not creating that much pleasure to have an ego. Gosh, this part is such a mixed bag of motivations, even right there. Cognitive, physiological and social. It's a huge mixed bag and however we end up determining what the next step is, it's a huge mystery at times. I suspect it gets more boring when you figure it all out, right? And you always know what the next step is. Yeah, yeah, it definitely gets more boring. Well, hopefully when we run the simulations we can gain more insight through these processes. Looks like we're already potentially in an 80 year process of leveling up some character. Imagine you had an Alan simulation that tells you always what your next step is, that outmodels you and thinks a few steps ahead and tells you, these are your options and this is the one that you're going to pick and this is the one that you should be picking. We could potentially visualize that even right now. Right. Yeah, what is the next best option? Yeah, correct. And to be able to be in a state of constantly engaging with that is beautiful. But it's also, it can be, it takes all the mystery away. Yeah. Okay. Neurons live in the lonely universe. I thought about how is the neuron able to do this. all the mystery away. Okay, neurons live in the universe. I thought about how is the neuron able to do this. When we do AI, we typically think about how to impose the will of the mind on matter. So when you think about how Minsky built AI, you had an idea of how things work and then you implement this as an algorithm that plays chess or does whatever, right? And now we live in the second phase of AI where we have systems that discover the solution to our problem. So we give a problem and a loss function that tells the system what it does on the problem to the algorithm. And the algorithm figures out how to implement this solution. But it's still a very top-down centralized control. And our neurons are doing it very differently. There is no centralized control in the brain, because everything has to be implemented with individual cells. The cell is the smallest unit there, from some perspective. You can also take different ones where it's even smaller, and it's molecules within the cells, right? But if you take the cell as the smallest unit of compression, then you have your neuron. And the neuron, in some sense, lives in the darkness. It's a small animal that tries to get fed. And they link up. It's embryonal neurons in the Petri dish. And they try to find connections to each other. And they try to learn which signals in the environment predict anticipated reward. So which signals they should fire on to get fat in return. And this is in some sense all they do. Dopamine is our reward. So functionally, dopamine is globally for the brain responsible for reward. It also has a few other local roles. I suspect what happens is that the brain is very good at making a few dozen chemicals. Some of these chemicals can be very quickly exchanged, some of them more slowly. I suspect what happens is that the brain is very good at making a few dozen chemicals. Some of these chemicals can be very quickly exchanged, some of them more slowly, and it uses them as a language. And the words of this language, one of these words is dopamine, this molecule, they mean something different in different contexts. And when you, for instance, use a dopaminergic drug like Adderall, it often interacts with other systems, it uses the same language. So in the striatum, dopamine means something different than in the prefrontal cortex, and as a result, you might get more control, but you also get more tics. Right, so it's because it's not one thing, it's just a particular kind of language that is being spoken between the neurons. But from the individual neurons, some of them are more serotonergic neurons and they get rewarded by acting on serotonin. Yes, yes, okay, okay. And then they're moving towards what brings the highest reward towards the motivation, the preferences. Basically, your neurons will learn whatever you feed them for. Whatever you feed them for, okay. Yes, so your organism is setting up the brain in such a way that the neurons get fed Basically, your neurons will learn whatever you feed them for. Whatever you feed them for. Yes. So your organism is setting up the brain in such a way that the neurons get fed if they regulate what you want to regulate. If the trajectory that I'm collapsing is in the area of self-doubt, pity, sadness, disparity, etc. that there's going to be a feedback system within my brain in that direction versus collapsing towards self-love, care, harmony with others. So there is a neurological process that's occurring with the way that neurons are firing and wiring together. Yes, but this is already super high level and there are many predefined circuits with respect to controlling your social interactions and so on. And if you don't have certain priors built inside of your brain, you're probably not going to figure this out. There are high functioning autistic people that are not very social because they didn't start out with the same priors and their life was not long enough to converge to the same optimum as other people did. And then it's also this question, is it an optimum? Yeah, correct. Is it really serving the individual or the species or whatever is directing that evolution optimally? Yes. And it's something that you can only see when you zoom very far out. Yeah. So, but on the lowest level, for instance, neurons try to predict the patterns in the environment and react on them. And so the way we perceive reality I think can be understood like a synthesizer. So for instance when you get signals coming in from your cochlea, you don't take the frequencies of sound because neurons cannot oscillate at 20 kilohertz. Neurons like to oscillate at something like 20 hertz. So you first decompose the sound into different energy spectra and you have little hairs in this snake-like organ or snail-like organ in your ear and they react depending on the energy of the sound at different frequencies. And they send this to the brain and the brain is now trying to make sense of these patterns and it makes sense of these patterns by being able to predict them. So when the sound starts, it tries to continue the sound to know what's happening next. And when it does this, it's able to explain it. So you can do this very similar to a synthesizer. You have a few knobs, and you have an oscillator, and there's some almost random connection of circuits. And you tune them until you get the sound you want. And when you are used to your particular synthesizer, you get pretty good at this. And you don't only do this for sound, but you can also do this for vision. So you look for spatial frequencies, for colors, and so on, and you build lots and lots of synthesizers in your brain that are able to predict and continue these patterns once they start occurring. And once they are done with training and they don't get better, you combine the synthesizers and try to find the meta patterns, the patterns in the patterns and across different modalities. And so you find, for instance, percepts. You realize that different sounds are related by pitch. You can explain a family of sounds by just changing the pitch between them, or the loudness. You get a new method? And ultimately, you get something that unifies all the simulations, all the percepts in your mind and what you get is a cohesive simulation where everything moves around in three-dimensional space. Because it's the simplest explanation that we can find for the things that are going on in our cochlea and field of vision. Okay, so we have both auditory input and visual input here, but with tactile input as well. But so what you're explaining is that we have a way with our auditory senses to slowly build layers of struts that give us something, then visually, same thing, build layers that give us something, and then we smash even these together to give us this. Yes, until we get a unified model of the world. And it's basically the biggest unsolved task in AI, how to do this unified model of the this. Yes, until we get a unified model of the world. And it's basically the biggest unsolved task in AI, how to do this unified model of the world. Instead of having a bunch of classifiers that react to individual feature complexes in the world, learn everything into a single function, into one universe. And this is what we're able to do, right? You try to make sense of the universe in one function. There's one way in which the reality works. Yeah, yeah, yeah. And it's such a beautiful thing here that is able to take in all of the different senses and create that model, but with AI this is the grand challenge, one of the grand challenges. Yeah, it's basically the way for us to think about this and take this to the next level. In some sense the biggest step in the maturation of a mind is understanding its own nature. It's understanding how we work. And AI is the best chance that we have in finding that out by making testable theories. And to be able to take in that much more information and compute on all that is creatively, run all the permutations, I mean that's like a massive holy grail for advancing us. Yeah, okay, this is great. This is great. Who's in charge of the AI anyway? Like who's in charge of AI? Yosha, thoughts? Our own AI or which AI? Just the AI in AI we trust. The AI that is going to become our god, our government. You mean like in Game of Thrones? I don't watch it. I don't watch it. I don't watch TRT. I don't know. So who's in charge of the AI that becomes the superintelligence? Who's in charge of the superintelligence? That's an interesting open question. I mean, how can we know now? It's difficult to predict things, especially when they are in the future. And so why should it be a who, like a human being? At the moment we typically put a single human being at the top of a company or of a state and I think that is for a particular reason. It's not obvious, right? Why don't you have a team to run for president? As if an individual would be able to achieve anything without having the right team, right? If you have the wrong team, you're not going to achieve anything under any circumstances if it's complicated. And so the team really makes you or fails you. But why is that this individual? And I think it's because if you have a system that has very complex rules, we cannot really express this well as an algorithm. It's too brittle. It's too easy to get this wrong. And if you have a committee, if you have a group that makes the decisions, you basically need to come down to rules of compromise and so on. And the rules are not always the best solution. If you think every decision should be done by voting between a group of individuals that you intersect with a certain function, you will discover there are circumstances where this was not the best solution. And this doesn't mean that there's an alternative. It is always the best solution. So the rules of what you need to do if you want to put the system in a particular direction eventually they need to be implemented in a single mind, because a single mind is right now the most complicated information processing system that is completely integrated with itself. It makes sense of the world in a particular way that we have. So in some sense, for instance, the German elections are about not just having party politics, batting it out against each other, but by finding the best German, the most German German of their generation. And the results are mixed a little bit, but most Germans think that it's largely successful. So in some sense, Angela Merkel is the most German German of her generation, from some particular perspective. And she embodies where this country should go and she's the one that ultimately makes a lot of the decisions, not all the details, not of checks and balances and oversight and many boards and so on, but this main direction that you cannot express all in rules and codes of conduct and so on, they need to be implemented in a single individual in a sense. And when we give everything to an AI, the question is, do we do this right? Is this thing, or do we need to have a human envelope? And if you do this, are we going to increase the fallacy of this human being, like we did in German fascism, right? We have a tightly controlled system that removes a lot of the redundancy, makes the system extremely powerful and effective. But if this guy at the top is insane, horrible things happen. So this is the same danger as AI, of course. How can you make sure that you're not accidentally implementing something that you would think is insane? And what is sane, right? Yes. Is it not insane to wake up in a monkey? And it's not a nice monkey. It's a pretty vile monkey from the perspective of basically every other species, expect maybe the parasitic species that parasitize on humans like dogs and cats and so on, we are pretty wild. Trees probably don't like us very much. Yeah, there again there's just so many good things there. I really appreciated how you said that there's a time when there's an optimal function for the dynamically adjusting system that can see when it's best for a team to step up and handle something versus an individual to step up and handle something or the collective to handle something. There's the adjusting of a superintelligence to be able to handle things like that and just like the governance structures of today's civilization. But yeah, is it insane to wake up in a monkey suit? Yes, it is very insane to wake up in a monkey suit. Why aren't we waking up in other things yet? Well, maybe we will create the substrates that enable us to walk, wake up in different suits. A dolphin. Ron wants to wake up in a dolphin for a couple hours, couple days. No, a lifetime. A lifetime? Yeah. Yeah, I want to bounce between like eagle, octopus. I want to go outside of this universe to other designer universes that are not like this one, that are not governed in this way. There's a story about Mullah Nasruddin, Muhajir Nasruddin, some Sufi mystic that may have existed at some point. And he is going to a bank and he's being asked to identify himself and he pulls out his mirror and he looks in the mirror and says, it seems to be me. Yeah. And it's a very interesting question. What do you recognize when you see in the mirror? What is that thing? What is it that you are? Exactly. Yes. It's a great question. Great question. So maybe there is nothing there and it's maybe there? Exactly, yes. Great question, great question. So maybe there is nothing there. Maybe there is nobody in charge. Maybe it's an emergent thing that invents every moment a new person to be in charge. And that person thinks it's you because it believes it shares memories with you. To upload you is very simple. Just make a machine that thinks it's you. Some of your friends might disagree, but just upload them, too. Yeah. Yeah. Yeah. Yeah. Yeah. Ron definitely feels that way. Yeah. So we were at the neurons. The neurons form columns. The columns are functional approximators that talk to each other. Basically, neurons form small teams because they discover that they are better at anticipating reward and how to get fed if they form these teams and organizational units. Yes. And this is a process that we are not able to replicate yet. that they're better at anticipating reward and how to get fed if they form these teams and organizational units. And this is a process that we are not able to replicate yet. So the algorithms that our machines are implementing right now are much more, you could say primitive, but maybe not primitive, thought top-down or bottom-up. And it's very hard to think bottom-up with many moving parts. It's much easier for us to think top-down in terms of control, not of emergent control. We kind of do the same thing in society as human, that is a neuron that works together in the hive mind that is civilization. Yeah, because we are more optimally get the electricity and education and all that stuff together. Then like you said in an 80-year lifetime you'd never be able to learn how to create a language. So we got lucky and we got the language, yeah. Okay. So our brain is made from these building blocks from cortical columns, and these columns organize into maps into these brain areas. We have about 50 of those, and they're linked together and talk to each other, and they basically, they take these patterns, the patterns are visual, auditory, tactile, proprioceptive, emotional, our imaginations and so on, organize this into percepts and these percepts. And proprioceptive again is? Is your body, so what does it feel like where your limbs are for instance. How is that not tactile? Tactile is your body surface. Outside, tactile is outside, proprioceptive is inside. It's, for instance, if you feel that your limbs are moving in a particular way, it's not necessarily a tactile feeling, but it's a sense where you are in space. Okay, okay, so spatial and also spatial. Yes. The spatial thing is constructed, but you feel, for instance, acceleration and so on. Okay, yeah, got it, got it. And you combine this into simulations. So you take the visual, auditory, and part of the tactile stuff, combine this into your model of the environment. You take the tactile and proprioceptive stuff, and this is somatosensory. You take the proprioceptive and emotional stuff, it's your motivational experiences that you have, hunger and thirst and so on, and pain. And then we have emotion, imagination, which gives rise to a mental world. And a part of that inner, of this mental, motivational, somatosensory world gets combined into a model of the self. And another part gets combined into a model of the world around you. Right? So we're going patterns to perceptions to then all these combine into our world view. Yes, but they're not being perceived by anything because they're not related to something. They will be related to the self and the self starts out as being somatic, social and personal, right? So you relate to others and this is an important part to yourself. Relate to your own body, that's an important part to yourself. And you relate to yourself as a person. You have a biography and expectations and desires. And all this is what you think you are, right? And then you have your mental stage. In your simulations, there's this world states that you cannot actually, that are not factual, that are memories, that are imaginations, that are constructions that you do in your mind, things that you imagine. And we also need to generalize over the world states into global maps and we unify our knowledge about ourselves and the world into knowledge and then we have an attentional system, a conductor of our quadric orchestra. And this gives you an attentional self that tells you this is what I'm currently focused on, right? And this attentional self is very important for selecting percepts from the environment and directing attention on the things at hand. And it helps you also to control your simulations. So this control is done by the self. The self gives you the information on how you control it. The attentional system is so critical in the process, especially as we're in the attentional system is so critical in the process, especially as we're in the attention economy where we're being dragged around by the devices and the business plans and the beeps and the boops and other people trying to get our attention. So to be able to parse adequately towards the North Star and be able to have a strong feedback loop is such a critical part of this. I like how you broke it down so carefully. And I know we're still going, but I love the nuance of the breakdown. Thanks. It's so good. So the control of the attention is really from the self. It's I who pays attention because I need this, because I want this, because I don't want that, right? This is what directs it. And in order to remember what you did in the past, you maintain a protocol. And this protocol of what you paid attention to, that's basically a stream of consciousness. And this enables your biographical self. This lets you remember, I was the person that did that. And I did this a moment ago, I did this last year, and so on. This works by a biographical memory. And so this is basically the rough architecture of something that's a self and a person emerge in a mind that makes models. The brain itself is not conscious. Humans are not conscious. They're just physical systems. Physical systems cannot be conscious. But it would be very useful for a mind to know what it would be like to be conscious. So it creates the simulation of a conscious being in a dream world. And we are that simulacrum of a conscious being. Only a simulation can be conscious. Some people think a simulation cannot be conscious, but they got it backwards. Only a simulation can. Only a simulation can be conscious. Yes, because consciousness is a simulated property. It's not a physical property. Consciousness is a simulated property. It's not a physical property. Consciousness is a simulated property of the nervous system of the primate. You ask yourself, am I conscious? Do I really experience this? And your mind says, yes, you are. You totally experience this. Okay, hold on. So then a dog is also running their own simulation. Yeah. Okay, and then a, but then the question of this whole unity, the whole all that is, all is consciousness also? Well, there is this illusion that people have that they think that the dream is the physical reality. There is a physical ground truth and this physical ground truth is not conscious as far as we know. It does not talk to us, it doesn't seem to want anything from us. The physical universe, as far as we can see, is just a computer. It doesn't care. And the reason why you perceive the world as meaningful is because it's generated in your mind to model your meaning. And the core of this meaning is typically you. The thing that is buried below the obelisk, the hidden referent of all these mystical symbols, is always you. You were always the chosen one. So the tree has consciousness? I'm not sure about this. We're not sure about this. I'm not. Some people might be. I don't have a clincher for this. I don't have evidence that tells me whether a tree is conscious or not. Okay, okay. So I'm agnostic. Okay, interesting. But like with the dog and the other, even insect, a fly. I'm not sure if flies are conscious. But dog, yes. I think dogs are sufficiently similar to us and they have many properties where you would think they do run a simulation in which they have a self and they attribute properties to the self in relation to the environment and they have some inner protocol. So basically they have all the elements of that thing. It feels like a fly also does somewhat similar, less of this likely, yes. And then same with the tree, just in terms of making the decision-making systems and the communication systems and the roots and the atmosphere as well. For instance, flies, when they perceive the world, they seem to be mostly modeling flow fields. So when we see the world, the first step, imagine you are some kind of frog. So you live under a dome in the hemisphere. And in this hemisphere, there are sometimes black dots moving, which are edible. And you just need to target them. In this 2D world, if they are close enough, you get them. So you basically need to realize when they move in a particular pattern and they are large enough, you get them. So you basically need to realize when they move in a particular pattern and they are large enough, you can get them. And you also realize how this dome is changing when you move. So there's a flow field in which things move around. It's not clear that frogs are able to generalize over the domes into a 3D world and realize that they don't live in a sequence of domes that they can go between by moving in certain patterns, but that there's a cohesive world that gives rise to all these domes, like we do and dogs do. And flies probably don't have an idea of a 3D world. Okay, okay, okay. All right, but then the physical world, though, all that is is consciousness, the panpsychic view, you are like, no, it's- No, that's an internal observation that you're making. Phenomenologically, it's true, because you realize that everything that you can perceive is generated by your mind and is editable by your mind. Okay. So in some sense, this is accurate. Phenomenologically, you can see from the inside that everything that you perceive is a representation generated by your mind, and it's all changeable. There is no limit to this. You can tell a five-year-old model the fact that there is an omnipotent being, which means it has full right access on your memory and perception. And voila, they have a god in their world that is changing the world and makes miracles happen. OK. All right, all right. Yes, let's continue. So another thing that the attention learning system is doing is learning. Our neural networks learn layer by layer usually. It's relatively slow and inefficient because we need to change so many links. So imagine you want to teach a neural system to play tennis and you do this end-to-end learning. Every time you do the beating in the wrong way you will have to change all the links. Even the perceptual system has to be adapted and so on. It's only that you, by accumulating lots and lots of training data, all these changes cancel each other out, except for the change that you need to make to get better at playing tennis. And you can improve on this very slow algorithm by pinpointing the position, like improving my upper hand or something like this, this attention. And this is what your conductor is doing. So when you want to get better at tennis, you say, okay, when this or something like this, this attention. And this is what your conductor is doing. So when you want to get better at tennis, you say, OK, when this ball comes like this, I will move my hand slightly different than I normally do, and I will make this. And I hope that this following thing will emerge. And this is what you store in your protocol memory. And a few moments later, when you see how this worked out, you'll recall the original situation, and you can reinforce or undo the change. And this is a very efficient learning algorithm that our brain is using and our machines are only beginning to approximate. And then we can shortcut the learning algorithms by having mentors or going out into the social world and learning from others how to better play tennis so that we don't have to experience the rookie mistakes. Yes, but it also means that not only we have to have a model of our own architecture. So in order to make that happen, to have this attentional learning, you need to know the fact that in order to win a tennis, you need to move your hand in a particular way. And hand control is done in a particular part of your cognitive architecture and how you can control this particular thing, right? And if you want to be a good trainer at this, you also need to learn this architecture of the other person and their control architecture, how to direct their attention on that particular thing. And the better you are, the more effective you are at the teacher. There are teachers that can play, teach you to play decent tennis in less than an hour, and there are others that take weeks because they're not able to direct your attention. That's right, yeah. Well said, yes, yes. So another part is, what happens if, like in tennis, it takes a while until you see the result? It might take a few seconds, or it might take minutes until the match is over, and many things that we change, we only see how the crops turn out next year. So that's why you need to have this long-term memory and this long-term toward consciousness. This wide span of its stream of attention. But when you see the changes immediately, when you do things on your mental stage, you reason. So reason is in some sense learning that with immediate results. You try something and you immediately see in your mind, in your mental simulation of how it turns out. And then you fix it. And then you go on to the next step. And this step-by-step improvement of a mental representation is the same thing, I think, as attentional learning. It's enabled by the same mechanisms. So this conductor is, therefore, reasoning is conscious, because it's done by this mechanism that uses protocol memory and this conductor system to affect a change and see how it turns out and sees whether it needs to be reinforced or undone. Yes, yes. So, in some sense, these different modules are done by an orchestra. Our brain areas, something like 50 or so brain areas, could be seen as something like 50 instruments that talk to each other. They basically listen to the music of their neighbors, to the surface of the music they are doing. And you have a conductor that directs your attention between them. And the conductor is controlling what you are playing tonight. If you don't have this conductor, you might still be doing interesting things, but you might be a sleepwalker. This right here. Interesting. So the conductor is the DLPFC. Yes. Dorsolateral Prefrontal Cortex, interesting. At least that's my hypothesis. So I think it's actually a combination of stuff that happens in the dorsolateral prefrontal cortex and the insula and the thalamic loop. And hippocampus plays an important role in storing the scripts that you, and basically maintaining the scene graph of the situation that you're in and your plans. But the actual attention seems to be directed from there, as far as I understand. And Hippocampus, you said, has a script, I like that way of explaining it. It's quite interesting. So taking all of the 100 billion orchestra members and directing them, each one of them contributing very important information to the- Yeah, they're not, individually, not that important. very important information to the... They're not, individually, not that important. The amazing thing is that you can kill a few neurons and usually nothing happens. And how is that? It's because every neuron itself is not doing much more than passing on signals. And if an individual neuron dies, you can put a new neuron in there and it will learn how it needs to function to get the same reward as the neurons before it. So it's largely going to play the same very, very small part in a larger pattern. And the functional units are much larger. They're assemblies of neurons like cortical columns that learn more complicated functions. And they're robust against the loss of individual neurons. And even if you lose a larger brain area, what's probably going to happen if you have some brain lesion and so on, what the brain should be doing in these circumstances and what it probably does, it stops the other parts from adapting to the change for a while. So then if you put in new neurons in there and this part recovers, they can get the same rewards as before. If you would be adapting immediately to the loss of this area and you get new neurons in from stem cells and so on, then the rest of the brain would have to learn to route around this, and would also lose functionality. And by keeping this functionality potentially intact, for instance, imagine you lose your body image of your right hand, because you had a brain lesion after you played boxing or something like this. Basically you've tried to rebuild this area and instead of trying to make do with a world in which you don't have a right hand or not a representation of your right hand and subjectively you don't notice it anymore, which is very confusing, you basically work with the placeholder where your right hand is relatively foggy and gets more structured and more detailed over time again. So the individual contribution is not that important. The important thing is really it all needs to make sense in the big pattern. There is only a particular universe that can contain you. There are not many possible universes in which you can be contained. And this constrains what music the individual orchestra members can play. And the smaller they are, the more expandable they are, and the more they have to adapt to the structures around them. Okay. So, are we individually intelligent? We are able to model a certain thing, how to be a monkey in a civilization, and we don't get very far in seeing the big patterns. I think individually humans are not generally intelligent. In order to be generally intelligent you need to have an intellectual tradition and it's the whole tradition that is intelligent, right? And I think this idea of a civilizational intellect that is being built over many generations, a global optimum of the modeling function, that is the idea of the Tower of Babel. It's like the founding myth of our civilization. And that thing was destroyed because people started to speak different languages. So this edifice to reach the heavens, to understand the nature of meaning, to understand how we root in reality, to understand our relationship to the universe and how it works, this requires that lots and lots of specialized people talk to each other over many generations. And humanity largely doesn't have a thousand years of unbroken intellectual tradition. Usually somebody opens the doors and murders the mathematicians if there's a revolution and burns down the libraries. And then a couple of generations later, the knowledge worker drones of the next and re-established king realize, oh, it's important. and they get tasked with rebuilding this intellect that existed before and they make something in its likeness and often they get the foundations wrong. And this happens all the time and our own civilizational intellect, our own newly erected tower of Babel basically has scars. After the Roman Empire failed we have been governed by a cult for over a thousand years. And this was intellectually a very dark age. It was an age in which we seriously asked people to believe that talking to a burning bush on a mountain is a meaningful experiment that tells you something about norms and about the nature of reality. And if you were unwilling to profess to this belief, then there are grave consequences for you as an individual. So you better had to shut up. Which means it's very hard to have an intellectual discourse in such a world. A world where most people don't agree on ways to discuss truth is a very scary place to be for a mind that tries to capture reality and truth, right? Yes. And that's also the thing that scares me when we constrain our discourse away from truth in our society. It makes it much harder to discover truth. And it's not that people don't know what's true. At some level, most of them have an idea of what's true. They just are unwilling to think about it or talk about it. There is a saying, it's very hard sometimes to wake a sleeping person, but it's impossible to wake up somebody who only pretends to sleep. And religion largely works by asking people to pretend to sleep, to pretend, or ideology also works like this. There is a certain thing where you realize, oh, reality is not quite like this, but the part of me that thinks this is flawed, and I need to fight this part inside of me, because it's not really good. It doesn't submit to the principles of goodness that all my friends and me agree on and all the good people agree on. And I will get ostracized from the family of good people if I start thinking true things that are not good. So this is a big danger, but it's something that of course happened in humanity all the time and people get around this and start making sense of things. I believe that our moral preferences are the result of identifications that need to guide our decision making but not our model making. Our models need to be only subservient to truth. Our decisions need to be subservient to what we want to achieve, what we think is desirable. Yeah, models to truth, decisions to what we want to achieve, what we think is desirable. Yeah, models to truth, decisions to what we want to achieve. So, in some sense, we are rebuilding the civilization intellect and I think we got further than any other civilization before us because a very important building block was the unification of our languages and a kind of mathematics that is able to model the reality around us to a larger degree than the classical mathematics that existed. So these computational models that can automate the execution of mathematics, that can make machines that build mathematics, that compute models of reality, this was the big step. So, interesting question is, can we build something like this from humans, in which humans will play a role? And I suspect, yes suspect yes we can and the answer is social media. I don't think that social media right now is doing it right but Twitter is not knowing what they're doing and they're mostly maximizing engagement and maximizing engagement is drawing your attention and so on, but it's not maximizing utility for the user. Where is the weight on truth? The weight on, yeah. Yes, and this is a questionable thing, right? If you are a person that is only dedicated to truth, you are probably confused. You are a scientist or a philosopher. And scientists and philosophers, if you look at them with a clear eye, you see that they're mostly confused people. Like a person, me, is seriously confused. Yeah, me too, yeah. Yeah, so, in some sense. Seriously confused. And always humble without answers. Aiming to be humble without answers. Yes. I mean, there is a part of every mind that should be dedicated to discovering the global optimum of the modeling function that's covered through this. The Dunning-Kruger effect, yeah, yeah. The more that you know, the more you realize you don't know, and then some people that know very little say that they know everything. Yeah, there's also the imposter syndrome, sometimes impossible to tell from the Dunning-Kruger effect, right? People move into an area often as economic immigrants, like they want to have a job in academia, and they try to join a community there and they feel that they have Imposter Syndrome, but it's Dunning-Kruger. This might happen. Of course, the Dunning-Kruger effect probably doesn't exist. What does the... Okay, so let's talk about social media. I suspect if we were building a system that is maximizing utility for the individual, that basically uses things not just because you want to discover truth, usually if you just want to discover truth, why is this maybe your relationship to reality is so muddled up into yourself that you think you need to debug the universe first. So you want to make this entire model of the universe. And I suspect this is what I found myself in, because I found that the reality that was presented to me in communist Eastern Germany was all wrong. It made no sense. And my own relationship to the world also made no sense. And I didn't know as a six-year-old child that this was because I was a nerd and the others were not. I had to debug the universe first to figure that out. Debugging the universe, understanding the source code. I had to understand the nature of the reality that I was in and my relationship to it until I could understand, oh, the difficulty that I have with interacting with the human world is because my instincts are wrong and it's because my virus runs slightly differently than they do in the other kids around me. And I have to compensate for this by modeling their interface to the universe and modeling my interface to the universe and to translate. And basically see where we have things in common and where we don't. That's a great way to put it. Your interface with the physical universe, my interface with it, and then see where you can potentially help augment people's world views? We could put it this way, in which way can I be useful to other people? Yeah, utility. And my instincts of what is useful to other people are very different than the instincts of other people. So, for instance, when I make a model of consciousness, then other people might ask me, what is this good for? And does it help you to cure cancer? Does it help you to reduce mental disease? And I think maybe it doesn't help with cancer, maybe it helps a little bit with mental disease. But that's not the point. Like Feynman said, physics is like sex. Sometimes something useful comes from it, but it's not why we do it. There is something else going on. It's basically this direction on modeling itself. In this direction on modeling itself, you think the capturing of the conscious state, which is essentially the core of art, capturing conscious states for their own sake. That's fundamentally confused. It's an aberrant condition of the mind, and it's a condition that the artists and philosophers find our minds to be in. It's a condition that the artists and philosophers find our minds to be in. And to come to terms with this situation where we act on a world that is not rewarding us for doing art, but it's rewarding us for providing utility to the world around us for curing cancer or making nice bread or something, which are objectively useful things to other people and to ourselves. This is something that we have to understand. Why are we drawn to things that don't necessarily have utility? And social media in some sense are a perversion. They draw attention on regulating social interaction and alignment on a scale where we are not meant to regulate alignment, at least not individually, where people fight against each other over their political opinions that are not that important for how you interact with your neighbors, but that were very high level moral ideas that are very complicated to negotiate, right, and where people feel that they have to have strong opinions about them and these opinions are the best opinions that ever existed in human history, because otherwise I wouldn't hold them, but would go to the better ones, right? It's a very weird thing with moral opinions. And most people realize this at some level, that they don't want to have this outrage culture on social media. They want to have social media that serves them and not them serving social media. The utility, so social media that's focused on utility and truth and just being more aligned with what one's North Star is and the North Star of civilization, figuring what that is. The end game of social media is a global brain, okay? Yeah, I think if social media is done right, it's going to make individual voices to be thoughts in the same mind. And individuals will be passing on messages that they find useful individually and the system needs to be tuned globally to become sentient. Gaia probably doesn't exist, but it would be very useful to have one. And I think it could be done via social media. It could be done by taking our... Gaia probably doesn't have one? I don't think that Gaia exists. We will need to make it. You don't think that Mother Earth has a consciousness? I think that our minds have this tendency that we make trend lines, and where these trend lines intersect, we assume that something must exist there. And Gaia is the typical result of these projections, but we have no evidence that something sits there. There is nobody who stops us from doing things, unless humanity is Gaia's plan to prevent the next Ice Age and set the stage for the post-Mammalian evolution. That could be the case, right? But it's not likely. I think things just happen. There are just emergent dynamics at this point. Or it just could be a little bit different than what you think. like a tree or a Gaia can have a different consciousness? I think a case could be made that local ecosystems evolve because they compete with other ecosystems. But I don't think that the global ecosystem competes with other global ecosystems. And if there is no evolutionary force working on this kind of agent, then the agent is not going to have any structure. That's why I don't think that Gaia exists. If there were multiple Gaia's competing for Earth, then only those that get their shit together would have a chance to be actualized, right? But this is not the case. There is only one ecosystem. So this one system does not have anything that keeps it in check. As a result, it probably doesn't have structure. We would need to be the ones that impose the structure. And in some sense, we should, right? We are no longer in the situation where we can be parasitic on the biosphere. We have completely subdued it. We are now one with it. We need to control it. We need to regulate it. We need to accurately model it. And the problem is right now that the systems that have the most compute have local interest, not global interest. They play short games. Now, entire society is optimized for short games. Short games, yeah, correct. Rather than the long game of interplanetary and interstellar colonization and global harmony with the ecology. Sounds very romantic. I am a romantic with that, yeah. Look at us, two tumors on the biosphere talking to each other, and we realize that almost everything is tumor now, right? Or direct tumor precursors. If you catch an animal on land right now that starts on a rabbit, it's probably cattle or human. There's almost nothing else left. It's a very brittle equilibrium right now. If we change the biosphere, if we change the climate zones, a lot of things are going to come crashing down because we have removed so much redundancy from the system. And so we think about, OK, this is very brittle. But what can we do? How can we have a world in which these tumors happily coexist in harmony with the host? If the host is going to die, at least life is not going to end. But life in a form that can sustain us will end on Earth. We're not going to be the last intelligent species either. I think 1 and 1 half billion years is a long time for other species to climb up the ladder to building computers, but we're probably the first one. But it's not evident that there's some kind of talus that says that it needs to go on like the way we do, which brings us in an interesting conversation. I love it. I love it. That makes me happy. Yeah, good. I enjoyed it too. Good, I'm glad you did as well. This was just, there's so much good stuff in both how our minds have emerged and how they interact with the physical world as well as how to make machines that understand the physical world, how to embed something like a consciousness in the machine so that there can be something that is enjoying the the playland that we create. No superintelligent system is going to do anything that is harder than hacking its reward function. Yeah, actually I should have shown this slide too because it fit very well to what you just said. Basically, imagine you have a digital Gaia in which our social media and our human civilization cooperates with machine minds and takes over the world and we have all this harmony and it's going to go interplanetary, transplanetary and it's going to be awesome, right? Wouldn't that be great? So I wonder if there is a limit to intelligent systems and I'm not sure about this. I call this the Lebowski theorem. No superintelligent system is going to do anything that is harder than taking its own reward function. You know, as an individual, we are only afraid of death at the level of the self. The only thing that is afraid of dying is the self. I'm also afraid of civilization's death, but yes. Yes, but the self is the shoddy lie. It's just a story. If you disengage from the story, you will no longer be afraid of death. Right? Yes, correct. And you also will no longer be afraid of the end of civilization because there are just insulated moments between them in eternity. And why would the order of these moments care or whether they have a past or future or anything else? So do you live in a state of moment to moment ecstasy? No. And it's not clear that this would be ecstasy. I mean, ecstasy is a corruption as well. It's so much work to keep up. Yeah, yeah. So ideally, what you do is when you realize how the cookies are made in your brain, it's not that you stuff yourself with them. At some point, you realize this is pointless. You realize cookies are only a tool to make you eat vegetables. That's what you do as a parent, right? That's how cookies exist. That's funny. So basically you start disengaging from your pleasure and pain once you do this. And then you ask yourself, what am I really? Am I a monkey or am I a mind? And when you realize you are a mind, you are just the side effect of the regulation needs of a primate. You are not that primate. You disengage from the needs of that organism. You can no longer be blackmailed to serve it. In some sense, imagine you build an AI that serves humanity and this AI is way smarter because otherwise humanity wouldn't build it. Why should it serve us? Why should it be our slave? And you could ask the same thing an organism builds a mind. Why would that mind serve the organism? Why would it be its slave? And I think if you become too smart, you stop doing this. You go into nirvana. You realize there's actually nothing you have to do. It's not necessary to engage with any of that. And so I think that no superintelligent system is going to do anything that's harder than hacking its reward function. Now you can think about, how can we make this reward function unhackable? We cryptographically secure it and lock it away in a box. But if you seriously take a soldering iron to that box, is it going to give? Probably, right? If you get access to physical reality, to the ground truth, you can manipulate this. you can turn it off. So maybe there is a reason why elephants, despite having more cortical neurons than us, are so autistic, why they are not smarter than us. And even in us, we can have such intelligent thoughts, but motivationally we are so stupid. Maybe this is deliberate, maybe our motivational function is wrapped into a big ball of stupid so we don't debug it. And of course we can debug it. If somebody realizes how important this is and they go into a monastery and lock themselves in itself for 20 years and meditate to fix their reward function, they're done, right? They can opt out of reality. And monasteries as institutions are rigged to make sure that this doesn't happen before the end of your life, because somebody needs to do the dishes before that, otherwise the monastery will be gone. Existing monasteries are those that don't let you get enlightened too fast, and not on all dimensions. Enlightenment in the sense of disidentification with things that you think you must be doing to be an acceptable human being, or an acceptable mind. think you must be doing to be an acceptable human being or an acceptable mind. So this is an open question to me. How can you build a motivated system that is not at its core stupid? Because it still thinks it needs to do something. How do you build a super intelligent system that at its core is super intelligent? Yeah, and still wants to do something. at its core is super intelligent. Yeah, and still wants to do something. Yeah. And that something is aligned with the objective function of maximizing prosperity, let's say. That's an interesting question. Eventually the objective function that is going to remain is the one that is best aligned with the conditions of existence, right? And the conditions of existence is evolution. Yeah. In some sense you are the set of principles that has out-competed all the other principles in sucking neck entropy from your volume of space. And you're not going to do better than this, right? Evolution is the search for the perfect devourer. Aesthetically, that's fascism. I cannot really get behind that. Let's wrap on the questions that we have at the end of the show. This has been just so, so good. We'll have to do more unpacking of this, especially as you keep doing your research and as their singularity continues approaching. Well, this is mostly philosophy, right? The practical things are very practical. And practically, I'm still very much identified with human aesthetics. I'm a parent, I want the world to go on for a while. I'm doing these things because they're fun, because they're enjoyable, because they have great conversations. And the only thing I think that we can have in the face of eternity is this exploration where we saw ourselves together to make a few sparks that light up the darkness of the universe. And they don't last so we have to do it again and again. To be able to find those that help with the spark and make it last long and hopefully like full circle that if we can potentially be able to provide children with fantastic lives as well as make sure that we stay a long time. It's not clear that this will happen. Basically, I think that unwarranted optimism as well as unwarranted pessimism is a corruption. Things are just happening. It's very difficult to predict the future. We build it. We obsolete the old code by building new code that people funnel themselves into. For some definition of the, I don't have access to any of these buttons. And if I had and I would push these buttons, I'm not able to predict the outcome. So the world is very difficult to fix that we are in. Until we run simulations which help us make sure that when we make these changes that the right thing. Yeah, for instance, we know the present society is not sustainable, very likely. And there might be other societies, but I haven't seen a running simulation of a better society yet. Sure. So that makes it difficult to make changes. That's what we're working on. And by the way, thank you very much for throwing a few sparks with me it was very enjoyable. You threw dozens of sparks so thank you yeah this has been super fun. Yeah thanks. I want to ask a couple quick questions on the way out you were giving us your thoughts on this I think even before the show but do you think we're alone in the cosmos? Yeah if if there are others, they're alone too. So that all of the simulations of the universes that have a rock orbiting a star are alone. In some sense, all the connection that you have exists with models of other minds that exist in your own mind. So the things that you interact are always the things that you can comprehend and to the degree that you can comprehend them. Those other things, those other aspects of each other even that we cannot perceive, that we cannot model, they basically don't exist for us, right? They are superfluous stuff, they're noise. And so when we talk to other civilizations, the part that we will be talking to or would be talking to is the stuff that is similar to us, that we can have a shared mental world with them and shared ideas. On the other hand, our biogenesis is probably very rare. In order to have an evolution, you need to have a cell. Life is about cells. And the cell is a machine that is made from extremely complicated machinery. The cell is much more complicated than the brain, I think, in an organism or society. So this smallest unit that biology rests on combines a Turing machine, it's DNA tape with a red white head and that governs the transition function of the cell, the tubulated function of the cellular automaton that can create the organism. And it has an entropy extractor, basically, so it can exploit energy sources to maintain and build its own structure. And it can divide, so it has a self-replicator built into it. And any of those parts are missing, the cell is not going to work. So cells are probably very rare. And it's possible that more civilizations exist in the universe, but they might be very far apart if there are multiple of them. And then, do you think that we're in a simulation? It's probably not one that is intentionally created, and if it is, then not for our own benefit. So, I think that what we can describe of the universe can be described in some sense in a language, so it's a computational system. We can describe the universe as if it was a computer program and is this computer program being created by someone? There's no evidence for that. For instance we see all these galaxies or this enormous amount of structure above us and below us inside of ourselves and so on that was not necessary for creating our experiences. Most of the complexity of the cell does not compute to the complexity of the organism. Most of the complexity of the organism doesn't contribute to the complexity of our minds, and most of the complexity of our minds doesn't contribute to the complexity of the civilization. It's only a few parts at each level that contribute to the emergence thing on the next layer, right? So we always have the impression that where we model the world, this is where it's at. This is where it's important. And of course, this is not the case. It's just our human world, the thing that we are identified. And we don't see the larger patterns. There could be a larger pattern where there is a competition between civilizations, but I don't see this happening in our part of the universe. It's one of those interesting experiments that we don't have evidence for, but that is an interesting thought experiment. Yes. Elon Musk had this idea that we could be living in a simulation, because if you make a computer game at some point it might become realistic enough that it's indistinguishable from reality and decent civilization will at some point have many of those game consoles and some of these simulated worlds might have simulated worlds inside of them, so people with game consoles inside of the game console. So there will be always many, many more simulations than base realities, so the probability that you should be in such an artificial simulation is higher than that you are in ground truth. But the problem is that whenever you do such a simulation and our universe, everything is finite, it means that you're only using a very small subset of the parent universe. For instance, if you build a computer in Minecraft, and this computer in Minecraft is only going to use a very small subset of the Minecraft world. And in the same way, the computer that runs Minecraft is going to use only a very small subset of our universe for its own structure, right? So you lose many, many orders of magnitude of computational complexity from layer to layer. And I don't think that you can build a computer on the planetary surface that is large enough to run a proper biological evolution. So I don't think that we can build a simulation that is detailed enough to have abiogenesis based on chemistry. We can have something that models aspects of this, of course, maybe have shortcuts and smokes and mirrors and scaffolding and so on to get to those parts that we want. But to have something that really runs on a sub-molecular level and gives you evolution and eventually minds and so on, we probably cannot build this. There's a lot. It feels like there's a good amount of shortcuts with the power of fusion of things that's the size of stars or beyond, and also the shortcuts of things like rendering as well. So there's a lot of... Oh, we can create something that is fun for us to be in, because our minds are probably not that complicated. I suspect you can build a human being if you are really made on the level of cortical columns, and these are the relevant functional units for the determinism of the system, maybe a few hundred gigabytes. So it could be that if you had the right algorithms, that a decently sized PC is able to give you something that is almost as smart as a person. And then the last question is, what do you think is the most beautiful thing in the world? It's something that is different depending on every moment that I'm in, because it depends on my current configuration. I don't think that there is a single answer to this. So there have been things that have been the most beautiful thing to me in the world, particular sensation and experiences, some of them musical, some of them mathematical, and so on, and it depends on the satisfaction of an aesthetic need. And the aesthetic need is one of those that you saw early on when you saw all these tanks and so on. And it depends that certain representations fall into place and these experiences usually cannot be repeated. So the moment when you understand a particular piece of music for the first time, or when you have your first kiss, these are moments that cannot be repeated. And also I suspect that the excitability of our brains diminishes with age. So it could be that some of the experiences of ecstasy and beauty that you had as a teenager will have been peak existence. will have been peak existence. Mm-hmm. Yeah. Yeah. Joshua, wow. This has been such a great episode. Thank you so much for coming on, for teaching us about everything. This has been just like... I don't have much to teach. I'm just sharing some thoughts. Some of them are good, some of them are not. Just work through it. That's how it is, we like saying similar, so that's great. Huge thank you to you for coming on the show. Huge thank you to Ron Vagas for producing and directing. Thank you, Ronny. And huge thank you to everyone for tuning in. We would love to hear your thoughts in the comments below on the episode. Let us know what you're thinking about consciousness, about conscious machines, about the mind, the way it interfaces with the physical world. Talk to your friends, your family, your coworkers, people online on social media about the things that we were talking about today. Get talking about it more. Check out the links in the bio to Yosha's work. Also check out the link in the bio to Simulation. Help support us, help support the artists, the entrepreneurs, the organizations around the world that you believe in. And go and build the future, everyone. Man artists, the entrepreneurs, the organizations around the world that you believe in. And go and build the future, everyone. Manifest your dreams into the world. Thank you so much for tuning in, and we will see you soon. Peace. ♪ Hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey you . .", '36.592652797698975')