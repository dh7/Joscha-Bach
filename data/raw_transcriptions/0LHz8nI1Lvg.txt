('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/0LHz8nI1Lvg"> </iframe> </center>', " This is the Convergent Science Network podcast. Leading researchers in the domain of neuroscience, brain theory and technology are interviewed by Paul Verschuur and Tony Prescott. Okay, this is Paul Verschuur for the Convergence Science Network podcast, together with my colleague Tony Prescott, and we welcome Joshua Bach, who is a speaker at the BCD Summer School in 2018, where he gives two lectures, actually. And in some sense, they all turn around, I think, the general questions of how we can build an artificial general intelligence. I feel this is really, I think, the core concept that you're, or challenge that you're dealing with, right? So, what's an artificial general intelligence in your mind? It's a system that has a mind similar to our own and beyond. I suspect that our minds have a particular kind of generality. And of course, you cannot really know this, whether we are universal functional approximators. But if it turns out that we can build AI similar to us, that's a strong indication that we are in that class of systems, right? So the whole enterprise is spawned by the question of who we are, what, what is our relationship to reality? What's our nature? What is consciousness? Why are things happening to us? What does it mean that something is happening to us? And we realized that psychology has become somewhat unproductive, and philosophy has become even more unproductive as a field, because you need to perform experiments that you cannot actually perform in psychology, and that you never performed in philosophy. So, we now can build computer models and implement our ideas and usually our ideas will not work which will force us to update our theories and this is the way we can make progress. So, okay, this is a bit the outlook, but to get there, you saw different stages in which then this search for AI has evolved. And so we go from our symbolic AI, this is sort of version one or first order, to an AI that's more like the current forms of AI that we know, where we use different machine learning methods to actually that can learn functions, right? This was the whole point as opposed to humans writing them out. We can try to learn them. And then we would jump into meta-learning and meta-search, right? So, what's the logic of these steps? What's sort of the driving principles that you would see sort of meta-search as a tip of that pinnacle. There are many ways of looking at this whole issue. I mean, there's a big rift in AI traditionally between symbolic AI and everybody else's AI. And before I came to Cambridge, I thought this was because of everybody else not listening to Marvin Minsky. And after I got to Cambridge and met Marvin Minsky, I felt it was basically the opposite, that Marvin Minsky, who co-founded the field with John McCarthy and a few others, realized that in order to understand cognition, we need to teach computers how to think. So we need to build cognitive AI in a way. And then he made the decision that this needs to be symbolic. And this was the wrong horse, but Minsky was too influential. He started yelling at people that did low-level cybernetics and neural learning because he felt what they were doing was too simplistic. And in a way, he was correct. What they were doing was very simplistic. But it turns out the way we can get to simple grounding and to the kind of operation that our minds are doing, they start from there. They don't start from symbolic representations, which are so highly abstract that we can only form them after generalizing over our perceptual representations and operations and so on, right? These symbolic operations, we get them by making our representations so discrete and low dimensional that we can apply analytic operators on them. It doesn't work the other way out. You cannot start with these analytic operators, or at least this project has not worked out. But what happened is basically this big rift, because Minsky basically started yelling at people for doing other things than he did. And this created an area of burn ground around cognitive AI, which was also symbolic AI, and the AI of other people, which never bothered to read Piaget anymore, and instead, mostly did neural learning and so on. But what happened in the first half of AI, basically since 1950 until quite recently was mostly that people thought about, well, when we want to perform the following task that requires human intelligence, like playing chess, or parsing natural language, What do we need to do to make that happen? And they came up with a clever algorithm and then they implemented this algorithm at some point because our computers are very deterministic and scale very well. These algorithms became smarter than people. So I'm born in 1973. There was never a point in my life where I had a chance to play better chess than a computer. That's in a way depressing, right? Even though these algorithms were so simplistic that I was able to write an algorithm that would play better chess than myself, right? Even though I'm not that good of a programmer compared to the people that put all this effort in this. But we know that our strategy of playing chess is very different than the ones of this simplistic algorithm. We are able to cope with much more uncertainty in our search space than our programs, so we can have much more fuzzy representations that are more perceptual and difficult to grasp what they are about. And to find such ways of navigating very complex, hard to define problem spaces, we cannot enumerate these problem spaces directly, but instead we go to programs that learn how to do this. So this new wave of deep learning is mostly about building systems that learn how to solve a particular kind of problem. And they're not general in the sense that we now have systems that can do this for any class of problems, but you basically set up a learning algorithm that learns how to play chess, or another learning algorithm that learns how to play Go. And we can now use very similar learning algorithms for the same thing and then this thing just plays against itself and abstracts over what it sees and then it outperforms everything humanity has done so far in this particular domain. And the next step might be that we go one step above this which means we don't write this learning algorithm by hand for this particular task, but we write another algorithm which learns how to find that learning algorithm, which means we go to meta-learning, right? And it is tempting to think of our brain not as a learning system, but as a meta-learning system, because many domains require very different learning strategies when we want to conquer them, right? And our brain is in some sense a machine that figures out how to learn a new type of thing. And from this perspective evolution could also be seen as a very slow and unprincipled search for meta-learning systems to which we are the first solution that has managed to build computers. But now, okay, this is fine, right? Okay, fine, I'll take it. But in some sense, I could also, well, this is not so bad. No, but I understood the words, right? I understood the words, I understood the logic. That gives me hope. But now I could also say, well, that's great because that's a scheme that just shows that the author of the scheme understands recursion, right? And then you just follow the logic of recursion to say, ah, so this is now the logic I should follow, right? Because they're unknowns. I have algorithms, oh, they're unknowns. Okay, then I have to cover the space of possible algorithms. Ah, but when I have a trick to cover on the space of all algorithms, there might be other spaces that I have to cover all the space of algorithms, there might be other spaces that have to cover. So the process seems to have a meta-level that's actually not so complicated. And indeed, when I was growing up in sort of this transition from symbolic AI to what then called was new AI or embodied AI, new artificial life, whatever, that was exactly already the dream. We want to use genetic algorithms to find algorithms that can define other algorithms, right? So, we're in a loop because this is late 80s, right? Yeah. So, what should give us any hope that also as a research program, this has any leverage and any traction? Because in some sense, also when I see the proponents of this approach going around for many years telling me these kinds of stories, there's not necessarily you, right? There's just general reflection on that field. If you then pose a simple question, okay, but then show me an example that I find impressive or that can convince me also from a scientific perspective. The examples are maybe a bit poor, right?? So in that sense, for me, things like AlphaGo, you can also say, okay, it just means it's not a game that's not interesting, right? So why should this program or this logic actually get us to this promised land, right? So why should I have any kind of confidence in that track, trajectory that people have already announced for the last 30 years, along which we have not found much. We're still in the desert. So you're going old grumpy AI programmer that has gone bitter on me now. Okay, no, I understand this. I can relate to this. It's more grumpy scientist who tries to understand the reality he finds himself in and he does that by trying to get traction on issues like explanation and prediction. So, I'm old enough to remember that we thought for a long time ago is going to be long for long time outside of our reach because the search space is so fuzzy that we cannot really enumerate it and the situations cannot easily be ordered like chess situation where you say my queen is gone this is probably very bad and there should be something making up for this if I go down that part of the search tree. And in Go all these things are much much harder to define. So we thought for a long time this might be out of our reach for maybe even our generation or the foreseeable future. And a system that combines human intelligence with machine intelligence is going to dramatically outperform the whole thing. And now we know if we add human intelligence to this it's going to get worse, right? There is nothing that a human player right now can add to AlphaGo to make it better. And so I think this is a reason to have a slight bit hope, but it's a very unprincipled reason. To get to a more principled perspective I think this is a reason to have a slight bit hope, but it's a very unprincipled reason. To get to a more principled perspective, I think we need to take a step back and wonder, what is it that a mind is doing, right? A mind is some system to make models in the service of regulation for an organism. And to make a model, you need to identify the meaning behind information. But what we get from the world is information, certain patterns. We try to predict other patterns with it. And the meaning of information is its relationship to change in other information. And that is what we call a model. So when you have a blip on your retina, the meaning of that blip is the relationship that you discover to other blips on your retina at the same point or at different points in time, right? And these models that you can come up with describe, for instance, the movement of people in a three-dimensional space while they exchange ideas and have an economy and all these wonderful things. And they are a simulation generated in your brain to explain and predict the patterns on your retina. You will not be able to do this for all the patterns. Some of them are just going to remain blips because you don't find a relationship. So we call those noise. And it's a lot of noise in our retina, right? So when we try to make sense of our universe, we get a few samples of bits at different points in time. We try to relate them by making these models. This is the task of our mind. So we already know our mind is a function approximator. And we also roughly know over how many data our mind is going to approximate, we know how many seconds we are alive, we know roughly the frequency at which neurons operate, we know the our surface that basically is our systemic interface to the environment that we operate in, and so on. So we have a rough idea of the ballpark in which we operate and over which we make models. Aren't you describing just one side of the elephant? Because now you're pulling at the tail and you say, okay, we need to compress information, right? So, that's why we build models. But on the other end, the other side of the elephant, we have also the need to generate action to control physical systems in real time. Yeah, in the service of regulation. That's fine. Yeah. But that doesn't necessarily satisfy this whole idea of compression and model building. Because now I have to sort of flip it around and sort of again decompress into a real world that is open. Right? You cannot necessarily capture all of that with the idea of model building and data compression. I'm not quite sure in which sense you would ever decompress. Because I don't interact with some decompressed world. I'm interacting with the world of my model, with the simulation, with the dream generated in my neocortex. This is subjectively the world that I inhabit. I'm never going to decompress you back into a part of the evolving state vector of the universe into some weird quantum graph. Instead, I always interact with you as a person that has intentions, that has a certain geometry, and so on. So I'm always interacting with this data compression model. Yes, but with decompression, I mean, you interact with me, I'm going to tell you things that you cannot fully predict, right? Yeah. I might say retro. Okay. So, now there's a decompression sense of the way you interact with the world. The world is open. So, the world now again generates highly variable feedback to you, right? So, you're compressing, the compressed states in the end get mapped to an action that you bring back to the world. Now, the world is again open. So, now again, you face a set of possibilities that collapse into one that's the one you're exposed to. And then again, you go back into your compression mode and you generate your next action. So, that's more what I meant with the decompression part of it, that your action is like this finger in this massive state space that generates back to you. Yeah. But basically what I do is I try to filter as many invariances from the world as I can and put them into a model and everything else is the state of that model in as far as it is relevant, right? So, I have a space of relatively few latent variables, just a few million of them, right, or a few billion of them in which I operate. And I have a much, much larger static set of constraints that captures the laws of perspective and biology and intuitive physics and so on to make the world interpretable. So, I can actually regulate my needs in a better way than a frog can. But with that, you're not necessarily saying anything that should knock me off my chair, right? This is rather... Well, that would surprise me. But this is a very reasonable way, right, to summarize what a mind or an embodied mind would do. Should I apologize for making, suggesting a reasonable way? I suspect that we have even converged in many of our perspectives on AI, have similar intuitions on how to get there. I don't know if our intuitions disagree on how long this is going to take and what is going to be necessary to get there. But it would be very surprising if we wouldn't be getting there even within our lifetimes. I think, David, I can understand where you're coming from in this sort of first, second, third or the fourth order systems, but people have been thinking about this for a long time and whether it could be so neat is, I mean, it's kind of like a physicist's view of how things could possibly be. And you you're a psychologist so it surprises me you take such a sort of a neat taxonomy of intelligent systems. Oh no, I'm just trying to tell a story that fits into a podcast. Say that intelligence is function approximation, this is a huge statement in itself. I mean that's one of the things it is. What else is it? It's the ability to act appropriately in real time and that may not require function approximation. The reason why I don't do this, why I don't lump this in this regulation is because we observe that there are people that are very bad at acting appropriately. And they might nevertheless be extremely intelligent. So I would distinguish the ability from reaching your goals, which we could call smartness, from intelligence, is the ability to make models or wisdom, which is the ability to pick the right goals. But we find very often when you observe people is that excessive intelligence is often a prosthesis that our brain is using to compensate for the lack of being smart or being wise. Okay, so we can have this particular definition of intelligence which maybe captures one aspect of human intelligence. Yes, of what minds are doing. Whether it can capture general intelligence, you know, in terms of all the different kinds of intelligences that humans have, you know, sort of in a Howard Gardner sense. There might be multiple kinds of intelligence, maybe they're not all function approximation. But then, you know, if we look at what learning is, what learning to learn is, what searching for the optimal theory of function discovery is, These aren't necessarily as you say a recursive thing of applying the same idea multiple times to itself. They could be completely different things. Oh, I fully agree. And so what we have in the current wave of AI is basically the optimization of inductive learning systems, but we still need deductive systems. We need to discover abductive learning systems, which reason beyond the data, which no current deep learning system can do, but humans can do. We don't know how humans do that. We need to also automate the process of scientific discovery, and we have no idea how to do that either. So, I think this... Wait a moment, you might be too bold in your statements here. You know... Me? Yes, I do remember when deep learning started to be... I think he's trying to be nice. When deep learning came up, basically everybody around me, including myself, started to make statements about what deep learning cannot do. And time and again, we had to revise our statements about what it can and cannot do. And it's a surprising thing to me still, it's surprising, because you have these essentially mostly feed forward network with very limited recurrence, because we cannot train this very well, and very few lateral links, because we cannot deal with those very well. And still, at some point, Google threw away everything that computer linguists have done over decades in terms of machine translation and replace the whole thing with an end to end trained, relatively dumb feedforward network. And there is this thing, you know, if, if you think about your neural network, your feedforward network, not as representing a brain, something like a neural architecture with all these current connections, but with a function that correlates adjacent brain states, it's not quite clear what it cannot do, right? The question is, do we find a sufficiently defined loss function and an algorithm that converges quickly enough on a solution to that particular function? So when you want to find an algorithm that converges quickly enough on a solution to that particular function. So, when you want to find an algorithm that discovers truth in a particular mathematical domain, for instance, by discovering new theorems, it's not entirely clear to me that we can prove that our current deep learning approaches cannot get there. No, wait, you're going too quickly here because if you take this example of machine translation by Google, right, we have to, I think, really think also about the epistemic autonomy of these systems, right, the old symbol ground and crawler. Why could Google get away with throwing away a lot of the traditional methods is they could also capitalize on the collective knowledge of a huge amount of humans, who would just add exceptions and new information to the lookup tables they could build. Right, so by virtue of that, the system actually worked. Right, so that means epistemically, it was not autonomous. It's a, so in that sense, it didn't solve the problem of translation as biological systems as ourselves would perform that. Of course. They found a workaround. Yes. Which has been really shrewd, if you want, and smart, and the way they capitalized on human knowledge is fantastic, great, you know. But it doesn't mean that they really solved the actual challenge. They found a workaround. Of course, the question is, what is the challenge in terms of providing machine translation? If you... No, but we were talking about it in terms of mind, right? And you were saying... Of course, this is not a mind. Nobody says this. We all agree. You were saying deep learning is surprising us, right? So yeah, it keeps surprising me. It keeps surprising me in the sense that I thought, oh, it does more than I thought it would do with these relatively simplistic methods. Because most of them we discovered in the sense that I thought, oh, it does more than I thought it would do with these relatively simplistic methods. Because most of them we discovered in the 1980s already. Yeah, and I think it, in the 1980s, certainly people like me, you know, I had the faith. I thought these systems would be super powerful once we scaled the computing power. I mean, I did my PhD in reinforcement learning. And, you know, it took me a week to train a little bug to move around and avoid obstacles. And now it would take half a minute or less maybe. So this, you can look at the algorithm and know that it's a super powerful algorithm. That given the right, you know, sort of, AlphaGo is probably an amazing design of network, but you can see that reinforcement learning in itself is such a powerful algorithm. Given a state space that's so easily defined as Go, it ought to be able to do really well in that. So the things where I disagree with you is, I do think it's possible, it's actually very likely that the current class of algorithms is not going to carry us all the way and we need to invent new classes. Yes, but the thing where we disagree is I cannot even prove that one and I haven't seen a proof that they cannot do this. So as long as there is no proof for this, we don't know the limitations of these things. You're arguing against yourself then, you're saying that we don't need to go to the plus one discovery mode? No, I'm just saying that you put in a certainty to the negative, where even that one is justified. We don't know that. I think we don't know how far the current approaches can carry us. Well, it's very difficult to find a proof for the absence of something. So, you're asking the impossible. So, no, well, you could say there are certain classes of algorithms that you can in principle not learn. If you could find a proof to show, okay, when Tony says something we cannot get this thing to discover new theorems with this. This is a pretty bold statement and you could probably formalize this. And then I could ask Tony what increases your, what is the evidence for that claim? Except vague intuition. Well, no, I think scientific discovery requires interaction with the world. That's what makes scientific discovery different. You can't just... You need to go out there and you need to make observations and you don't have a priori what to observe. None of these things neural networks can do at the moment. And people have tried to automate that. But I think you guys are much too cozy there together. Because I don't understand why you guys are not in a state of panic over this challenge of epistemic autonomy. Because this is actually, I wrote a few papers in the 90s already on that, analyzing Nettalk. Nettalk was one of the big breakthrough moments in connectionism because, oh, wow, we can map words to pronunciation features, right? And, oh, wow, we have emergent vowels and consonants in terms of the pronunciation features. But the thing is, if you analyze the data that was, the patterns that were presented to the network already encoded this distinct separation between vowels and consonants. So there was no magic there. It had everything to do with how humans had precoded that data. So that again means it was not epistemic, it was not epistemically autonomous. Right? And this is the problem that Bumper has again all the time. These systems learn fantastically well on human annotated data. That's not helping us. So as long as we're not really solving this simple grounding problem, we're not making progress. That's one issue. And I haven't heard an answer to that yet from your side of the table. Even though it's a round table, but okay. And secondly, you both seem to be very comfortable with this notion of intelligence, which worries me, because intelligence is actually a very circumspect construct that McCarthy just pulled out of his head in some sense in the 50s to just give the field a name, because they had computers, they had computers that would sort of imitate things that humans could do, like problem solving, and they had to give it a name. So he goes back to construct and go back to Golden, late 19th century and Binet, that were actually used to make distinguish, to distinguish between classes and races. But for the rest, it had no ontology to it, and which was also exemplified by Binet, who said, well, just with my test measures. And then when Adrian Owen a few years ago put humans in the scanner and asked them to do the Raytheon progressive matrix test for intelligence, he saw that actually, depending on the specific item you work on, different parts of the brain get involved. So there's not even one underlying process, right? So how can you guys even in this search for a synthetic mind be so comfortable with this knowledge of intelligence? I think it's leading us astray. It's not helping. And the second thing is, if you insist on that, are you going to solve this epistemic autonomy problem? So, first of all, I don't think that intelligence is such a problematic concept. I do think that the notion of being able to make models is a very useful notion to understand what our minds are doing. And it's also clear that this is sometimes domain dependent. So we have certain priors in which areas we pay attention, which means where we put more resources into learning new models. And this leads to different performance. So differences in people's innate or differences in talent is probably differences in need attention to some degree. There are certain things that we as organisms are paid for, to pay attention to, degree. There are certain things that we as organisms are paid for to pay attention to, like paying attention for instance to facial expressions or to the emotional states of others and so on, and this will create different degrees of resolution in our minds with respect to these domains and bias us in a certain way. There's this other thing what animates us to do certain things are the needs of the organism that are implementing some kind of reward function that gives us pleasure and pain to reward us for doing certain things or punish us for doing other things and then anticipated pleasure and pain which make us go into certain areas of the behavior space or avoid certain areas of the behavior space in advance and eventually lead us to generate models about the structure of that behavioral space and the policy space in which we operate. And so if you ever build a robot that plays soccer, it's not very difficult to make a robot want to go to the ball in some emergent sense, right? And if you have imagined that your robot has many more needs that it needs to satisfy, like recharging its battery from time to time, avoiding injuries, and maybe getting repairs from time to time when the injuries have accumulated to such a point that they impair its function. So you could imagine that this thing needs to satisfy a number of needs and achieve some kind of dynamic homeostasis about them. And suddenly you have a system with some motivational system that gives relevance to what needs to do. And then when you think about your mind as some general function approximator that has, so to speak, a bit of cortical real estate that it can use to model certain domains of its environment with more or less resolution, then what you should be doing is whenever you have a problem that you cannot solve with your existing feedback loops, you need to create secondary tertiary feedback loops, and at some point, this is not enough. So you need to spawn a completely computational model of that domain. So this is probably what our neocortex is doing, it creates a computational model of a domain. So it can regulate this domain, because you cannot regulate what you don't understand. That Ashby's good regulator theory, if you build a regulator, then the structure that it implements needs to be isomorphic to the system that it regulates and the world that we are in is a very complex, three dimensional dynamic physical world. And then it's overlaid with a very complex, even more complex social world and economical world and so on, that are in dynamic interaction with each other. And this is what our cortex in some sense is modeling. But not in an isomorphic way, how you do abstract, right, and compress, as you said earlier. Yeah, so isomorphic to what, right? There is a certain world that happens there physically, and we already know that what we perceive as being the world is very, very different from this. So, for instance, there is no sound in the world. There are air molecules, at least been seen from some perspective, right? They're actually also not air molecules, but from some frame of reference, we can describe this stuff as air molecules and their electromagnetic forces between them that give waves of traveling pressure. And then there's some Fourier transform happening in our cochlea that measures the energy of these waves in different frequency ranges. And this gets sent in the neocortex. And now the neocortex does its very best to predict the structure of these patterns. The result is some interesting geometric calculation that our neocortex approximates. And these geometric patterns we interpret as sound, right? And patterns we interpret as sound. Right? And then we organize the sound into things like phonemes or into musical patterns and so on. That's fine, but none of what you said necessitates the use of the word intelligence, right? Because now you talk more about induction, right? The ability to build models, right? The ability to estimate the sources of the sensory states that you're exposed to. I think that before you get to the sources, a lot of other things are happening. This idea that all the patterns that you perceive go to sources is already a major inference that you're making, right? So you make these inferences because they give good predictions, right? But what you eventually try to do is you try to predict patterns. And maybe a good metaphor for this is to think of a synthesizer, right? A synthesizer is a haverset arrangement of electronic components that have the property if you connect them, they go into weird oscillations and produce interesting dynamics. And you have a few knobs that you can use to fiddle until it makes the sound that you want. And imagine you have something like this built from neurons that the brain fiddles at the knobs until it's able to predict the patterns coming in from the cochlea in a few dimensions. And when it has done this exhaustively, it moves to the next layer, and then tries to identify the covariance between the knob states between the latent variables that it has done this exhaustively, it moves to the next layer and then tries to identify the covariance between the knob states, between the latent variables that it has discovered. And now uses this to generalize over the previous patterns. And it does this until it's able to fuse the different sensory modalities into a simulation of a dynamic free space with moving objects. It does that in the context of a superbly tuned body, a sort of physical apparatus which provides all the transducers which these, you know, sort of pattern recognizers, function approximators can work on and already decomposes the world in a way that is well set up in order to be able to make sense of it. So, I mean, in your notion of function approximation doesn't really, can't really stretch to include this aspect of what some people call morphological intelligence. I'm totally biased against that. I don't believe in morphological intelligence. It must be true that the body makes the challenge of the brain. Why don't you speak of intelligence again? Well, I don't like the word intelligence or morphological intelligence, but other people use the phrase. But it's certainly true that the body makes the challenge for the brain has much simpler, it makes it easier to control walking, because your legs are inverted pendulums. It makes it easier to do hearing because you're talking as if you have never seen a toddler walking with legs that are absolutely not fine to to walking. Your movement of your your geometry of your body is optimized for your adult state. If you are a toddler, it's actually much harder to walk because all the centers of gravity are off and you still manage to do this. In the same sense, you have all these nerves scattered over your skin. If these, this is not done in a very particular order. Your brain is doing the co-occurrence statistics over these nerves. That's not a battle you can win because there are always examples where just the biomechanics just helps you. Take grasping, right? You just push the center of the hand and it grasps. And this is true for any age or development, right? So, if you push hard enough in the hand, it closes like this so you grasp something. So, this helps you grasp something. This helps you. You don't have to plan this. Oh yeah. I'm not against the idea that your body is optimizing because it's subject to the same evolutionary constraints, but I wouldn't call this intelligence because your body is not making interesting models. But look, you and Tony, both were on the side of intelligence earlier. No, we're not on the same side here, because you're saying that these models aren't grounded epistemically, you know, Paul said that. Yes, they're not grounded. And I'm agreeing with him. And I'm saying that what grounds those models is that they interact with the world through a body, which is optimized through evolution to do that. So to think about intelligence in this entirely abstract way may not lead you very far because the embodiment is critical to the way that human intelligence operates and has evolved and we may not be able to build disembodies at AI's just by having recursively bootstrapping and learning systems and people are trying to do that have been trying to do that for decades and what I don't see is any breakthroughs in that or imminent breakthroughs so maybe we need to follow this more biologically grounded approach which is to say let's build embodied AIs. That would be my approach. I think it's part of your approach. I think it's a super popular approach. People like that approach. Well, because there's a little more. But if you go to NIPS, that's not what they're doing. Yeah, they do the stuff that actually works. Bodymentalism didn't work. Historically it's been a failure. When this new AI stuff came up, people have been building lots of robots. Like Rolf Pfeiffer's Klapparaturen in Zurich. They're very impressive, it's very aesthetic, but it didn't get us intelligent systems. And so I'm of two minds with respect to this. I think that for our human intelligence, there's something that's quite different from what a lot of machine learning is right now doing. For instance, when we train a vision system in machine learning, the typical approach is we give it an annotated database with a few million images, and ask it to generalize over this. Or we might even give it a non annotated database of a few million images and ask it to generalize over this. Or we might even give it a non-annotated database of video streams and ask it to generalize over this. It's going to figure something out. But it's figuring something out that is slightly different from what we figure out. Because for instance it's prone to adversarial examples. So you change a few pixels strategically in the input image and suddenly instead of a cat it sees an ostrich. How is that possible? It's probably because it's mapping this onto a space with a very different structure than the space that our mind is mapping these things on, which means it gives you a similar classification under most circumstances, but not under all circumstances, especially it's not good at dealing with novelty. So, if you throw an end-to-end train system, a picture for instance of a plane that is crashing into the tarmac, it might recognize, okay, there is a plane and there is a tarmac and it might figure out, oh, there is a plane on the airport and this is what it's going to tell you. It's going to miss out on the crucial interesting fact that this plane is about to crash and bad thing is going to happen. So, how can we do this? And I think it starts, in this sense, I agree with this embodiment thesis. We start with our proprioception when we are born, it's already set up. And we already have made a map of our body surface before we are born. And then we add stuff to that map. We probably add stuff to that local perceptual space. And whenever we see something, we can work based on the assumption, what I'm currently looking at is a window into the same dynamic free space. So everything that I perceptually sense is a part of the same unified world, right? This is a very interesting thing. So when somebody shows me a picture of a cat I don't see an isolated set of pixels that I now have to classify. I know it's a person giving a picture to me of a cat that has been produced by a machine and at some see an isolated set of pixels that it now has to craftify, I know it's a person giving a picture to me of a cat that has been produced by a machine and at some point was a window in a different part of the same reality. This is the way I make sense of it and it's very hard to fake such a universe, right? Wait, Joshua, before you go there, I already challenged you that, so you were saying, look, a lot of people believe in this sort of embodied view on mind. I'm explicitly not saying intelligence. And then I challenge you, I said, look, but if you go to NIPS, no one is doing it. And you say, that doesn't work. But the things that the people who go to NIPS believe work might work for the wrong reasons because the tools they build work because they rely on massive amounts of human annotated data, which is not the way forward. We know that we'll hit the wall somewhere, right? Well, then you said, well, and look at what role five, well, I brought robots to world five is lab in 1991. So I know roles work pretty well, right? And then we have to be clear, what does it mean to work, right? In our own trajectory, this whole embodied approach has worked extremely well, because we have really explained aspects of the brain to the extent that we could make testable predictions that were confirmed to some extent but falsified. So from the perspective of a scientific program, this has been extremely successful to the extent that now we are repairing the brains of stroke patients with it, right? So it also depends very much on how you define it worked, you know. So I think we should be very careful there. And so did we scale to anything you might want to call artificial general intelligence? No, that's the future. That's the challenge, right? But I think the situation for embodied models of mind are not that bleak. It just depends what you're looking for. And if your benchmark is what the computer scientists and the big tech companies are looking for, sure, then we might not be performing really well. But if you're interested in a more scientific program, I think we really have made huge progress. I think, and there is value there, and this might be an alternative approach to these questions than the more computer engineering-informed one. Yeah. I think we should also take care to distinguish the practical work from the industries of AI hype and anti-AI hype. These are two variable industries that feed a number of people to produce AI hype or anti-AI hype. And there is actual research work and there's also scientific work. And these are slightly different things. And if you look at NIPS, it's a very diverse thing. I mean, it's about as large as Burning Man, but sells out in a quarter of the time by now. And there are a very large number of people that do very constrained vision stuff or convolutional network things. And there's also many, many other people. And the absolute number of people that wanted to do a science of intelligence under the umbrella of AI was always a pretty small minority. And I think the ratio of people that do this is almost unchanged. So the vast majority of people in AI do engineering. They basically build useful applications for data processing systems at the forefront of what we knew about how to do clever data processing. And a small subset, a few percent of these people are interested in actually how the mind works and dedicate their careers to that question. And just because so many people now do AI doesn't mean that these others are not doing it. It's just, it's the same fraction. The absolute numbers are probably larger than ever in the history of AI. And still it's a small subset. And an absolute number of people that are interested in building models of minds and have very similar ideas to you and me with respect to this that are at NIPS are probably still a few hundred to a few thousand people. No, sure. I think this is really an important distinction. That's okay. So, now we got that out of the way. Let's move on to the second slide. Exactly. So, you did emphasize that consciousness has a role to play in reaching this synthetic mind. So what's your definition of consciousness and what's its function and why is it such an important ingredient to synthesize mind? So there are several aspects to consciousness and when we use this word, we often don't make clear what we mean by it. It also doesn't help. It's mostly a thing that we know by pointing and not by agreement about what we mean by its particular functionality. So there is something like alertness, the ability to react to stimuli in real time, and so on. Then there is the ability to learn and update, and the ability to reason about your behavior and reflect on it. There is the ability to attend to stimuli and high level stimuli that we abstract in your brain into mental simulations, and to remember that you attended to them. And to remember that the second order attention to this, so this reflexive consciousness above this excess consciousness. And then last but not least, there is this phenomenal experience, what it feels like to be in a particular situation, right? So maybe we start with this one. At the core of what it feels like, if you really meditate very, very deeply, you feel that there is, at least this is what appears to me, like a fundamental kind of disagreement with the universe at some level. And whenever you perceive a feature, it has a particular meaning with respect to this kind of disagreement. It gives you a particular kind of relevance. It's like zero-order consciousness. And then as soon as you start conceptualizing objects, before you think about them in terms of reflexive language and so on, you get something like first-order consciousness, where you see a particular thing and you feel, I am perceiving a particular thing. And when you go above this thing, then you are in the conceptual level already, that basically you now start carrying that thing around second order, like in a glass bowl, you see this glow of consciousness, with every step it gets a little bit darker, but it's a symbolic manipulation that you perform to shift that thing around in your mind and do things with it. And when I give a linguistic report about the whole affair to you, then consciousness is a far distant memory to the processes that produces, or it's like a myth, it's something that my linguistic apparatus is not actually participating in very much. So there is something like a subjective feeling of being participating in very much. So there is something like a subjective feeling of being conscious in actuality. And the weird thing is that we probably cannot be conscious in actuality in the sense that a physical system cannot do that. The brain takes a long time to process stimuli. For instance, when I stub my toe, it probably takes something in the order of 700 milliseconds for that to register in my neocortex in my integrated world model. But I have the impression not only do I perceive this in real time, but I also act on it in real time. Subjectively, I'm not just experiencing this with a nice time delay that can be filtered away with some editing trick in my mind, but I also have the impression it is me that acts on that stimulus that's happening right now, right here. And that is not possible, because the initiation of that action is going probably to take another second until I'm able to catch myself stumbling and react to that pain and integrate this all into a cohesive story. Which means the story must be instantiated with hindsight. At least if physics is correct and we live in a mechanical universe. Subjectively, I don't live in a mechanical universe. So what sense can I make of this dualism? Subjectively, I live in another universe in which I can react to me stabbing my toe in real time and experience that, right? Yeah, the thing is that you do, but your conscious self doesn't, but your body does. And so this identification with the conscious self, which is problematic, and your body is reacting as quickly as it can. Yes, so in a way it reacts as quickly as it can, but also not in real time. But the problem is that you're identifying with your your subjective consciousness not with the rest of your body So yes, you say I can't respond that fast. Yes, you can if you identify with your toe I'm with you know, the reflex systems in your spine That's as fast as you can respond but and then those systems report up to your conscious stream of consciousness that this has happened Yeah, but but I don't't understand why that's a problem. I mean scientifically we know that that's what must be happening. Exactly. Exactly. But this is what gives rise to this confusion that people, including many philosophers, have about that. Personally I don't think it's a problem. So there is an answer to that. Before you guys agree, Joshua is making an assumption, which is not declared, right? And which is relevant for this argument, because you seem to be saying there's something like behavioral time constants. I hit my toe, and now within a certain time frame, I will have to respond to that or I follow my nose, right? And the time constant of that process is so short that it's not reasonable to expect that the system that would support consciousness can operate at that same time constant. That seems to be the argument you're making. Yes, but it's not an assumption. It's something that we have established by measurements. We can put electrodes to somebody's skull and we can see how long it takes until the stimulus arrives there, right? So we know about this delay. It's not a subject of the before because it's on the table now. Yeah. But you didn't declare that before. Okay. That's, argon hinges on that. You say I have two processes, one fast, one slow, and they cannot be running, they cannot coalesce. So therefore, the slow conscious process by by necessity, is the story you tell yourself afterwards. So, no, it's even worse. The problem is that, yeah, that this conscious process, that thing that I perceive as happening, is happening instantaneously. It's happening in actuality. It's happening here and now without any delay. It's not even happening fast. It's happening instantaneously. So how is that possible that I act instantaneously on me stabbing my toe when physics does not permit my neurons to transmit signals instantaneously, but does so in time frames that I should be able to perceive? Right, because I can distinguish time frames that are happening at something like half a second or a third of a second. I don't get the instantaneous. I don't get the instantaneous. Can you explain that? So, when I act based on, when I act on my perceptions, subjectively, I'm acting on my perceptions in real time. Really? Yeah. I see you moving your head and I'm reacting subjectively, instantaneously on movement of your head. And of course the solution to this is this is a story that's been created after the fact to basically make it possible for me to learn from the past interactions that my nervous system has orchestrated with the physical world. Well maybe that's also a little bit the tricky bit of subjectivity. I don't experience it like that because I live according to my own theory, which is real-time is subconscious and cause experience is delayed. I don't experience stopping my toe in real-time. I don't, not at all. Oh, maybe you're not conscious. That's definitely possible. We cannot exclude that option. But it seems so. Why are we so surprised that folk psychology is wrong? I mean, and we grow up being... Well, it's not folk psychology. I'm talking about my subjective experience. Yeah, but your subjective experience is folk psychology, because that's what you've been told as soon as you were able to listen. These were explanations of events that happened. Nobody talked to me about consciousness when I was older. They talked to you about... Where did you grow up? They talked to you about the fact that you could control your toe and that if you stubbed your toe... No, my parents mostly stayed out of my way. I learned to control my toe by myself. But still, it doesn't matter, right Joshua? Yeah. You may have evolved your own folk psychology, but unfortunately you grew up into a scientist and then you discovered that it was largely wrong. So now that we've discovered that folk psychology is wrong, we can just put it to one side and not worry about it. No, the thing is that what I have to explain is the set of phenomena that is subjectively available to me. So the reason that people get confused about consciousness is that they have a particular kind of experience and that experience seems to conflict with what is scientifically possible, which is why people like Tononi get extremely, come up with very expensive theories, right? I mean epistemologically and ontologically very expensive theories. There is the reason why they're willing to carry that burden. It's because they're not going to give up on what you so divisively call folk psychology. Well, I think that's absolutely true, but I think they should. Oh, I agree. Julio ends up being a panpsychist. So that's, yeah. Okay. That depends what kind of, whose folk psychology you're buying, but okay, he certainly gave up mine. But the thing is, I think you tried to wiggle yourself out of a problem here, Joshua. And I would like to remind you of the problem, which is, I really believe that I remember that you said earlier that the constant experience of stubbing your toe was a story you told yourself afterwards in that example. You didn't say instantaneous. And then I was trying to help you, but of course, that means I was doing the wrong thing for the right reason, right? Or maybe it was all wrong. By saying, by just trying to look, you're making this fundamental assumption that we have a dual process system that have different time constants. Oh, no, that's not what I'm doing. There is something else going on. There is this traditional perspective of idealism, right? Idealist philosophy basically makes the statement that we don't live in a physical world, but that we live in a dream, in a magical world, essentially. One in which symbolic interactions are possible instead of just mechanical interactions that need to be performed on some fundamental, causally closed physical level, let's say some quantum mechanical level or below that. And in this symbolic universe, certain things are possible, like our minds exist. And the dualist perspective says that these two domains, this physical domain of objects moving in a space or information flowing in some kind of computational graph of the universe, and this mental domain where our mental phenomena take place, somehow need to co-exist. And what I want to suggest is that this is in some sense the case. We are literally living in a dream that is dreamt by a mind on a higher plane of existence, and the physical universe is that higher plane of existence, and the mind that dreams us is generated by the brain of a primate. So in some sense, these perspectives are complementary. Subjectively, I live in a dream world. The same circuits that produce a dream at night produce a dream during the day to track and predict my sensory data. And the properties of the... Yes. That's also a logical consequence of model building and compression and all that stuff, right? Exactly. So, and the difficulty that comes up with, which is the reason why people like Tononi are so confused or why strong embodimentalism has such an appeal, which basically there is a different kind of embodiment which doesn't say let's use robots because they're so useful to ground your perception, which I totally agree with. If you are a mind, you should be able to use your body, that's a useful benchmark, and it's a very good source of data if you're connected to a physical universe. There's a lot of interesting variants and invariants in it, right? And if you have some regulation goals that are maybe given from an organism that make you interested in cool stuff, right? So you build interesting models and interesting policies. I agree with that perspective. But there is a deeper perspective, which says, basically, some people get up from their meditation, and they realize, oh, my God, this is so awesome. This cannot be explained by a mere computational process, there needs to be something going on that is more than mechanical. And where does this stuff come from? Because my brain is just performing some boring computations, probabilistic excitations in these glorified fat cells that we use for processing information that we call neurons, right? So what is that thing? And for some people, they suggest, oh, maybe it's the touch of the mystical reality substance, maybe physical reality that's more than computation. And the mind is not something that just happens in the brain, but it's the result of some magical morphic resonance between your body and your environment. By getting in touch with the mystical reality of substance, you have this emergent and possibly extended mind that spreads outside into this universe that you're perceiving. And what you don't realize is that the universe that you're perceiving is not some direct access realist universe, but it's a dream generated inside of your neocortex. It's a simulation. It's a VR. Sorry, are you endorsing the idealist perspective? No, I'm basically saying we should interpret the idealist perspective as something that in some sense is true. You can perform magic, you can learn how to do, to levitate, you can learn how to look into the future, but you do this basically by editing your memories, by changing the way you produce, your neocortex produces these models and you pay a price for this because you cannot break the bank, because outside there's a mechanical universe. I think it's very straightforward because you already declared this earlier where you say well we by necessity we build models right and as soon as you start to rely on these models to inform your actions you live in a virtual world it's virtualized yeah so that's and everything else is just a consequence of that. But I mean that virtual and that's fine So, that's and everything else is just a consequence of that. But I mean that virtual... And that's fine. The mind lives in the physical body which lives in a physical world. Yes, that's the interface. But there's one thing... There's no magic then. Exactly. There's no magic. There's no conspiracy. It's good news. We can summarize the last hour in just eight words, okay? Which are? The mind builds models and as a result, you virtualize. But now you add, you say, you add to the physical world, you add an additional word, higher plane. Why is it necessary? Why is the physical world now the higher plane? Why is not just, we build models that are informed by the external world, but we abstract away from it, and those inform our actions. Oh, it's because the world that I live in subjectively is VR, right? Similar to Minecraft. It's a computer game generated in my brain, right? And the physical world is the parent universe to this. In this sense, it's the higher plane of existence. It's the substrate plane. You seem to suggest a dualism I don't like. Because in Tony's case, earlier you were sympathetic to this embodied view of mind, and that actually builds a very solid interface with you from the beginning that cannot be decoupled. Since it's physically instantiated, it's an embodied mind, it's by necessity always one with its physical world. No, no, no, you're going to woop to me. This is not true. So basically, I think if you would replace your neurons with something that is function equivalent and performs the same computations, you could still be living in the same VR. Look, if I allow you to make all these assumptions, the conclusion might be accepted, but I think these assumptions are extremely strong and you will not succeed in living up to them till the end of your career. Oh no, it's just you're not a functionalist, that's all. And you maybe, I cannot change your mind, I can try, but for the same reason that you can change the CPU in your computer to something that is functionally equivalent in the sense that it's able to run the same software on it, you can run the same program on your computer, right? That was not the argument. The point is, you described the external world as a parallel universe, right? That's the word you used. No, as a different universe. I didn't say parallel. Well, you could separate it. You could separate it. No, it's a different frame of reference. So, in the same sense as, for instance, the software that runs on your phone does not exist in the same physical universe as the phone itself, because it's a frame of reference that you're using to describe it, which doesn't make sense in that other universe. I think it's a shortcut you're making, right? Because all the words you pronounce now that are the product of this biologically instantiated germ machine that you believe you are, right, resonate back to you as you speak them through the real world, right? And even will hit your skin, you gesticulate. So the embodiment is implicitly there also as an integral component of these models in which you rely. It's not decoupled. Like, oh, and now they're independent from each other. No, I, I see. There's continuous feedback between them. I remember from reading your paper that you are basically an identity theorist, right? And I don't think that's a useful way of conceptualizing that space, because I am not a monkey and I'm not that body. I'm a side effect of the regulation needs of the monkey. I'm the result of that mind. I'm a story that the mind tells itself. There are people which... But it's only because you choose to identify with that. Joshua, I reject your story. To the extent that you're constructing an I, why not construct an I that identifies with the physical self as much as with the mental self? Because this identification doesn't hold. I realize that when I think about what this physical system is, that I really don't get behind it. I have difficulty to identify with a subset of the evolution of the state vector of the universe because this is not what I perceive myself to be. What I perceive myself to be is very similar to a character in a novel that is told in a very rich perceptual language. And this character in this novel is something that the brain uses to tell itself its own story in a way. But it's not the story of a brain or it's not the story of a primate or an organism or a bunch of cells or a bunch of particles. It's something else entirely. Because the thing is, a brain is a physical system. It cannot feel anything. It cannot experience anything. Neurons cannot experience anything. But it would be very useful for the brain to know what it would be like if it was somebody who could experience something. So what the brain is doing, it creates this VR of a world. And then within this VR, it creates a person, a simulacrum of a person. And that does think to that simulacrum that is halfway to a simulation. Wait, it does think? Yes. So basically, as an author of that dream that you are in, that's the system, a set of processes that generate your VR, and that create your personal self, your model of what you that you are in. That's the system, a set of processes that generate your VR and that create your personal self, your model of what you think you are. And then it does things to that model, like expectations and the result of past actions, and then witnesses the reactions of that simulation and uses it for learning. Yeah, but I mean, then you're identifying with this thing which is flittering on and off, because this simulation itself isn't going on all the time. Most of the time you're operating in the real world, your body's just doing things. Yeah, so the good thing is my brain gives that thing access to the language center. And here I am and talk to you. And here we are. You don't have to. It's totally optional. The door is locked. No, but look, this is partially all reasonable because because it still fits the same model building story, only now you say, say look but there is other sources of information related to the body itself which also build models. But now you have a problem because you indeed actually explicitly declared that there is something observing that model, right? So you have to solve a muculus problem. No. Okay, who's the observer? How does the observer work? Who cares about the observer? Why do you need an observer at all? There is not an observer in the sense that there is something looking at it. What is there is a system that takes parameters from that simulation and uses it for learning. So, there is a loss function that is generated by producing a virtual character with certain intrinsic tendencies, behavioral tendencies, so it's a model of my own internal regulation. And when something happens to me in my virtual simulated world, like my simulated love interest, my partner decides to desert me, which is something that might happen in my mental simulation, then my brain in some sense observes what happens to that character, which doesn't mean it's laying back and watch that movie. It just takes a few parameters from that thing and uses it for learning, right? So it would play that scenario into, let's say, working memory and then you can experience that scenario. Yes. So it's basically a simulated world that does things to that particular thing, because it's a very high-level compression of the state vector of the universe and available data at my systemic interface to say, okay, you have a partner and that partner just left you. What do you feel about this? Sure, but it doesn't matter whether it's high-dimensional or not, right? So, just, yeah, you have, so you have a self-model, you're saying, right? Then you have a model of the world, right? And these two models now interact with each other. Because now the self-model could say, okay, let's push some parameters in the world model to create some scenario that would affect me. Let's see what kind of states it would give rise to. So, it's like the coupling of two models. Yes. So, you can do this to some extent, right? You can imagine what it would be like if you would get a lot of chocolate in an hour from now, right? And you can evaluate your own reaction to this. But if you are working correctly, you are not allowed to believe that this is going to happen without evidence that it's going to happen, right? Because otherwise you become delusional. Which means you are not free to choose the environment that you want to live in. It's a very paradoxical thing because objectively you might suffer because the world simulation that you're in does horrible things to you and you might wish that not to happen. It's actually your brain at some level that does these things to you and you could go and reprogram that brain to give you paradise. But of course it might mean that this organism dies because the model does not give very good predictions. And yet, I think that most of what these Buddhist techniques of fixing your conflicts with environment are doing is to reprogram that simulation to get your brain to like you better, to like that person better, to do nicer things to it. Well, the Buddhist found a very simple trick to do that because, just say, you solve whole conflicts, you don't want anything. You stop wanting stuff. So turn down all your drive systems and the world is great. There is more than one Buddhist tradition of course. And what you describe is the Sutra. So the Sutra transition is ascetic. You basically get rid of your needs by stopping to satisfy them. And at some point this thing realizes I don't need to satisfy them and then look forward to this. So they don't create any discrepancy between target and current value anymore. You forget about this thing and you become free. And there's the tantric tradition, which does the opposite, which says basically embrace this maximally and satisfy all your needs. Accept that they are there and explore them and eat your shadow, integrate this all into a thing that actually really works for you. So since I've been in Barcelona, I've been having vivid dreams. Now, I can tell you more later, but when I wake up in the morning, I experience the world in a different way than I experience my model of the world in my dream. My model of the world seems very vivid and very real when I'm dreaming it. But then I wake up in the real world and if that is a model of the world that I'm living in now, it's different in some interesting and important qualitative ways from my dream world. So, I mean, whereas, would you recognize that the distinction there that the waking world that I'm in, is somehow closer to, to maybe this physical world that you're, say you're permanently separated from. So I think you can, and certainly that we've talked a lot about the mind building models, but we, you know, we talked a lot about the mind-building models, but we can also talk about the world being its own model. It's closer in the sense... ...about all the evidence that we actually don't model. Ideally, it's predictive of the patterns on your systemic interface. So at night, you are dissociated from your sensory input, which means that your dreams spin freely and you have no way to invalidate your hypothesis of the world state that you're in. Also, dreams tend to relax your priors, which means your model has more degrees of freedom than it would have during daytime. Which gets us back to grounding. So when I'm awake and when I'm not daydreaming I'm in a grounded state where I can't believe arbitrary things or you know sort of imagine or see cockroaches climbing up the walls which I did the other night. I can't do that because they're just not there. So the world is grounding me in this awake state. Yes. So this makes me closer to the physical world than I would be if I was entirely living in a mental simulation. Do you think it's a very useful category to say closer? I mean, how close are you to quantum mechanics? But I can believe in, with some confidence, that my perceptions in my everyday state reflect something about the physical world. So it's not that I'm trapped in my mind. I've got access. Let's not mistake the physical world for quantum mechanics. Those are two very different things. No, quantum mechanics is probably not an ontological theory. It's a particular kind of formal theory that allows you to describe low-level stuff. So, the only statement is valid. No, what I wanted to say is the world is more weird than quantum mechanics. The physical world is probably some computational system that progresses from state to state in a particular way, and we are an emergent pattern in this as physical systems. But nevertheless there are things that can't happen in my awake life unless they're true hallucinations. Yes. I mean it very much constrains what I can perceive by the physical world. Ideally you should only hallucinate things that are matching the patterns on your retina which have predictive values for the value for the patterns on your retina but the things that you are hallucinating are not actually like the things happening on your retina or the things that give rise to what happens on your retina. But they are interfaces so I mean they they're interfaces so that physical events can affect my mind and impact on my mind. You might you might you might come to interpolation between states of the retina that as such have never occurred. Why not? I don't see that problem. You're very literal in that sense. And then the other problem I have, among the many problems I have, with the one I will share with you now, is that you agree with this idea of the embodied view of mind, so the machine view of mind. But I think we disagree very deeply about what machine means, because I think I would even accuse you of only thinking about the machine in terms of a Turing machine. And that's why I think of the word computation is so central for you in the way you describe these phenomena. Well, again, I would see the Turing machine as just a thought experiment, as a very ideal kind of machine. Yeah, it's way too powerful. We need finite state machines. No, but also physically it doesn't exist. At best, it's approximated. It's a sub-symmetric machine. No, I agree. There are no infinite tapes. Exactly. So, therefore, it's only a metaphorical machine. Well, I want to talk about flesh and bone machines that really physically exist and have an ontology. So, isn't there a bias in that sense in analysis if we implicitly define machine as Turing machine? Wouldn't it be more useful to consider scenarios where we're less tied to a notion of computation. We say the universe is computation. No, that's the way you can describe it. Okay, let's try to disassemble this. So, first of all, a machine is not part of the territory, it's part of your map, right? It's a certain part of the language that I use to describe the world. And what I mean by this particular concept in my language is a system that I can describe as a state space and a transition function that correlates these spaces in deterministic or probabilistic ways. And what's characteristic usually about machine descriptions is that the machine is able to exactly revisit earlier states. So at least conceptually it should be able to do that. So it's a particular... But already that is problematic, right? Yes, it is. But this is not our point of contention here. So the important thing is machine is part of the language that I use to make sense of the world. And mathematics is the set of all languages, right? It's the domain of all possible languages. And then there is a set in which you can make proofs, which are the languages that you can formalize in a particular way. So, you get very interested in those. And most of what we do in mathematics is these formal languages. And it turns out that in these languages I can specify things that cannot work, like paradoxes, things that are conflicting with themselves, like the set of all sets that don't contain themselves. This cannot be implemented. And for something to exist, it probably needs to be implementable, which means it needs to be realizable in a way that it can produce certain functions, that it can perform certain things, right? That it can go from state to state and give rise to states at my interface to my environment. But I cannot know what that is is that thing, because I only get discernible differences. And even this, I don't know exactly, if I really get the discernible differences that I think I get, right. So I had to make very conditional models. And it turns out that mathematics allows me to reason about these conditional models, if I have evidence that my mind is cohesive enough to afford this reasoning. And the good news is the evidence that my mind is cohesive enough to afford this reasoning. And the good news is, the theory that my mind is cohesive enough to perform mathematics gives very good predictions. It's a very nice theory. So I can do some mathematics with my mind and I can start bootstrapping certain things. And one of the first things I discover, that mathematics is covered very late, is that there is a part of mathematics that doesn't have contradictions. And this is constructive mathematics. And the modern name for this is computation. So everything that happens as a computation cannot have a contradiction, your computer can never be in an invalid state, right. And the traditional definitions of computation, which use infinite tapes and other things, don't fully realize that implication. So this gives rise to the Gdel problem and the Halting problem and so on, which are features of language specifications that introduce infinity and timelessness and so on. But in computation these things disappear. Now in computation a few things are different from classical mathematics. For instance, pi is suddenly not a number anymore, it's a function. And it's a function that you cannot actually compute, because nobody knows the last digit of pi. In order to compute the last digit of pi, you would need an infinite amount of energy to produce these computations if you happen to live in a reversible universe like ours. But you need energy to delete bits, right? So you will never know the last digit of pi if you live in the universe that you can mathematically deduce to be probably in. So we are not talking about reality as such. What we are talking about is mathematical models of hypothetical worlds. And then we try to discover the class of mathematical models that can explain our observations. And subjectively, we live in members of that class. But we don't live in the low-level members of that class, because our brain is not sufficiently detailed to simulate them. But that's your hypothesis. Your hypothesis is that you do live in a subset of that class, because your mathematical languages and the statement of mathematics is only true for closed worlds, if the boundary conditions are defined and as life forms, the power of life is that we act against the second law of thermodynamics as open systems. So there might be aspects of these systems that you cannot capture with a set of mathematical languages that you now advance. I'm not part of that religion. I think that... No, that's fine, but that's a challenge. Still, you must convince me or... I'm trying. But you should explain why you believe that this specific language, that this ancient notion of computation, would be susceptible for giving us any kind of traction to these open systems that we call life. I don't think that life is about open systems. The meaning of life on earth is the hydrogenation of carbon dioxide, roughly speaking. Life is cells. What about the meaning, right? Only in the sense that people are asking for the meaning of life, right? Of course, there is no meaning, but in the sense that people are asking for the meaning of life, what is it there? What does it do? I was asking for the meaning of life. I was saying your computational language might not be appropriate to understand life, including consciousness and everything in between. Okay, I'm trying to show you why I think that is the case. No, of course, that's what I'm challenging you. Yes, so, okay. Why don't you show me that? So, what life is about, it's cells. At some point in 1650, we didn't know what was life in the same sense as people in 1950 didn't really know what was intelligence. You could point at it, but you didn't know what it structurally, functionally was. So, in the same way as I would say today, and you might disagree, that intelligence is the ability to make models, I would say today, or most biologists would say this, life is cells. A cell is a molecular machine that is able to extract neck entropy from a very wide range of environments, and it has something built into it that is almost literally a Turing machine. You have this DNA tape with a red white head, and this thing implements an operating system on the cell which allows the cell to become part of a cellular automaton to create an organism. It's not writing states. Not necessarily writing states on that tape, right? It's reading it. It's reading it, but it can also write a few things and some of it, it also writes in methylation. So we have this epigenome which is a flash drive. Yes. But there, even neurons seem to be breaking their DNA in particular ways to write something on it. But largely, of course, it's a ROM. We're mostly reading from this. And we also write state, but we do this in additional parts in the cell. So, this is mesentation and changing layout of the genome. Basically, a few bytes that we can write. And what the cell can do is it basically measures its state, which is a chemical configuration based on processes that happen inside of the cell and processes that chemicals diffuse over the cellular membrane from the environment of the cell, which might also contain other cells. And based on this, the cell is going to go into a new state. And then the process of this is either regulate something or it differentiates itself in another cell, which will behave differently when confronted with the same set of possible states, or it might go into apoptosis, or it might divide itself. So your cell combines a neck entropy extractor, this state machine, differentiation mechanisms that allow them to participate in evolution, and self replicator. In the absence of any of these components, your cell is not going to work. Once you have these components, which are incredibly complicated and probably require enormous amount of rolls of the cosmic dice, maybe in the same order of magnitude as the available planetary surfaces in a sufficiently isotropic region of the universe, right? You have that thing life. You have this evolution of things that grow up. And what life can do, what other dumb chemical processes cannot do, it can produce controlled reactions. We are in competition with dumb combustion processes and redox reaction on this planet, and when they start, we cannot outperform them. And there is a fire we lose against the fire. The fire is so efficient in closing like entropy gradients. But there are some things that we can do, like we can do photosynthesis if you are a cell. and there's a fire we lose against the fire. The fire is so efficient in closing like entropy gradients. But there are some things that we can do, like we can do photosynthesis if you are a cell. So you need to add a little bit of energy to harvest more energy in the end. And what happened initially was that the cells, some of them discovered after a while away to turn energy from the sun into their own structure by taking carbon dioxide from the atmosphere and changing it into their own structure by taking carbon dioxide from the atmosphere and changing it into their organic molecules and putting oxygen into the atmosphere. And they did this for a while and were very successful in this. And from the geological perspective this was the major oxygenation event, a major event where suddenly you had a lot of free oxygen in the atmosphere. We are in danger of just agreeing with each other. So, Paul said that animals are machines that can physically exist. You are saying that animals are computational entities, but you are defining those in a particular way, such that there can only be ones that can exist. So, we are largely agreeing. Yeah, so, there happened a singularity at this point. At some point, some cells decided to combine together and form organisms, which made them participate in the multilevel selection, and they mostly became carnivores. They started eating other cells. We're not making progress here, because the challenge was a somewhat different one. Oh, I'm giving you a computational description, and I'm trying to explain to you at some point why life is not an open system. You don't give me a computational description because you already admitted that the state machine of the cell is not a Turing machine, but it might be sort of... No, it's just a finite state machine. It doesn't have infinite tape. It's less than a Turing machine. Right, exactly. It's not more. But the point is... It's also not something different. The computational language might give you leverage in describing certain processes. I give you that, not a problem, but you seem to give us some ontological stages. That was my concern. But I mean, I think... But we cannot discuss ontology. What we can discuss is epistemology and metaphysics. No, but we can because you seem to give us some exclusive, some exclusive descriptive power that I want to question a bit more. And that's why I was challenging you on this notion, is your notion of machine a Turing machine? Because maybe if we think about, although we were talking about mind, synthesizing mind and consciousness, there might be properties to that that go beyond what we can capture easily from this computational perspective. It's basically, in in the limit it becomes a Turing machine. If you give it unlimited computational resources, then it's also in a sort of an open system, energy is flowing across the boundaries, otherwise it would die. So, this is just another way of describing what it's doing. this is just another way of describing what it's doing. So maybe there is a conflict here, but I think in some way that it reflects your view of the mind as being pure computation and sort of sitting inside this body, which in this physical world, with which it could never have any direct contact because it's purely in the virtual sphere. Whereas I think Paul, myself, have more of a view that these are really, in the end, the same thing. The physical world generates the virtual mind. Oh, basically what I would say, there are in some sense the same thing, but they are described with a different language. So, basically, the language in which you describe mental events and in which you experience mental events is a language that is different from the language that we use to describe physics. So, mental events don't exist in the physical realm. And it's also not all physical events exist in all languages of the physical realm. For instance, wetness doesn't exist at the level of molecules. Heat doesn't exist at the level of molecules. So, mental events, like memory states, are physical events, right? So, that's, you know, there's an identity relation. You will know examples of that, right? From where you could go to physiology, you look at place cells or something like this, right? So, my challenge was more to say, look, fine, you have a computational language and Turing machines are fantastic and Alan Turing is really a great example. Wait a moment, no, no, I'm not convinced of this. I'm not, no, no, that's an important distinction. There might be something going on here, it's not true. So it could be that there are certain, for instance, that I have memories of conscious states that the physical system has actually never been in. From my perspective, this is a conscious state that I inhabited, because I have a memory of having had this in the content, in the focus of my attention. But you might be able to show that my physical system actually was never there. So from my perspective, I have a language that describes something that's subjectively totally valid, the experience that as being valid, but this is actually never took place in the physical universe. Because because none of your memories ever took place at all reconstructions of, you know, of things and how you would like to think they happen. No, I think what you're doing is in order to sacrifice to save your identity theory, you basically come up with this derisive notion of folk psychology to invalidate my actual experience. I'm saying that none of your memories are true. No, my experience is not identical to what happens physically. It's something else. How do you know that? The nature of memory is that it's memory. Because I have experiences that are not consistent with my understanding of physics. Yeah, but that's a very introspective argument. Yes. But basically, I caught myself editing my own memories and I've had mystical experiences that are probably the result of my brain messing up its own memories or editing them. And then it's a necessary part of being a physical machine that you can't go back in the past and relive that experience. All you can do is reconstruct it, which is what memory is, and it's always going to be different from the actual experience. But how do you know that I ever experienced that thing? You don't. Yes. So, actually, I... You don't need to. I recorded it. Based on the timing, I know that most of the things that I experience cannot physically take place in the way that I experienced them. And that's all fine. So what? That's the fallibility of memory. No, it's just not just the probability of memory. Memory is also creative. It's constructive, you compress, you build models, right? So, okay, big deal. No problem. It's basically, it's something else. There is the distinction between a novel and the book. If you look at the book and you open it, you see characters in it which are actually just colored parts of the book. Natural categories, right? Yeah. There is no story in that book except from the perspective of a mind who reads it. And a similar thing happens to me. I don't exist, except from the perspective of a mind who generates me, and my own perspective who reflects on what I think I am. And that thing is distinct from this thing. So there is no identity at some level between the novel and the book. You could say there is one because the novel doesn't have a being that is distinct from the book and the reader. But this is where your metaphor breaks down. Because your mind is not the story that's in the book. It's a bad metaphor because the book always assumes there's a reader. So now immediately you have your homunculus problem. You don't want to have this homunculus problem if you want to have a scientific theory. So I like the metaphor to give a gist of this idea of the problem of natural categories. Fine. That was the idea of that metaphor. So you see that. As such I accept it. Yes. But you cannot push it too far and say, therefore, also the story I tell myself about who I am, me, me as a story. I'm worried that you're going too far in an attempt to save identity theory. I don't think it's very elegant. I don't think it works. Well, you should be arguing. But there's functionalism and there's identity theory. We don't have a disagreement about functionalism. There's weak and strong functionalism. Yeah. You can be, you know, weakly functionalist and say that you can have mental states in things other than brains, but there's a lot of constraints around what those sorts of things can be. And, you know, I think that's probably where I would be. I don't know if Paul is an identity theorist, I think he probably is at least weakly functionalist because he wants to build robots that can have mental states. So but then we can discuss what those constraints might be and I think we're making some progress on that. Yeah. But whereas you're quite strong functionalist so you think you could you know take your, maybe you could put it into some very different kind of computer and it will still be your mind. Whereas for me, maybe for Paul, it's instantiated by brain. It's very hard to imagine how it could be instantiated in any other kind of device. But I'm still a functionalist because in theory at least it might be possible. So, to take a small step back from this thing, when we describe reality, we don't aim for a single narrative, right? What we aim for is the map of all possible theories that could explain the observations to the degree that we can discover them. And then we want to assign confidences in that theory space, right? To our best knowledge and meta-confidences on why we assign the confidences if we space, right? To our best knowledge and meta-confidences on why we assign the confidences if we can, right? So this is some of the process that we are doing. So when we have a disagreement, we should be able to understand the perspective of the other one in such a way that we could state it in ways that the other would agree that we stated this correctly. And then we should be able to exchange arguments about the different confidences, right? So, for instance, there are people that are embodimentalists in the sense that they think that the mind must be the result of some resonance between body and environment, and brains are only incidentally present in that whole thing, which is a very strong thing. But I've met a number of philosophers that are in that camp. And then there are people which are strong idealists in the sense that they think there is no physical system behind that dream. There's nothing that computes God's mind, and I live in God's mind, and this is the lowest level of the thing that exists there, right? And then we have different ideas that that for instance, there's some computational reducibility going on in the brain, that leads to mechanisms that are so intricate, that they produce the mind in a way that cannot be functionally abstracted in something that has fewer moving parts than the brain. And then there's disagreement about how many moving parts we need, right? And this is now it starts to get interesting, because now we get in the domain of things that we can possibly empirically test, right? So, at this point we can start maybe identifying interesting disagreements. So, I cannot know whether it's possible to reduce a brain into functional units, for instance, at the level of cortical columns before somebody has done this, right? So it's only hunch that I'm having. And my hunch is currently then we can probably run a person in less than a terabyte of computer memory if we had the right kinds of algorithms, which we at the moment probably don't. Where does a terabyte come from? Oh, it's just an upper bound based on the amount of data that you collect in your lifetime, a number of concepts that you can form, the number of computations that a neuron can make per second, functional organization into cortical columns, the kissing number of the cortical columns and so on, which basically gives you the address space of these. So, this is a guesstimate, let's say. Yes, it's all a guesstimate, right? It's all a ballpark figure. And when I was at MIT and taught a class about the future of AI, I asked the students in that class to come up with their own estimate, what they think. So before we had discussions about different ways of making that estimate. And the estimates range between something like 200 gigabytes on the low end and several exabytes on the high end, depending on what people thought the level of granularity is. The majority clustered around the low end. But of course this doesn't mean that we know that this is the true answer, right? We will only know if and when we succeed in doing this. And then we must look at arguments that are not driven by our intuitions or human vanity, because we are, that's a special species that cannot be possibly explained by a dumb computer. It depends a lot on the physical machine in which you're instantiating the person. That's right. And there's another thing about memory that, as we discussed, also from the model building, it exploits models to be regenerative and constructive. That's why you can imagine stuff you never experienced. So that means it's more about what are the parameters of these models, right, and how can you store those. But to store a model might be more than just storing just flat bits, right? It's also relations now and functions that I have to store. Yeah, transition function. So it might be gone. But for me, the question on the theory, so I completely agree, we can make this theory space and we can start to compare and see what the disagreements are. But I think at the bottom of it, the fundamental challenge of any theory is to explain something and to make testable predictions and to control stuff. These are the three criteria for a theory, right? So, with respect to your theory, I don't think we will just discuss 5% of it by now, but okay, we can do a follow-up podcast. It's still on lecture one, I think. Exactly. No, slide one. No, I think it's slide two now, right? What's the prediction? what's the most convincing prediction about the theory, which has been validated, right? And what has to be the most fundamental aspect of mind that you think you have explained? So that's a difficult thing to say. For me, I've answered many questions that I had, and this was mostly conception and analysis. So for instance, when I started to go into the field of AI, I didn't know how it's possible that a system can have an emotion. And what does it mean to be in an emotional state? Because I thought of an emotional state as something like a parameter within the system. What does make this parameter special? What makes an emotion different from a number or from a scalar or from a bunch of scalars, right? And at some point I discovered that a way to make sense of this is that the emotion is a modulation of cognition. So you have parameters that are themselves not emotions like arousal and valence and focus and the securing rate which tells you you focus your attention inwards and valence and focus and the securing rate, which tells you you focus your attention inwards or outwards and so on. And these together put you into a particular kind of configuration. And this configuration means that your cognition is going to work differently. You will perceive the world as differently. You will perceive yourself as differently if you have a self-concept. And these changes is what we actually mean by emotion. So basically I realized, yes, this explains sufficiently what I meant is this notion of emotion. So I could before point at a certain phenomenon, and now this language allows me to perform a conception analysis that makes it possible for me to understand, oh, this is actually how emotion is probably implemented in an organism as a set of modulators that configure the cognition of that organism depending on the environmental and internal circumstances, right? So, in this sense, it's not a predictive theory, it's an integrative model. And I've done this for a lot of things. For the predictive things, you can say, okay, we can come up with a machine learning algorithm that, for instance, identifies a cost function. You should think that the value of a cognition is proportional to the increase in expected reward if you perform it, minus the utility cost of not doing something else with these computational units. So if you implement this principle and you have lots of computational agents, do they perform useful modeling of useful computations in a given domain, this is an experiment that you can test. And many things in machine learning are in that area. But they're not theories of how the mind works, there are theories on how to implement a learning system. And when you look at reinforcement learning, for instance, many of the breakthroughs that we had there were the result of such considerations, right? So in this sense, AI has made many interesting predictions and useful theories, but without being able to build a thing that works like us, we will not know whether our models are actually models of a mind, or of something else that only performs certain functions that are very mind like. I mean, the slide that I put up on day one said we can have an analytical approach, not a synthetic approach. The synthetic approach can give rise to an existence proof. And the way I would look at what you're doing, you're following a synthetic approach, which is inspired by biology, but it's not necessarily limited to that. And you're looking to build existence proofs of your ideas that do things which animals and humans do. We might call them intelligent. And that's how you validate your theories. Is that fair? Yeah, I think that's fair. The problem is if I would use an analytic approach, I would have to produce theories which have an enormous amount of free variables. And that would be very prone to overfitting of my models, it's very hard to make predictive models with very many free variables when you analytically look at a brain. So you've gave for instance, this example of is making a model of the conic tome of C. elegans, right. So we know this has 309 neurons and we have mapped out the conic tome. Unfortunately, we haven't really mapped out which of the neurons are excitatory or inhibitory because we haven't really gotten to a model to the level of the individual vesicles there. So, the search space is still very large. And if we map this out correctly, I think it's a good guess to say that this thing is probably going to behave somewhat C. elegance-like. At least it would be very surprising if it doesn't. Very interesting. But there are good reasons why right now our models are not really good enough for this. At least as far as I know. I haven't looked in that space in the last year or so and don't know of their new publications that have cracked the problem. But you probably would have heard about it, right? So it's also possible that the conic tome of C. elegans is computationally irreducible, which means it doesn't compress. It's possible that we don't find a description of what C. elegans is doing that is shorter than the full conic tome, right? So we would still expect that this thing does what C. elegans is doing, but we cannot say anything beyond this, except it's a function which has 309 moving parts and complex interaction between those parts, which means our minds are not very well equipped to do a further analysis on that model. But also this is a very bold thing and we probably can compress it a little bit and make identify some patterns in this and make sense of this on a higher level description, right. So this is what we would expect. But for me, the main prediction that I'm making right now is that at some point, humanity will build machines, building the current computational paradigms, which means building finite state machines that process information, that are able to perform similar intellectual feats as we do, if global warming doesn't kill us before that. That's basically my prediction. It will be testable in some sense. But on a bit longish timescale. Yeah, we don't know that. So there's of course this thing, Daddy, do you think that computers will ever be intelligent? Yes. Before I die, a little bit before that. Right, exactly. So, Joshua, before we hit the finish line, we made quite a tour through many aspects of the mind, how we can model mind, how we can think about it also from a theoretical perspective. And you have been really working very, very deeply, and thinking very deeply about these challenges. So, if we would like to follow in your approach to cracking this hard nut, what would be Joshua's law? It's a very interesting question. What would be my law in the sense, what strategy should I be using to get to results? What strategy should anyone use? Yeah, recommend it. But what should Tony be doing tomorrow? What should Paul's group be doing instead of the stuff that they're doing now? He needs direction. He needs direction. Give us a law. Joshua's law. It really depends. I mean, from a career perspective, my advice would be very different. It should be printable on a T-shirt as well. You cannot have too many words. So I think at some level you should be aware that you are a story that your mind tells itself. And when you make a model of the world, you do this by identifying relationships between information. The most important rule there is that you have to understand that the meaning of information is its relationship to change and other information. And when you try to evaluate your model, the first rule for that is that the weight of your confidence should equal the strength of the evidence to support it. There are no valid beliefs without priors. You basically need to track all of your priors until you form a ring, until you know the rules of your reasoning, until you know the basic axioms that go into your mathematical languages. And then you try to see if you can find a model that matches the data, that matches the patterns that you get and has predictive value. And once you've done this, you try to see if there are other rings, if there are other self-contained systems that have the same power. And- So how's the law? So the law is in some sense, follow the evidence, try to make models that track the priors, always follow your priors and resolve them until you believe stops being a verb. Because that system is now self-contained and no longer has a relationship to you. Once you understand a way to define arithmetic, that definition of arithmetic becomes independent of you, right? From your priors until you're a circular. Yes, until basically you get to a mathematically closed system. Every mathematically closed system in some sense is a tautology. Mathematics is a set of tautologies. Your destiny is to be a solipsist. No, why? I don't even exist, so how can I be a solipsist? A solipsist is somebody who thinks they exist. I have pretty successfully deconstructed my own existence. Don't take that away from me. You're saying what Russell said in that clip, focus on the facts and what you want to believe to be true. But in some sense I realized that Tony and Paul and me are characters in a dream that is dreamed by a higher mind on a higher plane of existence. And the best explanation for that thing is that it was created in the brain of a primate in the course of an evolutionary trajectory, right? So this seems to be a theory that gives good predictions, seems to be sound, seems to be compatible with all the solid scientific results I'm getting. We're not getting to our law now. No, the basic law is, I think, still what Francis Bacon discovered, the first law of epistemology. The weight of the confidence must equal the strength of the evidence. And I think from this follows everything else, if you take this thing, because you can take this and check all the alternatives once you have this hypothesis that you should be doing this. So, Joshua's law is Bacon's law? Yeah. Okay. Evidence is bound to be different. Oh, I cannot claim originality. I'm not a very original mind. I'm not that smart. So the last question to you is, Tony will chase you up four years from now. You might be in San Francisco by then or somewhere else, we don't know yet, or still at Cambridge. I don't know what you're up to, but it sure will be a great place. But Tony is going to check whether you succeeded in falsifying or verifying a specific hypothesis that you're going to share with us today. So what's the most urgent hypothesis that you would like to see really fully tested in this four-year time frame with respect to your theory of mind prediction. So one of the things that I find very interesting right now is whether the purpose of consciousness is attention based learning. So there is seems to be a problem, our brains are not differentiable, they are not easily decomposed in the succession of layers that both so which we can pipe a loss function with the chain rule. So something else needs to be going on, right. And I suspect what's going on there is among a few other things that I'm not seeing right now, an attention based learning algorithm, and that might work by capturing the current binding state, the current latent variables that define your part of partial configuration of your brain, and a change that you are making to this configuration with respect to a particular goal and the expected outcome, and triggering conditions that tell you when you have a possibility to evaluate whether the change was successful. And then you commit this to an index, and conscious attention might be the ability to make index memories. And then you go into the future, you basically experience a few other world states decompose this binding state to do other things. And then at some point, the result manifests in the world, you will re instantiate this original binding state and based on the outcome of your change, you will reinforce the change or you undo it. And this could be a learning algorithm that sounds a little bit awkward because we need to do so many steps, but eventually we need to touch way fewer links than our current machine learning algorithms do. And what I would like to see is if this kind of algorithm actually works and produces useful results and if we see a new family of learning algorithms that implement this or similar principles. Okay, very good. Lucia Boch, thank you very much for this conversation. Thank you too. The CSN podcast was produced by the Convergent Science Network of Biometrics and Biohybrid Systems, a project funded by the European 7th Research Framework Programme. For more interviews, recorded lectures or upcoming conferences in the field of biomimetics and biohybrid systems, go to csnnetwork.eu. And thank you for listening.", '45.874375104904175')