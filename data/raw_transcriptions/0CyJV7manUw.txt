('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/0CyJV7manUw"> </iframe> </center>', " Welcome everyone to our panel on motivation and autonomy. Today we have some very interesting speakers. We have Cristiano Castelfranchi, Christian Vulcanius, Dietrich Doerner and Josche Bach. We will be talking about some of the very pertinent questions such as how do we conceptualize and construct artificial agents with rich autonomy? How can we use computational models to understand agency in humans and in AI agents potentially? So our first speaker today is Cristiano Castelfranchi. Cristiano iselfranchi. Cristiano is the director of the Institute for Sciences and Technologies and National Research Council and the full professor of cognitive science at the University of Siena. His research focuses on the study of autonomous goal-directed behavior as the root of all social phenomena, as well as how social life shapes individual cognition. Cristiano, the floor is now yours. You may share your slides. Okay. My slide. Okay. It's okay. We don't see them yet. You can see? Not yet. Strange. Why? Sorry. What they have to do, share the screen. Scusi. Cosa devo fare? Condividere la scena? E' ok? E' ok ora? Sì, ora dobbiamo entrare in modo di presentazione. E' eccellente. Ok. Il foco del mio discorso, della mia intervenzione, è la necessità di un modello in hybrido della natura del mente, un'architettura in hybrido, sia per modellare la mente umana che per costruire una mente artificiale e effettiva. E questa natura in hybrido, mi concentro su due aspetti di questa architetture. Una è che abbiamo bisogno di due tipi di ragioni, la ragione della malattia, la scala, e la ragione della ragione, la ragione della ragione. L'altra è che abbiamo bisogno di un attacco e di comportamento di attenzione a un obiettivo, e di un obiettivo di metodologia, controllo del dispositivo orientato al obiettivo, il miroreattivo. Per esempio, dobbiamo passare in più al modello BDI, il modello del desiderio e del desiderio, in cui ci sono diversi errori in modo di vista. Dobbiamo fornire un modello molto più ricco di obiettivi, motivazioni e devisi. Per esempio, in Bbagliato nel BDI l'uso del desiderio come categoria generale per i motivi. No, il desiderio è solo un sub-modo di obiettivi e sono parte di una famiglia di obiettivi che noi sentiamo, come necessità, emotionali, non possiamo sentire tutti i nostri obiettivi. Ma alcuni obiet ma alcuni sì. Cosa significa sentire? Che receviamo un'indicazione dal corpo, una sensazione. Percepiamo il nostro corpo e questo lo rende un corpo reale, il nostro corpo, non solo un sistema di computerizzazione. Le creature artificiali simulano solo di avereozioni, le sentimenti, gli affetti. Non sentono nessuno di solito. Non hanno un corpo vero. La sensazione può arrivare sia qui e adesso dal mio corpo, dando l'informazione del stato del corpo, sia un'allocazione, un'attivazione della memoria di un'esperienza reale, come la somatica marca priori esperienza di felice. I desideri sono basati sull'immaginazione di felice sensazione. Le necessità sono basate anche sulla sensazione del corpo presente che viene ora dal mio corpo. Un'altra domanda che dovremmo abbandonare sul modello BDI, per esempio, è se è erroneo considerare l'intenzione come un basico istituto indipendente del mente. Credo che la mia intenzione sia così. No, di nuovo, l'intenzione è solo un sub-tipo di obiettivi, un tipo di obiettivi. Più precisamente, l'intenzione è quello che diventa un obiettivo nel suo processo dopo aver scelto e deciso di fare qualcosa. Per guidare la nostra accia diretta, diventa un'accia intenzionale e quindi è il doppio in intenzione, la parte originale, l'obiettivo originale, il desiderio originale e l'intenzione di fare, che è l'accia, il plan che abbiamo potuto trovare in modo da raggiungere l'obiettivo, l'obiettivo. Quindi dobbiamo modellare gli obiettivi, il loro processo e le loro tipologie. Ci sono un'antologia ricca di obiettivi che abbiamo bisogno. Desire, necessità, intenzione, obiettivi, progetti, speranza, aspettazione, etc. Includendo obiettivi, tipi speciali di obiettivi. Molto diversi da desideri e altre forme di obiettivi, special kind of goals. Very different from desires and other forms of goals, duties, are goals originated by norms and can be even disagreeable, different from desire, for example. And we need also an artificial agent, norm-sensitive agent, the ontic mindset, the ontic representation, the mind of the agent. So we need an agent able to mentally recognize a norm as a norm, il pensiero antico, la rappresentazione antica, la mente dell'agente. Quindi dobbiamo essere in grado di mentalmente riconoscere una norma come una norma, che è diversa da una richiesta personale, un favore, un intercambio, o da un uso, un abito. Una norma è qualcosa di speciale, è un obiettivo originato da una prescrizione, o da un'autorità, la volontà della nostra autorità. Ma una norma, una vera norma, per essere realmente rappresentati in mente, possiamo decidere di violarla, altrimenti non è una vera norma in mente, è una binda. Possiamo decidere di obbedere o di disobbedere, e questo crea un problema con gli agenti artificiali in grado di violare la norma. Questa è una domanda aperta. L'altro punto molto importante è che la mia scelta è la necessità di distinguere tra la teleologia, la teoria dei veri obiettivi, e la teleonomia, la teoria di altre forme di finalismo, o comportamenti finalistiche. Non abbiamo confuso il sogno vero come una rappresentazione anticipatoria mentale che guida il nostro comportamento, che diventa un'azione. Con altre forme di finalismo, comportamenti funzionali sogni orientati, ma non davvero sogamento di verità, in un senso cibernetico e psicologico. I veri obiettivi sono il centro della mente, il centro è la funzione di una natura anticipata per guidare il nostro comportamento verso. Non siamo confusi con veri obiettivi, in particolare con le funzioni del comportamento, che non sono obiettivi mentali. L'evento non inteso che i feedbacks riproducono e disegnano quella funzione o quel comportamento, come nella biologia evolutiva, in cui l'anima ha molti comportamenti, funzioni adattate, ma non capisce, non rappresenta questo in mente. Secondo, per distinguere due obiettivi dal B, il risultato aspettato del comportamento strumentale, il modello di apprendimento strumentale, che è il rewardo che rinforza quel comportamento reattivo. La necessità moderna per modellare B, l'aspettato risultato di un comportamento strumentale, è classificato, o meglio, anticipatoriamente classificato, da un'altra perspettiva. Quindi, anticipatoriamente classificato, condizioni, attività, risposta stimolata, rappresentando la condizione, l'esecuzione dell'attività. In anticipatoriamente classifica, non solo la condizione dell'attività, ma anche l'aspettazione del risultato. Questa regola è un comportamento basato sulla regola, è basata sull'apprendimento, l'esperienza precedente, e è molto importante che l'apprendimento di rinforzamento sia la base, ma che questa meccanica basata sull'anticipatoria rappresentazione, è l'experimento. Quando l'experimento menziona, quando c'è l'espectacolo risultato, c'è un sentimento positivo, sia per forzare la regola, la sua attivazione accessibile, sia per forzare l'aspettazione, la certezza che la crediamo. Tuttavia, l'ex in the classifier non è un obiettivo, e l'acciaio non è realmente un obiettivo diretto, obiettivo dirigito, anche se la finalità è un obiettivo orientato. In true goal, inizio la top-down dal motivatorio anticipatorio e anticipatorio desiderio, il desiderio, ad esempio, inizia dalla scercita di come, in fondo, cosa devo fare, posso raggiungere il obiettivo se posso riuscire a riempirlo per formulare un plan e un'azione. Questa è la tipica strategia di supernetico top-down. L'expert in acribatica vestifari è solo una predicca e una credenza del futuro che inferiorizza i feedbacks sul ruolo e lo rinforza. Ma il comportamento e l'agente rimangono solo condizioni di attenzione, stimoli di risposta, rimangono solo agenti rispondenti, reattivi al comportamento, al stimolo, all'econom comune è un processo di basso a basso, non si parla di cibernetica come in Millegranti e Privello, modelli in spiaggia alla cibernetica, dalla rappresentazione desiderata. Ora, la chiamata di architettura del menteo in ripeto, che ci servono entrambi, ci servono un agent cognitivo e mentale vero guidato dalla decisione vera, deliberazione vera, sistema 2 risposta alla scelta, alla calma, agli obiettivi, il plan, la decisione, etc. l'agent di deliberazione, ma anche noi ci servono agenti condotti con il meccanismo sistema 1, meramente reattivi, reflexi, risposta automatica, abiti. E' un sistema di reazione rispondente, non un sistema di reazione risonante. Ci servono due tipi di sistema in architettura. Ma questo sistema 1 e 2, contrario al dominante Lichardt, non sono solo paralleli e competenti con l'uno l'altro, no, interagiscono con l'altro, interferiscono, sono anche integrati. Considerate, per esempio, il nostro modello di comportamento diretto al obiettivo, governato dal obiettivo e formulato sub-obiettivi, sub-obiettivi, sub-obiettivi, sub-obiettivi, in un plan, questo è the plan, goals and plan. But the lower level, the level of the elementary action I had to perform, are not the same structure. They are more similar to behavioral networks. They use anticipatory classifiers, so the real goal obiettivo e il plan, e poi eseguiti e implementati in anticipatorio classifier, per cui costruiscono una struttura di molti livelli di compressione unica. L'altra fisica, l'architettura cognitiva, la mente cognitiva è basata sulle rappresentazioni, rappresentazioni es and goal, the two basic important representations and their processing and their interaction. In fact, these two basic representations of mind interact with each other. For example, goal processing is based, is grounded on beliefs, depends on belief. Consider, for example, our model of goal processing, in which there are different steps in goal processing, from the activation of a given desire of goals, then to the conflict, option, selection, chosen goals, and then formulation of a plan, and then formulation of intention, then execution of intention. The intention is just the final dei sogni originali, diventa un'intenzione e le azioni diventano intenzionali, ma passo a passo in questo processo di sogni il credito è filtrato, filtrato, filtrato, filtrato, filtrato e determina la vita del sogta e supportare il processo di scelta, è basato sulla creazione di credito. Un altro tema importante è l'autonomia in agente. Ci serve l'autonomia, ma l'autonomia non è solo l'autonomità di performance di seguito, ma l'autonomità del obiettivo, e così la scelta autonoma, la decisione vera, eesta dà spazio all'iniziativa, che significa fare qualcosa senza un richiesto, autonomamente, come un'iniziativa. Questa dà la possibilità di fare di più di ciò che mi è necessario, o di fare qualcosa di diverso dal mio richiesto, ma meglio per la soluzione, per risolvere il problema. per la soluzione, per la soluzione del problema. Ci servono l'autonomia completa, l'autonomia del obiettivo, in cui l'agente ha le sue preferenze e le decisioni. Se sono davvero autonoma del obiettivo, ho il mio obiettivo autonomo, il mio obiettivo autonomo. Non sono solo un'operatore obbediente alla tasca. Non sono solo in grado di eseguire la mia tasca. Ad esempio, posso decidere se adottare il tuo obiettivo, aiutare o no, se aderire alla tua richiesta o no, o rejeccare o rifiutare. E dobbiamo dire che l'Agenti AI autonomi per la reale interazione sociale, per la cooperazione efficace. Perchè? Per esempio, perchè abbiamo bisogno di iniziativa nel nostro partner, iniziativa nel nostro robot, che dovrebbe essere basata sul mio leggere e nel nostro mente, in autonoma evoluzione, autonoma imparazione, autonoma planning, autonoma acquisizione di conoscenza, professore, e così via. Ma questo autonomo, l'agente autonomo, veramente autonomo, con cui dobbiamo interagire, ha il suo progetto, ha i propri progetti. Questo crea un problema serio. Ci sono pericoli nel fatto che l'agente artificiale sia guidato dai propri progetti? i suoi obiettivi, questi problemi etici non sono stati derivati solo dall'obiettivo che ho messo in mente a mio richiesto. E questo obiettivo che ho messo in mente all'agente in favore di chi ha un obiettivo così? In favore, in interesse di chi? E a base di quali valori? Valori di condizioni, valori culturali, quali valori hanno un problema molto serio? e in base on which values, shared values, cultural values, which ones save any serious problems. Other point is that for a human life, a really social interaction, we need agent and robot able of mind reading and mind description. For real cooperation, for real conflict with our agent, theybbiamo rivolgare il nostro obiettivo nella nostra mente e la sua supporta in base al quale abbiamo questo obiettivo per adottare un obiettivo così. Dobbiamo capire, rappresentare, avere una teoria di mente nel nostro obiettivo per aiutarci a migliorare, per frustrarci anche il nostro obiettivo, per cambiare il nostro obiettivo, per aiutare di più, o anche per frustrare il nostro obiettivo, o per cambiare il nostro obiettivo, con persuasione, con attrezione, con meccanismi di influenza, ma in modo da farlo, la vera interazione sociale deve leggere il nostro obiettivo in termini di obiettivi e credenti, e viceversa. Quindi dobbiamo essere in stanza intenzionale, una interazione bas basata su scrittura di mente e leggere, comunicazione e cooperazione. Dobbiamo scoprire una architettura mentale basata su rappresentazione esplicita di crescita e obiettivi e la loro leggere. Questo è anche un fondamentale fondamentale per la famosa esplanabilità e trasparenza ora molto trendata. E' anche per la trasparenza di trasparenza abbiamo bisogno di una comunicazione e di interazione basata sulla scrittura mentale. Per rendere il nostro essere comprensibile o esplicabile è in termini di motivazione cognitiva il motivo per cui stai facendo ciò che stai facendo, per quale ragione, per quale credimento. Il sistema non è trasparente, esplicabile se mi proviene il suo usatore, non il suo ingegnere, il suo algoritmo, completamente obscuro, o il suo statistico probabilistico di data, la base del obiettivi e le assumptioni, le credenze. I motivi, perché lo facciamo così? Per cui obiettivi, per quale intenzione, per quale decisione e preferenza? E qual è il plano che indica il suo comportamento? E c'è una norma che impinge su questo, se tutti questi sono violazioni della norma, per e perché la ragione della sua attenzione. E lo stesso per l'assumptione, su quale base, su quale credenza, su quale data che non ho, o quale conoscenza, aspettazione, decide di fare ciò che fa, e fa ciò che fa. Al contrario non possiamo capire cosa sta facendo. Doveva esplicare in questo modo. Non solo dobbiamo capire il comportamento del sistema in termini mentali, il cognito risone. Probabilmente, per una buona interaccione, dovrebbe essere bilaterale. Devo leggere la mente del mio robot, del mio agente, in termini mentali, credere, desiderio, intenzione, decisioni, aspettazione, e deve leggere la mia mente in termini mentali. E' bilaterale. Ma ci serve qualcosa di più. E' necessario che l'agente rappresenti la sua mente in mental death. Deve avere una leggerezza di mente di se stessa, ascrivendo a se stessa la credenza e il desiderio, e spiegando la sua comportamento in termini di credenza, desiderio e decisione, questa rappresentazione mentale. Per avere una buona interazione, dobbiamo avere questa periodica ascrizione di gore, desiderio e necessità, e dobbiamo mentalmente condividere un plan, altrimenti non siamo realmente collaboratori. In me e in suo mente c'è un plan, un obiettivo, goals, belief, and I know this and he knows this. Okay, let's jump to this point, cognitive and emotional, sorry, for arriving to the second aspect of the hibernation of man, the different kind of reason. In our mind we need two kinds of reasons and two kinds of weights in our decision making. Goals have a double value, two kinds of reasons and two kinds of weights in our decision making. Goals have a double value, two kinds of weights. The reason and value, the utility, based on reasoning and decision deliberation, and the third value, attraction and repulsion. The first is based on argument. Belief can be explained, discussed, argumented, can change, si possono esplare, discutere, argomenti, si possono cambiare, persuadere, cambiare quello che credete e poi la tua intenzione, il tuo obiettivo, le tue prevenzioni. Ma il secondo valore della nostra decisione, è solo intuitivo, sentito, solo a causa di qualche affetto, di un'occasione, di un marzo somatico, etc. Non è giustificabile, esplicabile, ragionevole, argomentabile. L'attivo corpo percepito, il corpo, e questo sentimento, entra nel mente cognitiva, nel processo cognitivo, nel valore del obiettivo, non è più solo a causa del crescimento di una ragione, questo è meglio per questo consagramento, è meglio, no, solo la sensazione del corpo e più intensa la sensazione, più alto è il valore del soggetto più intenso è il valore del soggetto. Quindi il corpo può mettere un soggetto nella nostra mente e anche assignare un valore a quel soggetto, non solo un soggetto a base del ragione Analogamente, la relazione tra il credere e l'emozione un valore assignato a un obiettivo molto buono, non solo un obiettivo a base di ragione. Analogamente, la relazione tra la crescita e l'emozione deve essere anche reversa. L'approccio classico dal punto di vista cognitivo è l'apprezzo cognitivo. Credere in qualcosa e rilegare in qualcosa, attivazione di una reazione affettiva, visita e risposta affettiva. Questo è il modello classico dell'apprezzo cognitivo. Ma sappiamo che abbiamo anche all'altra parte. Sentiamo qualcosa e questa sensazione ci induce a credere in qualcosa. E più intensamente la sensazione, più sicuro di cosa crederemo. E' un modello classico bilaterale da credere in una risposta affettiva, ma anche l'altra parte, da una risposta affettiva a credere in qualcosa. Quello che in psicologia chiamiamo affetto come informazione, ciò che sento come la base per credere. In questa dimensione dobbiamo correggere la credenza di Damasio. Nella visione di Damasio, i mercati somati prevedono il tre di scelte. In un'altra termina, in un senso, decidono cosa faremo sul baso di ciò che sentiamo. Questa vista è deviante, è sbagliata. Siamo in grado di combinare, di calcolare insieme, ambaste, il valore sentito e la ragione della ragione. Ambaste. Dimensioni molto eterogenee, non è chiaro il meccanismo, il modello. in both very heterogeneous dimensions, not clear the mechanism, the model. We can decide, for example, to do something which is very important from the rational point of view. It's very usable and necessary. I have to. But disgusting. Oh my God, I don't like it at all. I have to do it because it's too necessary. Or vice versa. I can decide to discard something very attractive. I would like very much to do that. perché è troppo necessario. O viceversa, posso decidere di scartare qualcosa di molto attrazionevole, mi piacerebbe molto fare questo, mi piace molto, mi desidero molto, ma non posso, devo fare questo, è più importante, è necessario, usabile. Quindi possiamo combinare i due valori e un valore ragionale può prevedere il fatt fatto uno o viceversa. Combiniamo le due cose. Non è vero che il mercato somatico pruove la trea di scelta e eliminere la scelta che non è attrazione. È così vero che abbiamo anche un stratogio metastrategico e un layer heuristico, un layer heuristico metacognitivo, per gestire queste due dimensioni che interfere con l'altra. layer heuristic, at the middle cognitive heuristic about ourselves in order to manage this two-dimensional interfering one with the other. And we can, we have rules and strategy for stop one or stop the other one. For example, I can say to myself, stop, oh God, or maybe so impulsive, so impulsive, stop, keep a cool head, reason about in order to see something. Or viceversa, don't stay continuing to reason, it's a good inconvenience, not convenient, irrational, follow your impulse, follow your impulse. So we have a metaheuristic, a metacognitive level even for managing these two dimensions, a stop one, inhibit one, o l'altro. E come ho decido, abbiamo un obiettivo che implica sentimento, che posso sentire, come desiderio o necessità, più o meno intensi, e un obiettivo che non posso sentir, non elicita nessuna sensazione e sentimento. I obiettivi implicano un senso di piacere, come desiderio o necessità, o obiettivi senza come the desire or need, or goal without any pleasure, just rationally based. So, that's my main claim on the hybrid nature of architecture that we need for modeling human mind and understanding human behavior, and also for building new artificial mind and artificiale. Dobbiamo avere la ragione della Terra combinata con la ragione del motivo per cui i due sistemi interagono, competono e convergono. Dobbiamo anche avere una distinzione clara tra il comportamento del vero obiettivo, il progetto del obiettivo, il processo del obiettivo, il valore del obiettivo, e così via l'acciano intenzionale, dar dark-coded, dark-processed, dark-value, etc. dark-coded, dark-processed, dark-value, etc. versus a merely goal-oriented device, reactive device, versus a merely goal-oriented device, reactive device, classifier, system 1 and system 2, versus a merely goal-oriented device, reactive device, classifier, system 1 and system 2, and convergence and competence. Thank you for your interest. This is my research. Sorry for being too confused. Thank you, Greg. It was wonderful. It was a little bit too confused. Thank you. It was wonderful. It was a little bit too long. We went a little bit over time. I think our time was notable, but thank you so much. I didn't listen to talk from the left. Yes, it's fine. Sorry. Thank you so much, Cristiano. So Cristiano has also graciously provided an extended version of the slides, which I will be adding to the, a link for in the video description once it's on YouTube. So next up we have Christian Valkenius, who is a professor of cognitive science at Lund University. And he also is the director of the research school of the Wallenberg AI Autonomous Systems and Software Program for Humanities and Society, the goal of which is to study the consequences of AI for society. His research focus is on computational modeling of cognitive and emotional processes and their use in the control of robots. And his interdisciplinary research also looks at the areas of neural engineering, AI, machine learning and robotics, psychology and biology. Welcome, Christian. The floor is now yours. Thank you. As you realize, my research focus, it's not very focused, instead I try to understand how many different things connect together. And today I want to say something about how I've tried to approach motivation, emotion and attention, and trying to build the models of these phenomena. There are of course many ways to approach these areas. Many people look at the basic emotions or basic motivations, which is very often more of a linguistic analysis than an actual analysis of processes. And as an alternative, you could have a more phenomenological approach. But that's hard to do something that is useful for machine implementations or a robot. So the approach I think is perhaps most useful is to look at mechanisms and try to reproduce mechanisms in computer simulations in robots, as I will mention briefly later on. And when you do that, it always turns out that these labels, things like motivation and emotion, they don't fit very well. That's not a one-to-one correspondence between words and processes, but I try to ignore that and use these words anyway. So very generally I view this as something like this. Some motivational systems they tell us what to do, it's related to behavior selection, of course also to things like needs, specifically to future needs. Sometimes we have immediate needs, but most of our needs are future ones. And just as a simple example, something very simple like being hungry is not caused by starving right now. Instead, our body expects us to eat, and because of that it lowers the blood sugar. We experience that as being hungry. And that's not at all the current need, it's a future one, but we experience it right now, and we select the behavior that would fit that. So the anticipatory nature of motivation is very important. Then we have emotion or emotional processing, and that's perhaps where there are most different views about what it would be. The way I see it is that the system that tells us what we should have done, it has to do with evaluating behaviours, evaluating outcomes, situations and so on, which relates to learning in various forms and things like reinforcement learning in the simplest cases. Obviously emotion also has an experiential aspect of it that I don't really study. And of course the social aspect, we communicate emotional reactions to others and so on, but we'll ignore that for now. So one way of defining emotions that I like very much is one that was most put forward by Edwin that emotions or states elicited by reinforcing stimuli, which seems a bit simplistic but but it connects emotion to reinforcement learning and to evaluation of situations. It's actually four dimensional space. It looks like two dimensions, but there's actually four dimensions. So the basic emotional signals, it's not emotions in itself, it's emotional value. So one pair of dimensions goes from positive to negative, from a pleasure to pain, say hope to fear if we're looking at more anticipatory emotions. So that's one or two basic dimensions. They can actually exist at the same time. Something can be very pleasurable and painful at the same time. Actually, the most interesting things in life are not unidimensional. They have both aspects. And then there are two dimensions that have to do with expectations. So if we expect to be rewarded, but we're not, we might be frustrated. And of course, being frustrated could mean receiving no stimulus at all, could be a situation with absolutely nothing. So the frustrations doesn't come from the actual situation, but from our comparing our expectations to what what actually happened. Similarly, for relief, we expect something unpleasant that didn't have happened. So by combining expectation with sort of primary value in this way, we will get four dimensions, but we can go on forever, which Jeffrey Gray did many years ago in his theory. So we can expect to be relieved, but we were not, and that will be a more complex combination of these dimensions. But basically, emotions here, as I see it, is used for learning, for changing the evaluation of situations and behaviors and so on. And emotions can also be motivating, which complicates things. And motivational states influence emotions. So what's good or bad, to take something simple, depends on my motivational state. Again, a simple example, if I'm hungry, then maybe food will be a positive thing, give rise to positive emotions. But if I've already eaten, it will completely change. And of course, that goes for more complex things as well. And the thing that binds everything together then is attentional processing. So attention is interfaced between both the environment and the motivational and emotional processes and between those processes and memory. So the way I view memory is like a backup for the environment. If we can do something in the environment, we'll do that. If not, we'll do it in memory. But as I see it, it's basically the same processes going on. I'll get back to that in a minute. So what emotions do for the attentional system is to help decide what's interesting. It will evaluate possible stimuli, possible choices, possible objects and so on. So given that we have a simple situation with some objects in front of us, we could in principle attend to any one of them. But the emotional system will tell us that given this motivational state, some of them are more interesting. And we will then be more likely to do something with that object. Of course, this will also then influence emotional motivation. So if we see something interesting, we'll be more motivated to do something that's related to that. Again, taking the food example, because it's so simple. Say we're not hungry, but we see something tasty, then we'll perhaps feel more hungry because of that. I haven't done much about action selection here because in many cases, stimulus selection through attention is sufficient. there are not too many things to do with it. I mean, we can do a million things, but usually the action we perform is given by the stimulus or object itself. That doesn't mean that behavior selection is not important. It's just that it's not needed in the very many simple situations, which may be surprising. Now, so trying to model this kind of processing. Oh, Christian, I'm sorry to interrupt, but the audience is saying that the slides are looking static. I'm not sure if that's intentional. Yeah, that's true. I was just about to switch to the next one. Thank you. Yeah, so some 20 years ago, we started to model this and here's a new one. So the first picture I wanted to show is this one, which is a model of learning in the amygdala in the brain. And as I said, it was a long time ago before color was invented. So it's in black and white. This was a simple neural network model for the described processes within the amygdala, which is supposed to be involved in emotional evaluation, among other things. And its interaction with the orbitofrontal cortex, which is a part of the brain that inhibits especially emotional signals. So one of the things that this model was used to, was to describe how emotions become context dependent after a while. So initially we'll just react to something, but when our expectations are not fulfilled, they will be increasingly context dependent. That seems to happen through the frontal cortex. Now this model, we continue to work on that for quite some time. And most people have used it, they actually use it as a control system. So it's used in the industry now in the various processes, which is quite surprising. That is not something we expected. But if we look at the more current version, so here's one, and I don't have time to go into the detail, but my point here is more that today it's possible to go through the neuroanatomy of emotional processing and build computational models of that actually work and do the hypothalamus and also parts of the sympathetic and parasympathetic nervous system. So we tried to model, for instance, how emotional reactions influence pupil dilation and things like that. The nice thing about this kind of modelling is that we can The nice thing about this kind of modelling is that we can reproduce very detailed experimental data. So in the case of pupil dilation, we have been able to reproduce nearly every experiment we found with this model. And even though each of the components here, each of the circles is one small part of the brain, but when we combine all of it, we get a lot of very interesting properties. And we can do learning experiments, we can we can scale the model, see what happens when it, for instance, we shine light in the eye afterwards, which surprisingly is influenced by that. And so on. And in this case, we looked also on the effect of touch on trust, how it motivates approach or avoidance behaviors. And so this is just an example of a kind of modeling we are doing in my group, and this is not a complete model, but just as an example of what it looks like. I would like to show some more global views on the model. So here's one study we did recently where we looked at how emotional value processing could influence decision making. So the important part of this model is a picture to the left here and how everything is controlled by a loop basically starting with perception going through memory where whatever the model is looking at triggers both memory processes and associations with value and this is basically the old image and a model in a new version. So when we look at something or even think about something that will be accompanied with by an evaluation of HLO states, if we're looking around us, it would be a variation of stimuli around us. If we are imagining things, it will be an evaluation of that. And when we do that over time, looking back and forth or thinking about different things. We propose as well as an accumulator mechanisms that accumulate value for all those different states or objects, which can then lead to a choice or one of them, actually an attentionless choice which will lead to spatial attention, which will influence our perception again. And this goes in both ways. So if we look at something, we will accumulate value for that, if we think about something that could make us attend to it, and so on. So this is a very dynamic system that continuously works back and forth between the memory and the environment. What you see to the right here is just a simple neural network that do the accumulation for the special case of selecting between two different objects. So when we simulated this, we look at very simple choice procedures, but it turns out that even very complicated things seem to use a very similar mechanism. So this is a sketch of an experiment we did a few years ago, where we looked at much more complicated moral decisions. So people were asked things like, do you agree that murder is sometimes justifiable? And we had two choices like, yes, sometimes or never. And we looked at their eye movements, why were they deciding? And we you could see that as people are thinking about the different alternatives, they look back and forth. And the hypothesis was that they would look more at the choice they would actually do, which actually happened here, as well as in simpler situations. But one very interesting thing was that we tested whether we could interrupt people while they were looking at one alternative and ask them to make a choice immediately. And it turns out that people are very likely to choose whatever they are looking at. They are very forced to choose at that point. So that supports the idea that there is some sort of argumentative process going on here. So we could actually force people to think that not justifiable just by interrupting them at the right time which is quite interesting and this fits very well with the model we're working on So finally I want to show this overview of an alternative version where we looked at how value processing could influence memory processes. So basically, there are systems for identifying and localizing objects, storing it in working memory, and so on. Also controlling attention through a mechanism similar to what I showed previously here. And in the center, there's a value system that modulates everything. So it will enhance processing of location or stimuli, or things in the working memory that are valuable. And to that, we add a gain processing, which basically controls randomness of associations in memory. So that accounts for focus, or even things like creativity. If we increase the randomness, we'll get new memory traces with a combination of a lot of things. To the right, it just shows how this can be used, for instance, in a choice situation. So say we have simulated agent in the team ace and can go left or right. So it looks left or right, sees a stimulus there, and that starts an associative process within the brain, within its memory, that's what you see to the right, that transitions between different memory states. So essentially it fantasizes or imagines going through the maze and at some point it will reach a location like the gold state that has a value and which will then fix on that and lock onto that. And that could be used to make the immediate choice. So it links sort of the immediate situation with memory, imagination and then choice based on emotional evaluation. Since I'm now running out of time, I will just very briefly show this last picture as an example of what we're trying to do right now, as is to take this model based on neurophysiology, gradually building it bottom up, and also putting it into some of our humanoid robots that we built in my lab. So with that, I thank you for your attention. Thank you so much, Christian. That was an excellent presentation. So next up, we have Dietrich Dorner, who is a professor emeritus for general and theoretical psychology at the Institute of Theoretical Psychology at the Otto Friedrich University in Bamberg, Germany. Professor Dorner has also received the Gottfried Wilhelm Leibniz Prize, which is the highest honor awarded in German research. Famously, he is the main author of the cognitive architecture called Psi Theory. And I believe today's presentation will touch on that. Dietrich, the floor is now yours. You may share your slides. Well, I'm going to talk about motivation. Motivation in psychology is not very much estimated because it is a kind of counterprogram to cognition. And cognition is a modern direction of psychology. But this is completely wrong, I think, because I believe that, I believe it, together with Aristotle, the soul is a controlling device. For a body which by being controlled has life. It's a wonderful sentence because it brings two very enigmatic concepts together. That is life and controlling. And to identify whether you have bodies, you have stones and whatever things that exist on the world, and these are not alive. But if a stone becomes alive and has fear about a car which would crush the stone, then the stone would have life because he is controlled and he tries to raid the car. OK, if you, for instance, steer a car, then the steering serves a certain purpose. The car and you should arrive at a certain goal. Therefore, by steering the car, you strive for a goal. And the simple system which strives for a goal is a control circuit, feedback circuit, or Regelkreise in German. This figure shows a control circuit. There's a tank, and this tank should be filled until the level of this red arrow here. And that could be done by open a tap. But that must be opened, and the tap must be connected to some source of water to be that this works. And so the conductor works in the following way. There's a set point and the actual value. That point is called nominal value 2. And there is calculated the difference between the set point and the actual value. And if this difference is greater than 0, then you have a need for something. And you need water. Water is needed to fill up. And then an appetendum, that is an action which occurs for the filling of a tank, is activated and if everything goes well then the water tap is opened and the tank would be filled. And this is a control circuit in the normal form. We did a little more complicated here. We invented the appetender. And these appetenders serve to hinder disturbances to empty the tank. And this, with an abatendum, with an action to fight enemies of the tank, this control circuit is like a living being, the tiniest living being you can intend for. And if you look for the origin of life, you must find the point where the first natural control circuit came in life. I don't know whether there are speculations about how that happened, whether there are speculations about how that happened. But this is very simple. And such control circuits, they exist in the normal environment in the not living environment, too. There are control circuits which have no life, but exist. Okay, I have here a kind of example of such a living system. This is a, well, I would say it's a living system, and it has about three control circuits. The first one is for fuel. The system must have fuel to heat this tank and then we have water. This water is heated and so steam begins to exist and see steam for instance drive a system which drives the wheels or a steering system here and this system would be live and this system could be able to identify the environment and therefore we have a third control circuit here. That is, you must look for places where you could fill up the tank. And this is a normal environment. This is a gas station. You should be able to identify the situation of a gas station. And if you are able to do that, you can drive to the gas station, and you can fill up the tank again. This is the idea. And I just sketched such an environment for the steam engine car or living circuit or system consisting of three living control circuits. And here we have it. This is, again, the car here. And then we have a kind of town with some houses and so on. And some gas stations here, for instance Schell, here Aral, maybe unknown in the States, but it's in Germany and this system would be able to explore this town and find the guest stations and then when it gets 30 and the water is not enough, the system could find a place for water and would fill up the water tank again. And therefore, I would say it's not at all, it's not a non-complicated system which just is controlled by three control circuits, water, fuel, and information about the environment. Okay, that's it. Very simple system, but it works. And you could have such systems and they work. OK, but it is too complicated. Not enough complicated to have really a plant or an animal or even man. And we will now go on with how such more complicated systems could be realized on the basis on a system of control circuits. And you must now have first an idea what kind of control circuits you need. And the first you need some control circuits for basic needs, food and water for instance, that is energy and some stuff which is needed to change food really to energy, to sugar in our system. And then you have a system which is able to regulate itself. Well, another control circuit should control the absence of pain. That is, the absence of damages to the body. And they should be minimized, or they should be repaired. Then, next one is sexuality for a living system. It is not necessary. You could have a living system that's all sexuality. But to have a development, you need sexuality because you need that the system can die and new systems could be which have another genome and are better with environment. Sexuality, then comradeship, friendship, love, and power. Relations between the different animals could be there. And this is even very early in the development, comradeships, friendships, and so on. For instance, my koi fishes in my garden pond have a very complicated system of comradeships and friendships. I would call them friendships, which I have them. Although normally, this is not one of the attributes one would find in fishes. But they are there and rather in a rather complicated form. Then very important to knowledge about the reality. The very simple living systems could just look at random and then run around to find some sugar and to eat it up, and then they would live. But much better is to have systems which are able to acquire knowledge about the reality and to orient themselves in their reality. And therefore, learning is even very simple organisms, reality. And the system builds up a kind of memory, a kind of model of the environment, and then is able to act much more successful than without that knowledge of reality. These in my eyes are the most important systems. And I think this system is rather complete. And, but you couldn't speak much more about what, in what kind of the knowledge of reality is created and how, for instance, religions or ideologies come into the game and form and give you the form, the knowledge of both the reality you should take. But this is another question. Or it is not another question. It's a very important question. But we could not speak about it here. OK. These are the systems. And now we have here the image of the organization of the whole system. This is a complete soul, which you have here. And it is not uncomplicated. First, you have here the basic control circuits. I left out most of them, but hunger, pain, affiliation, that is the relations to the other members of the group is essential. The knowledge about reality and then competence. An estimation of the ability to cope with the environment, to cope with the reality. This is competence. Very important. OK. How does the system work? Well, quite simple. You have a number of these basic control circles. And they all have a set point deviation. That is a deviation between the nominal value and the actual value. And this depends the strength of the motive. Well, there might be for the different systems, there might be weights for the strength of motives. That is, for instance, pain has a higher weight than hunger. These strengths of the motive then are activate a system or a neuron, MS. OK. MS is the strength of the motivation, but to the strength belongs the competence. That is not only the set point deviation is essential, but the competence to gain something which fills the tank, the hunger tank or the pain tank, again, must be there. And this competence is connected with this competence tank. And therefore, when the competence is connected with this competence tank. And therefore, when the competence is low, then the motivational strength is low. And therefore, you must define the motive strength as the multiplication of competence times set point deviation. That's fine. OK. What then happens is a selection process. This is done by a suppressing mechanism, which is more and more activated, and therefore exerts a suppression to the MS neurons here, which has the same value, until only one MS survives. And this one is then transferred to the next layer, to this layer here. And this then fires cognitive processes. OK. Apertender, goal. Here we have a direct connection between a patendom and the goal. Here you have, for instance, cranberry bushes. And this could be used to bring, to diminish your hunger. Well, this is the simplest form of a cognitive activity, just remembering something. But I will not now go into details about the differentiated forms of cognitive processes. This is just this cognitive system. And the cognitive system is under control of the motivational system. system is under control of the motivational system. This is the system on motivation. Very easy to understand it and it is very, well, it's not very complicated. And it is the same in all animals or plants, but the control circuits are different and the kind of learning is different and so on. Okay, that's the core of psychology. And within 10 minutes, you can explain it. But you can explain it only in that way because you must have the concept of motivation. Otherwise, without motivation, there is no life and no living system because the psyche, the soul, is nothing else but a controlling system. And we have seen such a controlling system does. And it is not at all complicated. but the controlling system. And we have seen such a controlling system does. And it is not at all complicated. Okay, so let me introduce Yosha Bach in this case. Yosha Bach is a cognitive scientist and AI researcher who focuses on computational models of cognition in Neurosymbolic AI. He has, oops, I have screen all of a sudden. He's currently a principal AI researcher with us at Intel Labs in California. Previously, Yosha has taught and worked in AI research at Humboldt University of Berlin, the Institute for Cognitive Science in Osnabrück, the MIT Media Lab, and the Harvard Program for Evolutionary Dynamics. Josje, the floor is now yours. Thank you, Tanja. When we are looking at motivation here, it's in the context of a larger perspective on what we call the vectors of intelligence. The idea is that the systems of the future will be not evaluated in a single dimension of performance, but rather they will have many capabilities across fields. We will no longer build systems that are task specific, currently building systems that are more broad and flexible. And this is in this area of deep learning, we increasingly have models that are able to perform many, many tasks, some of them, which they were not even built for. But the question is, how can we get to systems that go beyond this flexibility towards generality? And this means that we are optimizing many dimensions. For instance, we need to get our representations away from operator-specific representations towards universal representation. So we can represent everything that happens in the world in suitable structures for it. Or we want to go from offline learning, and represent everything that happens in the world and the suitable structures for it. Or we want to go from offline learning, as we currently have in neural networks, towards online learning, and eventually to self-improving systems that have an idea of what they learn and why. We want to have systems that go from the open loop control that you currently have mostly when you act on the world, to closed loop systems that have feedback to the environment and eventually construct new feedback loops to self-extending. And with respect to autonomous systems, we want to go from task-directed systems towards goal-directedness and then eventually self-directedness. So basically systems figure out what they want to achieve and why and what the context is. And then you have collaborative systems. you need to go from systems that show what their inner state is to the user towards explainability so they can tell the user what their state is, explain it in terms that the user can understand. And that also means that they have to increasingly track the state that the user is in. So they have to understand what the user can understand in the given context. And eventually, these systems have to be able to understand what the intentions are so they can truly collaborate and adopt the goals of the ones that they're interacting with. So when we are talking about autonomous systems, we go from task-directed to goal-directed and then to self-directed systems. For collaborative systems, you go from transparency to explainability and then goal adoption. And of course, the question, does an AI system need motivation at all? It's conceivable that you are only dealing with oracles, right? An oracle is a system that is responding in a similar way as Google search to a question that you pose to it. And after that particular task is over, the system is done with its task and goes back into quiet sense. And it's different from a system that is continuously interacting with the environment and serves its own goals and its purpose. But there is not necessarily a very strict gap between such systems. Because for instance, when you look at GPT-3, that is a system that is not goal-interested by itself. It has no motivation. It's just a system that is basically doing autocomplete and is creating some story or response based on the autocompletion. But GPT-3 is very well capable of creating a story about a motivated agent. And so if you put an underlying motivation or combine it with a motivational system, then even such a system is able to switch from the oracle state to something that is more agentic. What is an agent? I think that the notion of agent can be best understood, and you come from cybernetics. So when you take the concept of a feedback loop, you have a controller and a regulated system, the controllers affecting the regulated system, there's actuators or effectors, and it gets information about the regulated system by a sensors, and the sensors tell the controller in which way the regulated system deviates from the ideal state from the set point of that system. And there's an environment which disturbs the regulated system. And the easiest example of such a control system is the thermostat. But the thermostat is not an agent because the thermostat is only modeling the deviation of the next frame. So it actually doesn't have a model of reality. It's just reacting. And you want to have a system that is able to integrate the expected deviation over a longer time frame and optimize over a longer time frame. And as soon as you have a controller that is able to model the future, then you will have a branching expectation horizon and the control becomes decision-making. And this is when you have an agent. So an agent basically is a controller that is able to model the future and is combined as a set point generator. So we basically have stages in which you can understand intelligent agency. The simple system that is acting on the environment is the regulator, is the feedback loop. And when you add a modeling system, you have a predictive controller. And when you combine this with a set point generator, you have an agent. And a sentient system is one that is so general in its modeling capabilities that it can discover itself. And to discover, model the relationship that it has to its environment so it understands what it is doing and why it's doing that. And you can also go a step further and think about agents that are capable of transcending themselves, which means they link up with other agents in combined collaborative agency that is coherent and is basically instantiating agents on the next level. And humans are of that type. For instance, when we have close relationships with others, we are going to link up into a shared agent. And we also state building, that is, our civilization itself can become a coherent agent that is able to act on its environment due to the way in which people are collaborating. And in our own mind, we have a variety of agents that interact. I think we can see the way in which we deal with reality as a perception agent, that is a system that is trying to track reality as it progresses and generates functions that predict the sensory patterns. And we have an attention system that is interacting with this perception system and is creating actively a coherent model of the world. And you have a motivation agent that is giving a motive force to the perception agent and the attention agent. And what we are looking at today is mostly the actions on the structure of this motivation agent. The core of the motivation is valence. Valence is what determines the value of the operations that we can perform on the environment. It's grounded in a multi-dimensional motivation system. And at the root of valence are needs that we have. And the big question, what is the minimal set of needs that you would need to become an interesting agent that acts on its environment. And the most radical proposal comes from Carl Thurston with the free energy principle. And what he says is, basically, you only need to minimize the prediction error. And this will lead you to structural discovery. And if you are trying to minimize the prediction error in a system that has setpoint deviations, you get all the other behaviors. And I would say that this prediction error is equivalent to an aesthetic need. It's a need to discover structure in the world at large. But I don't think that it's sufficient to understand human behavior. The GPT-3 is an example of a system that is minimizing prediction error and nothing else. And it seems that the order in which GPT-3 learns things is very different from the order and people learn things. Basically the meaning is if GPT-3 is able to capture at all the long tail of style. And so GPT-3 is only looking for an aesthetic structure in a way, and not even for symmetries in it. It's also in this regard incomplete, because it's not necessarily coherent. It's not directly optimizing for coherence. So I suspect that while if you zoom out long enough, there might be a convergence between systems that only minimize prediction error and properly motivated systems in order to get a system that within its lifetime, in the finite lifetime, is going to get to an interesting behavior on the world. You need to put in additional needs. And also for the purpose of learnability, you might even have to have something like transitional needs, motivational reflexes. For instance, when a baby is born, the baby does not have an understanding of hunger and thirst. These are two complicated desires, and they cannot yet be matched on actions that are available to affordances that are available to the infant. So instead the infant has hardwired reflexes. When the baby is born and gets hungry, it has a seeking reflex. So it will turn its head around and open and close its mouth. And it has a sucking reflex. If you put something into its mouth, it will reflexively suck. And it has a swallowing reflex. If you put liquid into its mouth, it will swallow that liquid. So if you have a newborn infant, you want to give it medicine, you just squirt it into its mouth and it'll swallow it regardless of whether it's tasty or not. But as a result of these three reflexes, the baby gets into a situation where it learns to connect hunger with swallowing and the satisfaction that comes afterwards. And once that learning is online, the reflexes are turned off because they would interfere with the properly learned behavior. And at this point, you can no longer give them medicine in this convenient way. And it's interesting to think about how many of these transitional reflexes we have. I suspect that many of our moral emotions are actually reflexes that are built into us before we understand the true symmetries, the economy and the game theory of the social reality. And the older we get, the more these moral reflexes are replaced by true understanding. And the moral reflexes are just no longer active because they're replaced by an understanding of what are the status of the world that we want to achieve, what are the behaviors that get us there. The individual needs can be understood as physiological needs, social needs, and cognitive needs. And physiological needs, you have hundreds of them, but there's sustenance, pain avoidance, rest, libido, and so on. And we have a bunch of social needs, for instance, a need to affiliate with others, a need to care, a need for affection, and a need for legitimacy, which means that we try to serve internalized normative goals, a need for dominance to rise up in hierarchies, and so on. And obviously not everybody has the same needs and the same strengths. There are individual differences between those needs. And we have cognitive needs, a need for competence, or being basically more skillful, a need for reducing uncertainty environment, which lets us explore, and our need for aesthetics, which lets us discover structure. And each need can be understood as a target value and a current value and the deviation. Yosha, your video froze. To pretend that we have left until our meat tank runs dry is basically defines the urgency. You froze for a second. Okay, thanks for letting me know. So we can, the urge indicator changes over time, and the stronger it changes over a unit of time, the stronger of a pleasure signal you get. If it goes in the wrong direction, you get a displeasure signal and those pleasures... that this pleasure signal and those pleasures. I think you're having connection issues again, Yosha. Signals of the universe. And in this way structure the world based on, oh, yes, I am. As far as I can see, everything is good here. Can you see, understand me still? Yes, but I broke off for about five seconds. Oh, okay. So both pleasure signals and displeasure signals, uh, reinforcement signals. And, uh, when we act, we do not act directly on these reinforcements when pleasure and displeasure happen, it's too late. We need to act on models of them. So basically on the on future pleasure and displeasure on anticipated pleasure and displeasure. And the urge signals are part of a larger architecture, they give rise to priming and modulation, and to learning and decision making and our cognitive systems. And the modulation gives rise to emotional states. So we get modulated based on our changes in our urges and the expected changes into different configurations that adapt us to the environment. And that when we perceive them ourselves, we perceive them as emotional states that we are in. And the same is true when we perceive them in others. And this gives rise to what I call the engine of motivation. So here you see a bunch of tanks. To the upper left, you see the physiological needs. At the bottom, you see the social needs and to the top right, you see the cognitive needs. Each of these tanks can run dry. And when they do this, they create pain signals. And when they, the level increases, they create pleasure signals. And they interact to produce an integrated sense of valence and urgency and urge strength. And they affect a number of modulators that configure the cognitive system at any given moment. And we can use our models of needs to map them on the big five and as they explain personality, basically the hypothesis here is that personality can be understood as parametrization of the motivational system. But are needs not a hierarchy? Don't we all know Maslow's hierarchy of needs? I am not a big believer in this hierarchy of needs because first of all, I don't think that these are needs. These are purposes, right? The needs are more basic things than self-actualization and self-esteem, which are conceptually quite difficult to understand. Safety, this is something that you cannot measure directly. You have to construct these structures that you see here. So this is actually a hierarchy of not of needs, but of purposes, of models of needs. And, but we see is that basically, on the lower level, you have organismic purposes, and the next level, you have relationship purposes. And at the top in Maslow's hierarchy, you have purposes that can be integrated with the lifespan of the organism, which is basically the ego. And in our own organism, or our own own mind, they are not organized into a strict hierarchy. Our needs are in direct competition. They're next to each other. They might have different strengths and might be replenished in different frequency, but they are directly competing. You are not just waiting when you want to have stage of revolution until you have eaten. Sometimes you will not eat for a couple of days because you have more important social or high level goals. And of course we have goals that are above the ego, basically what we call sacredness, purposes that are so important that we are willing to sacrifice the goals of the organism or of the ego for it. And once we have sacredness, all our other purposes become instrumental to satisfying our sacredness, our higher meaning. There's nothing esoteric about this. It's just the way in which most human beings are implemented. And this sense of having purposes above the level of the organism is what allows us to link up into higher level agents. And I think that our ethics depends on having this notion of sacredness, it depends on having shared purposes above the ego. Basically, ethics is best understood, I think, as the negotiation of conflicts of interest, under conditions of shared purpose. If you do not share purposes with anybody else, there's no reason why you should have ethics, right? You are a singleton that is interacting with other singletons. You're maximizing your own goals and there's only game theory that you have to obey. All your interactions are going to be transactional. And if you want to have non-transactional interaction, it means that you have to act on a shared purpose. It means that you are giving something to somebody else, not because you expect to get something in return from this particular agent, but because you and this other agent are part of something larger. And you want that other agent to achieve their goals because they're ultimately also your goals as part of that something larger. So you don't need this direct return from this other agent in this context. And these shared purposes need to be given by innate needs. You cannot infer them something. There are sociopaths, which are completely functional as cognitive agents. They don't have any cognitive deficit, but all their interactions are going to be transactional. And the interesting thing about our own species is that we can link up in this shared purpose, that we are able to build societies at scale, that we can build up in this shared purpose, that we are able to build societies at scale, that we can build coherent agents at scale. And when we think about AGI or artificial intelligence systems that are ethically interacting with people, it means that we have to build them in such a way that they are going to share purposes with us. And these shared purposes need to play out in a coherent world, which means before we can have ethics, we need to develop a shared aesthetics of what the world should be looked. Yoshi, you're frozen again. Should we look to not converge to a coherent reality that is sustainable? Okay, so it makes no sense if we have shared impulses that do not converge to a coherent reality that is sustainable. Okay, so it makes no sense if we have shared impulses that do not converge to a coherent reality that is workable and sustainable. This is it for me. Thank you, Joshua. Dietrich, are you able to hear us? Oh, Dietrich is a guest again. Let me make you a co-host. Dietrich is a guest again. Let me make him a co host. Dietrich. Can you hear us? You should be the co host now again now. Hello. I cannot see him. Can you see him? Yes, I can see him. He's unmuted. 这一步是为了让您的手机更容易运行,并且更容易使用。 如果您的手机是手机,您将需要更多的时间来运行。 如果您的手机是手机,您将需要更多的时间来运行。 如果您的手机是手机,您将需要更多的时间来运行。 I think we have a connection issue. Yes, that's very unfortunate. Okay, let's hope that he is going to be able to join us in the moment. It did work in the preparation. Yes. And maybe what we should be doing is for now to jump into the discussion. Mm-hmm. And hope that we can draw him into the discussion later. Okay, so at this point, in this case, I would like to invite questions from the audience. You can post them in the chat. To begin with, we have several questions to the speaker to jumpstart the discussion. So what kind of algorithms do we need to compute and distribute rewards in artificial AI systems? So that their goal aligned with the humans. Yes, I think you like to go first? I think that there can be various algorithms that do it. And eventually, what is important is the functional result of the application of these algorithms. And the easiest perspective that we can have on that is probably a cybernetic one where you have a regulation system with feedback loops, but it doesn't matter how you actually implement it. What matters is that the system is acting on something that amounts to a shared purpose. And I am not aware of a very good model for this in the context of AI that does this in a general form. But I think that Christian and Cristiano might have thought about this as well. Christian, do you have any thoughts? Oh, yeah. To some extent, I mean, some form of reinforcement learning system, but not like the ones we're using today, because they usually only have one general value function for the whole system. It should be something more complex with multi-dimensional value to start with, but also not necessarily reinforcing very simple associations or something like that, but probably influencing a lot of different processes. So perhaps if we develop something like that, this will be more like a control system like Yoko is suggesting. Algorithms are not to my domain, but let's say this point, which we need to model at different layers, emerging one on the other by self-organization. So we need to model in theory many layers, and then we need algorithms for each layer. abbiamo bisogno di modelli e teorie, moli strati e poi abbiamo bisogno di algoritmi per ogni strato uno è l'algoritmo del nostro cervello implementando attività neurale, inibizione, attivazione, associazione ma su questa base, a livello basso, abbiamo l'algoritmo del cervello il programma del cervello, il programma cognitivo che è implementato sul lato neurale, ma non è riducibile. Non possiamo capire la persona solo con la descrizione basale di attivazione neurale. Noi abbiamo bisogno di una spiegazione funzionale a livello basso, e poi un livello più alto di spiegazione funzionale al livello cognito, e così via. Quindi, in un senso, dobbiamo modellare un lato diverso di macchine algoritmiche. at the cognitive level and so on. So in a sense, we have to model a different layer of algorithmic machines. Sorry for this intervention. Christiana, actually a question for you. In your presentation, you spoke about intention as consisting of the, of what to do and choosing like of these two aspects of the goal and the actual choice of executing. How does this architecture lie within the system one, system two distinction? Okay, grazie. Per me, in basso, è parte del sistema 2, perchè è basato su crescita, decisione, argomento, ragionamento e poi lo facciamo. Ma, ma, come diceva anche Yosha e Christiane, dobbiamo integrare il meccanismo e i dispositivi del sistema 1 come la struttura di base implementando anche le attività intenzionali. Alla fine è realizzato da un meccanismo automatico. E viceversa, è possibile trasformare un dispositivo intenzionale e una decisione in abiti automatici, usando. Per esempio, quando inizio a imparare a guidare un'auto, mi racconto di come devo fermarmi, come devo... poi diventa un automatismo. reason about how I have to stop, or I have to then become an automatism. And vice versa, it's possible that automotism is explicit and become a decision-making process. So it starts to be automatic, and decide to control and decide. So it's possible the implementation of the two systems, and the conversion to system, the competition and collaboration, interference. sistema e imprimirlo, la conversazione del sistema, la competizione e la collaborazione, l'interferenza, ma è anche possibile che una sia tradotta in l'altra. A presenti, le persone con la teoria del sistema uno e del sistema due non sono così chiari su questo. Di solito presentano loro come due meccaniche paralleli e competenti che stanno unendo l'una all'altra. E' troppo semplice. Interagiscono, interferiscono, e poi si traducono l'una in l'altra. two parallel and competing mechanism stopping one on the other. This is too simplistic. They interact, interfere, and then translate one in the other. So it's a bit more complex. Thank you for your question. We have a question for Yosha and perhaps also for the others. Do we have a theory for high-level agency? It seems from what you said, Yosha, that game theory isn't enough, at least naively. This is from Mohammed Barakat. Yeah, so first of all, to the sister bond system two distinction, I think that Cristiano is exactly right that they're not separate, but they have to be understood as something that is working together. But there are different operations taking place in system one and system two. It seems to me that system one is primarily perceptual and it's getting to its understanding of reality with some gradient descent, which means it is following a deviation to a local optimum and perception. And if you are following a gradient, you don't need to have a memory of where you came from. You just need to follow that gradient until you end in the local optimum. But if the search space is very large, for instance, the space of things that we could be looking at at any given moment based on the models of reality that we know, we often don't converge. And so we need to construct a solution. And this construction of a solution, when you cannot follow a gradient, does require memory, because you need to remember which one you tried and why, and which things you tried worked and which one didn't. And it will discover its own index memory, its own stream of operations that it did. And so we have a reflective system that has memory and is able to perform analytic operations, grammatical operations, and get to a low-dimensional representation of reality that is very stable. And you have the perceptual system that is much more dynamic and more continuous, and that interacts with this reflective system. And I suspect that the reflective system is primarily a top-down process, the reflective system is primarily a top-down process, whereas the perceptual system is primarily working bottom-up, but they have to work together to work at all. I don't think that one can work without the other. Now, to the question, if we have models for emergent higher-level agency, I think that it's interesting to look at the work of Thomas Aquinas in this regard. Thomas Aquinas has discovered concepts or policies for rational agents. And he says that basically every rational agent can infer that when it's in some kind of social context, that it needs to optimize for four different things. One is, the first principle of rational agency is goal rationality. You need to pick the right goals and the actions that you can hope to achieve these goals. In his old language, he calls this prudence, right, wisdom. And then you have to have optimization of your internal regulation. You need to regulate your own needs in such a way that you don't fall apart, that you don't overeat, that you don't under-eat, that you maintain your homeostasis of your organism. And this is what he calls temperance. You also need to optimize the interaction between agents and keep them in balance and homeostasis. And this is what he calls justice. And then you need to maintain the correct balance between exploration and exploitation, which means you need to both learn and then you need to be able to act on your models. And you need to understand when you need to act on your models. And this is what you call it's courage. And these are the principles of rational agency. They are not giving rise to higher level agency. If you want to build a higher level agency, agent, then you first of all need to submit and serve a higher level agent, right? You need to recognize that there is a higher level agent that you want to be part of, and you need to be willing to serve it. And this is what he calls faith. And you need to be willing to link up with the agents around you to do this, not just an abstract sense that you abstractly serve some higher level agent, but you need to do it with the people around you that also share this need to serve the higher level agent. And this discovery of shared purpose above the individual, this is what he calls love. And then you need to be willing to do this in the absence of expected reward, because before you act on this, this higher level agent doesn't exist. Right before the higher level agent exists, it cannot give you the better environment that the higher level agent exists, it cannot give you the better environment that the higher level agent can construct for you. So you need to be willing to invest into this thing before it exists. And this is what he calls hope. And in the language of Thomas Aquinas, these rational policies are called the practical virtues, and the next level agent forming policies are what he calls divine virtues. And we are used to take these things as superstitious or religious, but the concepts that are connected to them as such, but they're not. I think that they're entirely rational in the way they were conceived. And it's just that there are, our world is so steeped in them because we used to be a religious world until 150 years ago, that they seem to be ubiquitous and part of the previous civilization. But I think that he made a genuine effort to discover principles for emergent next level. There was an interesting remark in the chat during Dietrich's presentation, dreams no longer a component. So it inspires me to a question of, well, what role does dreaming and imagination play in autonomy and goal formation? And I would like to ask Christian Vulcanius for his thoughts. Okay, so the way I view it, the sort of basic mode of the brain is just imagining things, more or less a complete mess, but tries to follow what's happening outside the head when possible. So it's like we have a world outside, including the body, continuously evolving over time, and what the brain is trying to do is to follow that with sort of an internal process that follows along as well as it can. But that's during learning, but it can also be decoupled from the environment, and that means that those state transitions that it has learned just goes on on its own inside the head, more or less. When they're not controlled, it will be daydreaming or imagination. And depending on this factor I talked about, a game control, it can be completely random, or it can be focused on a particular problem, or aim that some solving a problem, for instance. used for daydreaming and for imagination and for planning and problem solving. It's the same, the difference is to what extent it's controlled or limited to just states that are relevant for whatever we're trying to think about. But that's the short version, the long version is very long. Thank you Christian. Thank you, Christian. I have to remark to this, and the audience is dreaming the default state for which you can create the awake. And I think that's basically correct, in the sense that everything that we perceive is by its nature a dream. And it's, if we can perceive it, we must dream it. A physical system that doesn't have any awareness, you can only be conscious in a simulation. And what we perceive is the game engine that is generated in our brain that is tracking sensory data. And at night, this game engine is not tracking sensory data because it doesn't perceive any and it's dissociated from the sensory processing. And instead, it augments data by varying the states of the low-level systems so you can experience things that are not grounded in direct sensory experience. But the system is initially trained by having access to this outside world and by tracking these patterns. So our dreams tend to be heavily inspired by the patterns that we perceived in the real world interaction. But I think it would be very helpful if more people in general, for epistemological reasons, were aware of the fact that everything that they can possibly experience, think, every revelation that they have, and so on, ultimately is a brain state. Everything that we perceive is a configuration of our own brain, some kind of dynamic pattern that unfolds, and that is attended to by a reflective system that has memories of what it attended to. So we can only have access to the things that we remember having attended to and what we attend to are always states that take place in our own brain. At least that's the most sound theory that we can observe. That makes sense of what you can observe. And because this system is during wakefulness, coupled to the external world, we can make a model of what representations are the result of that coupling. And the functions that best predict the next sensory state are what we perceive as perceptual reality. I think this is how it works. But to this original question, I think that also implies that without the ability to something, it means we establish relationships into a universal, connected, coherent model of everything. And this universal, connected, coherent model is what we individually call the universe. Everybody creates a universe in their mind as a dream and relates everything to it. Thank you, Joshua. We have another question for you, which I think would also be great if we could get Cristiano's input on that. And the question is, so I'll just read it. This is more of a non-technical question, but are there any immediate strategies you are sympathetic to of applying these notions of coordination and higher agencies to improve or increase the agency of the social structures we currently live within? What do you think are the factors driving the seeming lack of agency within our current civilization model, and do you see a steady state dynamic emerging from the current situation? Yosha, I'm not sure if you would like to go first or whether you would like Cristiano to go first on this question. Cristiano, would you like to go first? I'm not sure that I fully understood the question, Volete andare in più? Non sono sicuro che abbia completamente capito la domanda, scusate perché non ho potuto leggere, ma l'interesse era sulla coordinazione, la coordinazione degli agenti, l'emergenza del risultato collettivo, la socialità e così via, se ho capito. etc., se l'ho capito. Ci sono due fasi. Una è che ci servono un'associazione sociale fondata e fondata, un'associazione sociale che non è solo cooperativa, ma che è anche conflittuale e competitiva. Il conflitto è parte della società, è una parte fondamentale. Non c'è conflitto, non c'è democrazia, per darvi un esempio. Quindi, è cooperazione e conflitto. Non è solo adattare, adattare, adattare il mio comportamento agli altri, ma è anche cambiare il comportamento degli altri, influenzare loro e cambiare la loro mente. Di solito, c'è una misinterpretazione del teorema di Tom. Si predica, comprende e non si adatta. No, Tom è basatamente per cambiare il loro bigore e la loro mente. Una parte della teoria importante è la fondare la socialità in diversi aspetti nel mento dell'agente. E non solo l'intercambio, il gioco, l'interazione, ma anche qualcosa di collettivo, l'emergenza di un volo collettivo distribuito, di un conoscenza, qualcosa che è molto molto... e ancora più difficile, l'intero legame istituzionale. Il contesto effetto di un certo atteggiamento e obiettivi sociali estaboliti. Quindi, differenti livelli di socialità sono implementati nella mente e funzionano attraverso la nostra mind, for example, not norms and so on. This is one aspect. The other aspect is we need a theory, a behavior-based theory and also evolution-based theory of sociality, the kind of interaction in sociality. And on that point of view, let me say that I have enjoyed very much Dörner's critics to the simplistic statistical use of correlation and so on. che ho apprezzato molto le critiche dell'uso di correlazione e così via, ma ho un'altra critica che si riferisce anche a qualche domanda sulle teorie del gioco e così via. Attenzione! Molte teorie sono in effetti tipiche ideali, sono in effetti teorie normative, una cosa molto strana. In economia è in questo modo, in teoria di gioco è in questo modo. Sono in modo ideale, racionalmente, ma non è una teoria completa scientifica. La teoria scientifica dovrebbe essere esplicata con che sia empirico, non solo una idea normativa. Perciò dobbiamo accompagnare anche il regolamento basato su diversi tipi di socialità, e lo farò molto brevemente. Per me, la base di socialità di tutti i fenomeni sociali esterni è la dipendenza, il fatto che gli agenti non sono in grado di raggiungere tutti i loro necessari. Abbiamo una limitata abilità, una limitata competenza, limitati risorse. Quindi siamo dipendenti dall'altro, abbiamo bisogno dell'altro. E' l'avvantaggio di essere diversi, complementari. La società è basata sui netori di dipendenza e sui rapporti. Non solo l'intercambio, ma di diversi tipi. L'intercambio, ci serve questo, ci serve quello, ma anche l'interazione senza interesse, come la teoria di Adasvig, o anche per la vera cooperazione. Condividiamo un obiettivo, siamo un obiettivo comune, dobbiamo associare il nostro potere per raggiungere il nostro obiettivo comune. Also for true cooperation, we share a goal, we are a common goal, we have to associate our power in order to achieve the common goal. But even more, even altruism, I don't expect any of that. I just do that for you and so on and so on and so on. So we need a more systematic theory of different form of social relation interaction. And for me, I'm based on different kind of dependency. And we need to ground that, to found that, in the mind of the agent, because he is the mind that controls the social behavior. Sorry if I've been wrong. Thank you so much, Cristiano. Dietrich is back. Dietrich, can you hear us? Yes, I can hear you. But I can only see you in a very blurred image. And I'm so sorry. But please don't take any, well, don't look at my problems. This is just continue the meeting. I try as good as possible to follow. Dietrich, what do you think are the minimal needs that you built into a system to give it sociality and social agency? Please again, I got only the half of your speech. Okay. What do you think are the minimal things that need to be built into a system to give it something like social agency? So that you have collaboration and competition and the emergence of normative structure. Do you have thoughts about this? Our mice have social competition and they behave socially. For instance, our They behave socially, for instance, our mother mice, they look for their children and feed them and teach them things, how to come to a certain place or not. And this is very developed by genetics. Genetics are great in simulation. Because if you have a gene pool, a lot of things you have not to program, but they develop for themselves. And this is quite interesting. I think this is a good medium to start with, to simulate just human development. And this is quite interesting. And then you get a different form of behavior. For instance, there must be They, well, there must be a basic need for contact. This is basically, but when you have this contact, and my fishes in my pond, they have that. They look at me and they know me and they like to have contact with me. They don't ask me, but they wait for instance that I give them food and they don't not wait then from other people, but for me. That is, they have a connection with me, fishes, and they have contacts even, they have social structures, these fishes, koi carps, have contexts even, they have social structures, these first of all, and everything in a rather primitive form. And to have a genome and sexuality helps a lot in development. I would like to go on with our main problem at the moment, to develop language, not just artificial language, easy to make, but to develop that kind of language which I talked about, about a quarter of an hour. Okay, we lost the trick again. The gods of the internet are not with us with respect to our connections to Bavaria. I'd like to briefly also emphasize this point that Cristiano just made, that collaboration and competition have a relationship between them. I suspect that our need to collaborate and our ability to collaborate is the result of an evolutionary pressure and our ability to collaborate is the result of an evolutionary pressure that was a competitive pressure. Basically, the agents bind together and form coherent behavior because they are forced to, because they are competing with systems that have discovered the power of cooperation. And once you have that power of cooperation, you are able to organize yourself in governed structures. And these governed structures mean that you are doing things that are not optimal for yourself, but that are optimal for the group, basically for the next level agent, for the next unit of organization. And as a result, you're able to collectively out-compete individuals that are only acting on their local own self-interest. And if you take away this pressure, if a system becomes too big to fail and the competitive pressure falls away, then the system loses the need, the evolutionary pressure towards collaboration and cooperation, which I think is what we might be observing in our own societies. And as a result, what we see are increasing failures of coordination. Right, so our inability to coordinate, for instance, in the face of a pandemic is probably the result of a change of governance structures in the last 50 years as a result of our society becoming very safe and too big to fail in a way. There was an absence of competitive pressure from the outside, the fact that we have weapon systems that protect us effectively from any outside attack and so on, and the fact that we have built an agriculture that is feeding us very well and a political system that is relatively stable have removed the need for deep cooperation. And so in some sense, that's the way in which agents organize to next level agency is the result of pressures that exist in the environments of the agents. And this is related to what Dietrich was describing in his simulation of the agents. And this is related to what Dietrich was describing in his simulation of simulated mice. He builds environments where there is a benefit from competition at first, because there are limited food resources. And the environment is also set up in such a way that there is a benefit from cooperation, so they can share information, they can find the food sources against other groups of mice, and as a result, there is an emergent social structure. I think we are on the top of the hour. And I would like to take this opportunity to thank all speakers for joining us today. And I'm very grateful for getting us together despite the connection issues. I'm very happy about the possibility of having such a conversation over the internet and via Zoom. I'm also very, very grateful of having met Christian in person and seeing Christiano again after many years and also Dietrich. We will cut this video into a form that we can release if everybody's okay with this. And I would also like to thank the audience for following along, asking questions and paying attention. And I will see you next time. Thank you very much. Thank you for your beautiful initiative. Yeah, thank you very much.", '53.57365536689758')