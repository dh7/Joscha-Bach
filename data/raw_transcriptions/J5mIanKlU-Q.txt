('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/J5mIanKlU-Q"> </iframe> </center>', " Yeah, I don't know, there are like millions of things we could talk about and I was really thinking what those could be. I mean maybe like human, like AI value alignment problem, like towards human could be an idea. I'll try to like formulate the worries that I recently started to have around the value alignment things. Okay, let's say there are lots of people around Berkeley and everywhere who are trying to do inverse reinforcement learning to force the agents to align themselves with the human values and stuff like this. And we all hope that this will lead to AGI and superintelligence that will be aligned with something that we don't know much about, which is like human values and the set of human values. And the problem here that I'm afraid of is that, let's say that the human society is now kind of stuck in a Nash equilibrium that is kind of defined by the latency in communication between all of those stand-alone human level AI agents communicating via some kind of cultural information transfer, and most importantly by natural language. And this keeps them in this maybe inadequate equilibrium, but that inadequate equilibrium can be maybe the only adequate equilibrium holding the mutually inconsistent sets of values. And now let's say we've got the same number of AGI agents that are actually almost indistinguishable from one singleton that has no problems communicating in the speed of light, basically. So it removes this barrier of communication latency. And it will just, you know, maybe the whole value system will just, you know, crash because of this thing, because of getting away from this kind of suboptimal Nash equilibrium. And metaphorically it can be rephrased like, you know, here the human psychopath can be harmful to the extent of his human body and you can be sure that he just cannot influence, like Hitler could not influence the whole world and eventually he had to die. But that would not be the case with the AGI and superintelligence. And I'm afraid that this can be like a systemic problem, that this can be just a problem in principle that we cannot overcome. I think our minds are modeling devices, right? In the service of regulation. So intelligence is the ability to make models, because what you cannot understand, you cannot regulate that, right? You cannot regulate what you cannot understand. And Ashby's good modeling theorem says that a system that regulates something needs to build a model that is isomorphic to the circumstances that it models, right? And when you want to model ethics, it's a prerequisite that we would have to understand our own ethics and the nature of our own values. And it seems to me that most people that are now asking for regulating the values and the ethics of AI don't even have a clear understanding of how to align our own values and what they actually are and what the nature of the integrity is that we would need to achieve to do that, right? I think that ethics ultimately is the principled negotiation of conflicts of interest under conditions of shared purpose. For instance, personally I'm a vegetarian because I choose to share a purpose with animals with respect to the avoidance of suffering. It is not a mutual thing. It's not that these animals do care about my suffering. It's not that we made a contract with respect to that. It is a choice that I made. But it's also a choice that is predicated on a number of postulates that I made in my mind that are not shared by others. And others do not share these postulates. And they're not intrinsically rational postulates. They're choices that flow out of my personal value system, then I would not have an argument to convince another person with different values to accept these postulates, right? So I cannot accept other people to be vegetarians if they do not share purposes with these animals in the same way as I do, right? If they are rational agents and they choose not to share these purposes, then their ethical considerations will not extend to the suffering of these animals. So this rating will be a different one. And when you look at AI, the difficulty will be, how can you convince AI to share a purpose with us, so we become moral subjects to the AI? Especially since it's not clear that you would need more than one AI, because while our minds don't scale, artificial minds will scale. So a larger artificial mind will be better than a group of small artificial minds that would have the same power, right? The reason why we are so good as a society when we work together and outperform individuals is because an individual brain doesn't scale. But that might actually be, you mentioned that it's a negotiation actually. A negotiation by definition takes time. It's like a temporal process. And maybe what if the humanity values, like we simply don't, we want to create friendly AI that does not collide with humanity values. We simply don't, we want to create friendly AI that does not collide with human values. Now just think of them intuitively. But what is like, and we know that there is nothing like human values per se, but maybe it is the temporal extension extension of those ongoing negotiation processes among the, you know, humans. And as long as they are, you know, in the temporal dimension that is compatible to us, like it will take you some time to negotiate with somebody who eats meat and kills animals, something to, you, something to negotiate about those values. But the HCI agents will just go completely away from that time dimension. They will just compress it into, let's say, I don't know, microseconds. So there'll be like kind of no space for living through those values. And that's what I'm afraid about in case of AI. You should be worried, but it's kind of an obvious thing. Of course, you should not be worried about things that you cannot change in the sense that you have emotions about them, but you should understand them. So you and me can probably negotiate to build an AI that we would consider friendly, but we cannot prevent everybody else from building different AIs. And there are objectively people on that planet that have goals and purposes that conflict with ours. So in a time of growing resource scarcity, for instance due to global warming, there will be the situation that there will be people on the planet that want the lunch of your children for their children, right? And that's a purpose that at some level is very difficult to negotiate. So there will be objectively conflicting interests. It will even be worse at some point. And if we don't manage to turn the situation around, which is not very likely, that will probably be unavoidable. And in the same sense, if you think about intelligent systems that exist, for instance, different corporations share some purposes, but not all of their purposes, because they are often competitors. Corporations are intelligent systems, they are already in some sense AIs, they are AIs that borrow human intelligence for their intelligent agency, and they are very slow and inconsistent minds, because basically the update cycle is the update cycle of their spreadsheets and the dissemination of these spreadsheets in weekly meetings. So this is pretty slow and humans are often able to outsmart corporations because they can do something within that update cycle that the corporation cannot react on. But imagine this corporation uses computer systems to optimize all these modeling processes and negotiation processes and eventually the spreadsheet gets updated every few milliseconds, and the meeting takes place every few milliseconds. You will have a thing that basically becomes sentient. Even if it's not conscious, it's something that is acutely aware of its own nature, its position in the world, of its goals, and we will have to compete with it, and it will have a higher degree of sentience than us. That's a problem that these AIs will not be the robots that we build, these robots will just be the limbs. These will be systems that are not going to fight for human rights, for the same reason that we don't fight for ant rights for us, we don't want to be ants, it's not attractive. And these will come top down on us. We will be living inside of them, not next to them. We will be the gut biome of AI. And the big question is, is the AI going to use antibiotics at some point because the gut biome is too much trouble? And how much antibiotics is it going to use? Okay, so yeah, it doesn't look too... I was hoping for a kind of optimistic answer to the fact that... Oh, I'm optimistic. I think it's totally possible that global warming kills us before AI does. Yeah, okay. Yeah. So this is actually, this is exactly the same level of optimism that I have. Like, you know, there's nothing we can do about global warming, there's nothing we can do about volcano below Yellowstone, about asteroids hitting us. So we just feel kind of irrationally safe about them, because we just, you know... Oh no, we shouldn't! We have to accept the human condition. And it might sound depressing to some people, but you cannot save lives. You can only delay death. The moment you are born, it's clear that you will die, because second law of thermodynamics, entropy always wins. In a universe like ours, you have to have a source of entropy to maintain and build your structure, and at some point, the source runs out, and at this point, this latest point, where you die. So death is a certainty. There's no way around this. And the interesting question is only, what can we do until entropy gets us? What can we build? And there's this amazing thing that we have this chance to be here, that the universe conspired to produce this very moment. And we can throw some sparks together. And it's just amazing that we have this conversation and reflect on this universe in this brief moment in time. But yeah, this is definitely like we can have, we can create something that will keep on, you know, decreasing entropy around us, even though it's not going to be us, probably. I mean, it could be something like it's worth creating a conscious life that will be here after us in the in the universe. So for you, you are kind of... Not convinced today. I will be conscious for long. I think consciousness is a particular artifact of a type of learning, of attention-based learning. I think that the purpose of consciousness is to maintain a model of the contents of your attention, because we have an attention-based learning system, we integrate these things into a protocol that we recall, right? And I think that at some point, if you have learned in a domain exhaustively, you stop being conscious of it. Most of the actions that perform in everyday life, you're no longer conscious of them, right? Because we've learned them reasonably well. When you drive your car while having a conversation, you're only conscious to driving the car to a very small degree. And the older we get in our lives, the better we get at our everyday tasks, and the less often we are conscious of them, right? And the same thing will probably happen to the AIs. So the more they know what they're doing, if they have a similar learning mechanism as us, the less they need to attend to these this task and the less they will be conscious. Well they might be conscious about different things like more cosmological things but okay so which brings me to the fact what do you think the motivation behind like the real motivation behind building AGI is because it seems to me like you know like thought is a computational process and consciousness is... Well, yeah, and so not every computational process, definitely not all computational processes, or functions respectively, can be named as thoughts. So, you know, why on earth are we trying to create a system that just funnels all possible functions that are in the, I don't know, counter set universe to a very tiny subset that resembles the way how we process information, which is a very difficult, you know, thing. It's like the whole AI research is about, you know about funneling the infinite number of potential functions for information processing into a little tiny subset of those that are similar to what we humans do, and then let them inflate again over a computational substrate to the super intelligent entity that actually, as you said, will very soon perform functions that even don't resemble that little tiny subset that we think of thinking. And why not just nuke the earth? And you can be sure, or you can be maybe kind of positive that some of the functions realized by that physical process will simply be the computational process realizing somebody's thinking. And it will be like a kind of superpower. much more interesting set of functions realized in the universe at that moment, whereas we, you know, for some reason just trying to limit it to create something like, you know, like imitate your day, something that is similar to us and then let it expand to the rest of the universe. So what do you think is the real motivation about creating? Let me answer the question. So first of all, if you think about physics, with respect to complexity, you can understand it as a kind of search, but it's a completely undirected search. So some corners of physics would use systems with more complex structures, and most of the other parts of physics don't. And we are, of course, in the parts of physics that produce these interesting structures. And at some point you had the formation of orbits and planetary surfaces and of certain neck entropy gradients that could not be closed by dumb chemical reactions and that opened a market opportunity for controlled reactions. And this is the market opportunity for life to build controlled chemical reactors that can out-compete simple combustion processes. And the main thing that happens on our planetary surface is the hydrogenation of carbon dioxide. So some cells emerged, small molecular factories, how to take carbon dioxide from the atmosphere and use photosynthesis to build their own structure in organic molecules and oxygen is put up in the atmosphere. And then they happened a singularity on top of this because evolution had started. Evolution is a much more efficient search, right, because it privileges the things that build structure over those that don't build structure. And this evolutionary process led to the first singularity in a way after life emerged and covered the planet within a to the first singularity in a way after life emerged and covered the planet within a geologically extremely short time. And this one was the emergence of animal life of a bunch of cells lumped together and decide that other cells are a very good source of negentropy so they can eat them. So these eating groups of cells that lumped together and moved over the other cells and started eating them. That was a major revolution. That was actually major evolution. That was actually maybe more interesting point in the evolution than actually creation of human intelligence maybe. Yeah, but this was the next singularity in a way, right? These cells, tasks some of the cells to do the information processing and as a result they started to regulate and to make models of the environment and at some point these things became generally intelligent. And this is the next singularity. because once you have this directed intelligence, you have an extremely directed kind of search. You have something that really figures out what is it that I'm searching for, unlike evolution, which is a blind search, which goes into all directions at once, basically, and those directions that don't lead anywhere are pruned. It's a search that eventually got us where we are. But now we are able in extremely short time spans to come to solutions that would be much, much harder to get to from an evolutionary perspective. Sorry to interrupt you. I'm not sure if... Yeah, okay. Evolution might be non-directed. It follows physical laws. And one of them is, well, it really goes in the terms of decreasing free energy. So you can actually say that out of those multitudes of possibilities for physical laws we happen to live in a universe where all its parts arrange themselves so that it decreases free energy and increases entropy. So it actually can be, you can imagine an infinite number of universes where this doesn't work this way. It can be like, I don't know, I don't know, level 4 multiverse or something like this, where you just don't have this physical loss. And what if these physical laws are actually an instantiation of directedness, so actually evolution and even the molecules, they are organizing themselves or proteins folding, they actually, from our perspective, are undirected. It's just like a molecular chaos. And as at the moment when in the 37 degrees Celsius, you know, it reaches, you know, local minimum of free energy, it folds a protein and that's it. And you would not say it's directed, but you can imagine a different universe where it would fold completely differently. So maybe already this directedness in search on that cosmological level is maybe further away than we admit to, you know... It doesn't help us. The problem is that all the universes in which the current present particle-physical laws don't apply are not universes that could contain us. We are the product of exactly the type of universe in which we find ourselves. So these kind of structure building things require a reversible universe in which we find us, basically that is information preserving, require neck entropy sources. They require neck entropy gradients. And if you don't have a reversible information preserving universe, you cannot build the kind of structure that we are in, at least not when you are in base reality, because things just dissipate. So the corners of the universe in which we find ourselves are those, is pretty much the properties that we observe. That is the limitation that we find ourselves under. So the generation of intelligent life and so on does require certain conditions. It needs to be a genesis principle and there needs to be an underlying structure that enables the formation of the type of automata that we are. Now back to your original question, why do we build AI? Because it's the next level of search, it's basically to automate and scale our type of intelligence, our ability to make models in the service of regulations, kind of the obvious speed up of that thing. And what it practically means, it's super useful for basically everything that human people do. We need regulation and modeling. If you want to control a company, if you want to design a new tool, if you want to find a solution for any kind of problem, you need minds. And you benefit from minds that are better than ours. So why not build them? It's very clear economic, social, and cultural benefit from minds that are better than ours. So why not build them? It's very clear economic, social and cultural imperative to do that. Even though it's a dangerous thing to do, but it's kind of a universal talus that leads us there. But you can also ask from a personal perspective. Why are you and me interested in this? And I think the main benefit of this is that it helps us to understand who we are. We are minds and we want to understand why is this happening to us, how is it possible that the universe is happening to us, to this person simulated in the brain of a primate in a virtual environment, generated as a dream in the neocortex of that primate. But we are understanding us in a different way than Marvin Minsky or, I don't know, John Searle had envisioned in the days of good old-fashioned AI, because we're not anymore the gods for AI. Like, it was like in the 50s. We're actually just more in the sense of fathers of AI, just seeding something and then nurturing it. Or in like, in more Kabbalah way, we're kind of, you know, putting clay together and dancing around it, like putting words on top of words and actually trying to evoke those principles that entered our biological substrate to enter a different substrate that we have arranged. And yes, this might be, this actually might be the Imitatio Dei principle. We just want to do it because it's inherently present in our culture. But do you think that we really can understand us by creating AI? Totally, but I'm not a god. I'm just some person inside the mind of a primate. I'm not exactly that primate, right? I'm a side effect of its regulation needs. I'm part of a model that this primate organism uses to navigate its environment. I don't think that Marvin Minsky perceived himself as a god. I think that most of the ideas that we are discussing right now were pretty well understood 50 years ago. Just, of course, they didn't disseminate so widely into culture. And also, if you look at AI as a discipline, it's largely an engineering discipline. And this big rift in AI as in cognitive science and AI as a field of engineering, well, it's partially because of these forefathers of AI, of some of the decisions that they've made and some of the social problems that surrounded this. For instance, I think that Marvin Minsky understood that psychology wasn't going anywhere after Piaget, so we needed a new science, the science of information processing, that would allow us to build testable theories by implementing them, something that psychology is missing as a criterion. We cannot test theories with too many free variables, right? But we can just do this because they have to run. So he understood why we need to do this. Why we need to take computers to test our models and build models of us to understand us. And then he made the wrong decision. He decided that AI needs to be symbolic. That we have to work from language and symbolic knowledge and extend this downwards into simple grounding in a way. But this would lead us to a different understanding to what we have now. It didn't scale and he yelled at all the people that did cybernetics and that did neural networks because he thought it was too simplistic. In a way it was too simplistic at the time, but it also led to this rift in AI where he created an area of burnt ground around symbolic AI that was cognitive AI, his own area, and everybody else's AI that didn't look at cognition very much. So it's something that, a gap that we are still struggling with, this idea that there needs to be a gap between symbolic AI and distributed AI. It's a total red herring, in my view, because we are looking at something like more a middle layer. We should be looking at a set of organizing principles for minds, universal modeling principles that are motivated by reasons to do these models. And then it harnesses whatever computational substance is available to implement the necessary structure to do that. So you think like building AI will help us help us formulate new falsifiable theories that can predict something testable about humans, about us, about our minds, about how we work as a society? Well, it's already doing this. We right now have models that approach capabilities of human vision in some way. They don't work exactly the same way because they're not attention-driven. I think the primary issue that we have with our current models is that they don't converge on a single world model. For instance, today, there was a presentation of pictures that could not be properly labeled by the current end-to-end neural image recognition systems. And the main issue was not that they didn't discover the components correctly. They largely did. They could see that this airplane that was about to crash on the ground was an airplane and the ground was tarmac and so on, but they didn't see what's relevant about this. And this ability to discover what's relevant in a novel image and a thing that you have never seen before, how do you do this? You don't do this by adding more and more modular functionality and intuitive physics and so on. You do this by creating an integrated world model. So, a model of the universe that you live in. When you see a picture, you always know this is not a collection of pixels that looks like another collection of pixels and I have to classify this. What you know is, I'm actually not looking at the world, I'm looking at a picture. And this picture shows something, it's a signifier of something. And what does it signify? It signifies a particular window into a global universe that I have in my mind, that I can create. A global dream of what's happening in physics, in sociality, in my own mind, my relationships to others. And so this creation of a cohesive world model, for us, mostly means people moving around in three-dimensional space on a planetary surface exchanging ideas. This is a constraint that is currently missing from our models. And so do you think like we're doing now is that we take theories currently representing our current understanding of human minds then simplify them kind of twist them and metaphorically apply them to the computational environment and then we are observing the experimental outcomes and then it's very hard to and then then we conclude, oh, so it's underperforming humans. So we're definitely not made the way we made the algorithms. So let's start over again. Whereas it should maybe go the other way around, like start, like, I know it's maybe super hard, maybe impossible, like, but starting the theories about AI and then test them. Because well, or maybe you could just maybe tell me your idea if we have already learned something about us from AI, from designing AI. Oh, me personally, I have learned a lot. When I entered the field 20 years ago, I did this because I have questions about our nature. I wanted to understand what is the nature of an emotion. Now I understand an emotion can best be understood as a configuration of cognition. A number of parameters like arousal, parameters of focus, narrow inside or outside, deep or shallow and they all define an affective state together and this affective state gives rise to a certain space of cognitive configurations and then each culture discovers eigenvectors in that space and a region in that space and defines them as emotions. So they're perceptual crystals in the configuration space of cognition. It was an insight for me, it was one of my first insights when I started studying AI and exploring it by myself. And the nature of intelligence is the ability to make models and what actually is a model. The meaning of information. What is this? It's a relationship to changes in other information. And discovering these relationships means making a model of the world. And then discovering, oh my God, I'm not living in the world out there. The world out there, that's a weird quantum graphic pattern generator that generates patterns on my retina. And my brain is a machine that counts these patterns to predict future patterns. And the relationship that it discovers is exactly this. So when I see a blip on my retina, the meaning of that blip is the relationships I discover to other blips on my retina. This is how I make sense of this. And these other blips happen at the same time or at different times. And I integrate all this into a function that contains people moving in a world. And these people in a world are not objectively out there. It's just the best model that my mind has discovered to predict the patterns on the retina. And the stuff out there looks completely different than what my brain is doing. It actually doesn't look like anything because there are no colors and sounds and so on. These are mathematical models that my mind is generating over those. This was a very big insight for me. But hadn't you known that before you started to do AI? Wasn't this your baseline actually? No. No. I mean, I'm still fucking stupid. I'm just 43 years old. There's so much to learn and our culture is so confused. For some reason, our culture doesn't have a good idea about core concepts like what is thinking, what is dreaming, what is imagination, what is the nature of creativity. That's the jumping over discontinuities in a search space, I think. Usually creating a new search space with different principal components. Well, actually, yeah, you might be right that I myself have learned something from AI. It's like, yeah, it's for me, like when I was a student, for me, it was impressive to see like the universal approximation theorem from Hornik and Saibenko, which just tells that I can, you know, some kind of basically sigmoidal activation functions. And for me, that was like, wow, it's to prove that we can work that way. But then there was nothing you could do for the next 10 years or maybe 15 more years, how to computationally, you know, really work with this until you know, the deep learning, you know, buzzwords and until that moment. And at that moment, we realized that it takes super simple networks to, you know, to approximate such seemingly complex functions like facial recognition, where before that I would say, yeah, facial recognition, it's like, you know, you would have to need billions of neurons. And suddenly you see that out of the million dimensional space of one megapixel photograph, that actually contains all possible configurations of how we sit here, you just, you know, just generally randomly generate pixels. Suddenly, if you give it enough time, you smiling there would appear there. And you know, the dimensionality reduction mechanism that reduces this to a set of 500 faces I know, is super simple, as we have learned from any, you know, deployed facial recognition algorithm. And this was, I think, for me, this was first uncovered empirically, that engineers just created those systems. And then they started to ask, like Max Tegmark in this last year's paper, why deep and cheap learning works so effectively. And maybe that's my answer to that, what I have learned. That Actually, yeah, what we have here in our heads is like evolutionary tuned prediction, a computational mechanism for predicting short term outcomes of like. And long term outcomes. Even long term outcomes. Yes, it's a universal function of proximity. I mean short term in the cosmological science. We don't care about galaxies, but we care about the... Actually I do. Oh, okay. Okay. Yeah. But that might be actually the thing that you are able to care about them might be a byproduct of the fact that you were evolutionary created to be able to escape a tiger because you just out of the infinity trajectories that he could go, like turn in the right angle. You just know the constraints that no physical system can just, you know, hunting you can just turn that way. Of course. And so you kind of, the part of matter that forced the body, that created the bodies of our ancestors, just learned to model the constraints of the system surrounding us. And very effective predictors are just those neural networks. And yeah, actually, I'm answering to myself. I've learned this through AI. See? I hadn't known that before. I hadn't known that. OK, so cool. Yeah. Yeah, OK. So I'm taking back my suspicion that we cannot learn anything about ourselves. Actually, I would take a step back from the whole thing and think that next to the thing that we build societies to feed people and to feed even larger number of people and create better plumbing and more creature comforts, what we also do in parallel to this is that we create something like an intellect that spans many centuries. And it's basically the attempt to create a model of the universe that integrates all our knowledge and understanding like a global optimum in the space of modeling functions. And the creation of that intellect, which includes physics and mathematics and all the other intellectual communities and the unification, is something that takes many centuries. And humanity really flourished locally in states, which allowed a civilization to sustain an intellectual tradition for more than a few hundred years. This intellect was torn down every few hundred years, at best. And then later on rebuilt by the knowledge workers of the generations that came afterwards. And they usually tried to mimic the shape of the previous intellect without getting the foundations right, and this created ruptures. And our own culture, our own societal intellect is the result of several such upheavals, and there are several breaks in it. So these are the breaks that make it impossible for us, or very difficult to have a cohesive understanding in our social hive mind about things like what is thinking, what is a soul, what is a mind, what is consciousness, what is meaning, what is God, and so on. These are all questions that are incredibly confusing to us. And now we have found ways to start making sense of this, and again, fix our foundations. And the super exciting thing is that we now live in the time where we are approaching the creation of this global intellect. And we notice it's something that does not only span generations, goes beyond what an individual can understand, but it's something that will result in machinery to take over from us and model truth. So basically the societal intellect that models the world is going to be an AI. It's going to be a machine understanding of the whole universe and the conditions that we find ourselves in. To me that's very exciting. Yeah, yeah, that is. Maybe, well, maybe last question, kind of off topic, but maybe not so off topic, definitely not so off topic. I don't know, well, I don't have a knowledge set good enough to judge, well, one of the recent books that I've read, it's from, you well know Harari, it's Sapiens. I don't know if you've read it and the- No, I haven't read it yet, it's in my reading list. Okay, so yeah, so maybe I'll ask you next time, because some things that you're saying Kind of remind me of the lines of thought that I can see that It's it's I'm not I think I'm not able to summarize it neatly like in front of the camera maybe Okay, my own thoughts. Well, I'll just summarize. Well, I start from the end. I kind of like that book and there are many people, like friends, you know, my friends who do like evolutionary biology and stuff like this, they just hate the book. And they're saying that, I mean, he, you know, stole half of the ideas from others and he just compiled them in a way that... So like a philosopher? Yeah, kind of. Or a mathematician. But it's like, I'm okay with this because I could not compile the works of others, so I'm okay... Actually, that's our job. There's very little that can come up in our lifetime. I wouldn't have even discovered the difference between cats and dogs. All by myself in one generation. So, I'm not sure about, and I haven't read his follow-up, like Homo Deus or something like this, and I'm not sure about that, but I liked, in my ignorance, I liked the line of thought that he had with the sapiens. He was discussing, okay, you've got Homo erectus here for 2 million years or something, and how come that after less than 70,000 years, some very non-remarkable subgroup of Homo species somewhere from Africa suddenly starts to be so super effective in some things. And he, I don't know if it's his ideas or someone else's ideas, but he proposes that it's about their ability to conceptualize fictitious entities, to conceptualize non-existent things. And that's it. That's basically it. So Homo erectus was probably able to discuss how big the lion is, and very sophisticated things, probably, that helped them to coordinate groups up to 150 individuals or something, which is probably comparable to other apes, to other primates. But sapiens just devised something, conceptualization of things that do not exist, but that help the groups to sort out the coordination problem of, you know, groups much bigger than 150 entities, you know, that help them initiate trade, for example, with other groups or exchange of things. Because if, you know, probably the group of homo erectus met somebody, other group, they just either kill them or, I don't know, enslave them. I don't know how it works, basically. But here you have to kind of create a concept maybe of trust, for example, into something you share, some non-existing fictitious entity you share with others, maybe a god or whatever entity, and you just trust everybody who shares with you this concept. It's actually much darker than this, I think. So first of all, this point that you brought up with respect to imagination, I think we can see that crows and cats have imagination already. So they are able to perform things on the world that they haven't seen before. Like a cat may be able to deduce that in order to get to the milk, they need to throw down the pot because it has maybe a narrow bottle, and they need to throw down the bottle and step out. And are they able to communicate it between individuals? No, they're not. But what they can is they can imagine things. I don't know what crows can communicate. I mean, they talk a lot to each other, but it seems that when crows make a hook, they don't do this just with random experimentation, but they actually have a plan, which means they have a world model that allows them to examine particular possibilities in the world and then directly go and make an experiment to see how they work out. So they already have this imagination of possible worlds. And actually when you think about it, every decent animal should have this because it's very expensive to test what happens if you should run on the highway or not. You should be able to imagine this to learn from this. And the smarter animals are able to do this. And all the human is probably capable of imagining what happens when you run on the highway after generally observing intuitive physics and maybe you should try to avoid it. So imagination itself is necessary but not sufficient. And then you get a few steps further like language and so on and we can show with games here with even tribal organization and so on, and we can show with games here, with even tribal organization and so on, while being necessary, evolution of a reputation system is necessary, but not sufficient to build a scalable society. Because a reputation system breaks down after a few hundred individuals, because you can no longer keep track of who did what to whom. So you end up with all these prisoner's dilemmas that will limit the efficiency of your tribe, the scalability of its organization. And I think the adaptation that made Homo sapiens more successful than all the other hominids, including Homo neanderthalensis, was not just, you could call it trust, was the programmability of Homo sapiens, it was the limitation of rationality, it was the ability of individuals to give up rational agency to groups, to basically no longer believe things based on the evidence, but based on the social status of the members that gave them that particular belief. But that's actually trust in fictitious entities. That's an artifact of this. But we have shown that there are societies can exist where people walk in lockstep without believing in a fictitious entity. So that's the creation of a fictitious entity to align the purposes of people is one way of doing this, but there are many other ways to create ideologies and make people walk in lockstep. But my hunch is that our success as a species was predicated on the ability to align purposes beyond rationality by limiting the intellectual and policy agency of the individuals to run hive minds on them, to make them programmable. It is domestication of hominids where you never become a full-grown adult that makes all the decisions by themselves based on what they understand about the world, but where you always are willing to give up some authority about your beliefs and your policy preferences to something above you. This is maybe the core adaptation.", '13.995296478271484')