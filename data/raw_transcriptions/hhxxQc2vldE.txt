('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/hhxxQc2vldE"> </iframe> </center>', " Howdy, this is Jim Rutt and this is the Jim Rutt Show. Jim Rutt Show Listeners have asked us to provide pointers to some of the resources we talk about on the show. We now have links to books and articles referenced in recent podcasts that are available on our website. We also offer full transcripts. Go to jimruttshow.com. Go to jimrutshow.com. That's jimrutshow.com. Today's guest is Yoshua Bach. He's vice president of research at the AI Foundation. Previously, he's been a research scientist at MIT and Harvard. He's the author of the book, Principles of Synthetic Intelligence. Psy, interesting kind of joke at multiple levels, an architecture of motivated cognition, and he's published many papers. I should also point out he has one of the most interesting and entertaining Twitter feeds I know of, at Plinz, P-L-I-N-Z. Check it out. This is the second time we've had Yosha on the show. Last time we talked quite a lot about AI. We talked a little bit about his micro-psy project. Today we're going to kind of split to the very high side and then get down to some details on micro-psy. So I think we're going to start off talking about the biggest of questions or one of the biggest of questions in this field is how does mind arise from brain and can it arise from the brain? And I think that's a really good question. And I think that's a really good question. And I think that's a really good question. And I think that's a really good question. talking about the biggest of questions, or one of the biggest of questions in this field, is how does mind arise from brain? And can it arise from an artificial brain? Josje? I think it can, obviously. And so, the question is, of course, what is mind? And there, sometimes we need to clarify our terms, not so much to usurp them, but to make sure that we are talking about the same thing. So typically when I say mind, I refer to essentially the software that runs on the brain and that enables everything else. And what is software? I think that software is best understood not as a physical thing, as something that has an identity, but as a physical law. It's a very specific physical law that says whenever you put things together in the universe in this particular arrangement, the following thing will happen. And the following thing is a description in terms of certain macro states that have a causal structure. And in a sense, you could say that, for instance, the text processor in your computer doesn't have an identity. It's a physical law. And a physical law says that when the gates in your computer are in this and this arrangement, they can be described as a text processor. And when you interact with it, the following thing will happen when it's currently in this and this state. So the same category of things is mind. It doesn't have an identity, it's a principle. It's a software that runs on your brain. And it doesn't run on your brain because it is corresponding one-to-one to a configuration of atoms in your neurons, but because there is a coherent causal structure emerging over the activity of the neurons that you can use to describe it. It's a lens to look at the activity of many neurons. And there is not a reason why this should not happen in other substrates that are under the same functional constraints and can implement the same principles. In the same way as we can implement software on many, many different types of computers, as long as brains are Turing-complete, you can build minds on different substrates if you are implementing the same causal principles. Of course, people would argue that human mind, the human consciousness, we'll make a distinction later about consciousness and mind, but let's for the moment keep them together, is strongly embedded in its bodily substrate. And that, in fact, some people like Antonio Damasio would argue that the actual root of our conscious being is deep in the brainstem and is more related to our body, actually, than it is to our higher brain. And if that's the case, then while we could create artifacts that were analogous to the mind in some sense, they essentially can't be the same if they're not embodied in the same kind of structure that the human mind is embedded in. If I change the interface that you have to the universe, so you are situated a different way, for instance, you are no longer located in your body, but you are connected to sensors all over a city, and your incentives are aligned with the incentives of the city, and you also identify with these goals and so on. You might turn into a city at some level, right? And this still means that you have a mind. Even if I change your affordances, in the same way, if I limit your affordances to things that are internal to your own mind, as it happens, for instance, you're having dreams at night, it's not that you lose your consciousness in that moment. And I think that you can also be perfectly conscious while playing Minecraft. And even if you are embedded in a VR that only gives you access to Minecraft, that probably has enough complexity to, without loss of generality, be sufficient to be conscious and to form a bond to an external world. We can take Minecraft and implement this on a chip. We can implement this on neural tissue in principle, at least, and implement this within the brain itself. So you never need to leave your brain and go into your physical body in order to have the experience of the body. Your embodiment can be entirely virtual. What you need to have is an implementation. The implementation needs to have a realization on the physical substrate layer of the universe. But it doesn't mean that you need to have a physical body that is exactly human-like. You only need to have human-similar fortiances if you want to end up as a human-like mind. Yeah, the old brain-in-the-bottle argument, right? But particularly your own work has put a lot of emphasis on emotional states and emotional valences, etc. And there's a substantial school of thought that says that the reality is that the physical reaction of emotion, such as heartbeat and respiration, skin tone, et cetera, actually happened before the feelings, i.e. the cognitive states associated with emotion. So you'd have to also essentially have inputs that, you know, directly, at least for humans, if you want to try to extend them to a non-body state, brain in a bottle, you'd also have to essentially fake out the physiological, emotional responses, wouldn't you? We probably noticed that consciousness is not in charge when we control our interactions with the environment. Some meditators say that consciousness is like this little monkey sitting on top of an elephant and it can direct the attention of the elephant by prodding it. But eventually, at the end of the day, the elephant is still going to do what it wants. And if the monkey thinks it's in charge, then the monkey is often in for big disappointments when it notices that it makes decisions, but the elephant is not reacting to them. And this elephant part of our mind seems to be much older, especially with respect to the high level concepts that the monkey is developing to analyze its own behavior. The attentional control system is old, but the analytical systems that the monkey employs as result of its analytical and attentional reasoning and so on, They are rather young. And I think that feelings are a way in which the emotions and motivational impulses of the elephant are accessible to the analytic monkey. And so in this sense, you could argue that there is a lot of stuff that precedes the feelings because they are a way of communicating between subsystems in the brain. It's the perceptual system that does the majority of operations in something like a distributed neural network, this nearly continuous dynamics, and then this analytical grammatical engine. How do you interface these two systems? And you basically need to take the features that are being computed in this distributed perceptual architecture and translate them to the localist discrete analytical model. And you do this by projecting them into a space. And the only space that is consistently available is the body map. So feelings are projected into the body map to disambiguate them. You could also take the perceptual space just outside of the body-mat, but this changes all the time and we would not self-assign it. We could also have a specific emotion space that is distinct from the body-mat. Why don't we have feelings outside of the body? It's a weird thing, and I suspect it's because feelings have been implemented as an afterthought in evolution, so they had to be mapped into an existing brain region. So when you feel love in your heart, anxiety in your solar plexus is not because your gut is involved in these computations. Your gut is completely occupied with hurting bacteria. I think it's simply a projection that happens. And if you are a paraplegic and you don't have access to sensations in your body, you still feel emotions in your body. Yeah, the linkage between emotions and feelings is interesting. And at one level, you could say they go both ways. Again, there's been some very interesting psychology lab experiments where, for instance, I think one of the more famous ones is a person who walks across a dangerous bridge versus a non-dangerous bridge and then meets an attractive member of the opposite sex who asks them some questions for a questionnaire. And the probability of a male to ask for the questionnaire person's phone number to ask them for a date is way higher if they go across the risky bridge than if they go across the not risky bridge. So in a sense, the bodily sense of fear or challenge operating unconsciously, well below the conscious mind, through the body itself seems to program the mind itself into a different state. Yeah, it's interesting to speculate why that is. If it's simply that an achievement to deal with a subjectively life-threatening situation leads to a higher sense of competence, as the psi theory would predict, and this basically would lead to greater risk-taking behavior in the case of that agent. But to see if that is actually the reason, we would need to look into this in much, much more detail. I've read about these experiments myself, and I haven't really made up my mind what the actual cause is, and which systems crosstalk and so on. Why is it that we fall more easily in love after we have been in a life-threatening situation? I don't really know, even though I can easily come up with a few hypotheses, which is the dangerous thing. Yeah, exactly. Well, why don't you let's do it. Come up with one hypothesis. Love to hear the Yosha Bach take on that famous result. Yeah, so the easiest hypothesis would be if you basically master a risk, then your willingness to deal with future risks increases momentarily, because you have a higher self-ascription of competence. It's also the case that you put lesser value on the perception of risks that would normally fill your attention. Basically, you normalize risk. If you are, for instance, find yourself with cancer in a hospital, and you are threatened with possibility of death, then all the things that normally would give you existential fear, like losing your job or losing the friendship of a person you don't particularly like and so on, suddenly are no longer important. And as a result, you might start ending relationships that you're no longer interested or asking somebody out that you would never dared asking out because the risk is just not that high compared to the actual existential risk that you have just been facing. So it's basically a rescaling of your perception of what's important and what the valence of the important things is. That makes a lot of sense actually and again I know in your architecture and in other architectures, those states decay relatively rapidly. And that's why it may manifest in that particular experiment, where more or less immediately after having a challenging experience, the test is given. If it came back two days later, it might not be the case. Yeah. So in some sense, the prediction would be that for most people, there is a range of emotions that adapts to the situation that the agent is in. So you have roughly the same emotions and roughly the same proportions as long as you don't suffer physical pain or actual threats day to day that put you under real stress. As long as you are existentially safe, you feel roughly the same things at most of the times. And your emotions will adapt to the range of events that happen to you. So it's a normalization that happens in this way. And if you introduce new events into this stream of events, or if you change the stream of events, then for instance, the amount of anxiety that you experience about things might still be the same, even though the events are quite different. And which means you assign different valence to the same events, if they are in conjunction with others. It's probably an interesting lesson there for organizing your own life. So for instance, many people suggest that you always make sure that in all ages you introduce a lot of novelty into your own life. For instance, many people suggest that you always make sure that in all ages you introduce a lot of novelty into your own life. You always remain young, you always store new experiences and information and put value on them, and that you will undergo frequent changes of your environment. I think I've been pretty good at that in my life. I'm always getting into some shit or other, right? I never stand still, right? Congratulations. Let's see, what else? What are we going to talk about next? Get your thoughts on some of the other theories of mind. I'm sure you have them. One theory that was prominent, I don't know, 20 years ago is that mind was essentially a brain-wide set of interlocking frequencies, some of them phase-locked and some of them rhythm-locked. Any thoughts on that theory? So, when I was confronted with Singer's theories about that, that consciousness is something like a frequency in the brain that was also adopted by Christoph Kork, I didn't understand it. that by Christoph Koch, I didn't understand it. And mostly because it didn't make functional sense. It didn't explain how this is necessary and sufficient for producing the phenomenon. I thought that oscillations in the brain are necessary because neurons are not able to be in a constant state and make that constant state to each other. They need to fire. If they are connected in some causal structure, they need to fire. If they are connected in some causal structure, they need to fire in synchrony, right? There will have to be a wave of activation that passes through the brain. This wave will have to be periodic. So you will look at cyclic activations in regular frequencies. And these regular frequencies, that if you can pick them up on EEG, it means that large parts of your cortex are firing in sync. So they don't disturb each other and you can see a common signal. But the signal would be the result of the synchronization that leads to consciousness. It's not that the synchronization itself causes consciousness. And there is, on another hand, some interesting insight from the perspective of a neuroscientist who thinks about how is it possible that neurons solve the binding problem? How is it possible that they talk to each other at all? And then you can think of neurons as something like, for instance, like a neural lattice that is acting like an ether. And the signals that are being broadcast to the brain are moving like electromagnetic signals would move through an ether. This is a model that you can use as a crutch to imagine what's going on. And if that happens, the question is, if the signals propagate with this neural lattice, how do the individual neurons tune into the different programs that are being played out in the brain in parallel? And it's tempting to think that there is something like RM or FM encoding, basically some encoding where the neurons know at which frequency to tune in to become part of a certain computation. And maybe this is a viable model. So it's a viable model for how the brain solves the problem of signal transmission and synchrony of different processes over large distances in the neocortex. It's probably one of the mechanisms that plays a role. But it's not necessarily the best metaphor or the best engineering principle for an architecture that we would implement in a digital computer, because in a digital computer, we can have random access where we can relate everything regardless of spatial proximity in the computer, because we can have a bus with a very high throughput of data. And the individual memory cells in the computer can hold arbitrary values without any need for oscillation. Yeah, I always think that's an important point when you're trying to map from the biological to the machine. Mother Nature solves problems in a very odd way, you know, a teeny step at a time through evolution over billions of years. And it's amazing the stuff works at all. And if we're going to design one from an engineering perspective, in many cases, we would make very different decisions that we might get some functionally similar results. Back to, you know, brain-wide phenomena, one of my favorite cognitive models, and he very honestly says, I have no idea how the mechanism works, is Bernard Barr's global workspace theory where he believes that the representation of the sense of consciousness, for instance, the sensorium that we live in, the movie that we live inside in, is somehow broadcast to wide areas of the brain and the various functional areas of the brain are then able to process that information. What are your thoughts about Barr's and his global workspace theory, even though he himself admits he has no neurological argument to support it? I think it's partially born from intuitions that are the result of introspection. And I think that introspection is not given enough credit as an important tool in neuroscience and philosophy of mind and psychology. There is no real methodology for introspection, I think. It would be great if you could develop that further. And it does exist in the different meditation schools, of course, as an important tool. I think that we don't distribute consciousness across the brain into the different areas, but it needs to be the other way around. The core feature of consciousness is that we remember what we paid attention to. If we don't remember that we paid attention to it, then we are not conscious of it in hindsight, right? So it needs to be integrated into a common protocol where it can be accessed later on. And this integration means that it's somehow local. So it's the localization of information that before existed in a distributed way in the brain. So it's the protocol that pulls from different regions of the brain. It's interesting to contrast this and to compare this, this notion of attention in machine learning that is currently gaining in prominence. Yeah, that is interesting. My own work that I do in the area of scientific study of consciousness, attention has always been absolutely key in my own model. And as you say, it's quite interesting that in the newest neural net architectures, they're using something they call attention. It isn't quite the same, but it's at least analogous. There is something that is very close to the attention that we have in our own mental attentional learning. The difficulty is how can you train a system that is not organized into neat layers? Our brain is not a system that's organized into neat layers, so we could train it as a whole using Bragg propagation with stochastic gradient descent. And even if we could, it would not be the right way to do it because it's not very efficient. It takes an enormous amount of time to train a system, many, many operations, to get it to do the same thing that a human being can do. So arguably, a human being is much, much quicker in converging on a model of interpretation of visual stimuli than a neural network is. Yeah, in fact, I've used that as a very important probe in my own work, which is the way the human brain does it must be rather different than gradient descent or other similar methods that are used for learning in neural nets. Because I may have mentioned this on the last show, one of the things that really got me interested in thinking about this problem of learning mechanisms, that's when I was playing a very advanced and difficult war game. I don't know, this was about 2013 or 14. And I realized that I was able to learn the game well enough to beat the pretty good AI really quickly, like after playing seven games. And I realized I was using transfer learning from the other times I'd played war games of different sorts. And I started adding it up. How many times had I actually played relatively sophisticated war games to a conclusion of a game in my life. And the highest number I could come up with was 5,000, and probably the real number was something closer to 2,500. So a remarkably tiny number of examples of analogous games that I could presumably mine for transfer learning, and then apply it to this new game, and then learn to play it at a level good enough to beat the AI after seven games. And we know that today's artificial neural net architectures not even close, even the new good stuff like AlphaZero still take hundreds of thousands to millions of plays to learn even a very stereotyped game, chess or go or something like that. And these games are vastly, vastly higher dimensional space than those games. So high you can't even contemplate how high they are. I figured at least 10 to the 60th move possibilities per turn, just to give a rough sense of it. So, yeah, I think that the next frontier in thinking about learning and cognitive systems and what we can learn from humans is that the rate at which we learn as animals, and not just humans, of course, animals learn very rapidly as well, have to be somehow fundamentally different than what we're seeing so far from the world of artificial neural nets. We can learn things super fast if we can pay attention, right? The difficulty is how can you learn how to pay attention in the right way? The difficulty is how can you learn how to pay attention in the right way. There is a famous video on YouTube where a coach trains somebody who doesn't know how to play tennis to play a relatively decent tennis within the space of half an hour. And this is because he's able to direct the attention of the subject in this very great acuity and tells her exactly what to pay attention to. So she quickly updates her behavior in the parts that count. And if you don't know what to pay attention to, you have to, in some sense, brute force the problem by applying the error function very broadly in your system and hoping that the errors accumulate eventually in those parts of the architecture that make the biggest difference. But it's a very wasteful way of doing it. When I was in New Zealand in the 90s, I worked at Ian Witton's laboratory. It's the same one in which Ben Goertzel studied for a time and Shane Leck of DeepMind, which Ben Goertzel studied for a time and Shane Leck of DeepMind, which is a happy coincidence, we never met there. But Ian Witton, I think has a great and underestimated influence on modern artificial intelligence. He's the father of arithmetic coding and he takes a data compression perspective on cognition. And while he never prominently took a stance in terms of cognitive science. His main motivation was to understand how minds work when he studied these issues. And so when I was working for him, he put me in a lab and asked me to find structure and language automatically. So what I did was first look at Ngram statistics, and I realized that adjacent look at n-gram statistics and I realized that adjacent words in English language, there are so many different words in the English language, that you cannot really do good statistics over, for instance, trigrams or quadrograms or pentagrams. You mostly are restricted to bigrams, to pairs of words. And this means that, for instance, you'd lose the predictive power that an article has for a noun if there is an adjective between the article and the noun. And if you have multiple adjectives between the article and the noun, the earlier adjectives are not going to predict the noun anymore. So you forget, you'd survey most of the structure. And our own minds don't work like this. They do identify the structure over the entire sentence. So how can I discover the structure? How do I do the statistics? And I started using ordered pairs of words that didn't need to be adjacent and tried to go beyond that. It was very difficult because you have such large statistics that you need to do. And in a way, you need to do curriculum learning like our own learning does. So we give children very short and simple sentences that are short and don't require a lot of pointers to maintain so the child knows what to pay attention to. And then we go to the next one and the child dismisses sentences that are too complicated to understand for the child. And if you try to do this in the learning system, you typically don't have these nice data sets where a human being interacts with the system and teaches it language as we teach our children and make sure that the problem is learnable. So instead we need to have a mixture where the system is trying to have candidates for meaning from complicated sentences and then builds up from there. And back then the memory that I had in this computer and the cleverness of the algorithm that I had available was very, very limited. I was able to discover grammar eventually, but I was not able to discover style in the way as GPT-3 can do this. And when I looked at the transformer algorithm that underlies GPT-3, it was fascinating to see to me that they basically found a solution to the same problems that I had to deal with in the 90s and couldn't solve by myself. And this is to basically make statistics over all the parts in a working memory and find out which ones of those are related to each other. And it's still too simplistic because it's a fixed working memory window. It's not able to throw things out of its working memory and put things in it. Instead, it has 2048 adjacent tokens in the working memory. No more, no less in this implementation that they use in GPT-3. And so, for instance, you can use this algorithm to interpret images, but only very, very small images. So those that don't have more pixel elements than those that fit into this fixed working memory. And so this algorithm in its current form is not able to comprehend large images or video. It's not able to relate the early parts of a book to a late part in a book. It's only able to keep two pages in memory at a time and relate concepts within two pages to each other using massive computational power. Yes, I played a little bit with GPT-3 and it's both remarkable and frustrating, right? You go, wow, what it can produce in terms of emulation of styles and plausible completions, etc. for, as you say, within a small domain. Frankly, I see it not working well at the, if you try to get it to generate 2000 characters of text, you're pretty much out into garbage land pretty quickly. But you know, in a shorter frame, two or three sentences, it's remarkable. And yet, and yet it really doesn't have any language understanding. It's just, it seems like it's just the biggest so far collection of associative statistics. I think we have to give it a little bit more credit. So I wouldn't say that it's a real shot at AGI, at least not in its present form. But it is able to understand certain things, or at least consistently act as if it understands it, which means I can give it a task. The way that I give it the task is weird, right? It's all auto-complete. So I give it the beginning of a text about the completion of the task. And then it's able to continue that text. So for instance, I could say the following is an extraction of sentiments from paragraphs. Then I give it two examples. And now I give it new paragraphs and it's going to extract the sentiment from these paragraphs. And if that is simple enough, the system is quite reliable. So I can ask it, for instance, to one of six key emotional categories in a part of speech. And whenever I prime the request to it with telling it that it's currently continuing a text that is extracting these sentiments from paragraphs, it's going to do that just fine. And this in some sense is a semantic operation. It does understand at some level what it means to extract a sentiment from a paragraph. It's only not related to a global model of the universe, a global sense of meaning that we have. When, when you and me understand something, it means we have found its place in the universe of that thing. Of course, the universe that we are talking about here is not the physical universe, which you and me probably don't really understand. Physics doesn't really understand it yet. Or at least those physicists which don't understand it yet have not found a majority that lets everybody of us understand it in the same way. And so what we mean by the understanding of the universe or the universe itself is our model of the universe that we have in our own mind. And this model of the universe means, okay, there is this roughly three dimensional world that has gravity in it and four forces. And most of the interesting, juicy stuff is electromagnetic including light and sound. And it's all organized in aggregates of matter that we can interact with in a certain way and it implements us and we are contained in it. And it's implementing the following set of mechanisms more or less, right? And in this big interconnected model that connects everything with everything about the universe that we are part of, we find a relationship, a causal structure that explains how that thing comes into being in a certain way. And GPT-3 is not doing that. It does not have an idea about the universe that it's part of. It's just babbling. But it can create plausible universes to some degree. It's not fully coherent. At some point, the coherence falls apart, and much faster than it does in our own minds. Most people are not fully coherent, right? I think maybe no human being is fully coherent. But the incoherence of GPT-3 is much, much more blatant than ours. Maybe we can improve this by putting a coherence loss on its learning instead of just a surprise minimization loss. Maybe, but of course, what I think you're talking about here is the famous symbol grounding problem. GPT-3 doesn't have its symbols grounded at all. No, it has them grounded in language. Yeah, as you say, they're the self-referential back into language. It's not grounded into... Yes, it's only language, but it does have a proper grounding and language for many of the operations that it performs. So when it's able to do two digit arithmetic quite reliably, there is, in some sense, a grounding by treating the symbols that it manipulates as symbols that are subject to arithmetic operators that are properly evaluated, right? So as a functionalist mathematician, you should be satisfied. It's not mapping this onto the same understanding of axiomatics as proper mathematicians do it, but arguably not all mathematicians do have the same understanding of mathematical axiomatics and this doesn't make them completely impotent as mathematicians. Yeah, though of course we know that GPT-3, while it does fine on two digits, starts to fail at three. Yes. Then again, it's trying to predict likely texts, right? There are not that many human texts that have good arithmetic over long numbers, because we don't write texts about this. It's not a human activity. And so it would be inhuman to do that. Yeah, it does not understand, for instance, the idea of the algorithm to do the arithmetic. One time, I think it was just for fun, I wrote the algorithm to do multiplication, manipulating text strings. Just because I wanted to make it clear in my head how to do it, it took me a day to write that. It wasn't that hard. But GPT-3 doesn't have anything like that. It has no ability to generate anything like that. If you ask a machine learning system to find a decently short state machine that explains all the arithmetic that we throw at it, then the system is quite likely to discover the operations that we want to discover quite quickly. There are deep networks which are able to do better integration and differentiation than Mathematica and able to solve things in a short amount of time or things that Mathematica, a symbolic system that is handcrafted using proven mathematics, cannot do. So it's able to learn things that mathematicians can do, but mathematicians don't yet know how to do in the right way. And it can learn a learn things that mathematicians can do, but mathematicians don't yet know how to do in the right way. And it can learn a few things that mathematicians maybe cannot do yet. And GPT-3 is just not trained for doing this. And the implication might be that Carl Friston is wrong. That the free energy minimization, which comes down to a surprisal minimization translated into the terms of physics, is not the right principle to build a mind. It could be that you could still say that you can minimize surprise a little better if you are able to assign the right value to your meta-learning algorithms, but you cannot make those proofs if you are a simple algorithm. And I think that's the reason why simple organisms are not using a single principle to organize themselves, but have multiple needs. And these act as reflexes that satisfy many, many dimensions of needs of the system and the environment, and they are evolved. And we can probably supersede most of them. They are reflexes. And though, interestingly, and I think this is important when thinking about how to get to AGI, it's one again, one of my own little design principles, is that when you look at things from a symbolic perspective, at least, attempts to solve these high dimensional problems that even a very relatively simple organism has to solve, inevitably run into the combinatoric explosion of options problem. One of the things about these simple-minded algorithms is they essentially collapse all that. And I'm relatively convinced that our attention algorithm is essentially a hack to get around the combinatoric explosion of options problem. Let's move on to another topic. We're talking about ways of thinking about how mind emerges from matter. Another one that's become popular over the last eight or ten years is Tononi's integrated information theory. And, you know, Christoph Koch has also been a supporter of that. You know, it's a head-scratcher. It's kind of interesting at one level, but it's relatively easy to create high I.I. scores that are clearly not a mind. What do you think about the integrated information theory with respect to mind emerging from matter? I think if you go to a workshop of the integrated information theorists, it's a little bit like going to a climate denialist conference. The fascinating thing is you look at the conference program of the people that don't believe in global warming is that they don't agree with each other. There are some which will argue that there is no global warming, others will say that global warming is not man-made, some will just point to errors in the way that the mainstream of the scientific community talks about global warming and so on, and they all accept each other's papers. Why is that, even though they are so fundamentally not on the same page, there is much more disagreement between them than there is disagreement that they basically have with the mainstream. And it's because they are defined by the opposition to the mainstream. And the same thing happens with these workshops about the IIT theory, that is the integrated information theory. They disagree about how to compute phi, for instance, this measure for integrated information on what the status of phi is, on how relevant it is, on how to interpret the concept of integrated information in the physical universe and so on. They are mostly opposed to functionalism. And they try to find an alternative to functionalism. And I suspect that's the main driver for the popularity within the IIT community. Within the physics community, it's probably the reason that Max Tagmark has decided to make this one of the planks in his boat that tries to integrate over everything in the universe from the perspective of a physicist. And so in some sense, the adoption in physics that this is taken seriously is probably a little bit political. I have great personal respect for Tononi. He basically is a neuroscientist that earns his money and respect as a sleep researcher. And he is also a philosopher and an autonomous intellect. So it's somebody who is not part of a particular school in philosophy. And as such would be a hard time being accepted in philosophy. He has a new solution to an age old philosophical question. And if that is the case, it's usually bad news because all the good answers that you can find without labs have been around for a very long time. You just need to find and translate them into the modern language or into your own concepts. And so if he has found a new solution, the question is, what is the solution for? What does it really express? And the suspicion that I have is that the core of a theory is not really being published. That was also the impression that I got from personal interaction. The core of a theory is not phi, it's not this measure. This measure was introduced as a need to produce a theory with quantifiable statements that can be mapped to experimental predictions, because that's somehow seen as the gold standard in the sciences, but not in philosophy. For him, it's more an attempt to find a theory that explains or that talks about panpsychism and reintroduces it. And it's mostly because he doesn't see how functionalism can solve the problem of consciousness. And he does latch onto this idea of a distributed information in the brain, but the distributed information that happens in the brain is probably not the same thing as it happens, for instance, in quantum mechanics. And I don't think it's non-computable. And it's a little bit ironic if you try to make a information theoretic theory and phi is an information theoretic measure for consciousness that is not functionalist because functionalism and information theory are strongly intertwined. It's an oxymoron to make an anti-functionalist theory using information theory, I think. It would be helpful for our audience if you could describe what functionalism means. I could try it, but I think you could do a hell of a lot better job than I. So very simply put and very cursory is that functionalism treats a phenomenon as a result of its implementation. So for instance, what is a bank? A bank is a thing where you can have an account and you can have an interface to it, you can store money in it, you can extract the money from it, it is conformant to a certain legal interface, it's giving you certain guarantees that are compatible with the legal and monetary system and so on. And if an institution is fulfilling all these principles, then it doesn't make sense to not call it a bank. And Dennett uses this as a metaphor to explain his opposition to the concept of a philosophical zombie, a system that is identical in all its features, except not having phenomenal experience to us. And a zombie bank would be a bank that is basically a thing where you can store money and retrieve it again, and you can ask for your account level and so on, but it's not a proper bank because it lacks the essence of a bank. It's a true bankness, the thing that you cannot really touch and that has no causal influence of its interaction with the outside world. And this, we would reject this notion of a Zombank as nonsensical. It doesn't make sense to distinguish reject this notion of a Zombank as nonsensical. It doesn't make sense to distinguish between a bank and a Zombank because there is no essence of a bank beyond its functional interface. And why would this be different for consciousness? Is there some kind of a hidden essence? And so in some sense, functionalism is the rejection of the notion of a hidden essence, of something that does not have causal properties that we can observe. Everything can be explained in terms of its causal properties, and all causal properties ultimately can be explained as functions that are implementable, which in my perspective means computable, realizable in a physical system. Yeah, I must say when I read Dennett's Consciousness Explained, while I didn't necessarily buy his pandemonium theory, I did find his rejection of philosophical zombies reasonably convincing. What's your thought on that? The pandemonium theory, by the way, goes back to Selfridge and has strongly influenced Minsky, who was Selfridge's student, and has led to the Society of Mind. And I think it's a very beautiful theory. It says that the mind is basically a bunch of agents that implement different behaviors. And you can train your mind with new demons. And demon is a good metaphor. In computer science, it's a program without a user interface, in a way, without a user facing front end. But it's an automatic behavior that basically can be instantiated, can run multiple of them, they can interact with each other and produce combined behavior in your computer. And in this pandemonium theory, you have a stage where you have the active behaviors that are currently populating your working memory and perform certain tasks. And there are others that are sitting in the audience and can basically evaluate what's happening on stage. And they can pull others on stage or boo some off stage. And those that are on stage can also call up the alleys, elite behaviors and get them on stage to enact a certain scene. And so it's, I think, a very powerful metaphor to imagine how the self-organization of behaviors in the brain might work. It's not doing much more than this. It's not yet an implementation, but it's, I think, a good start to basically see a self-organizing system that is adapting itself to a variety of unknown situations in the world by composing a team of behaviors at any given time. So this is the power of the pandemonium theory. For Dennett's theory of consciousness and self, I was always wondering why Dennett seemed to be so ineffectual among philosophers and among philosophy students, a lot of philosophy students don't like Dennett very much. And also a lot of philosophers don't like Dennett very much or analytical philosophy for that matter. And I suspect it's because Dennett seems to miss the problem that people try to explain. He is not talking about that problem very much. It's not because he cannot explain it, but because he doesn't seem to think that phenomenal experience is that important at all. It doesn't take center stage in his philosophy. And I think that could be because Dennett is such a nerd. Maybe Dennett doesn't have that much phenomenal experience. Maybe Dennett is somebody who is extremely constituted on a conceptual side. And I can relate to that because I am also very constituted on the conceptual side. It's only that I started observing this difference in people who are constituted on the side of feelings, that are constituted on the side of what the people of science mean are intuitions, maybe pre-scientific intuitions that scientists are very, very of. I think that a scientist or a true philosopher is born as an aberration. It happens when a child is so desperate that it decides to permanently trust its ideas more than its feelings. And this is often the case when you are a nerd. You have the best of intentions. You have very pure feelings about how you should interact with the world. You have compassion for your environment maybe, but you might have poor empathy because some of the wires in your brain are not wired properly during your development. And so you cannot guess the mental states of others. You don't have a sense for the mental states of others. It's difficult for you to figure out what you should be believing, that you should modify your beliefs to fit in and so on. And it's maybe repulsive to you to modify your beliefs just to fit in or to pretend that you modified them because you don't like the innate sense of how important that is to play ball with most humans. And so as a child, you fail in your social interactions with non-nerds. And because nerds are few and far between, it's probably less than 6% of the population on that spectrum, you learn that you cannot trust your feelings when you want to interact with other people. You fail if you do that, right? So what do you do? You need to build rational models for interacting. And I suspect that's one of the reasons why so many people that are very, very good at rational analytical models are nerdy and end up in the sciences and are not very good at intuition and at intuitive empathy and at the resulting social interactions. It's almost a trope that good scientists are poor in social interaction. And it's a disaster, I think, that we are cherishing scientists as a lifestyle archetype and as the goal of the educational system that every child should somehow become a scientist. No, a proper scientist is very useful to society, but is a defective human being if they are trusting analytical truths, which is very brittle and hard to prove, more than they trust their feelings and intuitions. Science works like this. Science works by always finding analytical truths and only believing in what you can prove. But this is not viable for life because almost everything in the world is more complicated than you can deal with human logic. So science is able to deal with the edge cases but it's not as good in dealing with the mainstream of problems that you have in life. Scientific theories don't help with most of the everyday problems. They only help with the difficult edge cases where your intuitions fail. So in some sense, science exists to deal with our darkest emotions in a very literal way, because, or darkest feelings. It's those feelings that are extremely difficult to disambiguate. That's why they're dark. They don't help us, they're our feelings in these darker regions. They don't tell us what to do. And this is where we need logical reasoning. And so I do think that scientists are important. They need to be protected, but they are the result of a particular mindset. And this is also true for people like Dennett. Dennett is not able to talk to people very successfully in the same way as Deepak Chopra does, because Deepak Chopra might not be analytical, completely clean and pure, but he's able to resonate with people at a very deep level where he can tell them what they really care about, what they want to have explained in their everyday life. So in some sense, if we want to make computationalist philosophy attractive to the mainstream, we have to explain to them how functionalism is dealing with intuition, with phenomenal experience, with the sense for the greater whole, with our sense that we are not confined to a single self in a single organism, but that we stretch far beyond that, that we are somehow distributed in the universe. Experimentally, that's true for normal people. It's probably not true for most scientists. Most scientists are confined to their own intellect at some level, including myself. Yes, and I mean, I know a lot of scientists, and certainly in the scientific community, the kind of folks you're describing are overrepresented. In fact, probably the greater ones are even more overrepresented, but it's by no means 100%, probably not even 50%. Probably the greatest scientist I personally have met and spent time with was Murray Gell-Mann, and he's an amazingly social character, right? He has social intuitions. You know, he knows a lot of things that aren't about science and he can weave them into an extraordinarily interesting conversation. Unfortunately, he's now passed. He was a great guy out at our Santa Fe Institute, but, you know, he was certainly an amazing scientist who nonetheless was a quite complete human being. So I would make sure we don't over... Oh, absolutely. Well, I agree. Statistically, it's true. All these geniuses that are very good at all the fields, they do seem to exist, but they're very rare and far between. It's basically people that are able to integrate so well over so many areas that they reach that stage of development in all the areas, right? I also would not say that scientists are in a moral or experiential sense incomplete human beings. They are just different from your run of the mill homo sapiens, or many of them are. Even somebody like Noam Chomsky, who I very deeply respect, or Marvin Minsky, they are people that have very specific minds, right? They are very good at some things, and there are other things that they don't specialize in. Basically, they have indications of Asperger's, of a mind that is hyper-focused on some areas at the expense of others. And there is the suspicion that this hyper focus is the result of a compensation that you get super good at writing books. And Chomsky is probably one of the greatest minds of his generation and also in terms of writing. If you ask him something, he's going to respond with a complete chapter, including footnotes and references. Most other people that I know only think in paragraphs. And actually it's rare. Only good thinkers can think in whole paragraphs. I know some that do. Let's get back, though, to philosophical zombies and Dennett. That's an interesting perspective that Dennett doesn't see the problem. Maybe the real payoff is you go one step beyond philosophical zombies and go to David Chalmers' hard problem, which Dennett also rejects as sort of being a non-problem. And I think that's relatively close to his rejection of philosophical zombies. What's your position on Chalmers' claim of the hard problem? I suspect that Chalmers, in a part of the time sees ways out of the heart problem. But he's still mostly on the side where he doesn't. His current philosophy is often focusing on the question that we don't need to explain the heart problem itself, we need to explain why people think that there is a heart problem. Right, so we need to explain the psychological certainty. And we can come up with theories that explain the psychological certainty. So we don't have to explain the phenomenon how a physical system can have conscious experience. We need to explain why we think that we have conscious experience. That's interesting. Right. And I think that this goes in the right direction. There is something that struck me when I was looking into parapsychology. And I, in some sense, grew up with a part of parapsychology. For instance, in the 70s and 60s, when LSD was around and still not illegal, a lot of people at the CIA experimented with LSD. And incidentally, during the same time, there were a lot of experiments that were similar to, if you've seen the show Stranger Things, the MKALTRA program and related programs, were looking also into clairvoyance and into far sensing, where people would focus their mind on different parts of the world and would use out of body experiences to sense what's going on there. And a number of papers and books came out of this by people that were not completely loopy and that had proper state funding. And when I read this stuff, I concluded this is probably incompatible with physics as we know it. It's certainly not compatible with the standard model because we need more than the four forces that are compatible with the standard model to explain what's going on here. And the quantum non-locality explanations that come up here are not explaining it because quantum non-locality does not allow the transmission of information that normally would require a photon to send. It might be possible that there are different regions of the universe that share state, but you don't know which regions of the universe these are. So you would need to have a classical back channel to that region to know about the entanglement between these different regions. And so it's not obvious that there is some kind of quantum mechanical explanation for psi phenomena. And as a result, either significant parts of physics that work extremely well in the lab are wrong, or these phenomena are not correct. You know, our mutual friend Ben Gertzel is a fan of these psi type things. Yes. He is also incidentally a big fan of psychedelics. Yep, I'll confess to have done a little psychedelics in my day, though not in 40 years, and they are an interesting altered state. So there is a correlation between psychedelics and the ability to look into the future without winning the lottery. Right, there's a weird thing about psi phenomena that don't seem to change the physics of the universe or the statistics of the universe very much. If we could evolve the ability to reliably far sense or look into the future, we should see a lot of animals that do this and predator-prey dynamics. We should have a small subset of the population at least that quite reliably wins the lottery without cheating. Yet the banks still win. How is that possible? It's an argument that, for instance, has been made by Stanislav Lem and I think Summa Technologia in one of his purely philosophical books that don't make a concession to the reader. It's a very beautiful one and where he argues against Psy in this regard. Also, I had a very good friend at my time in Harvard, she's still a good friend, I hope. She had prophetic dreams every night, lucid dreams. It's very difficult to deal with this because she was often able to look into the future during these dreams. And she started writing this down and she noticed that when she was experiencing things that were costly irrelevant, that basically would be the equivalent of winning the lottery, that would change the future in any way, she was unable to write them down. And it's like the men in black basically prevented her from doing that. I think there's only one good explanation for that. The explanation is that your memories are changing retroactively. So you don't remember when your memory changed. You don't remember when your construction of reality changed. She was not able to write it down because she didn't know it at the time that these lottery numbers would come. These lottery numbers come and together with the knowledge about these lottery numbers you instantiate the memory of having foreseen what these lottery numbers were. This is the easiest explanation. Yeah, in which case you actually did not know the lottery numbers, right? You had the memory of knowing the lottery numbers without actually knowing them. Yes, but there is a deeper phenomenon here. So, if the spiritists and the psi theorists and parapsychologists are right, and I'm not completely ready to dismiss them, and Turing was also not doing so, right, it is, if the same as 1950 papers makes explicit and affirmative references to the high probability that telepathy is real. So he even asked the question, could an AI system be truly intelligent if it's not telepathic? Or would this be a sufficient intelligence if it's not telepathic? There is a deeper implication that he doesn't discuss. If telepathy is real, if there is a possibility to use unknown physics to send information across minds that are not in any kind of known signal relationship to each other. So there are adjacent, so you can observe the other one and entangle yourself as their own vibrations in their mind, just using your visual sense or other known senses, right? If that is true, if there is something like telepathy using an unknown physical causal mechanism, how can you guarantee that your consciousness and your mind is computed in your own brain? How can you guarantee that your brain is not merely an antenna and the consciousness is computed elsewhere, maybe outside of the universe, and you are partake in this universe, right? In this consciousness. Yeah, that's the Pedro this universe, right? In this consciousness. Yeah, that's the Penrose hypothesis, right? That the microtubules are somehow quantum antennas that allow us to get signals from across the multiverse, perhaps. I don't really understand why Penrose is affiliating himself with that so much, right? It's difficult because his theory is different from Hameroff's theory. I've never seen Penrose actively endorsing the microtubuli in the same way as Hameroff does. In my view, and I really, really like Hameroff as a person, Hameroff is building a psychedelic sculpture garden with his theory. It's a theory that is making more predictions every year or more explanations every year using the same mechanisms. And so it's a theory that explains how anesthesia works, how psychedelics work, how consciousness works, and now also how evolution and emotion work, all using the pi resonant quantum underground. In a way that I was at the Science of consciousness conference sitting in the audience and he was putting stuff on the screen and I felt that my basic understanding of physics was sufficient to understand where the physics ended. And I asked the people next to me, are you not concerned that you don't understand what he's talking about in this physics about the pi resonant quantum underground? And I said, don't worry, be shut down as soon as he mentions that. It's not the important part of the theory. I truthfully have not checked his stuff out. I will have to. It's super interesting to read it. And it's really beautiful. I like it. It's also very poetic in a way, but I think it's more art than science, because it's integrating observations, it's projecting things, it's connecting loose ends, but it's not doing this in the same way as a scientist would probably do it, which explains its lack of resonance in the sciences, right? There's almost nobody outside of his circle who is citing these papers and working with this and thinks that they are in any way real and worth discussing. The reason I think why Penrose is affiliated with this is, it's, you know, if you are excluding all the viable explanations except one, or all the probable explanations, then the improbable explanation has to be the reason. And Penrose thinks that computation cannot explain all of consciousness. And I think it's because of his way he interprets Gdel's proof. Gdel has in some sense proven that computation is insufficient to do all of mathematics. There are parts of mathematics that cannot be done in a computational paradigm, assuming so leads into contradictions. And Penrose believes that human mathematicians can do these parts that computation cannot do. I think the resolution works the other way around. These other parts of mathematics were never real. Mathematics was ill-defined. The truth theory of classical semantics is wrong. You can only claim that something is true if you can actually compute that truth. You can only claim that something has a value if you can actually compute that value. It's basically constructive mathematics. Constructive mathematics is roughly the same thing as computation. And what Gdel has shown in my view is that constructive mathematics is real, or can be real in the sense that it can be implemented, but classical mathematics that contains infinities cannot be implemented, cannot be real. Nothing in the physical universe relies on having known the last digit of pi. But there are mathematics that pretend that they are, that this works, and some of these mathematics are even used in physics, but they're not real, they're not computable. They cannot be implemented in any physical causal structure. So this was the implication, and this is an implication I think that Penrose has not seen. He believes that the ability of our minds to be conscious are related to the uncomputable part of mathematics and the uncomputable parts of mathematics go beyond known physics, which in some sense is all computational, right? Even quantum mechanics takes a bunch of numbers and performs some expensive computations on them, and then you get the next bunch of numbers. And this is how we explain the universe. And the only part that is not explained in this paradigm so far is quantum gravity. So the culprit must be quantum gravity, right? It's the only thing that's left from Penrose's perspective. And the only one who tries to offer a theory that uses quantum gravity to explain consciousness in the brain is Penrose using his microtubule. And I think this is how it comes together. Interesting, yeah, I've truthfully read it when it came out, read a couple of things on it and I just put it aside and says, well, I'll know that it's out there but but I'm not going to pay it any mind until somebody comes up with some useful experimental results. I'll have to check out this Hemerov character. I don't know about him. But let's go slightly back to this big distinction between physics and the parapsychology, which leads us into a different world. I think that there are only two possibilities. Either we live in a mechanical universe, and this is the hypothesis of physics. It says that everything is at some level, there's a causally closed mechanical layer. And this doesn't mean that the causally closed mechanical layer needs to look like what the universe looks like to us, right? The Einstein space and our theories of quantum mechanics are probably high-level descriptions that don't describe the causally close lowest level. But it's still mechanical. It's still built in such a way that can be expressed as a computer program. And the alternative to that is that we live in a dream. What's the difference between a dream and a mechanical model? A dream has magical interactions. They're symbolic interactions, which means you sacrifice the cat and the comet appears. Or the comet appears and it predicts your career. There is no known physical force or plausible physical relationship that we could discover that would explain this kind of interaction. It just means that somebody is messing with us. There is a conspiracy, right? The same conspiracy that exists in Minecraft, if you open up a console and set time set day and the sun suddenly rises in Minecraft, but you basically supersede the basic low level causal mechanics of Minecraft using a higher level of causation that is outside of the basic game dynamics. And this would be a world in which Psy is possible, in which Psy in the esoteric, parapsychological sense, where you can use telekinesis to overcome physics, or where you can use clairvoyance to overcome the limits of information transmission via subliminal photonic transfer of data between different regions of the universe. So how is that possible? And I think the reason that this is so attractive, the theory that we essentially must live in a dream, is that we actually live in a dream. We do live in a dream. It's possible to have these experiences. It's possible to experience telepathy. It's possible to experience clairvoyance and so on. And it's because we live in a dream that is generated by a mind on a different level of existence. And this different level of existence happens to be physics. And our culture is a little bit confused because it assumes that the world that you and me see in everyday life is physics, is the physical world. But there are no colors in physics and no sounds in physics. What we perceive is still the dream. And our brain that is out there in physics does not really look pink and squishy in physics. It's only a thing that, this is what it looks like in the dream, the dream that the brain is generating. And to get this right, that is the important thing. Yeah. And well, we all know, we've known for a long time that we don't experience actual reality. We have an interpretation of reality and then we have meaning maps, right? Yes. So we don't experience a simulation of reality. It's a simulacrum of reality. It's completely as if it's not isomorphic to the world out there in physics. And it's at different levels of abstraction, right? I mean, the human perceptual capability can't tell us about atoms, for instance, right, without instruments. And, you know, the idea of colors, even, as we know from anthropology, the different cultures divide the spectrum up in different ways. So some tribe in the Amazon might not describe the brain as pink and squishy. It might be blah, blah and squishy, which is a very different, narrower part of the spectrum than pink, for instance, or probably much broader. I think actually we have a more fine-grained color than many cultures. So yeah, they're meaning maps. And I think that's where people get confused. I still remain a naive realist at heart. I do believe there is an objective universe out there. Can't prove it. You know, as we well know, I can't logically prove the universe wasn't created five seconds ago with all the objects in it, including our memories and all ballistic objects in motion. But if only for parsimonious reasons, I assume there is a real universe out there. And as you say, we have some physical laws, but they're a very long way from the Planck scale. So there's probably a shitload we don't know, right? And yet so far they have shown themselves to be remarkably lawful. We haven't been able to detect, at least with a high-fidelity signal, any psi, any real even deviation from deep lawfulness. And so, at least tentatively, I side with the physicists. Though I do encourage Ben, which he does regularly, send me new papers on psi, because as you say, if we're wrong, or if I'm wrong and psi is real, then we have to have to reevaluate, is the universe actually lawful? And maybe it's not just a dream, maybe it is a simulation, but for the time being, I reject that on grounds of parsimony, if no other reasons. Yeah, so the only reason to not do it is to not accept this theory is because you don't think that scientists are able to explain the things that need to be explained, which is why does reality appear real to us? From a machine learning perspective, it's pretty clear that if a learning system does not in some sense implement the belief that the universe is learnable, then it's not an effective learning system, right? You have to believe in a learnable universe to learn it. It's implicitly. And so the weird thing that we have to explain is, I think, not the qualities of qualia. The qualities of qualia are easier than most people think, because this is just the geometric calculations that your perceptual systems are making. It's basically the dimensions, the parametrizations of the geometric architecture that is computing the perceptual models. And in some sense, this has been neglected for a long time because scientists have focused on the linguistic, the analytical models too much. And only with the widespread takeoff of the machine learning paradigms and deep learning paradigms, I think it has gotten more into the common consciousness of cognitive science, that perception is more akin to the deep learning systems than it is to linguistic systems, to symbolic systems. Right, the paradox is you can, of course, implement the deep learning systems on top of symbolic systems. We actually have to. They're completely implemented on top of symbolic systems. But there are symbolic systems that are very different from our symbolic reasoning. Our symbolic reasoning is arguably limited to a very small stack size and to very few elements at a time. We cannot hold more than like five to seven elements in our focus of attention at a time and relate them to each other. Unlike GPT-3 that is looking at 2048 and relates them all to each other at the same time. And it's doing this in many, many dimensions, and with extremely high resolution and reliability and no lapse of attention. So this is a very different way of doing symbolic operation than our mind is doing. It's doing this on the level of low-level automata. But we have to explain to people how the property of realness comes about. And the property of realness itself is paradoxically not a feature of physical reality. Physical reality doesn't feel like anything. There are no feelings in there. Physical realness can only be experienced as part of a model because it's itself a model property, right? It's a label that the mind attaches to some of these parameter dimensions. And if you look at them, you distinguish the non-real imagination from the real world that you experience by this label that your mind says, this is indeed predictive of your next batch of sensory patterns, as far as they can make it. And this includes the internal sensations that you have about your own self, in your own thinking processes, and reasoning processes, and your experiential processes. Your experiences are real experiences because they are predictive of your next experiential features, right? They are models of what you experience. And the realness itself is a model of the fact that they are predictive. Yeah, though of course it's true that for humans, again as I said before, the resolution of our measurements and perceptions are pretty gross. And we're able to go way beyond that with our scientific instruments. Yes, we evolved as if we are in a lawful universe, because that's what works, right? But it may well have been that we were so coarse-graining the universe due to our low-resolution vision, you know, the acuity of our feeling, the ability to decompose matter, that we could have easily been fooled by something going on deeper down, right? At least unless our instruments are lying to us, we can probe, I don't know, 20 orders of magnitude smaller than we can get at as unaided humans and 20 orders of magnitude larger. And yet lawfulness still seems to prevail in both the microcosm and the macrocosm, even though our formulations of the laws are almost certainly very substantially incomplete. That seems to me much stronger evidence for reality, deep reality, than merely our sense of reality, though I do take your point that, you know, reality is not an attribute of the universe itself, but it's rather an experience of a conscious agent living in the universe. But I would say we should have more confidence in reality than merely our naive animal consciousness because of the fact that we've been able to extend our probes a long way in both the microcosm and the macrocosm. I think it's important to hold it somewhat tentatively. I think that to have an enlightened relationship to reality, it is necessary to realize that what you perceive is a representation. This includes yourself and your relationship to the universe. This is all, in some sense, a representation that you don't perceive as a representation, but as an immediate reality. And you need to make it visible as a representation. You need to pay attention to that. You also need to pay attention to attention in such a way that the attention itself becomes visible to you, that you can notice how your attentional system works and how it's constructing your reality, if you are interested in that. And for a normal human being, the only reason to be interested in that is because it doesn't work. And since these attentional processes tend to work very well, most people don't pay attention to them and just take them as given, the processes of reality construction. And the people that are familiar with these processes are those that are naturally in altered states of minds because they fell down the stairs headfirst at some point or have developmental issues, or because they are in an existential crisis, for instance. And this existential crisis makes it necessary for them to understand their own relationship to meaning and their own self-construction. Yeah, let's step back a little bit. We've been talking about mind emerging from matter. We've talked about a lot of the leading theories. We've talked about some of the out there theories. From your perspective, what's important for the next step in the science of explaining mind emerging from matter? Where should we be looking next? What's the next 10 years look like? It's, of course, always very difficult to make predictions, especially about the future. Yes, thank you, Yogi. Yes. I suspect that one of the very interesting areas that we need to look at is attention-based models. And the transformer is only the beginning. From my perspective, there are at least three obvious things that are wrong with GPT-3. Interestingly, the fact that GPT-3 is not an agent is not one of them that can be easily fixed, right? GPT-3 is obviously not an agent, it doesn't have a context. So if you tell GPT-3, hey, you are GPT-3, what are you, then GPT-3 will produce a relatively random sentence, because GPT-3 will produce a relatively random sentence because GPT-3 has been trained on statistics and language until October 2019, and GPT-3 didn't exist in October 2019, right? So unless OpenAI has built something into GPT-3 explicitly to teach it about GPT-3, what it should say, it doesn't know what it is. And if you tell GPT-3 explicitly to teach it about GPT-3, what it should say, it doesn't know what it is. And if you tell GPT-3, I am talking to the AI GPT-3 and so on, then it will assume in some sense implicitly for some meaning of assume that it's talking about some kind of science fiction context or a technique context in which a human is communicating with an AI system. And then it might guess a number of things right, but it doesn't know which ones are right or not. It also doesn't know whether it relates to any kind of reality, where there is no sense of an underlying reality. There is no fixed context. But you can add this fixed context by making additional commitments. And you need to make these additional contexts commitments. I think that is an implication of Loeb's theory, which is a recasting of Gdel's proof. A system is not able to basically break out of its own axiomatic system and make statements about axiomatic systems above it. In order for a system to reason about itself, it needs to recreate itself within its own formalisms, and then make statements about this recreation of itself. So for formal systems that you have created, of course, you can build them in such a way that you can recreate the formal system, within the formal system, so the system can make proofs about itself. But strictly speaking, you cannot make proofs above yourself. You can solve this problem of agency in GPT-3, I think, by building, in principle, a vision to speech module, or vision to text module that is interpreting the camera images of a robot, sends them to GPT-3, and then GPT-3 tells a story about a robot in that world. And then a parser is reading these statements and translates them into motor actions of the robot. And we continuously play that game. And now arguably, language is not the right level of resolution. We want to have something that is sub-conceptual. We want to deal with perceptual stuff. But GPT-3 is able to deal with that, right? There's ImageGPT, which is able to learn the statistics of images in the same way as a gun would, or even arguably in a better way than a gun would, within the limits of its small attentional window. So the main issue, I think, is the creation of a larger attentional window. At the moment, GPT-3 has massive retrograde amnesia. It's not able to remember anything from two pages ago, except the extractions that it made from that. It's able to change its neural network as a result, even though it doesn't do online learning. But during training, it's of course, it takes something from it has seen before, but it's not able to reestablish the context in which it took it, took these insights. When I read a book, I might, in a later chapter, recall an earlier chapter and create a context that's merging the current chapter with the previous chapter, and then rewrite everything that I learned from the previous chapter in the new light. Because I can reconstruct what I learned from the previous chapter and where I light, because I can reconstruct what I learned from the previous chapter and where I got this knowledge from and so on, right? So for instance, I have an idea that I misunderstood or that the world is too simplistic and I need to revite this idea. And I ask myself, where did I get this idea from? Why and how am I revising this, right? And now I create a new working memory context in which I direct my attention. And this working memory construction is a thing that GPT-3 cannot do yet. So we need to extend attention in such a way that it's able to change working memory context actively and construct working memory contexts. And we need to change the level of representation from language to a multimodal representation that is agnostic to what it represents and addresses. And you mentioned something in passing, which I want to call out, which is rewriting. You know, one of the things that we know humans do is our memories are not only low fidelity, but they're also quite subject to be rewritten, right? And certain kinds of linguistic processing may be implementable as rewrite rules. And, you know, I think we both have had some exposure to the OpenCog system and the OpenCog system, a lot of it's based on the concept of both local and global rewriting. And that seems very far from GPT-3. I mean, it is what it is. It doesn't have any essentially dynamic rewriting capability going on in it. Exactly. So this is the second thing that is wrong with GPT-3 in my view. The first thing was the working memory window, which is limited to 2048 adjacent tokens. And basically, there are hard constraints in there in which the working memory is used that don't exist in our own mind. It's not necessarily that our working memory is larger, I suspect it's much smaller than the ones of GPT-3, but we are able to construct the contents of our working memories with way more degrees of freedom. So this is the first thing. The second one is online learning. GPT-3 is only doing offline learning, which is good for an industrial production system that is meant to behave in pretty much the same way. But if you want to build a system that is working like us, it needs to continuously learn. GPT-3 has stopped learning in October 2019. So it doesn't know about anything about COVID-19 or George Floyd, lives in a different universe. And we need to have an agent that is constantly learning and tracking reality in real time around it. But this is something that also can be overcome, right? It requires massive changes to the algorithms that are being used, you cannot use the same neural learning algorithms that are currently implemented in GPT-3. But it's nothing that is completely out of this world, this can be done. And the last thing is relevance. GPT-3 does not care about relevance. The relevance sensation that GPT-3 has is because humans don't write everything into texts. They write things into text that are relevant to them. So by minimizing the surprise on all the available texts that have a decently good scoring on Reddit means you are probably learning something interesting. It was interesting enough for a human to write it down. So just by learning from this, this is way, way better than random stuff. But this is not sufficient for a system that is interacting with the world and takes in rich sensory data on many modalities at a higher bandwidth that you can process in real time. So in this case, you have to focus on those parts of the model that are most promising. And for this, you need to have a motivational system. And I think that in practice, you will have way better results from systems that are able to assign relevance to learning and meta learning in ways that the current GPT-3 is not. So GPT-3 can do style learning in ways that no human being can do, because it learns all the stuff which we find is irrelevant. When you read a textbook, you don't care about the style so much. You care about the content. And GPT-3 does not think that the content is more important than the style. It only looks for style and goes for so deep style that ultimately it often bottoms out in content. Say a little bit more about the affective part and how that might be added into GPT-3. The affective part is basically something where the size theory I think is still one of the best theories today. The psi theory posits that you can describe an agent like us using homeostasis as a guiding principle. So there is a homeostatic balance that keeps the system stable in the face of an environment that disturbs it. Our mind is in some sense solving a control problem in many dimensions. And these control dimensions are given to us as needs that when frustrated, produce pain signals and when satisfied, produce pleasure signals. And this, when we act on these needs, we act on purposes and models of our needs. And we have strong biases on what kind of models of our needs we form. And this hierarchy of purposes that we form that becomes coherent is in some sense the structure of our soul. It's not a random set of behaviors that is just sitting next to each other and randomly arranged. It's a system that strives for coherence. And the more coherent it is, the more our mind appears to be a singular solid thing that has a definite structure. And in some sense, you could say that our true soul is the platonic form, the ideal form of all these purposes arranged in the right hierarchy, including our transcendental purposes that go beyond the individual and its present time and space that it occupies. Okay, I think we're about out of time. We didn't get to any of the low-level things I wanted to talk about, but that was alright. I think our conversation was good and rich and went a long way. I hope the audience appreciated it. I know I certainly did. So, Josje, I'd love to have you on the show again sometime to talk about your thinking about MicroSci3 and things in your own workspace. But I really want to thank you for this amazingly broad-ranged and yet deep discussion. I thank you too. I really enjoyed talking to you again. And let's do it again sometime.", '28.252381086349487')