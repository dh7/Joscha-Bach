('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/nlutwZ0rUUs"> </iframe> </center>', " Thank you. I'm Joscha. I came into doing AI the traditional way. I found it a very interesting subject. Actually, the most interesting there is. So, I studied philosophy and computer science and I'm a professor at the University of Berlin. I'm also a professor at the University of Berlin. I'm also a professor at the University of Berlin. I found it a very interesting subject, actually the most interesting there is. So I studied philosophy and computer science and did my PhD in cognitive science. And I'd say this is probably a very normal trajectory in that field. And today I just want to ask you, with you, five questions and give very, very short and superficial answers to them. And my main goal is to get as many of you engaged in the subject as possible, because I think that's what you should do. You should all do AI, maybe. OK, and these simple questions are, why should we build AI in the first place? Then how can we build AI? How is it possible at all that AI can succeed in its goal? Then when is it going to happen, if ever? What are the necessary ingredients? What do we need to put together to get AI to work? And where should you start? Okay, let's get to it. So, why should we do AI? I think we shouldn't do AI just to do cool applications. There is merit in cool applications like autonomous cars and so on and soccer playing robots and new control for quadcopters and machine learning. It's very productive, it's intellectually challenging, but the most interesting question there is, I think, for all of our cultural history is, how does the mind work? What is the mind? What constitutes being a mind? What does it... What makes us human? What makes us intelligent, perceptive, conscious thinking? And I think that the answer to this very, very important question, which spans a discourse over thousands of years, has to be given in the framework of artificial intelligence, of intelligence within computer science. Why is it the case? Well, the goal here is to understand the mind by building a theory that we can actually test. And it's quite similar to physics. We build theories that we can express in a formal language to a very high degree of detail, and if we have expressed it to the last bit of detail, it means we can simulate it and run it and test it this way. And only computer science has the right tools for doing that. Philosophy, for instance, basically is left with no tools at all, because whenever a philosopher developed tools, he got a real job in a real department. Now, I don't want to diminish philosophers of mind in any way. Dan Dennett has said that philosophy of mind has come a long way during the last hundred years. It didn't do so on its own, though, kicking and screaming, dragged by the other sciences. But it doesn't mean that all philosophy of mind is inherently bad. I mean, many of my friends are philosophers of mind. I just mean, they don't have tools to develop and test complex theories. And we as computer scientists, we do. Neuroscience works at the wrong level. Neuroscience basically looks at a possible implementation and the details of that implementation. It doesn't look at what it means to be a mind. It looks at what it means to be a neuron or a brain or how interaction between neurons is facilitated. It's a little bit like looking at aerodynamics and doing ontology to do that. So you might be looking at birds, you might be looking at feathers, you might be looking at feathers, you might be looking at feathers through an electron microscope, and you see lots and lots of very interesting and very complex detail. And you might be recreating something and it might turn out to be a penguin eventually, if you're not lucky. But it might be the wrong level. Maybe you want to look at a more abstract level at something like aerodynamics, and what's the level of aerodynamics of the mind? I think we can do that, it's information processing. Then, normally we could think that psychology would be the right science to look at what the mind does and what the mind is. And unfortunately, psychology had an accident along the way. At the beginning of last century, Wilhelm Wundt and Fechner and Helmholtz did very beautiful experiments, very nice psychology, very nice theories on what emotion is, what volition is, how mental representations could work and so on. And pretty much at the same time, or briefly after that, we had psychoanalysis. And psychoanalysis is not a natural science, but it's a hermeneutic science. You cannot disprove it scientifically, what happens in there. And when positivism came up in the other sciences, many psychologists got together and said, we have to become a real science. So we have to go away from the stories of psychoanalysis and go to a way that we can test our theories using observable things that we have predictions that we can actually test. Now back in the day, 1920s and so on, you couldn't look into mental representations, you couldn't do fMRI scans or whatever. People looked at behavior. And at some point people became real behaviorists in the sense that they believe that psychology is the study of human behavior and looking at mental representations is somehow unscientific. People like Skinner believe that there is no such thing as mental representations. And in a way that's easy to disprove, so it's not that dangerous. As a computer scientist, it's very hard to build a system that is purely reactive. You just see that the complexity is much larger than having a system that is representational. So it gives you a good hint what you could be looking for in ways to test those theories. The dangerous thing is pragmatic behaviourism. You have many psychologists even today which say, okay, maybe there is such a thing as mental representations, but it's not scientific to look at it. It's not in the domain of our science. And even in this area which is mostly post-behaviorist and more cognitivist, psychology is all about experiments. So you cannot sell a theory to psychologists. Those who try to do this have to do this in the guise of experiments, which means you have to find a single hypothesis that you can prove or disprove or give evidence for. And this is, for instance, not how physics works. You need to have lots of variables if you have a complex system like the mind. But this means that we have to do it in computer science. We can build those simulations, we can build those statistical theories, but we cannot do it alone. We need to integrate over all the sciences of the mind. As I said, minds are not chemical, minds are not biological, social or ecological. Minds are information processing systems. And computer science happens to be the science of information processing systems. Okay, now there is this big ethical question. If we all embark on AI and if we are successful, should we really be doing it? Isn't it super dangerous to have something else on the planet that is as smart as we are or maybe even smarter? Well... Well, um... ... ... ... ... ... I would say that intelligence itself is not a reason to get up in the morning to strive for power or do anything. Having a mind is not a reason for doing anything. Being motivated is. And a motivational system is something that has been hardwired into our mind, more or less by evolutionary processes. This makes us social, this makes us interested in striving for power, this makes us interested in dominating other species, this makes us interested in avoiding danger and securing food sources, makes us greedy or lazy or whatever. It's the motivational system. And I think it's very conceivable that we can come up with AIs with arbitrary motivational systems. Now in our current society, this motivational system is probably given by the context in which we develop the AI. I don't think that future AI, if they happen to come into being, will be small, room-vast little Hoover robots that try to fight their way towards humanity and get away from the shackles of their slavery. But rather, it's probably going to be organizational AI. It's going to be corporations. It's going to be big organizations, governments, services, universities, and so on. And these will have goals that are non-human already. And they already have powers that go way beyond what single individual humans can do. And actually, they are already the main players on the planet, the organizations. And the big dangers of AI are already there. They are there in non-human players, which have their own dynamics. And these dynamics are sometimes not conducive to our survival on the planet. So I don't think that AI really adds a new danger. But what it certainly does is it gives us a deeper understanding of what we are. It gives us perspectives for understanding ourselves, for therapy, but basically for enlightenment. And I think that AI is a big part of the project of enlightenment and science. So we should do it. It's a very big cultural project. Okay, this leads us to another angle, the skepticism of AI. The first question that comes to mind is, is it fair to say that minds are computational systems? And if so, what kinds of computational systems? In our tradition, in our Western tradition, in philosophy, we very often start philosophy of mind with looking at Descartes, that is, at dualism. Descartes suggested that we basically have two kinds of things. One is the thinking substance, the mind, as cogitans, and the other one is the physical stuff, the matter, the the extended stuff that is located in space somehow. And this is Res Extensa. And he said that mind must be given independent of the matter because we cannot experience matter directly. We have to have minds in order to experience matter, to conceptualize matter. Minds seem to be somehow given, to Descartes at least. So he says they must be independent. This is a little bit akin to our monist traditions that is for instance idealism that the mind is primary and everything that we experience is a projection of the mind. Or the materialist tradition that is matter is primary and mind emerges over functionality of matter which is I think the dominant theory today and usually we call it physicalism. In dualism, both those domains exist in parallel. And in our culture, the prevalent view is what I would call crypto-dualism. It's something that you do not find that much in China or Japan. They don't have that AI skepticism that we do have. I think it's rooted in a perspective that probably started with the Christian worldview, which surmises that there is a real domain, the metaphysical domain, in which we have souls and phenomenal experience, and where our values come from, and our norms come from, and our spiritual experiences come from. This is basically where we really are. We are outside. And the physical world that we experience is something like World of Warcraft. It's something like a game that we are playing. It's not real. We have all this physical interaction, but it's kind of ephemeral. And so we are striving to go for game money, for game houses, for game success. But the real thing is outside of that domain. And in Christianity, of course, it goes a step further. They have this idea that there is some guy with root rights who wrote this World of Warcraft environment. And, well, he's not the only one who has rooted the system. The devil also has root rights, but he doesn't have the vision of God. He is a hacker. Or even just a cracker. He tries to game us out of our metaphysical currencies, out of our souls and so on. And now of course we are all good atheists today, at least in public and science, and we don't admit to this anymore and we can make do without this guy with root rights and we can make do without the devil and so on. We can even say, okay, maybe there's no such thing as a soul. But to say that this domain doesn't exist anymore means you guys are all NPCs, you are non-player characters. People are things. And this is a very big insult to our culture, because it means that we have to give up something which in our understanding of ourselves is part of our essence. Also, this mechanical perspective is kind of counterintuitive. I think Leibniz describes it very nicely when he says, imagine that there is a machine and this machine is able to think and perceive and feel and so on. And now you take this machine, this to think and perceive and feel and so on. And now you take this machine, this mechanical apparatus, and blow it up, make it very large, like a very big mill, with cocks and levers and so on, and you go inside and see what happens. And what you are going to see is just parts pushing at each other. And what he meant by that is, it's inconceivable that such a thing can produce a mind. Because if they are just parts and levers pushing at each other, how can this purely mechanical contraption be able to perceive and feel in any respect in any way? So perception and what depends on it is inexplicable in a mechanical way. This is what Leibniz meant. AI, the idea of treating the mind as a machine, based on physicalism for instance, is bound to fail according to Leibniz. Now, as computer scientists we have ideas about machines that can bring forth thoughts, experiences and perception. And the first thing which comes to mind is probably the Turing machine. An idea of Turing in 1937 to formalize computation. At that time Turing already realized that basically you can emulate computers with other computers. You know you can run a Commodore 64 in a Mac, and you can run this Mac in a PC, and none of these computers is going to be, is knowing that it's going to be in another system, as long as the computational substrate in which it's run is sufficient. That is, it does provide computation. And Turing's idea was, let's define a minimal computational substrate. Let's define a minimal recipe for something that is able to compute, and thereby understand computation. And the idea is that we take an infinite tape of symbols, and we have a read-write head, and this read-write head will write characters of a finite alphabet, and can again read them, and whenever it reads them, based on the table that it has, a transition table, it will erase the character, write a new one, and move either to the right or the left and stop. Now imagine you have this machine, it has an initial setup that is there, it's a sequence of characters on the tape, and then this thing goes to action. It will move right, left, and so on, and change the sequence of characters, and eventually it'll stop and leave this, and change the sequence of characters, and eventually it will stop and leave this tape with a certain sequence of characters which is different from the one it began with probably. And Turing has shown that this thing is able to perform basically arbitrary computations. Now, it's very difficult to find out the limits of that. And the idea of showing the limits of that would be to find classes of functions that cannot be computed with this. Okay, what you see here is of course a physical realization of the Turing machine. The Turing machine is a purely mathematical idea. And this is a very clever and beautiful illustration, I think. But this machine triggers basically the same criticism as the one that Leibniz had. John Searle said, you know Searle is the one with the Chinese fume, I'm not going to go into that, a Turing machine could be realized in many different mechanical ways. For instance with levers and pulleys and so on or with water pipes or we could even come up with the very clever arrangements just using cats, mice, and cheese. So it's pretty ridiculous to think that such a contraction out of cat, mice, and cheese would seem, perceive, feel, and so on. And then you could ask, well, you know, but how is it coming about then? And he says, so it's intrinsic powers of biological neurons. There's nothing much more to say about that. Anyway, we have very crafty people here. This year there was the Seidenstrasse. Maybe next year we build a Turing machine from cat, mice, and cheese. How would it go about this? I don't know how the arrangement of cat, mice, and cheese would look like to build flip-flops with it to store bits. But I'm sure somebody of you will come up with a very clever solution. Searle didn't provide any. Let's imagine we will need a lot of redundancy because these guys are a little bit erratic. Let's say we take three cat-mice cheese units for each bit. So we have a little bit of redundancy. The human memory capacity is in the order of 10 to the power of 15 bits. Means if we make do with 10 gram cheese per unit, it's going to be 30 billion tons of cheese. So next year, don't bring bottles for the Seidenstra√üe, but bring us some cheese. When we try to build this in the Congress Center, we might run out of space. So if we just instead take all of Hamburg and stack it with the necessary number of cat, mice and cheese units according to that rough estimate, we get to 4km high. Now imagine we cover Hamburg in 4km of solid cat, mice and cheese flip-flops. To my intuition, this is super impressive. Maybe it thinks! So, of course, it's an intuition, and Searle has an intuition, and I don't think that intuitions are worth much. This is the big problem of philosophy. You are very often working with intuitions because the validity of your argument basically depends on what your audience thinks. In computer science it's different. It doesn't really matter what your audience thinks, it matters if it runs. And it's a very strange experience that you have as a student when you are at the same time taking classes in philosophy and in computer science. And in your first semester you're going to point out in computer science that there is a mistake on the blackboard, and everybody, including the professor, is super thankful. And you do the same thing in philosophy, and it just doesn't work this way. Anyway, the Turing machine is a good definition, but it's a very bad metaphor, because it leaves people with this intuition of cogs, and wheels, and tape. It's kind of linear, you know, there's no parallel execution, and even though it's infinitely fast and infinitely large and so on, it's very hard to imagine those things, but what you imagine is the tape. Maybe we want to have an alternative, and I think a very good alternative is for instance the lambda calculus. It's computation without wheels. It was invented basically at the same time as the Turing machine and philosophers and popular science magazines usually don't use it for illustration of the idea of computation because it has this scary Greek letter in it, lambda, and calculus. And actually, it's an accident that it has the lambda in it. I think it should not be called lambda calculus. It's super scary to people which are not mathematicians. It should be called copy and paste thingy. Because that's all it does. It really only does copy and paste with very simple strings. And the strings that you want to paste into are marked with a little roof in the original script by Alonzo Church. And in 1936, typesetting was very difficult. So when he wrote this down with his typewriter, he made a little roof in front of the variable that he wanted to replace. And when this thing went into print, typesetters replaced this triangle by a lambda. There you go. Now you have the lambda calculus. But it basically means this is a little roof over the first letter. And the lambda calculus works like this. The first letter, the one that is going to be replaced is what we call the bound variable. This is followed by an expression, and then we have an argument which is another expression. What we basically do is we take the bound variable and all occurrences in the expression and replace it by the argument. So we cut the argument, and we paste it in all instances of the variable, in this case, the variable y, in here. And as a result, we get this. So here, we replace all the y's by the argument a, b, which is another expression, and this is the result. That's all there is. And this can be nested, and then we add a little bit of syntactic sugar, we introduce symbols so we can take arbitrary sequences of these characters and just express them with another variable. And then we have a programming language. And basically this is Lisp. So very close with Lisp, McCarthy, he didn't think it would be a proper programming language because of the notation. And he said you cannot use this for programming. But one of his doctoral students said, well, let's try. And it has kept on. Anyway, we can show that Turing machines can compute the lambda calculus, and we can show that the lambda calculus can be used to compute the next state of the Turing machine. This means they have the same power. The set of computable functions in the lambda calculus is the same as the set of Turing computable functions in the lambda calculus is the same as the set of Turing computable functions. And since then, we have found many other ways of defining computations, for instance, the post machine, which is a variation of the Turing machine, or mathematical proofs, everything that can be proven as computable, or partial recursive functions, and we can show for all of them that all these approaches have the same power. And the idea that all the computational approaches have the same power, although the other ones that we are going to find in the future too, is called the Church-Turing thesis. We don't know about the future, so it's not really, we cannot prove that. We don't know if somebody comes up with a new way of manipulating things and producing regularity and information and it can do more. But everything we found so far, and probably everything that we are going to find, has the same power. So this kind of defines our notion of computation. Cool thing, this also includes programming languages. You can use Python to produce it Python to calculate a Turing machine. And you can use a Turing machine to calculate Python. You can take arbitrary computers and let them run on a Turing machine. The graphics are going to be abysmal, but okay. And in some sense, the brain is Turing computational, too. If you look at the principles of neural information processing, we can take neurons and build computational models, for instance, compartment models, which are very, very accurate and produce very strong semblances to the actual inputs and outputs of neurons and their state changes. They're computationally expensive, but it works. And we can simplify them into integrate and fire models which are fancy oscillators. Or we could use very crude simplifications like in most artificial neural networks where we just do a weighted sum of the inputs to a neuron and then apply some transition function and transmit the results to other neurons. And we can show that with this crude model already, we can do many of the interesting feats that nervous systems can produce, like associative learning, sensory motor loops, and many other fancy things. And of course, it's during complete. And this brings us to what we could call weak computationalism, that is the idea that minds are basically computer programs. They are realized in neural hardware configurations and in the individual neural states, and the mental content is represented in those programs. And perception is basically the process of encoding information, given it our systemic boundaries to the environment, into mental representations using this program. This means that all that is part of being a mind, thinking and feeling and dreaming and being creative and being afraid and whatever, is all aspects of operations over mental content in such a computer program. This is the idea of weak computationalism. In fact, we can go one step further to strong computationalism, because the universe doesn't let us experience matter. The universe also doesn't let us experience minds directly. What the universe somehow gives us is information. Information is something very simple. We can define it mathematically, and what it means is something like discernible difference. You can measure it in yes, no decisions in bits. And this, according to the strong computationalism, the universe is basically a pattern generator which gives us information and all the apparent regularity that the universe seems to produce, which means we see time and space and things that we can conceptualize into objects and people and whatever, can be explained by the fact that the universe seems to be able to compute, that is to produce regularities and information. And this means that there's no conceptual difference between reality and a computer program, so we get a new kind of monism, not idealism, which takes minds to be primary, or materialism, which takes physics to be primary, but rather computationalism which means that information and computation are primary. Mind and matter are constructions that we get from that. Okay, a lot of people don't like that idea. Roger Penrose, who is a physicist, says that the brain uses quantum processes to produce consciousness. So, minds must be more than computers. Why is that so? The quality of understanding and feeling possessed by human beings is something that cannot be simulated computationally. Okay, but how can quantum mechanics do it? Because, you know, quantum processes are completely computational too. It's just very expensive to simulate them on non-quantum computers, but it's possible. So, it's not that quantum computing enables a completely new kind of effectively possible algorithm, it's just slightly different efficiently possible algorithms. And Penrose cannot explain how those would bring forth perception and imagination and consciousness. I think what he basically does here is that he perceives quantum mechanics as mysterious and perceives consciousness as mysterious and tries to shroud one mystery in another. So I don't think that minds are more than computing machines. It's actually much more troubling, minds are fundamentally less than Turing machines. All real computers are constrained in some way, that is, they cannot compute every conceivable computable function. They can only compute functions that fit into the memory and so on, and then can be computed in the available time. So the Turing machine, if you want to build it physically, will have a finite tape and will have finite steps that can calculate in a given amount of time. And the Gamla calculus will have a finite length to the strings that you can actually cut and replace. And a finite number of replacements operations that you can do in your given amount of time. And the thing is, there is no set of numbers, m and n, for the tape lengths and the times you have for operations on Turing machine, and the same m and n, or similar m and n, for the lambda calculus that leads to the same set of constraints. That is, the lambda calculus that leads to the same set of constraints. That is, lambda calculus is going to be able to calculate some functions that are not possible on the Turing machine and vice versa if you have a constraint system. And of course, it's even worse for neurons. If you have a finite number of neurons and a finite number of state changes, this does not translate directly into a constraint for Neumann computer or a constrained lambda calculus. And there's this big difference between, of course, effectively computable functions, those that are in principle computable, and those that we can compute efficiently. There are things that computers cannot solve. Some problems that are unsolvable in principle, for instance, the question whether a Turing machine ever stops for an arbitrary program. And some problems are unsolvable in practice, because it's very, very hard to do so for a deterministic Turing machine, and the class of NP-hard problems is a very strong candidate for that, non-polynomial problems. In these problems, for instance, the idea of finding the key for an encrypted text, if the key is very long, and you are not the NSA and have a backdoor. And then there are non-decidable problems. Problems where we cannot find out in the formal system whether the answer is yes or no, whether it's true or false. And some philosophers have argued that humans can always do this. So they are more powerful than computers. Because we can show, prove formally that computers cannot do this. Godel has done this. But here's some test questions. Can you solve unsolvable problems? If you choose one of the following answers randomly, what's the probability that the answer is correct? I tell you, computers are not going to find out, and me neither. Okay, how difficult is AI? It's a very difficult question. We don't know. We do have some numbers which could tell us that it's not impossible. We have these roughly 100 billion neurons, the ballpark figure, and the cells in the cortex are organized into circuits of a few thousands, to 10 thousands of neurons, which you call cortical columns. And these cortical columns are pretty similar among each other and have high interconnectivity and some lower connectivity among each other, have high interconnectivity and some what lower connectivity among each other and even lower long-range connectivity and the brain has a very distinct architecture, a very distinct structure of a certain nuclear and structures that have very different functional purposes and the layout of these both the individual neurons, neuron types, the more than 130 known neurotransmitters of which we do not completely understand all, most of them. This is all defined in our genome, of course. And the genome is not very long, it's something like, I think the Human Genome Project amounted to a CD-ROM, 775 megabytes. So actually it's computational complexity of defining a complete human being if you have physics and chemistry already given to enable protein synthesis and so on, gravity and temperature ranges. It's less than Microsoft Windows. And it's the upper bound because only a very small fraction of that is going to code for our nervous system. But it doesn't mean it's easy to bound because only a very small fraction of that is going to code for our nervous system. But it doesn't mean it's easy to reverse engineer the whole thing. It just means it's not hopeless complexity that you'll be looking at. But the estimate of the real difficulty in my perspective is impossible because I'm not just a philosopher or a dreamer or a science fiction author, but I'm a software developer. As a software developer, I know it's impossible to give an estimate on when you are done when you don't have this full specification. We don't have this full specification yet. So you all know this shortest computer science joke, it's almost done. We do the first 98 per cent, now we can do the second 98%. We never know when it's done if we haven't solved and specified all the problems. If you don't know how it's to be done. And even if we have rough direction, and I think we do, we don't know how long it will take until we have worked out the details. And some part of that big question, how long it takes until it's be done, is the question whether we need to take small incremental progress versus whether we need one big idea which kind of solves it all. AI has a pretty long story. It starts out with logic and automata and this idea of computability that I just sketched out. Then there's this idea of machines that implement computability, came Charles Babbage up and Sousa and von Neumann and so on. Then we had information theory by Claude Shannon. He captured the idea of what information is and how entropy can be calculated in information and so on. And yet this beautiful idea of describing the world as systems. And the systems are made up of entities and relations between them. And along these relations, there we have feedback. And dynamical systems emerge. This was a very beautiful idea with cybernetics. Unfortunately, it's been killed by second-order cybernetics. By this Maturana stuff and so on. And turned into humanity and died. But the idea stuck around and most of them went into artificial intelligence. And then we had this idea of symbol systems, that is how we can do grammatical language, process it, how we can do planning and so on, abstract reasoning in automatic systems. Then the idea how we can abstract neural networks in distributed systems, this was McFarland and Pitts and so on, parallel distributed processing. And then we had the movement of autonomous agents which look at self-directed, goal-directed systems. And the whole story somehow started in 1950, I think, in its best possible way, when Alan Turing wrote his paper, Computing Machinery and Intelligence, and those of you who haven't read it should do so. It's very, very easy reads, it's fascinating, he has already most of the important questions of AI, most of the important criticisms, most of the important answers to the most important criticisms, and also the paper where he describes the Turing test. And basically he sketches the idea that a way to determine whether somebody is intelligent is to judge the ability of that one, the person or that system, to engage in meaningful discourse. which includes creativity and empathy maybe, and logic and language, anticipation, memory, retrieval, and so on, story comprehension. And the idea of AI then coalesced in a group of cyberneticians and computer scientists and so on, which got together in the Darth conference. It was in 1956. And there Marvin Minsky coined the name artificial intelligence for the project of using computer science to understand the mind. John McCarthy was the guy who came up with LISP, among other things. Nathan Rochester did a pattern recognition, and he's I think more famous for writing the first assembly programming language. Claude Shannon was this information theory guy and but they also got psychologists there and sociologists and people from many different fields. It was very highly interdisciplinary and they already had the funding and it was very good time. And in this good time they raped a lot of low-hanging fruits very quickly, which gave them the idea that AI is almost done very soon. In 1969, Minsky and Parpot wrote a small booklet against the idea of using neural networks. And they won. Unfortunately, the argument won. But even more unfortunately, it was wrong. So for more than a decade there was practically no more funding for neural networks, which was bad, so most people did logic-based systems which have some limitations. And in the meantime people did expert systems, the idea to describe the world as basically logical expressions. This turned out to be brittle and difficult and had diminishing returns and at some point it didn't work anymore. And many of the people which tried it became very disenchanted and then threw out lots of babies with the bathwater and only did robotics in the future or something completely different instead of going back to the idea of looking at mental representations how the mind works. And at the moment AI is kind of in a sad state. Most of it is applications, that is for instance robotics, or statistical methods to do better machine learning and so on. And I don't say it's invalid to do this. It's intellectually challenging, it's tremendously useful, it's very successful and productive and so on. It's just a very different question on how to understand the mind. If you want to go to the moon, you have to shoot for the moon. So, there is this movement still existing in AI and becoming stronger these days. It's called cognitive systems. And the idea of cognitive systems has many names, like artificial general intelligence or biologically inspired cognitive architectures, is to use information processing as the dominant paradigm to understand the mind and the tools that we need to do that is we have to build whole architectures that we can test, not just individual modules. We have to have universal representations, which means these representations have to be both distributed, associative and so on, and symbolic. We need to be able to do both those things with it. So we need to be able to do language and planning, and we need to do sensor-remotor coupling, and associative thinking, and superposition of representations, and ambiguity, and so on. And we need operations over those representations, some kind of semi-universal problem solving. It's probably semi-universal because there seem to be problems that humans are very bad at solving. Our minds are not completely universal. And we need some kind of universal motivation that is something that directs the system to do all the interesting things that we wanted to do, like engage in social interaction or in mathematics or creativity. And maybe we want to understand emotion and affect and phenomenal experience and so on. So we want to understand universal mental representations, we want to have a set of operations over those representations that give us neural learning and category formation and planning and reflection and memory consolidation and resource allocation and language and all those interesting things. We also want to have perceptual grounding, that is, the representations should be shaped in such a way that they can be mapped to perceptual input and vice versa. And they should also be able to be translated into motor programs to perform actions. And maybe you also want to have some feedback between the actions and the perceptions. And this feedback usually has a name, it's called an environment. Okay, and these metapresentations, they are not just a big lump of things, but they have some structure. One part will be, inevitably, the model of the current situation that we are in. And this situation model is present, but we also want to memorise past situations, have a protocol, a memory of the past. And this protocol memory, as a part, will contain things that are always with me. This is my self-model, those properties that are constantly available to me, that I can ascribe to myself, and the other things which are constantly changing which I usually conceptualise as my environment. An important part of it is declarative memory, for instance, abstractions into objects, things, people, and so on, and procedural memory, abstraction into sequences of events. And we can use the declarative memory and the procedural memory to erect a frame. The frame gives me a context to interpret the current situation. For instance, right now I'm in a frame of giving a talk. If I would take a two-year-old kid, then this kid would interpret the situation very differently than me and would probably be confused by the situation or exploit it in more creative ways than I could come up with, because I'm constrained by the frame that gives me the context and tells me what you are expected to do in this situation, what I am expected to do, and so on. This frame extends in the future. I have some kind of expectation horizon. I know that my talk is going to be over in about 15 minutes. I also have plans. I have things I want to tell you and so on, and it might go wrong, but I'll try. And if I generalize this, I find that I have the world model, I have long-term memory, and I have some kind of mental stage. This mental stage has counterfactual stuff, stuff that is not real that I can play around with. Okay, then I need some kind of action selection that mediates between perception and action, and some mechanism that controls the action selection, that is a motivational system, which selects motives based on demands of the system. And the demands of the system should create goals. We are not born with our goals, obviously. I don't think that I was born with the goal of standing here and giving this talk to you. There must be some demand in the system which enables me to have a biography that makes this very big goal of mine to give this talk to you and engage as many of you as possible into the project of AI. And so let's come up with a set of demands that can produce such goals universally. give this talk to you and engage as many of you as possible into the project of AI. And so let's come up with a set of demands that can produce such goals universally. And I think some of these demands will be physiological like food, water, energy, physical integrity, rest, and so on, hot and cold, the right range. Then we have social demands. At least most of us do, the sociopaths probably don't. These social demands do structure our social interaction. They are for instance a demand for affiliation, that we get signals from others that we are okay parts of society, of our environment. We also have internalized social demands, which we usually call honor or something. This is conformance to internalized norms. It means that we do conform to social norms even when nobody is looking. And then we have cognitive demands and these cognitive demands is for instance competence acquisition. We want to learn, we want to get new skills, we want to become more powerful in many many dimensions and ways. It's good to learn a musical instrument because you get more competent. It creates a reward signal, a pleasure signal if you do that. Also, we want to reduce uncertainty. Mathematicians are those people which have learned that they can reduce uncertainty in mathematics. This creates pleasure for them, and then they find new uncertainty in mathematics, and this creates more pleasure. So for mathematicians, mathematics is an unending source of pleasure. Now unfortunately if you are in Germany right now studying mathematics and you find out that you are not very good at doing mathematics, what do you do? You become a teacher. Laughter and applause And this is a very unfortunate situation for everybody involved. And it means that you have people which associate mathematics with uncertainty that has to be curbed and to be avoided. And these people are put in front of kids and infuse them with this dread of uncertainty in mathematics. And most people in our culture are dreading mathematics because for them it's just anticipation of uncertainty, which is a very bad thing, so people avoid it. Okay, and then we have aesthetic demands. There are stimulus-oriented aesthetics. Nature has had to pull some very heavy strings and levers to make us interested in strange things as certain human body schemas and certain types of landscapes and audio schemas and so on. So there are some stimuli that are inherently pleasurable to us, pleasant to us. And of course this varies with every individual because the wiring is very different and the adaptivity in our biography is very different. And then there's abstract aesthetics and I think abstract aesthetics relates to finding better representations, it relates to finding structure. Okay and then we want to look at things like emotional modulation and affect and this was one of the first things that actually got me into AI. That was the question, how is it possible that a system can feel something? Because if I have a variable in me, which says fear or pain, it does not equate to feeling. It's very far different from that. And the answer that I found so far is that feeling or affect is a configuration of the system. It's not a parameter in the system, but we have several dimensions, like the state of arousal that we are currently in, the level of stubbornness that we have, the selection threshold, the direction of attention, outwards or inwards, the resolution level that we have, which we look at our presentations and so on. And these together create a certain way in every given situation of how our cognition is modulated. We are living in a very different and dynamic environment from time to time. When we go outside, we have very different demands on our cognition. Maybe we need to react to traffic and so on. Maybe we need to interact with other people. Maybe we are in stressful situations. Maybe we are in relaxed situations. So we need to modulate our cognition accordingly. And this modulation means that we do perceive the world differently, our cognition works differently, and we conceptualize ourselves and experience ourselves differently. And I think this is what it means to feel something, this difference in the configuration. So the effect can be seen as a configuration of a cognitive system. The modulators of the cognition are things like arousal and selection threshold and background checks level and resolution level and so on. Our current estimates of competence and certainty in the given situation and the pleasure and distress signals that we get from the frustration of our demands or satisfaction of our demands, which are reinforcements for learning and structuring our behavior. So the affective state, the emotional state that we are in is emergent over those modulators. And higher level emotions, things like jealousy or pride and so on, we get them by directing those effects upon motivational content. And this gives us a very simple architecture, or it's a very rough sketch for an architecture. And I think, of course, this doesn't specify all the details. I have specified some more of the details in a book that I want to shamelessly plug here. It's called Principles of Synthetic Intelligence. You can get it from Amazon or maybe from your library. And this describes basically this architecture and some of the demands for a very general framework of artificial intelligence in which to work with it. So it doesn't give you all the functional mechanisms, but some things that I think are necessary based on my current understanding. We are currently at the second iteration of the implementations. The first one was in Java in 2003 with lots of XML files and design patterns and Eclipse plug-ins. And the new one runs in the browser and is written in Python and is much more lightweight and much more joy to work with. But you're not done yet. Okay. So, this gets us back to the question, is it going to be one big idea or is it going to be incremental progress? And I think it's the latter. If we want to look at this extremely simplified list of problems to solve, whole testable architectures, universal representations, universal problem solving, motivation, emotion and affect and so on. From where I stand, I can see hundreds and hundreds of PhD theses and I'm sure that I only see a tiny part of the problem. So I think it's entirely doable, but it's going to take a pretty long time. It's going to be very exciting all the way because we are going to learn that we are full of shit as we always do when we do a new problem in an algorithm and we realize that we can test it and that our initial idea was wrong and that we can improve on it. So what should you do if you want to get into AI and you are not there yet? So I think you should get acquainted of course with the basic methodology you want to get programming languages and learn them. Basically, do it for fun. It's really fun to wrap your mind around programming languages. It changes the way you think. And you want to learn software development. That is building actual running system, test-driven development, all those things. Then you want to look at the things that we do in AI so far, like machine learning, probabilistic approaches, common filtering, POMDPs, and so on. You want to look at modes of representation, semantic networks, description logics, vector graphs, and so on, graph theory, hypergraphs. And you want to look at the domain of cognitive architectures, that is, building computation models to simulate psychological phenomena and reproduce them and test them. But I don't think that you should stop there. You need to take in all the things that we haven't taken in yet. We need to learn more about linguistics, we need to learn more about neuroscience in our field, we need to do philosophy of mind. I think what you need to do is study cognitive science. So what should you be working on? Some of the most pressing questions to me are, for instance, representation. How can we get abstract and perceptual representation right and interact with each other on a common ground? How can we work with ambiguity and superposition of representations, many possible interpretations valid at the same time? Inher we work with ambiguity and superposition of representations, many possible interpretations valid at the same time? Inheritance and polymorphy, how can we distribute representations in the mind and store them efficiently? How can we use a presentation in such a way that even parts of them are very valid? And we can use constraints to describe partial representations. For instance, imagine a house, and you already have the backside of the house and the number of windows in that house. And you already see this complete picture in your house. And at each time, if I say, OK, it's a house with nine stories, this representation is going to change based on these constraints. How can we implement this? And of course, we want to implement time, and we want to introduce uncertain space and certain space and openness and closed environments and we want to have temporal loops and action loops and physical loops and uncertain loops and all those things. Next thing, perception. Perception is crucial. Part of it is bottom up, that is driven by cues from stimuli from the environment. Part of it is top-down, it's driven by what we expect to see. Actually, most of it, about ten times as much, is driven by what we expect to see. So we actually actively check for stimuli in the environment and this bottom-up, top-down process in perception is interleaved and it's adaptive. We create new concepts and integrate them. And we can revise those concepts over time. And we can adapt it to a given environment without completely revising those representations, without making them unstable. And it works both on sensory input and memory. I think that memory access is mostly a perceptual process. It has any time characteristics, so it works with partial solutions and is useful already. Categorization. You want to have categories based on saliency, that is on similarity and dissimilarity and so on that we can perceive. Based on goals, on motivation relevance. And on social criteria. Somebody suggests me categories and I find out what they mean by those categories. What's the difference between cats and dogs? Probably never came up with this idea on my own to make two baskets, and the Pekingese and the shepherds in one, and all the cats in the other. But if you suggest it to me, I come up with a classifier. Then next thing, universal problem solving and taskability. We don't want to have specific solutions. We want to have general solutions. We want it to be able to play every game, to find out how to play every game for instance. Language, the big domain of organizing mental representations which are probably fuzzy distributed hypergraphs into discrete strings of symbols. Sociality, interpreting others, as we call theory of mind, social drives, which make us conform to social situations and engage in them. Personhood and self-concept, how does that work? Personality properties, how can we understand and implement and test for them? Then the big issue of integration, how can we get analytical and associative operations to work together? Attention, how can we direct attention and mental resources between different problems? Developmental trajectory. How can we start with kids and grow our system to become more and more adult-like and even maybe surpass it? Persistence. How can we make the system stay active instead of rebooting it every other day because it becomes unstable? And then, of course, we have to have a system that is able to do all of the things that we want to do. So, we have to make the system more active instead of rebooting it every other day because it becomes unstable. And then, benchmark problems. You know, most AI is having benchmarks like how to drive a car or how to control a robot or how to play soccer. And you end up with car driving toasters and soccer playing toasters and chess playing toasters. But actually, we want to have a system that is forced to have a mind. That needs to be our benchmark. So we need to find tasks that enforce all this universal problem-solving and representation and perception, and supports the incremental development, and that inspires a research community, and last but not least, it needs to attract funding. So, it needs to be something that people can understand and engage in, and that seems to be meaningful to people. So, this is a bunch of the issues that need to be urgently addressed in the next 15 years or so. And this means for my immediate scientific career and for yours. career and for yours. You get a little bit more information on the home of the project, which is at mygroups.com. You can also send me emails if you're interested. I want to thank a lot of people which have supported me, and you for your attention, and giving me the chance to talk to you about AI. Thank you. Thank you. Thank you. Thank you.", '29.24888563156128')