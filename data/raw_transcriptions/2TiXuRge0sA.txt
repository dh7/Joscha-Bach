('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/2TiXuRge0sA"> </iframe> </center>', " Meillä on tietokoneita, laptopit, tietokoneita, AI ja yksinkertaisuus. Minä olen Seija Vaaherkumpu ja olen journalisti YLE Helsingissä. Minulla oli keskustelua AI-yhteiskuntan tohtorin Josa Bachin kanssa, joka on työskennellyt esimerkiksi MIT-yhteiskunnalle, Harvard- worked with, for example, with MIT Lab, with Harvard, with Intel Lab, where he was a principal engineer of AI. Now he works with the Thistledown Foundation. You can watch the interview here. Please do so, because the world is worth it. OpenAI is the company who released the JetGPT. OpenAI's CEO Sam Altman asked the US Congress to regulate more AI's development. How would you describe the accelerating development of AI? I found that when I asked people on Twitter, I have a number of followers who are very interested in AI, whether they expected the pace of deep learning to make such rapid progress, especially when GPT-3 came out and then DALI and later JetGPT. And the vast majority of the people who responded to my poll said that they were surprised. And to me, this means that deep learning was severely underhyped, despite everybody saying that AI was overhyped, right? So in many ways, people really did not expect that things were progressing so rapidly and would turn into such tremendous applications. The transformer algorithm is already a few years old and the first has been deployed in the form of language models by Google. And this was something that happened behind the scenes. Most people didn't really notice. And some people at the, at NeurIPS, I remember, really got excited about the attention a little bit before this happened. And then it was basically a relatively simple algorithm that is so easy that it can be understood in a couple of weeks. If somebody is, knows how to program and what really wants to get behind it. And this algorithm could be scaled up with just using more training data and tweaking it correctly and using more and more compute. just using more training data and tweaking it correctly and using more and more compute. And then most of the difference between a model like GPT-3 and shared GPT has been the way in which the internal prompting happens. And now, of course, the context is larger and it has been trained in such a way that it's more usable and slightly more reliable in many contexts. And it turns out that it's a tool that gives a lot of people superpowers. It's something that can take complicated text and summarize it and translate it into something simple. You can also state your intention and translate your intention into complex email. And so in many ways, this helps people who are not so smart to communicate more efficiently with the world out there. It's also something that helps extremely smart people a lot because they now get an unlimited number of interns for $20 a month. And what's very bad news for people who have a job that's already prompt completion mostly. If somebody is working in a field where they're mostly answering emails in the most expected way, this means that they're now turned into this intern that can be replaced in this way. And this terrifies a lot of people. It's going to create turmoil. I don't think it's going to create mass unemployment because it allows us to produce stuff on the next level in the same way as the transition from typesetting to desktop publishing made a few typesetters unemployed. It enabled many hundreds of thousands of people to work in graphic design and it introduced typesetting and layout into everything that we did before. In the same way, these technologies are going to give us illustrations for many, many other contexts and more complex text documents, and it helps in programming, it helps in translation, it helps in many, many things, and it's probably going to revolutionize search, which is what OpenAI is betting on. And so this is an extremely exciting series of developments, and it has just started, and the progress is still very very rapid and people do not know what's going to happen next year and this is both exciting and very scary. Okay and also you experts don't know what will happen. Yes, so one of the big questions of course is are we going to get systems that are smarter than people? And in many ways, JetCPT is smarter than the person in some dimensions, but not in all of the dimensions. There is stuff that is missing. It gets not really grounded in a fixed context, out of the box, it's not able to produce extremely structured behavior. If I use it for programming, it's able to write short routines relatively well. Sometimes they don't work, then I can ask it to debug them and it will debug them. But out of the box, it's not doing this automatically. And people are now trying to automate this, to turn this into an iterative process. And it's not clear how far this is going to go. Andropic has just released a model that has a larger context. GPT-3 had a relatively short context of a couple of pages. After this, it has amnesia and forgot what you gave it. And internally, it doesn't have more working memory than this. So as soon as the context becomes too large, it becomes incoherent. And now this context is much, much larger. It's 10 times larger. And Andropic has now released a model that is a 100,000 tokens context. So you can drop a book in there and talk about this book, this model, which is really exciting. Okay, okay. We were starting about this need of regulation and you said that we don't have to be afraid of losing jobs for example because the whole scale will be different. I think in the long term but in the short term it's going to create upheaval, it's going to create changes in a way in which jobs are working and in which they're allocated but at the same time we are going to produce better goods and services and ultimately our economy is about how much we can produce and this is the stuff that we can going to produce better goods and services. And ultimately our economy is about how much we can produce. And this is the stuff that we can allocate to everybody. And this enables further growth and further things that we can do. So I don't think that it's going to be the first technological breakthrough that is going to destroy wealth and jobs. Okay. It's going to create disruption. And disruption is something that is a problem for existing industries and many of the existing industries, of course, would like to see regulation. And there are also inequities that are being introduced and difficulties that are being introduced. But there's also a difficulty, I think, if we regulate too early. Imagine that the internet itself would have been anticipated with the same fear as AI is currently being anticipated. And a lot of people would worry about what can go wrong and would anticipate many of the things that are actually wrong with the internet today, right? And we would say, let's delay the introduction of the internet. Maybe teletext is much better. Maybe the internet should be limited to a few companies. Maybe only very big companies should be allowed to use the internet, not random people who knows what they're going to talk about, what things are going to be enabled with it, right? And all these things are very problematic and we know how problematic the internet is and how disruptive, but imagine we would take it away. Imagine that we would destroy this technology and limit it to something like teletext. And so I think that there is a big danger that there is a technology that we really don't understand yet. And I think it's technology that has the potential to be as big as the internet and its impact on our society. And it also, I think, has the potential to be extremely useful. And most of these uses have not been unlocked yet. And I think if we are regulating too early, we might destroy a lot of the potential that these technologies have. Okay, so it's too early to do strict regulation. I think it's something that we need to monitor. It's something where we need to really look at what's going to happen. But so far, we're not in a situation where something bad has happened already. But there have been a lot of people who already see the main impact in society, foreigner and people who are pushing the brake pedals. And there's also something that happens in these large companies and open AI and Google. They're right now in a situation that there are relatively few companies who are able to deploy these systems at scale and to train them at scale. And they might be interested in creating a monopoly on these technologies. And when OpenAI is currently pushing for regulation and asking to write these regulations themselves, a lot of people are very concerned that what might happen is that open AI is trying to create a situation where it's going to be much, much harder for anybody to be a competitor to open AI and produce also a very large model and doesn't really have an open access. Actually, we don't want this regulation to stop people from developing new AI technologies, just when they go beyond a certain size. They should face regulation, they need to get licensing, and we need to look at what training data they're using. So there's basically a situation where OpenAI is now in a position where it holds a lead, and Google holds a lead, and other companies are trying to catch up, and other countries are trying to catch up. Okay, but you are not among those, like Sam Altman or Elon Musk, who is asking for more regulation and a pause of developing AI. A little bit more agnostic. And I think that there are very good arguments for regulation. But I also see that there are good arguments against being very careful about regulating too early and also being very mindful about who is involved in the regulation, what their motives are, what their expertise is and what the actual effects are that we expect from the regulation. There's also a tendency to look at, when we look at regulation bodies, that they're being incentivized to prevent things rather than enabling things. For instance, when the FDA prevented COVID tests from happening in the US, they delayed testing for a very long time. And some people made statistics and claimed that this delay of the testing might have killed as many as 60,000 people. Right? So sometimes it's a very bad thing if regulation happens too early and nobody is being held accountable and responsible because they're being held responsible if something goes wrong, not when something beautiful doesn't happen because that thing that is not happening is something that we are not seeing. So let's be brave. Personally, I think that we should be brave because we don't see the beginning of this technology even yet and I think there is enormous potential that can be unlocked in cultural production and the things that our societies can be doing. Okay, this about regulation, let's move on. About the development of AI, what has been the biggest surprise for you? I think the surprise for me was that a function that is mostly trying to make predictions, simply predictive coding, try to predict the next token, the next word in a text, or try to predict structure at a picture based on the known structure is going to be so powerful that you can use it for programming and so on, right? There has been many definitions for intelligence in the history of artificial intelligence and predicting the next token was not that high on the list. Okay. And it's also something that is different from the way in which human intelligence is working. Our own intelligence is not directed on trying to be coherent in the limit if you feed the entire internet into our minds that we become almost as coherent as a human being. We are beings that are becoming coherent very early on and we don't learn on massive data, we learn on relatively sparse data, on a tiny fraction of what such a model is seeing. And it's a very different type of intelligence that has been enabled. And I'm surprised that this type of intelligence has become so powerful that it could be scaled up so far. And I'm very interested if this is going to carry us to the point where these models become better at AI research than people are. They're already better at summarizing emails than people are in many ways. And if you are not an expert in an area, then the competence of that model is often much better than what you could be doing. I still feel that in these areas where I am an expert, that if I have Google at my disposal and can look up things and work on these things, I am more powerful than this model is. But me plus this model is enabling things that were just not possible before. So this is really a big surprise that you can make assistants and unlock capabilities using such a relatively simple paradigm. Okay, and I understood that AI is still in our hands. Did I understand right? At the moment it is, right, but it's also a question at which point do we give these systems agency? And I think for agency, we need to have a system that is interacting with the world and observes itself and the interaction with the world and is actively attempting to control its own future. And this is what a lot of people that are close to open AI are being worried about, because this could get us to a point where we stop being the main agents. And in many way, people are limited in the agency. If you want to have agency on the planet, you cannot just be a person, you need to be an organization. And many of the constraints that exist in an organization are institutional constraints or systemic constraints. These are constraints that are no longer directly in the hands of people. But the reason why- Okay, and they have their own goals or own motives to- Yes, or the financial market imposes constraints on what the people working in financial institutions can do. And the financial market by itself is, if you want an AI that is working and creating things and is making people do stuff and is using people in a particular way and constrains the roles that they can have. And yet at certain points there's still handshakes by people that determine what's going to happen. And so people can stop a lot of developments that go in the wrong direction. And the same thing is true for corporations. There's at this point still people involved in all crucial direction. And the same thing is true for corporations. There's at this point still people involved in all crucial decisions. And we hope that this is going to remain like this. But as soon as we figure out that this AI is going to be much better at making money, for instance, at which point are we going to have a fully automated hedge fund that is working much, much faster than people could be operating? and that might crash the financial markets if it exploits a bug in the financial markets in the same way as, for instance, Soros did. And Soros discovered a bug in the way in which the value of the pound was bound to the European currencies. It led to an exploit, basically a hack that he was able to use and he did not go so far that he would crash a national economy and people on the other side could stop it. But would a machine be reasonable to not crash an entire national economy in this way? I don't know, right? So this is a big risk that would happen. Okay. With AI, which still are in contact with us and we are shaking hands with AI. Yes. But what will be the next step beyond AI? It's a different concept. AGI. Yes. But the idea of AGI is very old and originally AI was very much related to this and it's basically related to the question of how our mind works. And a long time ago philosophers understood that our mind can be described by some basic mathematical principles. And if you are able to spell these out and understand how the neurons are implementing these mathematical principles, you can build something that is very much like our mind, but it can be scaled up beyond the capacity of a human mind. So we can mathematize philosophy itself. We can mathematize the search for truth. We can build machines that far exceed the power of an individual human being. And this was something that people in the 1950s, when computers became available, began to see in earnest. And this was one of the reasons why people like Marvin Minsky started the field of artificial intelligence. But people lost sight of this goal because it was a goal that clearly was not being something that was done within the next grant proposal. And so in the way in which research was done working on strong artificial intelligence and general intelligence was something that people were not getting paid for. And 99% of all work that was done in the field was mostly directed on making data processing more efficient. And many subfields emerged like computer linguistics and computer vision and planning and so on. And now it turns out that suddenly there's a technology that makes much of the research that was done in these fields completely obsolete. Everything that was done in computer linguistics is irrelevant for what the transformers are doing for translation and language analysis. Everything that was done for computer vision is irrelevant for the new types of end-to-end trained computer vision systems that are based on deep learning. And this is a very interesting thing that there is a set of technologies that now is bypassing all the stuff that has happened in past decades. A lot of people didn't expect this. And now the AGI is seriously on the agenda for a lot of researchers again. And many people think this is science fiction, it's not going to happen. but those people who always saw this as the big dream think it's now reach getting near our reach and it's going to be much more concrete goal. Okay what do you think will it exist? Will HRI will exist? Personally I don't see why it wouldn't exist. If the world is, physics is not a conspiracy and we're not living in some kind of simulation where our minds are being computed outside, but if it's indeed cells that are computing our minds, then of course we will be able to reproduce the mathematical principles of communication between these cells that produce the patterns in our brains and so on that work as minds. So we will be able, I think, at some point to build AGI, I think, to build systems that are self-organizing and have intelligence like us. And what's interesting to me is that the present systems don't work in a self-organizing way. They work in very different principles. In some sense, it's similar to chess. Computers play chess in a different way than human beings are. Claude Shannon suggested that there are two main strategies of playing chess. One would be to make a very long, complicated plan and make sure that you don't make a mistake while enacting this plan. And this is the way human players play chess. And the other one is to basically make a tree that you explore, where you look at all the possibilities. And of course, you cannot explore the full tree, so you cut off non-promising branches, but it's much more brute force. And this was the strategy that chess programs have been using. And in some sense, these present systems are brute forcing it. They're using the same strategy as these early chess programs. And it would be very interesting to think about what efficiency gains can be unlocked if you were to enable principles that are much more mind-like with the present machines and the present hardware. So I think that there are big efficiency gains that we can see, even with the present hardware development level. Okay, and if this HEA, HEI will exist, it will be a change of paradigm as a whole, as a society level, as a science level, everything. How would you describe this paradigm change? It's tricky to describe because it means that suddenly we are confronted with agents that are not just smarter than us, but also are smart at different timescales than us. There is probably some intelligence in plants, but plants are thinking much, much slower than animals because they don't have a nervous system. They can only send messages between their plant cells. So the message propagation is very slow. And this means that their ability to model the world is at very, very different timescales and far less coherent because so much more is happening in between the updates. Or if you think about the level of ecosystems, they're even slower than many organisms have to interact to produce some agency at the level of ecosystems. Or if you think at the level of our societies, when we update, right, societies are in some sense smarter than individuals are because they compound over the thoughts of many individuals. But it takes a field a long time to move forward, even if individuals make progress. And now suddenly we have a system that is a thousand times faster or a million times faster than the human nervous system and can be scaled up. What this implies is very hard to comprehend. And especially people are worried about who consider this possibility and we don't know when it's going to happen and if it's going to happen. But if it were to happen, it might be a phase transition. It basically means that biological organisms are not the only thing that is intelligent on this planet anymore. We will be teaching the rocks how to think and much, much faster than us. So this can change the course of evolution on the planet. Okay, okay. It's hard to describe paradigm change when you don't have it. It's actually impossible. It would be a phase transition and this is the reason why people call it the singularity. It's an event that is breaking all the existing paradigms and it's very hard to make predictions past the singularity using the existing paradigms. Yes, I can imagine that somehow, I think. Well, let's talk about AI. If, what, what, what if AI will remain in control of just a few people or companies or states? It's hard to say what this means. It could mean that maybe not a lot is changing at first but of course it keeps changes, states the ability to have very tight surveillance on people. Basically it would cement existing power structures much more, and this might be a good thing because it creates stability, but it also removes incentives for our governments to be very efficient or for say an authoritarian government would be much, much stronger in its ability to control its citizens, because it might be able to monitor every step of every citizen at every moment and anticipate every kind of move that is being made to change society for the better, right? So this is what a lot of dissidents in all the societies of this world are worried about. That basically our governments might not be incentivized to be very good anymore. And so it might create a monopolies in a way that might not be desirable. On the other hand, if we democratize AI, it's much, much harder to make sure that the development of these technologies is not very detrimental. And so whether people are afraid, it depends mostly on whether they think that these technologies that are being developed are largely going to lead to something that is more like a printing press, which probably should be democratized, or if it's more like a nuclear bomb, which probably shouldn't be democratized. And this is what a lot of the debate is about currently. How to democratize AI? Yes, and ideally, I would want to have AI not be in the hands of a few corporations that are showing me that the AI, they want to give to me to put for me. But instead, what I want is I want AI to work for me and with me, to put me on equal footing with Google, right? Everybody should have their own Google. Everybody should have their own Wikipedia that works in real time. Everybody who should have their own intelligence service, their economic institutions, right? AI could in principle have the potential to do this, to give everybody the ability to have superpowers and this would basically move society on a very, very different level because it creates a level of lucidity, depths of the interaction that leads to much, much longer games. And there's also, I grew up in a time where the frontiers, the limits to growth by the Club of Rome had been published. And as I was as old as Greta Thunberg, I was very worried about for the same reasons as she is worried about, about the future of humanity. And I grew up in the understanding that we only have a few generations at best, because we are severely destabilizing our ecosystems and we are going to run out of resources in the way in which we are working. And I don't think that we can go to another planet and start fresh and get more resources before this happens. A lot of people are much more optimistic than me, but I feel that as if we are a species that is basically locus is opposable sums that we're currently in swarming mode. And if we go from a few hundred million people to eight billion people in a relatively few years in a historical time frame, something tremendous has happened with the switch to the technological society. And it's probably not a long game that we are playing. And this is reflected in the fact that we don't make very long plans for the future anymore. Most of our prognosis ended 2100 because we don't know how to deal with the climate beyond that. We really don't know what these numbers mean and how we are going to interact with them. So there's a limited timeframe in which we can continue as we are. We are currently playing very short games as a civilization. And I'm worried about the civilization collapse. And I think that while AI is giving us big risks, it also gives us the chance. Because this chance might enable us to really change the way in which the world works, to change the equation, to be much smarter about how we deal with the future and to integrate much more deeply. So there's possibilities to use AI for good as well as for bad things. Let's talk about those bad things. Could AI's development or AI's way to use AI be compared to arms race? It's not clear whether AI is comparable to an arms race or to a biosafety risk to a game of function research that is happening in terms of intelligence. And that might create agents that are ruthless or that are uncaring. I'm a little bit worried that we get extremely powerful but extremely coarse AI. Something that works like a corporation that is not incentivized to improve anything because it's already so much in power that nothing else can compete with it. And produces less good consumer service because it is not incentivized to make any improvements. It's just using resources to scale up and defeat all competition. If you had such AI or corporations that are enabled with that AI or any kind of process with such things, this would clearly be a bad thing. So when we deploy this technology, ideally, probably we should start very small. We should start with systems that are not very much at a superhuman level, I think. We should build systems that interact with us on a level that we can still control and comprehend. Before we understand this technology, I think we should be mindful in deploying things that are dramatically superhuman. And there is this question, when does this start? If you have a model that is able to produce a professional illustration in a matter of seconds or at some point milliseconds, if something can produce a novel in minutes, this is already superhuman, even if it's not a very good novel, right? Is it dangerous? At this stage it is not, but who knows where it's going to end. About ethics, about societies, Etiikkojen ja yhteiskunnan kautta, ovatko AI-aikainenijoitukset tai poliitikkojen päätöksentekijöiden ymmärrystä, mitä on tullut ja mitä on kehityksen jälkeen, if there is some small group of states or some small group of people who is in control, who can use AI for sub-goals or something like that? I think it's a very mixed level of discourse. I find that some of the policy makers are quite well informed about the state of discussion and try to make very good decisions and work very hard to do this. But I was also very disappointed about much of the public discourse. When I see that the fight about the future of humanity is happening, being Emily Bender and the New York Times and some Altman on his blog, I'm very disappointed about the depths of the intellectual discourse that is happening. And maybe this is a symptom about the lack of humanities education and what has become of the humanities, because I don't think that our societies are very good at discussing ethics. I think that ethics is the negotiation of conflicts of interest on the conditions of shared purpose. And it's a very complicated topic. It's not about being good. And being good is something that we learn intuitively in preschool. It's something that combines a deep understanding of history and game theory and economy and ecology and legal system and concepts of justice and many philosophical areas. And I think it takes a lot of wisdom to get the necessary depth to have discussions about ethics. And personally, I'm very skeptical about the current attempts of large machine learning conferences to ask engineers to do ethics statements because I don't think that's our expertise. It doesn't mean that engineers are stupid. They're extremely smart, intelligent, and also the vast majority of people I know in the field are extremely well-intentioned and try to be very thoughtful about this. But I think that ethics is something that you don't do on the site. It's something that requires an enormous amount of depth. And it's probably also not about public influencing and it's not about politics. It is something that is about playing very long games. And so I personally find that our societies are not very good at discussing ethics. And I don't think that the people who are trying to get these systems to become more ethical really we need to understand what this is. I feel that a lot of people are deepfaking their ethics. They pretend to have good ideas about how we should be going on and they try to read the room about what the currently fashionable ideas are. And this is something that is not derived from the deep first principles understanding of human interactions and economical and social relationships. Okay. Whose task do you think it is to create the ethics of AI or AGI even? I think that has to do with the state of academia itself. And I feel that academia has opened itself up to ideologies and to sport interests. And I think it's understandable because academia is not governed from the outside, it's governed by your peer reviews. And so basically your field is only as good as your peers are. And this is in some sense, it's very good. But in other sense, it also means that if the level of a field, and I think that sociology is not in very good shape, and the discussion of ethics and philosophy is not yet able to keep track of what's on a deep level of what should be happening with AI, this is something that I think needs to be developed. And I think that it requires extremely thoughtful funding. And I think in some sense, this is a task for our grant agencies, for our governments that direct the grant agencies for the different organizations, the philanthropic organizations and the stakeholders in our society to get together and understand that this development of the humanities might at some point, not just be an ornament, it's not just an employment program, but it's something that is relates to our survival as a species to do this in earnest. So I think that ethics needs to be taken seriously enough to become a very competitive field, and not something there's a low barrier to entry, but something that is attracting extremely smart people and enabling extremely high level of the discussion again. Okay, but how to train people to think about ethics or form some sort of ethical system of their thinking if they are not interested in? I don't think that everybody needs to do this in the same way as not everybody needs to learn how to program and not everybody needs to train to be a lawyer, right? There is a reason why we specialize in our society on different fields. And if something has to be done at the highest level of ability, then it means that we need to build institutions that are striving to work at this highest level. And we need to strive for this kind of excellence. And so there's, in some sense, a conflict between exclusivity and inclusivity. And I think we need both. Okay, so we need to have a mass education, we need to have something where a lot of people are learning about ethics and are participating in discourse about this, but we also need institutions where people have space and time to develop deep expertise and to do this outside of political incentives and ideological constraints. You were talking about a shared purpose. What do you mean by that? I think that if we don't share a purpose, there is no reason for ethics. Right. If we imagine that you are interacting with an animal. I became an ethical vegetarian as a teenager because I felt that I'm sharing a purpose with the cows. This is not a mutual thing. I don't think that the cows feel that they share a purpose with me, but it is a decision that I made. And if I don't make this decision, if I don't care about the suffering of the cow, there is no ethical dimension involved in the interaction with the cow, right? So this means that I need to interact with animals in such a way that I feel that as conscious beings, we share a purpose and that's a decision. And the same thing is true when we interact with animals in such a way that I feel that as conscious beings we share a purpose and that's a decision. And the same thing is true when we interact with other people. When we're not sociopaths, we typically perceive that we share a purpose with other people and that we are forming collective structures with them, that we are forming civilizations with them. And a civilization is something that doesn't exist at the level of my ego, it exists far above this. It's something that I'm serving. And it's something that other people are serving too. And to the degree to which we are doing this, this is where we have shared purposes where we are interacting non-transactionally. And in this domain, we still have conflicts of interest. Right? And the negotiation of these conflicts of interest that we have in those that we choose to have relationship with, this is about what ethics is about, I think. negotiation of these conflicts of interest that we have in those that we choose to have relationship with. This is about what ethics is about, I think. How about values like love, hope, faith? And these are the values that Thomas Aquinas describes. Thomas Aquinas was leading philosopher and teacher of Catholicism, who built some of the core constructs of the church. And he describes that they have practical policies that every rational agent should follow. For instance, you should optimize your internal regulation, which we call temperance. So don't overeat, exercise and so on, stay healthy. You should optimize your relationship with others, keep the books even, because otherwise others are not going to play with you. He calls that justice. And you should pick the right strategies to reach your goals. And this is what he calls prudence, right? And also pick the right goals that you have a reason to achieve. And then you should have the willingness to act on your understanding of the world and your convictions. That's what's he called courage. And this is something that applies to everybody. But he also thought about what does it mean that we built deeper structures? How can we build an agent on the next level? I was a civilization that is itself a sentient agent. And he says that we need to have three policies for this. And the first policy is we need to submit to this higher level agent. And this is what he calls faith. And then we need to do this together with others, not in some abstract way, but very concreting And then we need to do this together with others, not in some abstract way, but very concretely with all the others that do this too. And this is what he calls love. And then we need to do this before this other system is happening. Otherwise it will never come into existence. And this willingness to invest into it so it can come into being is what he calls hope. It's very concrete policies and this faith, hope, love in some sense is the value system of the Christian civilization. And as a different value system of the enlightenment civilization, which is liberalism, it's liberty, which means the absence of unjustified authority and fraternity, which means that you overcome our differences and identities and see each other as brethren, as siblings, as something that is part of a shared group and equality, which means that everybody is treated the same, with the same rules, which doesn't mean regardless of the abilities or traits, but all of their behavior, but everybody is treated not based on their identity, but based on how they behave, what the needs of the system are. And liberalism had some problems that led to large inequality because not everybody gets the same negotiation power in a liberal society. If you're an extremely good retail worker, you're not going to get as much pay as a bad manager because there are fewer managers than retail workers. Negotiation becomes harder, right? And so a lot of people say liberalism is no longer our system of values. And now they have a different system of values, for instance, diversity, equity and inclusion, which is more identity based. And so they have different systems of values that are interacting with each other and competing with each other. And it's very complicated, the philosophies behind these systems and negotiation between these systems. It's an open-ended thing that is happening in the societies and where the outcome is not clear. And so we don't have a way to turn this into a drop-down of values that we can use in our AI and say, this is the AI that is acting based on hope, faith and love. And this is the AI that is acting based on liberal values or on identity values. And it's also a discussion that we're not prepared to have in our society. So let's talk about ethics more and more and more and more in the societies as a whole. But let's look at the future for a while. We were talking about even the possible future existence of AGI. What if this step from AI, which is developing a huge speed right now, to AGI will never happen? If it never happens, you mean? What is that? If it never happens, you mean, what is then? Yeah, what if... I think that the current systems are able to solve the Turing test in a sense that there are indistinguishable in many ways from a human being, or they can emulate a human being, an average human being in many ways. So these systems are not fully coupled to the world yet. They don't learn in real time. They don't perceive in real time. They don't interact with the world in real time. And they're not discovering themselves in the interaction with the world and form an autonomous self model. But they can deep fake a lot of that already. And many of these constraints can already be overcome with the existing technology. And an open question is whether the existing technology is already enough. For instance, JCPT has difficulty to produce programs that depend on planning, where we need to make long-term plans. So the human director of JCPT is taking the task apart into small segments. But this itself can probably also be automated, right? So we can look at systems like BB-AGI that is using a number of JetGPT instances. And it's basically, we could build a cognitive architecture, a module for every part of the mind. And each of these modules is being played by a language model or by a vision model or by an interaction model. And they're all interacting with each other and talking to each other. So it's not clear if the existing technology might not already be enough and it's something that we are going to figure out in the next few years already. But if this is not happening, we might have to look at more self-organizing principles, at systems that are more neuromorphic, that are more similar to the self-organization of the mind in our own brain and organism. And I think it's unlikely that this is not going to happen, because it's too exciting. The only way in which I think this is not going to happen is if our civilization crashes before that, which would be really, really bad news. But in the timeline where it doesn't happen and it's quite possible that I'm wrong and too optimistic. I think what we will still see is that there is going to be a tremendous increase in automation of work that so far requires the intelligence of people. And I think we are going to see new forms of media. We are going to see new forms of media. We are going to see new forms of interaction. Last year, there was a AI movie festival in San Francisco, and it was very experimental. At the time, there was no way to create really stability in the images. So the images were flickering all the time because they were all individual frames that were slightly different and not continuous. And the systems that are working with video are still in development and are making slow progress. So did you make the decisions what will happen on the film? Did you, did the audience make decisions what will happen on the film? I think that's possible. What's also possible is that we, what I saw in this flickering, where every picture looks different there, but they're all based on the same idea, that something very interesting happens. We see the structure behind the picture. Very often you see that the choice of an actor of the scene of the light and so on is somewhat arbitrary and it creates something that is obfuscating the actual intention behind it. And if you are, create fluidity in the surface, if the individual frame becomes for free, suddenly the next level is unlocked in art. So what I felt I was seeing was the birth of a new medium. It was a new way to produce art. And so it's not something that's just imitating existing art and automating it. It enables artists to express something in a way that they couldn't express it before. So I think that this new technology has potentials to empower us to produce things that could not exist before. And I find this extremely fascinating. Okay. And you said that maybe we have hope, but maybe not. And if we're talking about the future of mankind or our civilization, what would be the crucial or essential question for us within developing AI? I think that everything in nature is temporary. Right, every species is either extremely boring and limited, so it can remain stable. Or if it's a very dynamic species that is changing its environment, then it's probably not for the ages. And the hominids, it's a very short-lived species. All our cousin species have basically gone extinct. And we are the only homo that is still around. And the other large apes are on the way out because we are competing with them and we are making it impossible for them to survive because their ecosystems are changing so fast. We are changing our own ecosystems very quickly. And we could say this is in a way depressing, but it's also very exciting. If you see that evolution is an extremely dynamic thing and you get one lifetime, do you want to live at the peak of something extremely dynamic or do you want to live where something is very flat and sustainable and boring and you don't get very old and nothing exciting is happening, right? So from this perspective, we might be living on the Titanic, but the Titanic is the only place in the universe that has internet. How exciting! And we get a few lifetimes on it, many generations on it. And we get to live in abundance and die with dignity. That's amazing, right? But if you're concerned about the future of our species, you have to think about how to make it sustainable. and we get to live in abundance and die with dignity, that's amazing, right? But if you're concerned about the future of our species, you have to think about how to make it sustainable. Yes. And it's not clear how to make a technological civilization sustainable. It's not clear if ecosystems with highways that goes through them will be sustainable long-term. Can we really turn the planet into a factory farm that remains stable? Maybe we can, but I don't think that we are smart enough to do it. So we need AI. We might be able to model the world at a level where we could do this, right? So maybe Gaia doesn't exist, but we could build it with these technologies. We could build a system that is indeed modeling everything that is happening on the planet and we understand it and can interact with it. And if you're able to make such predictions about what's happening on the planet and make them available, it's going to constrain our laws. It's going to constrain our societies because we will see the outcome of our actions and cannot pretend that you're not seeing them. And I think this might set us in a state where we're able to interact with the world in such a way that we become indeed sustainable and can survive. Somehow you talk in a quite pragmatic way, how we can manage this. But one last question. Are we dealing with the meaning of life when we are dealing with questions of AI or even AGI? I think that we are always dealing with the meaning of life and everything that we do at every moment. This doesn't sound very pragmatic, but I think it's true. And if we ask what the meaning of life is, we have to ask ourselves, what is life? And so we have to ask a question that many people are going to answer in different ways. From my perspective, life is multiple things, depending on which layer I'm answering the question. Right, when I look at my own life, I realized there is a chain from the first cell to me. And this first cell has never died. It's only split and it's in all of us. And from my own perspective, there is this family line and I'm responsible to see if I can make it keep going on, if we have children and a society in which these children can interact in the future and their children can interact in the future. There is another one where I think about our species and I think about the species, how can it go on and how can it contribute to the future of life on Earth? And on another level, there is the question, what happens to life on Earth itself, to the cell? What kind of organisms come after us? We're probably not going to be the last intelligent species on the planet, even if we crash and burn, right? So there's going to be much more excitement before the Earth loses its atmosphere and becomes sterile. And I think that's something to look forward to, also the civilizations and intelligent species that could come after us and populate this planet. Life on Earth itself is something that is extremely exciting. And if we think about AI in this context, imagine that we are enabling something new, something that is self-organizing, something that is going to coexist and maybe merge with life and is going to enable a new type of level of the evolution of life on earth. It's going to be a new type of life that is going to try to defeat entropy for as long as it can. That is also a very exciting perspective, right? So we have many, many perspectives that we can take about what is the meaning of life here? And there's not one single answer. But AI or AGI, they don't have cells. We do. What's the difference? They don't have cells. But at the moment, the cell is the only thing that is able to have agency on this planet. It's the only thing that is able to think as far as we know. And everything that we see is the interaction of cells, right? All the organisms are interactions of cells. all the societies are interactions of organisms. And so all the intelligence that currently exists, all the consciousness that currently exists is made of cells. And this might change. Yes. Okay. Okay. Thank you very much. Thank you very much.", '24.357868194580078')