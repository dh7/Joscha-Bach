('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/lKQ0yaEJjok"> </iframe> </center>', " Good morning, dear fellow conscious beings, and good day to you, fellow human beings in front of the displays at home. I would like to talk about four basic questions. Actually probably I'm not able to answer them completely today because they're sort of the first questions that we have. It might not be quite true. I think the first questions that we had as humanity was where's the food and then how can we push back entropy far enough to get warm and to get the dirt off us? And then once that was solved, where can we find love? Because love is important. And when this was solved, these questions remained. That is, what's the nature of reality? What does the universe look like? What's the state of affairs out there? And then what can we know about it? What's accessible to us? Who are we? What are minds? And what should we do? Philosophy hasn't found final answers to these four questions But it has asked them very nicely and I think that if you want to turn to answers We should look into individual sciences. And I looked into computer science mostly for the last 23 years or so. And I would like to focus on a particular small sub-question, and that is the question of how it is possible that a mind experiences a universe and makes sense of it. How is it possible that signals enter our mind somehow and are assessed by arrays of finely tuned classifiers that react to them and then create dynamic representations of objects and things and give rise to desires and needs and wishes? And then this whole thing triggers associations in our memory and our mind and this all gets integrated into a coherent experience of awareness. This spreads out and stabilizes throughout our mind. And then this gives rise to new memories and imaginations and goals and plans and expectations. And then it feeds back into the world somehow. How is that possible? I think this is basically the most interesting question of all. Reality is not given directly to us. The only thing that we get from it is appearances. Or as Husserl said a bit more than 100 years ago, we get phenomena. And somehow our mind is able to turn these phenomena into ideas and concepts that Husserl called noema. And the process by which he does it is what he called noesis. We cannot really say much about the state of affairs out there beyond the fact that it's able to yield phenomena, appearances. And because we cannot say anything definite about how the structure of the universe really looks like, Husserl said we shouldn't say anything about it, we should just shut up about it and he calls this epochee. So far so good, but then his pupils came along and then they tried to apply this idea of noesis upon noesis itself and they realized if we try to explain noesis all we end up with is noema again. It's all the explanations that we possibly get about the state of affairs are part of our noetic sphere. They're just ideas and concepts. So to formulate them we already need to presuppose noesis. Science cannot reduce noesis itself, Max Scheler said, and therefore it must presuppose the existence of a subject in the first place. This has given rise to continental philosophy and eventually to this division of most of philosophy and the sciences that we experience throughout most of the last century, which I think is very unfortunate. But there might be a way out of this. If we look upon the idea of how a robot perceives the world, it has an interface to the environment. This interface might be a bunch of sensors, and these sensors yield a vector of bits. That is, discernible differences, patterns that are systemic boundary that might be used to reduce uncertainty. And these bits are encoded into some low-level encodings. And then you might have a cognitive architecture implemented in the robot, some processing, that turns this into a conceptual representation that allows the robot to reason about the world and to tell itself a story about what the world looks like and what itself looks like in that world. But of course to a robot there are no phenomena, it's nothing what this feels like to be in this world, what the appearances are and so on. To the robot there are only bits at the interface to the world. But perhaps it's possible to build a cognitive architecture in a robot that is able to encode these bits at the systemic interface into something that looks like percepts to the robot, that looks like phenomena to the robot, and that gives rise to concepts in a very similar way as noise does in a human subject. The phenomena in this way might result from a robot's information processing architecture. And if we buy into this paradigm, of course we might ask ourselves if humans are robots too. And this idea that humans are actually information processing machines is what underlies most of the cognitive sciences, whether they are aware of this or not. How do we perceive reality? You probably know this well-known picture. I think I turned it upside down. Okay. There is something odd with this picture going on. Yeah, this is the normal version. The thing that was odd about it in the first place was that the eyes and the mouth have been turned around. Maybe some of you didn't notice at first. Why is that the case? I think it's because we don't process and take in faces as a whole, but rather we process features in the face independently. And then we combine them into a face schema that we mentally scan. If we look about visual information processing, first of all we have a retina and this retina gives us a vector of bits. That is information, patterns that are systemic boundary that reduce uncertainty that we can encode to form concepts about the world. Data on the retina is not a very good semblance of what's out there in the world or whatever. There's a very sharp spot in the middle that is hard enough to read, it's pretty small. Then there's a lot of mushiness around. Color vision is only there in the center. There is a blind spot in which the optic nerves connect to the retina and where we have no photoreceptors. But we don't see any of this, of course. This data is first projected into what neuroscientists call the laterate geniculate nucleus, which is a fancy that is Latin way of saying that is somehow sideways, knee-like looking protruding core. It's in the center of the brain, and it's part of the thalamus. And the thalamus just means chamber, and it sits conveniently on top of the spinal cord. And it relays a lot of sensory data into the cortex. So you have a crude touch and temperature and pain and so on. This all gets transmitted by the thalamus into the cortex. You can think of it like a switchboard. And this switchboard projects information into the primary visual area, the striate cortex. And this thing, if you look at the cross-section of it, is basically 2mm of highly packed neurons on top of lots and lots of wiring. In the brain there is something like 180,000 kilometers of wiring, depending on your age and gender, this varies a bit. And on top of this wiring in the cortex we have these two millimeters of neurons that are organized in columns. And in the visual cortex these columns have the highest density of neurons, which is probably because graphics is so expensive to compute. And right now, most neuroscientists believe that the primary organizational unit of these neurons is the mini column. A mini column is something like a circuit that is made up of something like 200 to 400 neurons, half of that in the rest of the cortex. So if we do the math, there are about 160,000 million neurons in the primary visual cortex. We get about something like 300,000 features that can be encoded in the primary visual cortex. And if we look into this, if we do measurements in the brain there, we find that in the middle there is an expanded area because there are more features detected in the middle and it gets compressed around the corners. There's even the blind spot somewhere in there. And the different neurons in this spatial alignment, they code for orientation and for frequency and for color. The primary visual area V1 then projects into V2, which has a stable visual map, so all the movements of the retina are combined in something like a coherent visual field. It is somewhat stable. It also has more complex patterns and textures. It also projects into V3. And in V3, in the visual area V3, we have color and some motion direction. And into V4, we have simple geometric shapes. Before we have mostly looked at non-human primates, people probably have an equivalent to that. It also, these areas project into V5, which has object motion and ego motion in V6. And you can see that now these processing paths split up and we found that there are two primary directions of processing in the brain that work in parallel. One is the what path, it's down there in the brain and it tells you about the nature of objects and the where path which tells you where they are. The where path reaches into those areas of the brain that are then responsible for grasping and attending to objects and their positions. This area that encodes for the what of objects is what we call the inferior temporal gyrus, which again is Latin for the lower sideways bulge. And this lower sideways bulge helps object schemata and enables us to separate figures from ground and also has numerosity, that is, the ability to discern numbers without actually counting. So if we look at, for instance, the side of a dice and we see with a single look how many dots there are on this side of the dice, this is numerosity and it's done in the inferior temporal gyrus. And there's an area in the inferior temporal gyr it's done in the inferior temporal gyrus. And there's an area in the inferior temporal gyrus which is called the fusiform gyrus, the spindle-like ridge. And this spindle-like ridge is responsible for face recognition. If it is not working in your brain or you don't have it, you have prosopagnosia. That is, you don't recognize human faces and emotions and so on very well and you have difficulty to discern people by their faces. Surprisingly common. Then there is an area in the brain that is able to direct attention and this is the dorsolateral prefrontal cortex. It's somewhere in the forebrain and you can think of it as an area that has a lot of pointers in the rest of the cortex. Using these pointers, it can pump activation in there and create a coherent pattern of activation. It directs our attention and it scans the schema. For instance, if you have a face representation or this representation of the Mona Lisa, it can scan down from the face level to the eye level and so on and to the mouse level and discern features there. The great thing about a model like this is that we can actually turn it into a specification for a computer program, and then we can implement this and test whether our theory about how the mind works, all these observations that we have taken from neuroscience and from hard engineering-like thinking, if you are in nature, how would you do this, we can turn this into a coherent model model and then we can test this model and see that it actually is able to perform pretty good vision. Of course, this is a simplification and a lot of the details are missing, but it can do most of the tricks that we wanted to show. So I think that we are pretty much on the right path here. So what types of representations do we have? We have schematic representations, this thing that distinguishes between faces and bodies and eyes and so on. And then we have non-compositional representations, those that we cannot combine, that we cannot reuse the features throughout the representation. This is more associative and vague and so on. And it's hard to turn this into language. We have these language representations that enable us to organize our schemas on a high level and to communicate them to others. And then we have a class of representations that I would call generative. And these generators allow us to build simulations of the world, dynamic simulations. So for instance, if you think about music or sound, what we need is something like a synthesizer in our mind. For instance, here an artist has made a representation of what the sound of a canary would look like if you take it from the visual into the visual domain, like synesthesia. I think this looks a lot like synesthesia, if you ever had a synesthetic experience. So in a way I think this is an equivalent of what the brain does to model sounds like this and if you imagine sounds like this somewhere in the brain a computational structure must be built that looks a lot like this. And it's not a schema, it's not just a static neural pattern, but rather it's a generator. The difference between a perceptual representation and an idea is that the perceptual representation is tagged as being verified somewhere in the sensory area. So it gets projected down as a hypothesis into the sensory areas of the brain and the nervous system and then it can get a verification. And when it's labeled as verified, our brain takes this as being real versus being an idea. So if we try to generalize this, we have the dorsolateral prefrontal cortex that somehow accesses distributed hierarchical representations. And these hierarchies of distributed representations are picked out by the attentional selection. Usually most of the time there is a superposition of many alternative interpretations. And it's very hard to translate this into language. But using the dorsolateral prefrontal cortex, we can make this coalesce into a coherent representation. Imagine you have superimposed images, superimposed photography where you have different layers projected onto the same image. We are still able to discern the different layers. And this is because we can direct our attention onto a part of the representation and then make it coherent by suppressing those parts that are not compatible with it. And this intentional filtering is orchestrated apparently by the drosolateral pavement of the cortex. And this bottom-up, top-down perception extends down into the sensory perceptual areas. That is, we get predictive attention and motor control that allows us to pick us stimuli from the environment to verify our hypothesis, what's actually being the case. And then as a result, we get back filtered representations that form these intermediate stage. Not all of our representations have sensory grounding, of course. Our world model and our self model have some sensory grounding, in proprioception, for instance, or in visual perception. But our mental stage, where we can freely manipulate ideas, does not. And our memories, of course, don't have any more. You could also see memory, by the way, as something like visual perception or something like a perceptual process, where you ask the memory what hypotheses are true and as a result you get back object abstractions and episodic episodes that happened in the past and procedural memory which tells you how to do things and templates, frames that allow us to interpret the current situation. The sensory perception basically is an interface to the universe. The universe gives us your bits and we use those bits, encode them into the mental stage, into the world model, self model and so on. And at the level of attentional awareness we are able to perform symbolic operations, that is we can do conceptual thinking based on these representations by manipulating them. In dreams something interesting happens. Because we don't have access to our sensory organs anymore. And what would normally be the output of those lower level sensory areas gets replaced by pretty much random noise. And this random noise then is encoded in the same way as the sensory data was encoded before. So we get something that looks like a world model and a self model, but it looks pretty weird. Thankfully, our long-term memory formation is turned off during that time too, so usually we don't remember this. It actually might be a useful process, maybe some links are getting strengthened, we find new associations and so on, and some housekeeping is being done on the mental representations. So the purpose of dreams is not completely explored and fully known. But I think the nature of dreams is caused by the fact that the lowest level is replaced by random noise. We do meditation. We can completely tune out this interface to the universe if we want to. We also can turn out the framing of mental representations in terms of the templates that we have acquired in the past, so we get more awareness of what's actually the case. And if you go very, very deep into meditation, you can even turn off the model of self. How is it possible that we can be conscious without a self? I think that's because our self is not identical to our mind. Our self is also not an agent. Our self is a representation. It's a story that the mind tells itself about what happens with it in the world. And we need this story to do high-level learning on what we do in the world. Because, for instance, if you play ping-pong, you need to react very, very quickly in the course of a few 10 or 100 milliseconds, and this doesn't leave enough time for qualitative processing. But this reflex arc is unable to determine whether we've just won a match. To decide whether we'd won a match of ping pong and use this as reinforcement learning to improve our moves, we need to have this high-level story. And this high-level story is what we perceive as the self. Likewise, will is not what drives our decision-making, but rather the will is a representation on the level of the self. It's a representation of the fact that somewhere in the mind, a motive has been raised to an intention, that we have committed to do something. And free will is actually a social notion. It means that this decision could have been influenced by discourse, by cognitive thought, by conscious thought, and by talking to other people. So free will is actually a social notion. It determines whether we can be made socially responsible, discursively responsible for our actions. So what's consciousness then? Is it a behavior? It's tempting to say because, for instance, if you have this mirror test that's quite famous, if consciousness, for instance, is the ability to recognize yourself in the mirror, this is observable behavior. Psychologists love observable behavior. So, they took a lot of animals and humans and put marks on their faces and put them in front of a mirror and then they test whether they were able to recognize themselves in the mirror and thereby prove that they have a concept of self and consciousness. Difficulty there is that it's actually not a test of consciousness, but it's a test of your ability to recognize that the mirror shows your body surface, a test that I might fail on a bad day. And then relate between that mirror image and your body, and then be able to care about this. For a long time, people thought that elephants don't have a concept of self because they don't remove the marks on their faces before people realized that elephants usually don't care about marks on their faces. That is about two out of three elements to elephants don't care about marks on their faces. That is about two out of three elephants don't care. Also dogs fail this test and maybe dogs fail this test because they recognize each other mostly by smell and by hearing and not so much by vision. So this is not a really good test for consciousness because it's neither necessary nor sufficient for consciousness to be able to recognize yourself in the mirror. Maybe it's neither necessary nor sufficient for consciousness to be able to recognize yourself in the mirror. Maybe it's something else entirely. Maybe it's the awareness of what happens in our mind. I think it's a pretty good idea that John Locke had. But we don't really get much awareness of what actually happens in our mind. For instance, this continuity of experience is fake. If you look at these small eye movements, the saccades, they are very small and jerky. And during these small and jerky movements, the world doesn't jerk around. How does this work? It works because our brain blanks our vision during those movements. It also blanks our vision during blinking most of the time. So the world doesn't turn black and bright again. And yet we perceive this as a continuous stream of experience. So somehow this story of this continuous experience that I have with you, this visual experience, is a fake. It's being stitched up into a coherent experience. Then came Giulio Tononi who suggested that consciousness is something else entirely. It's something that is emergent over the integration of information in your brain. Giulio Tononi is a neuroscientist and he has suggested that if representations are very disconnected in your mind, then we are not conscious. And if they start to form a coherent image, we start being somewhat minimally conscious. And if almost all of the cortex is recruited for a coherent representation, then we are fully conscious. The beauty of this is that we can actually measure this and we can come up with information theoretic measures, which he calls phi, and phi somehow stands for a degree of consciousness that a system can have. The drawback of this theory is that we can build a very trivial computer program that has a very, very high fi and is obviously not conscious. So I think it's a good idea if you look for consciousness to look for large coherent representations. So somehow that's necessary, but it's certainly not sufficient. I don't think it's a very good idea, but it's a much better idea than earlier neuroscientists had like Bolf Singer who suggested that consciousness is a 40 Hz oscillation in the brain. I think neuroscience is really coming along here. I think that consciousness is actually a set of functions. It's a suitcase term that stands for a bunch of very different mechanisms. And these mechanisms are qualitative. They need to be present. If these mechanisms are missing, you're not conscious. So it's not just emergent in some magical way, but rather it's a bunch of things that the brain needs to do. And for instance, it needs to create a local perceptual space and experience of where we currently are. We need to have access to perceptual content. We need to have the current world model, which also includes the things that we are not currently looking at. We have to have some directed attention, and this attention can either be focused or wide, or it can be inward or outwardly directed, but it needs to be there. And we need to have access to our concepts and simulations and linguistic content and so on in the mind be able to follow along on connections that it has, so we have this stream of consciousness. And we need an ability to create new concepts and to manipulate them. And we need to be able to put them onto a mental stage and observe them on that mental stage. If these features are missing, I think we are not conscious. We need to have these features. Of course, there are different states of consciousness. And these different states of consciousness have different functionality. For instance, during dreaming, we don't have access to our outer perceptual data. We don't have access to our motor functions. We don't have access to agency, so we cannot control what we do in dreams. Except in lucid dreams. In lucid dreams, these are somehow an intermediate stage. We get additional functionality that bootstraps our agency so we can have goals and plans and reflect upon what we do to some extent, but we still don't have access to our bodily experiences and we don't have access to our muscles, except maybe the eye muscles. So we move around in our own mental cyberspace during a lucid dream. And of course we are conscious in that state. Then we have a bunch of competencies that are nice to have. Things like the ability to learn from experience, or the ability to create intentional models of other agents, or the ability to create mechanical objects of processes in the world, and so on. If these things are missing because you have a neurodegenerative disease or because you are a very small child, you are still conscious. But you could argue that you are conscious to a lesser degree. Then there is meditation. In meditation, you tune out some of your abilities and you get access to new functional mechanisms. And you can learn, for instance, to control voluntarily some parameters of your physiology or of your cognition that would normally not be accessible. And then of course there are the altered states of mind, for instance states of psychosis or of psychedelic states in which you also have access to different mechanisms and lose access to others. So in the middle we have this core consciousness and we have a bunch of functionality that may or may not be present. And consciousness in this sense is really a big set of functions. It's similar to love. Love is also a set of overlapping states that are given rise to by different mechanisms, for instance, a need for closeness or a need for affiliation or a need for caring for others. And all these together form this big compound that we call love, but it's actually a set of very different mechanisms. Love is important. So in my perspective, the mind is an information processing system. To perform all this information processing, what we need to have is computation. We need to have some substrate that gives computation to the mind, and currently we tend to believe that this computational substrate is mostly the brain with its neurons. But how can we account for the regularity of the patterns that the universe throws at us? How is that possible? What can we say about the universe? Apparently, these patterns that we have at our systemic interface are quite regular. That's why we can encode them so successfully into concepts and things like time and space and so on. To produce these regularities and patterns, the universe needs to compute too. This is the only thing that we can say with some certainty. It's necessary and sufficient to produce regularities and patterns that you can compute. We can define computing in very abstract ways. So one way of computing a universe would be You can define computing in very abstract ways. So one way of computing a universe would be that we have a machine. We could call this machine that computes the universe in honor of Kurt Gödel, for instance, Gerd. And this machine, Gerd, that has a set of possible instructions and it has a current instruction at each time and it has a current state of the universe, which is a vector of bits. It's really many, many, many bits and these many, many bits are basically small discernible differences, small yes and no's. And then we need a position at least of a single bit in the universe that we're currently working with. And each instruction has this format that it takes a bit at this current position and depending on the value of that bit whether it's 0 or 1 or Yes, and no We write in a new bit at this position and then we advance one position to the right or one position to the left And then we pick a new current instruction and that's it with this we can define a Turing machine And there are many other ways to define computation There are many other ways to define computation. For instance, we could use multiple bits at each position and we can write or read more than one position with one instruction. Or the transitions between the instructions, that is between the different states of good, that could be probabilistic. Or we could have one good at every position of the universe and not move them at all, but rather have them be static and just read the positions of the neighbors, the data from the neighbors. In this case, we define a cellular automaton. And all these different approaches to computation can do the same stuff. Some of them much, much faster than others, but who cares when you are inside the universe? What we just saw is something like a dualist solution. This dualist solution, we have two computational substrates that are separate, and the mind can read data from the universe, but it cannot write it back. This has some drawbacks, because it means that nothing that you experience can be causally relevant for what happens in the universe. This is called epiphenomenalism and it's a bit inconvenient. A way to get out of this would be to make the universe somehow duplicate the equivalence of some states of your mental processing. This is called occasionalism in philosophy. But why should the universe do this? So a solution is to have the mind as part of the universe, to embed it into the universe and let the universe compute the mind. Because the universe is computed in its own computational substrate. It can do computation. That's why it can throw all these nice regular patterns at us. It can also compute the mind. and throw all these nice regular patterns at us, it can also compute the mind. So we can have nested computational systems. Like for instance we can build a computer from redstone blocks in the game Minecraft. So for instance in this game of Minecraft it's powerful enough that we can build a computer with a display and inputs and it's powerful enough to play games on it. But very, very, very slowly. Looks like this. We have this Minecraft game and in the Minecraft game we have the Redstone computer and the Redstone computer does the computation based on Redstone blocks and the Minecraft game itself runs on a PC or a Mac or a tablet but the Redstone computer will never know because it doesn't have access to the subset of the Minecraft game, which is its universe. It's good. And this is pretty similar to our current ideas of physicalism. Physics is sort of the science that tries to find a possible implementation of how the universe does its computation so it can produce the observations that we can make. And our currently dominant set of theories says that the universe is based on quantum computation. And in there we have computation that is mostly classical, there is neural computation that produces the mind. Actually, it's a bit of a simplification because we can also use other things than brains to do thinking, or to help us thinking. For instance, we can use the internet or we can use notebooks and so on and put parts of our minds out there. And in a more general sense, what the universe uses to perform the computations that are happening in our mind is it increases entropy to allow us to build structure. To think, to compute, we need to create structure. And to create structure, we need to reduce structure somewhere else. That is, we increase noise. And if you have a universe that is maximally noisy, there can be no more cognition or computation in it. That's why we need to push back entropy. Entropy is the enemy. It could also be the other way around. It could also be that the universe actually is embedded into the mind. In that case, our mind is good. Probably don't know it, but the universe would be a dream. This is the situation in our dreams, right? The regularity in dreams is there because it's caused by our mind, it's produced by our minds. And there's no way to disprove that we could be living in a dream, right? It's just maybe not the best possible theory. Could we be living in a simulation? Well, I don't think that there is any difference between reality and a simulation. It doesn't really make sense to distinguish between reality and a simulation anymore than it makes sense for a robot to distinguish between a simulation that it's currently in and a reality. The only thing that matters is what patterns are thrown at my systemic boundary. What encodings can I make over those patterns? If we buy into this, then we have to realize that concepts cannot really refer to facts in the world and there is no truth that we can get by referring to facts out there. We have to abandon the notion of meaning, our life becomes meaningless. What we have to do now is we have to resort to something else that does the job that meaning did before, and I would call this suitable encoding. The good news is that suitable encoding can do everything that we wanted to have from truth and meaning only better. We can no longer tell whether our picture that we make of the world is true or not, but what we can say is whether it's a suitable encoding if we compare it to an alternative. It doesn't work all of the times, but most of the time it works. So most of the time we can find a criterion that tells us that this encoding is better than another encoding. And criteria for these encodings are things like sparseness, for instance. How many representational entities do I need to represent this? Stability, how much do I need to update my image of the world if my observations change? Consistency, how many contradictions do I have in my representation? Coverage, for how many of my observations can I account for? Integration, how well does it fit into my existing representations? Then how relevant is it to my needs that I have as a system in the world? Then how adaptable is it to my goals and plans over time? And how difficult is it to build those encodings? How difficult is it to acquire them? How difficult is it to store them? And how difficult is it to maintain them? And difficult is it to store them? And how difficult is it to maintain them? And these are all criteria that I can define. I can define them in information theoretic terms, which is very nice. I can have very robust non-metaphysical definitions of those. But if there is no truth, you might ask me how it's possible that I make statements like that mind and universe are computational and there is no truth. Well, I don't say it's true. I just say it's four most suitable encodings. What's the nature of reality? I think that the universe produces patterns at our systemic interface. Information, little discernible differences that reduce uncertainty. And computation is necessary and sufficient for that. What can we know? Well, information. And we can encode this information into percepts, into concepts, into simulations, into world views, into culture. And who are we? I think that we are computational systems that encode information and that store that information and process it and interpret it in very, very specific ways. And we can find out details about this class of computational systems that we call minds by doing models of those. Computational models. These are called artificial intelligences. I think it's super important to build them to find more about the state of affairs. About what minds are, what we are, how our relationship to the world is. And this ties into the first question. What should we do? First of all, of course, we should push back entropy. Because to think, we need to put energy into our bodies and we need to push out wastes and push waste away from us and get clean again. And then we need to laugh. And if we have still time, I think we should build artificial intelligence. Thank you very much for this great talk. We still have about 20 minutes for questions, so I encourage you to line up at the microphones and just go ahead. Number two. Thank you Dr. Bach for absolutely brilliant talk. I was jiggling around in my seat. I thought it was going to be a left brain materialistic explanation of the brain and consciousness. And I thank you so much for just pushing the boundaries further there. But I've got some questions. As far as the brain and consciousness. And I thank you so much for just pushing the boundaries further there. But I've got some questions as far as the brain being a manifestation of an informational computational processing system of information coming from the universe. I was very interested in the end part of your talk where you're inverting whether our mind is inside the universe or we're embedded, the embedded model. Very interesting. I'm wondering if you could make comment, because I'm sure you've looked at this literature on the following. I'll just leave these with you. Observation and two-slit experiment. I think it's an interesting one. I'd be interested in your impressions if you read the literature on near-death experiences. At first, this was a phenomenon, but now I think the documentation on this, that is perception in states of no brain function. I'm wondering if you have any comments on that. Could you repeat this last bit? The phenomenon, and it's well documented in the literature now, I think from a scientific perspective we can say it's a phenomenon. It's a phenomenon of perception in the absence of brain function, for example, in times of cardiac arrest. Yeah. Once these studies, for instance, there was one at University Hospital in the UK, and they did studies on people with cardiac arrest in the ER, and they looked at things like extrasensory perception, out-of-body experience that these people had, and they apparently formed memory while they were recording flatlines. And once they started to talk about this, I wrote emails to the people that did this survey to get to know more about the data. And what they have found, for instance, they put a board on top of the beds for those people that make out-of-body experiences and wrote something on top of that board and wanted to see whether people floating out of their body could read what's on top of the board. They never could. Which doesn't mean that it's impossible, it just means that they couldn't show evidence that this out-of-body experience is something else than our sense of position in the local perceptual space getting disattached from our body image, which apparently can also be triggered by, for instance, ketamine. You can trigger out-of-body experience with some degree of reliability with certain drugs. And it's a dissociative phenomenon. So we have to account for the fact that these people were flatlines on the EEG while they were doing those processes. And I think this can be accounted for by the fact that our EEG doesn't pick up the activity when it's very low, but this is still sufficient to do the necessary computations in the brain. So I think the easiest and simplest explanation is that our measurement was not good enough to really measure whether there was really conscious activity going on in the brain at this point. So you would say that was a hypothesis for further examination. The hypothesis is that the level of measurement does not detect brain activity when it still may exist. Yeah. Okay, thank you. Microphone number one. Yes, thank you very much for this presentation. I think it was really good. It also really resonates with things I like and I think with most people here in the room. But something that keeps concerning me with these comparisons with computation and human being is that it feels like it's a bit presentistic. I've read literature from a hundred years ago where people thought, yeah, we figured out what human beings are. They are valves and they are pipes, just like the whole steampunk stuff, because that's the mode of thinking. And is there anything that makes you believe that right now this is special, that it's not just our way of thinking applied to the current times? Well, the number of thinkers that existed in the pre-modern age compared to the number of thinkers that exist in the modern age is pretty small. And the thinking of the pre-modern age is mostly available, at least those parts that we know of course, which is tautology, to our current thinkers. So I think that it makes sense to assume that there's a bit of progress. And I think a very important bit of that progress that was not accessible 100 years ago or even 50 years ago is the notion of computation. We simply didn't have this. Before that, we had this mechanical worldview. And it's very counterintuitive that a mechanism, something that makes us into it, wheels and cogs and pulleys, can produce things like a mind. And now we have abstracted this into a more general causal mechanism, a computational machine. And this mechanism that the world provides is only incidental. It's only important because it is able to produce computation. And this notion of computation, in my view, is pretty watertight. In the sense that it can account for the fact that the mind is an information processing phenomenon, and we can actually prove properties about this. So we got these nice abstract and synthetic concepts about how minds could be implemented. And I think that even in an idealistic universe where mind is primary, we would have to account for the fact that the mind is able to process information that is somehow appearing at a systemic boundary. So in some sense, it's a general theory. But as experience shows, better theories tend to come along. It just appears to me now that this is a pretty watertight theory and a paradigm that is going to carry us very far. Thank you very much. I believe that it is a much better model still than what we used before, but how can we say that it is? And you would say that it's a suitable representation, probably. But yeah, you mentioned computation, that still can emerge from mechanical systems, of course. We can make fully mechanical computers. So would you then say that the mind emerges from the physicalities of the body? If we admit that emergence is not a thing out there, but that emergence is actually a relation between different systems of description, then yes. So in a way the universe is providing us with a causal system that is somewhat insulated from the causal structures of the universe itself. And within that computational causality we can be minds. Okay, thank you. Next is a question from the internet. Hello. Okay. Do you think if we could build a machine that more or less works like the human brain and give it the necessary sensory input, would it become conscious by itself? Yeah, I think that consciousness is a set of functions. If these functions are implemented in the system, the system is going to be conscious. If these functions are missing, the system is not going to be conscious. All these functions are information processing mechanisms. I think that we can disassemble them, that we can research them, and then we can combine them into a coherent whole. I think it might take many decades, and I think that it might take many more decades than we have civilization left on our clock, but in principle it's possible and I don't see why not. Thanks. Next from Thor. Thank you. Thank you very much for your again for your challenging talk here. I totally agree with the love thesis and with the, let's say, a monist point of view. But at the moment, I think we have to accept that if the universe, it's a basic feature of the universe to create consciousness, whatever consciousness might be or not. But at the moment, we have to accept that our living systems will have cognitions and, for example, volitions. And if you look at the basic construction principles of living systems, you have a violation of the Russell theory of types. Seen from the point of the DNA, a protein is an operator and the DNA is an operant. And seen from the point of view of a protein, the DNA is an operator and the protein is the operant. You cannot get that logical. So my question is, do you think that our actually available formal concept of math and logic is sufficient to lead to a really constructed artificial intelligence? Yeah, I think so. Computation is a true subset of mathematics. Mathematics is more a language in which you can produce statements, like for instance Russell's Antinomy, and these statements need to be meaningful. Computation is basically those part of mathematics that runs. Okay. And I think that we need to build computational models. So in a way, I'm arguing for a new kind of philosophy, a subfield of analytical philosophy, which is the part of philosophy that could be expressed in a formal language. I'm arguing for a philosophy that is computational, that relies on theories that actually run. But, okay, they rely on theories which run, but you have a broken hierarchy in living systems. And every computational system is a hierarchical system, not a heteriorhical one. So, how can you solve this contradiction? I think this is simply because this idea of hierarchy and heterarchy are conceptual ideas. They're not parts of the territory, they're parts of the map. And in many cases, these descriptions break down, they're just not suitable encodings anymore. And so we have to refrain from concepts that are tempting, for instance the concept of auto-poiesis is in that category, because they seem to be a good idea at the time, and then we try to make our ideas of the world conform to this idea that is almost right, until it breaks down. So we shouldn't be invested into those concepts, we should pick concepts that work. And I think in those case, we have a description of our domain that is not working, that is breaking down at some point. So we really need to find a new description. So with autopoiesis, I agree with you because it's not a formal concept. It's a narrative concept, but there are formal concepts which are non-hierarchical, and at the moment, they are not, I'm sad about that, they are not scientific mainstream. For example, poly-contextual logic by Gotthard-Gunther. Mm-hmm, yeah. I also agree that we probably need to develop better formalisms to describe a lot of things. Mathematics is an ongoing area of research, but I'm not sure of if we need completely new paradigms. So I think more work needs to be done. Thank you for the question. Okay, next is two and then three. Thank you for the really beautiful talk. As an engineer, I accept everything. As someone, as a psychonaut and someone who studies philosophy, I have issues. I think it was Alan Watts who said that trying to define yourself is like trying to bite your own teeth. And I think it's the same with consciousness. I think consciousness is a little bit more fundamental than this. In quantum mechanics, you need a conscious observer to manifest matter from energy, right? No, I don't think so. I think that the observer in quantum mechanics and the observer in relativity and the observer in cognition are very different entities. The observer in relativity, for instance, is a system that could interact with another system. The observer in quantum mechanics is a system that actually interacts with another system. The observer in quantum mechanics is a system that actually interacts with another system. An observer in cognition is a conscious system. And I think these are entirely different notions. I was under the belief that you need a conscious observer to manifest matter. I don't agree at this point because I don't see why you would. I think you need to have a system that interacts. Quantum mechanics is a theory on the possible implementation of the universe. And at no point in the formulas of this theory, because it's just a bunch of mathematical formulas or other computational formulas that tell us how this is implemented, does it state that there is consciousness? What do you think about E is equal to MC squared? C being the spirit of light. I think consciousness works on light. We think in the form of light. I think light is the currency of consciousness. I think that's very poetical. It is poetic and it's really metaphysics at this point because I have an issue with people who like to discount anything that's beyond's beyond empirical as as foolish because what is empirical? Empirical means we have a limited set of tools based on the knowledge that we have now to Demonstrate certain things and it's always changed as our species have evolved Right that does not necessarily mean that just because we can't measure it, that we say, hey, this is not true. Instead, as real engineers, as real scientists, we should maybe take it into account and maybe come up with stuff that could make it empirical. So the issue that I have as a psychonaut is that you try to scope down consciousness into a set of empirical models models and you try to say that this is consciousness. You take a JPEG image, a 640 by 480 image. In it is the possibility, if you permute the faces of every single person, everything that has happened, everything that could not have happened, that JPEG image has the capacity to show, right? The problem is that we can't do it because we don't have computing systems that can deal with the beauty of infinity. And until then, I think we can only create pseudo-consciousness and that's not the same as real consciousness. Well, my problem is that I probably cannot deal with the beauty of infinity either. I can trick myself into this, but I don't have any experience of infinity. I don't have any experience of those things that you find so mystical. I can create those concepts, and I don't see any reason why a computer program couldn't create equivalent concepts. But I don't think that our minds can fathom concepts like infinity, which is also just a metaphysical idea. If you come down to it. Not yet. Right? We are constantly evolving as a species. Right? We like to think that we are the apex of evolution. We're not. We're constantly changing. It's happening in the form of races. Yeah. Yeah. It's a mental construct. And I really appreciate the work that you're doing and I think we need stuff like this to advance the species. Thank you so much. Thank you. Okay, next is microphone three. Hello. Thanks for the invigorating talk and thanks for putting some Minecraft in the mix. You had a very nice list, kind of like a checklist for consciousness, which seemed basically like a feature list almost. So I was wondering how much of that do you think will be portable on chips in the sense of interplaying software hardware. The second part would be, once we get there and get all the stuff on that list implemented somehow, chips, sensors, software, whatever, do you think the implementations will be more – or in numbers, there will be more complete AI implementations in the sense of my, I don't know, my toaster will talk to me or be my psychologist or it would be a subset which is more kind of like on a neat basis. I think this is more a question about how society is going to pan out and economy is going to pan out and the world that we how society is going to pan out, and economy is going to pan out, and the world that we live in is going to pan out. It's very difficult to say how this is going to be. I think once we are able to build fully sentient, fully intelligent, and fully self-conscious systems, these systems are probably not going to be Roombas that are going to battle their way up the cognitive hierarchy and try to appeal for human rights. But it's more likely that they are going to descend uponombas that are going to battle their way up the cognitive hierarchy and try to appeal for human rights. But it's more likely that they are going to descend upon us from top down, because they're going to be business intelligences, organisational intelligences. They are going to augment the non-human agency that already exists on the planet. And so, in a way, I think these intelligences are already there, they're just not very conscious yet and they're not very efficient yet, because they have to borrow human intelligence for their task. And in a way, they're probably going to push the agenda of the existing system much further. As an engineer, I'm interested in how we can get there. So I was just wondering what your guess was, because in the past, always had like very basic computing power and then we extracted more of those possibilities in software and this seems kind of like the other way around where most of the computation or like everything is simulated in large supercomputers or clusters and will people are trying to bring it back down to hardware basically. So the process seems very new to me, so I'm just wondering if you have any guess on how that might... I think that computer power is not the issue anymore for quite some time now. If you look for instance at birds, crows are able to build sledges and slide down roofs with them and play and solve little problems and do plans. Gray parrots are able to learn some basic language and able to learn negation and so on. And so it seems that we don't need all the neurons that we have in our brain. And then if you look at the neurons, we find out that they're very noisy and that our brain is built for redundancy in the most part. I think that the main issue right now is not so much computational power. We can get computational power very cheaply from Amazon. What we need is better algorithms. We need to have research into the mechanisms. And in many ways we are making tremendous progress especially in the last few years with deep learning. But we are still away from understanding all the algorithms and the computational structures that are involved. Thank you very much. Thanks. Could those people migrating between rooms please quiet down a little bit? We have only time for two more questions. I'm really sorry to those who have been lining up for a while, but I think the speaker will gladly answer them after the session. So number one and the signal angel. Yes, I'm here first. But did I understand correctly that you didn't talk much about identity and difference in relation to the minds that did I understand the correct correctly? Yeah. Okay, then how do you connect it? I mean, how do you describe this kind of concept of identity and difference and also maybe alienation and even violence? Because these are, I think, also things that are kind of, from a society perspective, I think very important to consider. I mean, how we relate as humans to each other. So identity and difference, alienation, violence. How do you talk about this in terms of the other language you're using? I will try to answer your question on the lowest, simplest possible level, because it leads very, very far into cultural implications. And on this lowest level if you try to build a little robot that runs around in a physical or in a virtual world and then encounters objects, at some point you realize that it will have to make a determination whether it just found objects that look very similar to objects that it found before or if it just found the same objects that now has a different location, maybe has different features than before. It's a different way of processing things, of storing things. In one case, you have different objects that may be similar or identical properties. In the other case, you have objects that might have similar features, but that are identical. And this notion of identity basically means that we stick things, the representations of objects, onto a common world line. That we have different instances of objects, any object of the mind, share a biography. It's the way cognitive processing works. And we apply this sense of identity as an extension in ourselves, as part of the concept of self. The concept of self in the simplest case might be the set of features that is somehow always with us, that is always accompanying us and always mostly accessible to us, whereas there are features that are changing all the time, this is what we call the environment. And if we find a structured representation of all those features that would be part of ourself and stick them onto a world line on something like a biography, we have what we call personal identity. And this personal identity is augmented with intentional interpretations, which is one of our main modes of interpreting the world. So we have this mechanical mode of interpretation, if we can see a mechanism. We have intentional interpretations, where we attribute desires and needs and wants and intentions to objects in the world, to agents. And then we have chaotic representations where we just take the events to be random. And we take this intentional explanation and apply it to our self-concept. And then we expand this into a social persona, into somebody who relates to the world, to the environment, to society self-concept. And then we expand this into a social persona, into somebody who relates to the world, to the environment, to society, and so on. And this is what we, I think, would call our personal identity. It's not a thing in the world, it's a representation. It's a construction. And alienation violence? I think I would need more than an hour. I'm sorry, but I think we should talk afterwards. Okay, thank you very much. One last question. I think I'm switching from the internet to this little guy on microphone number four. Good job. Thank you. Thank you very much, Josje. Thank you, too. Thank all of you. It was very pleasant. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.", '29.244338035583496')