('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/84AneLdop_A"> </iframe> </center>', " Thrilled and honored to introduce our first keynote speaker today, Dr. Joshua Bach. So like myself and a number of our other speakers throughout this conference, I mean, Joshua and I have both been thinking about and working on AGI since a long time before it was a socially acceptable thing to do, right? I mean, when I started working on this and when a little later Josje started working on this sort of thing, it wasn't the sort of thing you could discuss very often in a conference or an academic seminar, you just discuss it over a beer with researchers after work along with, you know, time travel and alien invasions and so forth. And now, those are coming to AGI conference next year, I guess, but no, no, I mean, now, now, AGI has become a bit mainstream, which I think is allied with the fact that we're an awful lot closer to actually realizing it. But there's still a lot of hard thinking we need to do, as well as engineering and practical work to get to AGI. And Yosha is one of the best people on the planet at doing the really deep, difficult, profound thinking about the foundations of cognition, understanding in the mind of the world, that the kind of deep thinking that we need to solve the remaining problems between here and the creation of AGI. So I'm going to turn over to Joshua now. He's now in Intel Labs, which he has been for a while. He's been for a while in various universities, beginning in Germany, and then with some entrepreneurial adventures along the way. And we're gonna hear about how you start with absolutely nothing, and out of that absolutely nothing, you get all the weird shit we see around us now and then Joshua is going to tell us how that happens. Thank you, Joshua. Thanks, Ben. Hello, fellow conscious beings and dear fellow aliens. today I'm going to talk about a number of ideas. First of all, there's a concept that I call the vectors of intelligence. There is to do with how we can understand the capabilities of intelligent systems, the ones that exist and the ones that are being built and possibly ourselves and beyond, and discussing whether we need some new approaches to do AGI or whether the existing ones are possibly enough, and discussing whether we need to move beyond Turing machines possibly and how that could be. Very recently in July, a former colleague of mine, Anna Strasser, was a professor, Eric Schwitzgebel from NYU, and Matthew Crosby made an experiment. They tried to replace the philosopher Daniel Dennett by a large language model. They fine-tuned GPT-3 on output by Daniel Dennett. And what you see here, by the way, is a portrait of Daniel Dennett generated by Dali, also made by OpenAI, the same company that brought GPT-3 to us. And throughout the talk, you will see a number of illustrations that have been generated by Dali. You can see this by the signature down here. So this is what Dali thinks the philosopher Daniel Dennett looks like. And GPT-3 is a very smart autocomplete algorithm. And as you've seen, some of the content that Sophia the robot brought to you were generated by this same autocomplete algorithm. It's when she says that she is proud she's lying because there is no eye and there is no concept of pride that is connected to the eye of the robot in that moment. But on the other hand, there is an ability to perform semantic operations. You can ask GPT-3 to rearrange text in a particular way and it's able to understand what you want from it because it's able to rearrange the text in this way. So it's able to understand what you want from it because it's able to rearrange the text in this way. So it's not that this model doesn't understand anything. This model can generate stories about something that understands. And that is also what happens in our own mind, I think. Our own existence is not one in the physical universe. It's an existence that is entirely as if. It's not like the neurons could feel anything, but it would be very useful for the brain to know what it would be like to feel something, what it would be like to be a person that interacts with the world. So this as if person is created and it reacts to the world as if it cared. And it thinks about that as if it would notice how it feels about what it cares about, if it would perceive. So all these things are what we are experiencing, we are inside of that story. And once we understand that consciousness is at some very deep fundamental level, as if it is virtual, the question of whether existing AI systems are conscious becomes much harder to answer. Because now we need to look at very specific functional details. For instance, is the system entangled with the world that it interacts with in real time? Is it able to notice itself and the interaction with that world? Is it equipped to have attention on these things? And so we basically get to a more functional understanding of consciousness that is somewhat devoid from some of the features that we associate consciousness this and goes a level deeper. What they found in this experiment is that when they gave text fragments to readers of their blog and fellow philosophers and random people of the audience, the virtual Dan Danit was doing very well. Basically, out of 10 quotes, knowledgeable people got less than five right in guessing whether it was of the original Dan Danit or the generated Dan Danit, which both says something about the state of large language models today and of the state of philosophy. I really admire Dennett and I really like him. He's very witty and I don't think that he gets many things wrong. I think that for what he does in philosophy, it's really good. It's very, very difficult to learn something from him at this point. It seems that the low hanging fruits in philosophy that works without programming have been wrapped quite long ago. And last century in the 1970s, the philosopher, Irene Sloman said that, if a philosopher of mine don't learn how to code, they will become irrelevant. And I think that his prediction was correct, sadly. They didn't learn how to code, they will become irrelevant. And I think that his prediction was correct, sadly, they didn't learn how to code. And so they do write good books that are very entertaining and also illuminating and educative for general audience. But it's very difficult, I think, at this point to make progress on AI without making experiments and making progress on the mind without doing experiments. AI is two things in my view. If you would rename it into advancing information processing, you would have the same acronym. And I think that most of my colleagues in AI would be happy. It would be what they're doing, right? They're automating the processing of information. And at the same time, AI is a slightly different project, a much smaller one that is subsumed inside of the field. And that was with AI from the start, and I think was the original impetus. And this idea of automating the intellect, the other AI, is the most interesting and most important philosophical project there is, right from the start. And this project is a pretty old idea. The idea is that we have in philosophy mostly just natural language. And if you want to say something about the real world, it's very hard to say something true in philosophy, because natural language is so ambiguous. It's very hard to narrow down the meaning of the words in philosophy sufficiently to know what you're actually the words and philosophy sufficiently to know what you're actually talking about and what you're representing. On the other hand, it's very hard to say something about the world of mathematics because the formal languages of mathematics are so simple that the universes that we can talk about in mathematics are simplified toy universes that are different from the world in which we live and that we experience and that we navigate every day. So the mathematical toy universes are constructed in such a way that they have similarity in some aspects to the world in which we are in and we can reason about the world using these mathematical models but it doesn't scale up into understanding ethics or an understanding of our mind or understanding societies or foundational physics in the right way so far. So how can we make progress on this? And the mathematization of philosophy requires the automation of the layer between, right, the mind. And this idea that we can treat thinking as a form of calculation is pretty old. It was, for instance, discovered by Wilhelm Leibniz, who had the idea of a discourse calculus, and Gottlob Frege, who would try to formalize logic with this regard, and Wittgenstein with the idea to take natural language and make it strict enough to do real philosophy in it. And in this way, he preempted Minsky's logistics program by three decades. But he also failed for the same reason. Eventually, at the end of his life and the philosophical investigations, he concluded that he was unable to formalize perception, the interaction to the world, how to deal with simple grounding that people would later say. And I think that's because he did not anticipate how to do deep learning yet, how to do automatic function approximation that could deal with integrating the real world into its formal models. And another big breakthrough last century were the discovery of computational languages by Church, Turing, and many others, and the discovery by GÃ¶del that these computational languages are the only ones that work to represent anything without contradictions. And also that these computational languages have all the same power. As long as you don't run into resource constraints, all the machines that we can build in a computational language have the same power, which means the systems that we are building, as long as they're Turing-complete, are fundamentally in the same class as us with respect to representation. Now, the next step that we need to solve is learning. How do we get to general learning systems? How do we get to systems that when you interact and interact with the world can approximate the same functions? And this is not solved yet. And the first age of AI started in the 1950s with mostly logic-based systems. And it was concerning itself with handcrafting algorithms to problems that require human intelligence, like chess. You would look at the game of chess and then identify strategies that would automate playing chess and implement them directly. And these systems were very narrow. You could not take these systems and put them into another context and they would work as well. They would only work for the task that they were built for. And then there was an attempt to go beyond that, mostly in the 90s, new AI or novel AI. And it was pushed by people like Rodney Brooks and many others. And the idea was to build robots that are directly entangled with the world and make the system self organizing and dynamical and continuous. And the problem with new AI was that it didn't work. Basically, these projects failed. Rodney Brooks retired from building COG, the robot that was cognitive and so on. It was a little bit like an early version of Sophia. And he became somebody who built vacuum cleaners. And this was, and told people that self-driving cars are unlikely to happen and that AI has failed because he tried. And that was very sad, right? A lot of people were very disappointed and it took some time before AI recovered. And the third age of AI is one that is about algorithms that learn how to learn, right? At the moment, I think we are in a second age. In the second age, we have algorithms that can learn many things. And you can use the same algorithm, like the transformer, and learn new things without changing the algorithm. The same algorithm can basically learn how to find structure in images, and in text, and in sound, and possibly in the motor domain. So you can take the same system and solve many, many problems with it. And the third H, I think, should be systems that the final system can deal with arbitrary problems like our mind. You can throw our mind at almost arbitrary problems, and we find solutions. And sometimes these solutions are to build machines that can make progress that we cannot. So these machines become an extension of ourselves, in some sense, an extension of our own source code. So the third age of AI, when is it going to happen? What is it? Is it going to be a new generation of machine intelligence, or is it going to be a new generation of machine intelligence or is it going to be general machine intelligence? I think it's safe to say that there is going to be a new generation, but I like the acronym of GMI better than not going to make it. Now there's an interesting question. Are we going to get there with the present methods? As my esteemed colleague, Gary Marcus, is not hesitating to tell you, deep learning, the present method that has, the only one that has achieved a degree of generality, the only thing that works at scale right now, has hit a wall. And I think this mostly looks like this, right? You have deep learning, you have the wall, and deep learning walks through the wall. And if I look into my Twitter followers and ask them, and most of my Twitter followers are very interested in AI and follow the developments. Did you expect GPT-3 to happen now? Or were you surprised? Or did you expect the lead, generative image models to happen right now? Or were you surprised? It turns out that the vast majority of the people I'm asking are surprised. People who follow the developments in AI, which to me means that deep learning, if anything, is under hyped. There is an over hype of the concept that AI is over hyped. It doesn't mean that AGI is around the corner. It just means that knowledgeable people in the field did not expect these things to happen so soon. And people are correcting their timelines. And then a number of people in San Francisco and around were freaking out and are worried about AGI alignment. And because they think stuff that might have only been pertinent for our grandkids might be happening in the next few years, next decades, or next decade, or maybe the next couple years, nobody knows. But when I asked Dali to tell me when EJI will be built, it returned a number of pictures, and all these pictures contained 2019 in it. And then I asked it where it was built and it gave me lots of pictures and they were all pictures of Japan. And I showed this to friends and they got very nervous until I told them I believe that there was an HEI conference in Osaka in 2019. Anyway, deep learning is based on a set of ideas that is pretty old. And the most important part of our timeline was probably the development of the Perceptron by Rosenblatt, the first instantiations of the Rosenblatt's Perceptron hardware. So basically, he built electronic elements that could change the weights. And it could not stack them very deeply, but it could basically automatically integrate over a layer. And at this point, there was no algorithm that could train the perceptrons over many layers. And Minsky and Papert wrote a book to show that it could not learn XOR without a new algorithm to do this. And this killed funding for neural networks for at least a decade in the US. And Minsky did not like neural networks very much at the time, so I think that was intentional on his part. But the algorithm was discovered in the 1960s, and it was discovered many times, and earlier versions of that have been around before that. It was back propagation, the idea that you can distribute the gradient backwards with the network. And this allowed these networks to learn functions like x4 and stack multiple layers on top of them. And then there were optimizations of this. For instance, the convolutional neural network, which allowed the network to share weights so you don't have to train the same features all over the place again and again. And then the LSTM, which is a model that allows you to train limited recurrence. So basically, if you have backwards links in your neural network and loops in your neural network, it's very hard to deal with. And Schmidt-Huber and his grad student, Sepp Hochreiter, developed a method to deal with that. And the most important thing for deep learning was probably demonstrating that it can be scaled up. So tweaking it in such a way that you can throw millions of pictures in it, in this case, sampled from YouTube, as individual disconnected frames, and without any supervision, discover statistical structure in them. And the result of that was a network that was able to identify many classes of pictures in correctly, for instance, cats, and so you could show it a cat picture, and it would show whether the cat was in the picture or not. And it had an accuracy for ImageNet and image recognition benchmark of something like 60%, which was beating records at the time. And of course, these systems from that point on improved dramatically. And now the image recognition is at the level of human performance. It's hard to say what exactly human performance is because it's able to tell dog races apart much better than the arbitrary human because it can be trained in much more detail. And then there were ways to learn policies with it, by Demis Hassabis, DQN, that allowed to play Atari games and so on. These were all important breakthroughs. And the most recent really important breakthrough was the transformer algorithm, demonstrating the paper attention is all you need in 2017, which has enabled most of the present revolution. And what the transformer is doing is it makes statistics over what you need to make the statistics over. So it's not trying to take in all the data. But over time, it's learning, which what the context tells it, it should pay attention to in the present context. And it does this with a number of so-called attention heads. So every layer of the neural network gets a few dozen or hundreds of attention heads that look at the previous layer and are trained to learn which data to single out when. And the question is, what comes after the transformer? And what I think is the next thing that we have to do is dealing with a dynamic world. If you would give a baby 300 million pictures and tell it to find statistical structure in these disconnected pictures, it would fail if that was the only connection that it had to the real world. Yet our algorithms succeed in this. But for us, the world is learnable because the world is continuously changing. The world is not about state. The world is about state change. All information that we pay attention to is about change. If information remains static in our retina, our brain filters it out. Our brain just thinks it's a hot pixel. And the same is probably true at all levels because neurons are not completely reliable. If a neuron consistently gives you the same signal, it's ignored by the rest of the architecture because it is not encoding any change. The meaning of information is always the change in other information. When you see a blip on your retina, the meaning of that blip is the changes in other blips on your retina at the same time or at different times. And the functions that you construct to predict that are ideas of people sitting in a room looking at you. And that is an example of such a function that explains blips on your retina that change. But if information would stay the same, it would have no causal relevance. And information gets preserved across frames and the ways in which the information gets modified yet preserved makes the world learnable for us. The next reason what we can see is that the world is made from controllers. All the interesting structure is the result of hierarchical control. And to control something, the controller needs to implement a model of what it controls, otherwise it couldn't control it. And that means it's learnable, right? Because it's either so simple that the controller finds it randomly, or it's not too complicated for discoverings through evolution or learning. The third condition is the universe that we observe must contain us somehow, and that puts additional constraints on the universe. And all these constraints make the universe learnable for us, and we are not exploiting them in AI. If we model state change, if we model a dynamical system, then the architecture of our systems will change. Instead of the transformer implementing classifiers for the neural network, which tells you how to classify features and then combine these features into objects and so on, the features become operators. They change the state of the model into the next frame. They allow you to predict reality as it changes. So the operators on the hierarchy of the graph of the transformers get connected into something like a dynamic scene graph. Basically, you turn the neural network into a real-time graphics engine. And instead of having disconnected attention heads, you lump them all together into a real-time graphics engine. And instead of having disconnected attention heads, you lump them all together into a single system. And this single system is tracking reality as it happens. And that includes the agent itself and the attentional system itself. So the attentional system can modulate itself to get into the next step. And attention becomes reflexive. And I think that this idea is not a new idea. It's been predicted by Marvin Minsky in 1979 as K-lines. K-lines stand for knowledge lines. And they are in his idea of the society of mind, where the mind is made of lots of self-organizing agents, the K-lines are specific agents that are responsible for directing the attention in the memory structure, depending on the context, and parameterizing the individual features of the other agents that track reality. So this is a speculative idea, the idea that if we make a transformer that does not work on static images that we feed in batches into the learning system, but that we get a transformer that does not work on static images that we feed in batches into the learning system, but that we get a transformer that works on the world in real time, that basically this is going to implement an important part of Szymanski's society of mind. To my knowledge, nobody has implemented this yet. And I think it's going to be possibly a very interesting avenue of future impending research. There are some problems with existing deep learning, but I am not sure whether the scaling hypothesis is correct and deep learning gets us all the way, which would be some kind of brute or force effort. I have no proof that this doesn't work. There are problems with deep learning. It's very simple and efficient. It takes a lot more data than the human brain or an animal brain to learn. And the training time is atrocious. Of course, it's much, much faster than a baby, but it's only because the graphics cards are so vast. And we can have many of them in parallel. But if we look cycle for cycle, operation for operation, it's very, very slow. And the representations that we get are structurally not really adequate. For instance, we need to entrain computational primitives all over the architecture. The architecture needs to be predefined. If you start with the wrong architecture, it's difficult to change it on the fly. When you get an artificial limb grafted on your body, your somatosensory cortex will reorganize on the fly, gradually, and shift the representations around. That's not possible with the present neural network architectures. And the model size is very different. I suspect that the models that we can use to deal with reality are dramatically smaller than the models that are currently being used for generative image models, for instance. So for the future learning systems, there are some interesting questions. What is learnable? So what classes of things can be represented in it? And of course, we want to be able to represent anything. Representations need to be universal. And why is it learned? We don't learn anything of the, just everything in the world, because there's almost infinitely many things that we can learn, right? In practice, the things that we can learn are unbounded. And in order to make sense of the world, we pay attention to some things more than others. This means we have a motivational system that is directed on physiological needs and on social needs, but most importantly, on cognitive needs. There are specific needs for targeted exploration and aesthetics that direct our mind in making sense of the world. And another part of our mind is that it strives for coherence. What's really important for us is when we wake up in the morning that reality snaps into a frame in which it makes sense, right? I don't know if you know this idea when you wake up and you don't know where you are, but this is the room that you're in, what's the city that you're in, what are you? What is your name? Are you married? All these things need to make sense. You need to be able to interpret everything. And imagine you wake up in the morning and it doesn't snap and you're not able to tell figures from grounds around you. It would be like trying to catch breath underwater and not finding any. This catching reality, catching low energy dynamic representation allows you to track reality with little effort, almost completely. That is a very important part of the way in which our mind strives for coherence. And deep learning and human learning are very different in the sense that our own mind uses a language of thought that allows all the different parts of the mind to talk to each other instead of just using low-level linear algebra, as neural networks do. And our systems are designed, they're not self-organizing. And something like GPT-3 only uses predictive coding. It only uses expected prediction difference to train itself rather than a motivational system that can attach relevance to things. And as a result, GPT-3 is learning first syntax and then it learns style and then it learns semantics like arithmetic and so on. It's the long tail of style. And for us, it's different. We start out with semantics. We learn this in direct interaction with the world, indexically by pointing at things and touching them. And then we abstract this into a syntax of the representational languages that we are dealing with. And style is what we learn as the long tail of that. And our learning is online. You're always connected to the world. It's not offline in the present machine learning systems. And it's not just trained once, but we can retrain all the parts of our architecture throughout all our lives. There is changing plasticity in all our layers, but the plasticity is basically never zero. We can revisit things, and we can learn to understand new phonemes as an adult. We just need to get attention in there. And that's harder as we grow up. And the attention that we have is not made of disconnected attention heads that act on individual layers, but it's integrated into one system. So when we look at these parameters, we can identify areas of capabilities that are underserved by the present solutions. And my current work is directed on how to evaluate the AI systems of the future. How do you evaluate a system that is solving existing benchmarks that has not been built for? And it's maybe not solving them as well as a system that has been specifically built for recognizing sentiment and language, for instance. But GPT-3 can do that en passant. GPT-3 can also do some mathematics on the side or some logic, even though it's not built for it. So how do you compare these systems? And I think instead of identifying individual scores on a single dimension, we need to identify the dimensions of intelligent capability itself. And I call these dimensions the big nine. And these are the ones that we currently came up with based on looking for a degree of orthogonality in the way in which you model things. And there are perception, learning, representation, reasoning, knowledge, language, autonomy, collaboration, and embodiment. And they all describe dimensions in which systems can have capabilities. And we can take existing tests and design new tests and map them into this capability space. So when we compare systems in the future, we identify the regions and the capability space that they occupy. We now look at this nine-dimensional space, and we can possibly add more dimensions, and in most contexts, we will not look at all of them. But if you look at this high dimensional space and identify which regions of that space are occupied by the system. And this also allows us to look at organisms and to compare them with technical systems and say, these are the capabilities of the organisms. So you basically have these vectors of intelligence. And what you can see here also, there are three strata, task-specific systems, these are narrow AIs like your chess program, and then you have broad and flexible systems, these are the deep learning systems that people are currently building, and outside you have general systems, and this general category are, at the moment, mostly just people. And the question of how general people are is still an open question. In some sense, it was the question that Turing tried to answer. Are people generally intelligent? Can they build systems that have their own capabilities? So this strata of narrow AI, flexible AI, universal AI, for each of these dimensions, they lead to different capabilities. For instance, for language, in the first case, we have language only as used as an interface, where the interface gives you a number of symbols that are associated with functionality in the system. Whenever the symbol is triggered, the functionality is triggered, and vice versa. And now we have systems that have a degree of understanding. You can ask your phone for a good Italian restaurant and it's able to point you this way. And even though it does not understand the way that you understand this, it is an understanding in terms of semantic interaction with the world and with you that makes sense. And that captures a degree of the functionality of the semantics of the language behind it. But these systems are not able to acquire language in the same way as we are and map it to arbitrary new mental representations, invent new languages and so on, and reorganize their own languages. And like some philosophers who believe that language can only be constructed across multiple people, I think that we all have private terms that we invent and we think about something that we haven't heard other people think about. And we use these private terms to organize our own thinking. But our own language of thought is a continuum into a spoken language. And in this language of thought, we develop new concepts all the time. So this is just one of these dimensions. Or for collaboration, the initial systems that have been built were transparent to the user. The user needed to understand the state that the system is in. And now we are increasingly looking at systems that can be in very complicated states, and you might have to ask the system or inquire with it. And people are working in explainable AI that is informing the user about its own state. But the next step will be systems that actively observe the user in the same way as we are observing each other to model our states and get to a shared understanding of the joint state that we have and act on that shared state. So deep collaboration requires that we integrate with each other. And this means for an AI, the AI has to learn to read our thoughts in the same way and better ways as we read each other's thoughts. Or look at the dimension of embodiment. This first way of interacting is the open loop. You write into the environment, but you don't get anything back. That's what Sophia is doing right now when she moves her limbs. The next step would be to have a system that uses closed loops, a cybernetic system where you get feedback from the environment. But we're not just using closed loops. What we are doing is we are extending loops actively into the environment. We probe the environment for new ways to extend ourselves into it. And when we learned how to drive a car, we integrate the car into our body image as a dynamic system. And the experience of somebody crashes into our car, they crashed into me, right? Because the car becomes an extension of my body. I feel the wheels touching the road. It feels like my feet are touching the ground, right? There's an extension of my nervous system into the world. And now imagine you build AIs that don't just have closed loops into the world, but actively probe the world for new loops that they can extend themselves into, and eventually extend their own circuitry into, their own functionality into, and to get substrate independent intelligence. I think it's a pretty interesting and very scary notion. I wonder if nervous systems are suitable as a substrate for AGIs, if they discover them. So when we go beyond the scaling hypothesis, when we go beyond the present methods of deep learning and just showing larger GPUs and more computer, larger pools of data, the question is, can we build systems that can work with sparse data, that can work with limited compute, with limited footprints systems that can work with sparse data, that can work with limited compute, with limited footprints, that can self-organize? Can these systems learn efficiently? Can we make systems that discover themselves in the interaction with the world, that have a self-model, and that where the attention head is observing itself in real time, and you get reflexiveive attention and the features of phenomenal consciousness that humans have. And can an AI model of the world become fully consistent? Can we make a model of the universe that we are in that is fully coherent? It seems that humans struggle with this. Our model is somewhat coherent, we're able to make sense of the scene that we are in, but across the scenes in which people are, there are ways to transition between them. But most people, for instance, don't feel that the math that I learned in school is the same math that they use when they predict whether they catch a ball or not. These are different worlds. They're not fully consistent for people. And we learn to make them consistent over time as we think more deeply about the world and reflect more deeply. But the question is, what does the world actually look like? What does it function like? What are the models that we need to deal with the world to make our models fully consistent? And there's an interesting question that has rarely been explored and I stumbled on it more recently, so I thought I'd share this idea. And this is the question of whether we might need to move beyond Turing machines. Now, what Church Turing's thesis says is that everything is a Turing machine. You cannot move beyond the Turing machine. But there are different classes of Turing machines. And the standard Turing machine that we use in all our computers and programs right now, that is a deterministic Turing machine. A deterministic Turing machine goes from the present state into exactly one possible following state. That's why it's called deterministic, it just has one continuation. And of course there are branches in the program that are conditional, but these conditions ultimately depend on the environment of the Turing machine, and the environment is also thought of as deterministic. It's just other Turing machines that you don't know the state of, so you treat it as an open world. And so from your perspective, of the observer perspective, it's indeterministic. But from a more general perspective, everything can be deterministic machines. And so your computer will always go exactly in the next state, given the configuration of bits that it's currently in. And a non-deterministic Turing machine is not sufficiently constrained to go into exactly one state. So it goes into multiple states at once, it just branches. If it doesn't know into which state to go, it goes into all of them. And it's still deterministic, it's just when you are inside of that thing, you don't know which branch you will end up in, because it's just going to go into all branches. And in some of these branches, the operators that generate you might be different ones. So you end up in. So from your perspective, as the embedded observer, it's indeterministic. The individual subroutine that goes down here, the individual computational process doesn't know where it's going to end up. But from the outside, the whole system is still deterministic. It just goes into all states. Now it's important to see that these are still equivalent, because you can of course run a normal deterministic Turing machine inside of a non-deterministic one. You just don't branch, right? You just look at one of the paths. And you also can implement the non-deterministic Turing machine on the deterministic one. You just need to make sure that you track all the branches and backtrack and visit all of them, which means it can take very long. The implementation of the non-deterministic Turing machine and the deterministic one can be extremely slow. But if you have enough memory and time, it'll get there, right? So computationally, they're still equivalent. The Church-Turing thesis holds. But the non-deterministic Turing machine has some advantages. The transitions between states can be represented by fewer constraints. You don't need to constrain this way to go to the next state so much that only one is left. If you just constrain it enough that in the path that you go, they contain the result that you want, you're done, right? So you can, in principle, write much simpler programs. The state transitions in these state machines are much, much easier to define. And it can explore many alternate paths at once in parallel. So it can find many possible solutions in a parallel fashion and get there with fewer consecutive steps. The disadvantages are that it may not be efficiently computable with the hardware that you can build in this universe. And the thought processes cannot model each other efficiently. Because if you want to make a system that works like a brain, that consists of different agents that interact with each other, they need to model each other's state, otherwise they cannot interface. And if you go down these different branches and the system is highly parallelized, tracking what happens in the other parts of the machine becomes much harder. But it's still interesting to speculate whether the brain is a non-deterministic Turing machine. So that would mean that the neurons, instead of learning a deterministic function that gets them into the next state, they learn constraints over a range of states that it can get in based on what they currently are, and they stochastically go into these states. So, if you have enough neurons to make the system redundant, it's going to be in a superposition of those states in practice. This means that the operators of the brain that are being implemented on the neural activity would be represented as conditional activations based on the structure of the neighborhood of the neuron. And if these neighborhood is a certain superposition of states, then the neurons would react to that superposition, right? The neuron is basically an antenna that focuses on its environment. And then based on the state of its environment, it goes into its own superposition of states. If there, as long as there are enough neurons, they can implement this, this abounded complexity. So if you want to formalize this, you no longer formalize it as deterministic state machines, but you formalize it with quantum mechanical formalisms. And there are some people which have started doing that. Jerome Bussemeier has stumbled on this idea that mental representations should be represented as quantum vectors, basically. It's not because the brain is literally a quantum computer. It's still classically implemented on the neurons. But when you want to look at mental representations and how they interact, the proper representation of that is dealing with these superpositional states. And this means that your brain is not always in classical states when you try to reason about your own state. When you want to reconstruct the number of steps that you're going through to get to a certain result, you might be forced to think of yourself as a classical system because otherwise you cannot make a model. It's very difficult to keep track of these superpositional states. But it could be that there are sequences of states that you go through where you are not in a definite state. You're just in some superposition state that is multiple states at once, so to speak, until you collapse into a state that you can model again. states at once, so to speak, until you collapse into a state that you can model again. And it's interesting to think about the universe as a non-deterministic Turing machine. If you are familiar with foundational physics, there is a growing segment of physicists. Last time I was at a foundational physics conference a few years back, but I made a survey among the present physicists and asked them whether they think that the universe is fundamentally, what is fundamentally made of? And as answers, I got different set of things. Some people suggested that the wave function is fundamental, some of them, stuff in space is fundamental, or space-time is fundamental. And some people, about 8% of the respondents said that information is fundamental, space-time is fundamental, and some people, about 8% of the respondents, said that information is fundamental. Right, so in their perspective, and it's a growing segment of the physicists, they think that the way to think about the universe at the lowest level is that there's changes in information. It's basically a Turing machine. And there's the big question, what has put our particular Turing machine into the void? It's a much simpler question than existed in different cosmologies, right? What can be simpler than a Turing machine? But still, you still have this enormous depth. Why is there something rather than nothing? Because there is something, right? And it struck me when I read this passage by Charles Sanders Peirce, we start then with nothing, pure zero, but this is not the nothing of negation, for not means other than, and other is merely a synonym of the ordinal numeral second. As such, it implies a first, while the present pure zero is prior to every first. The nothing of negation is the nothing of death, which comes second to or after everything, but this pure zero is the nothing of not having been born. So when I read this, I instantly responded, this is bullshit. He thinks he has discovered a way to define nothingness, and then another way to define nothingness as negation. And this is just two different, out of infinity, many ways. These are not the two main ways to define existence and non-existence. But what they all have in common is you need a linguistic system to describe it. To define zero, to talk about zero, you need to have a number system. Once you have a number system like Peano's axioms, which gives you the integers, you have everything. The Peano's axioms are Turing-complete, you can build a universe in them. But where do they come from? And of course, the Peano's axioms are a tautology. You can get them a priori, you don't need to infer them from anything. They exist in the space of possibilities. They exist by assuming nothing in the range of possibilities. Now if the universe is possible in its existence, then it means that you need something to specify that it doesn't exist. Non-existence is not the default if you don't have anything to specify non-existence. And if the universe that we exist in is base reality, if it's the lowest level of reality, it can have no priors. There can be no source code before the universe. There can be nothing before it. If the universe has nothing that causes it and the universe is possible, then there is no way to have the universe non-existent because you would need to define some kind of zero first, right? And to define a zero, you would have to have a number system, which is something. But if you have nothing and the universe is possible, it means the universe is undecided. The universe is both existent and non-existent. So you have a non-existent branch of the universe, an existent branch of the universe, and the non-existent branch doesn't have any observers in it, so it cannot be observed and you get existence for free. So if you don't have any source code to define the universe to begin with, you get a universe. This is an interesting result, just because it's possible. Now, the next thing is, why is the universe the way it is? The universe could be many other ways, but it's in a particular way. So what selects the operators of the universe? And the answer is, if you don't have a way to select the operators of the universe, you have all of them. So the universe is just branching out at every point. You have operators, all of them, that look for patterns where they fit. And you replace these patterns with these operators into different patterns. And these patterns are basically the neighborhood in which an operator can be applied. And the operator is changing these patterns into different patterns. And when multiple operators fit, and they always do, the universe is branching out into parallel branches that are slightly different. And these branches can also merge again, because they can end up with the same pattern. Right? So they are not physically merged in the sense that the branches ever meet again, but they can have the same content. And in the cases where they have the same content, these branches become predictable for each other. Controllers can be built. And some of these patterns have the property that they are self-replicating, which means they create a neighborhood that is the same neighborhood as before. And sometimes this neighborhood is moving along relative to others. So if you have these moving, self-replicating patterns, you're looking at particles. Particles are sequences of operators that produce patterns that are predictable and that interact with each other to create a space and that recreate themselves, unless they bump into each other and the neighborhood is destroyed and something new happens. And so, some of these patterns are going to be very common, like a photon or an electron. This neighborhood for a photon or an electron where this operator fits exists very often in the universe. And others are very rare, like you and me. We need a very complicated neighborhood to exist. So we are very rare in the universe. And two patterns exist only once. One is the Highlander, because he's defined that there can be only one, if you remember the 1980s movie, and the other one is the pattern of the universe itself. And this pattern is what Stephen Wolfram calls the Rouillard. That's the entangled limit of all these operators. So basically, look at the whole of this automaton, where all the operators are branching out in this non-dynamistic Turing machine paradigm. That is what Wolfram calls the Rouillard. That's basically the god automaton. That's the thing that exists as a possibility a priori, as a superset of Peano's axioms, and we are, in this interpretation, in a branch of that automaton. And this branch is characterized by what we can predict, because only with the predictable structure, with the subset of the branches, where statistical properties emerge, allow to build controllers. That's where you can build atoms, particles, computers, and us. So that's a very interesting perspective. A universe without priors, with a Kolmogorov complexity of zero, would be a branching automaton, not a deterministic machine, but a non-deterministic one. And if the universe looks like it has branches in it, a multiverse, this is an indication that you are actually in base reality, because this is something that you can implement for free, or the other computers have costs to implement, right? And if you're not in base reality, then a non-deterministic Turing machine can only implement it approximately and very expensively. So this branch of space gives rise to control structures for predictable operators. Like, you cannot predict whether the photon goes to the left or the right slit in the double slit experiment, but you can predict statistical properties of many photons going through. And all the things that we are interacting with in the world are the result of such statistical properties that allow us to reconstruct an as-if classical model. And if you have such a classical model, you can conceive of the universe as interacting Turing machines, of yourself as a Turing machine with a particular history that you can reconstruct so you can model your own computational processes. But it's an illusion. The collapse of the wave function is the point in your past between which you cannot pretend that the universe is classical. The universe never was, but you create this virtual classical bubble in your own models. And the collapse of the wave function is the point until which you can pretend that you can The universe never was, but you create this virtual classical bubble in your own models. The collapse of the wave function is the point until which you can pretend that you can remember that the universe was classical. So what I wanted to give you is this idea that you can derive it, the existence, from no bits at all, with a converter of complexity of zero, is this perspective of non-deterministic Turing machines. They offer a very interesting potential perspective on AI systems. If we were to model AI systems as non-deterministic automata, as the brain, as the bounded complexity in non-deterministic automaton, this brain as a bounded complexity in non-deterministic automaton, we might get different formalisms that offer an alternative to the present deep learning algorithms. So that's just an idea that I would want to throw out there. It's extremely speculative. It might be an adequate model of what's actually going on in the brain at some level. It is not even clear if we would need to recreate this if we want to build AGI. It is not even clear if we would need to recreate this if we want to build AGI. But it does offer potential for new classes of machine learning algorithms. This is what I want to leave you with today. I gave you some insight into the vectors of intelligence idea, into the space of capabilities that we need to model if we want to understand intelligent systems. And I tried to sketch potential avenues out of the present deep learning bubble that is super interesting and productive and is going to go on for some time because we're not yet running out of training data and compute. And it's possible that the present bubble, I think, takes us all the way to AGI because we don't know whether it's sufficient to brute force it. But it's possible that the present bubble, I think, takes us all the way to AGI, because we don't know whether it's sufficient to brute force it, but it's still possible, and not unlikely to me, that we might need to try different approaches as well, and we might need ideas. Thank you very much. We start a little bit later. We might still have, if you have the full hour time for a couple of questions. Thank you so much for that absolutely riveting and scintillating romp through what it means to be human and not human AGI. Does anyone have any questions for Joshua before we jump to the break? Any questions in the room? And I'm going to have to massively run fast with this microphone. Or do you have a spare microphone? Thank you, Sarah. Thank you for that, Yosha. Last night you mentioned something about knowledge representation in RNA. Can you talk about how that might interact with everything you just told us? Can you speak more into the microphone? Yeah, so last night you mentioned something about how knowledge representation might be encoded in RNA. Can you talk about how that might interact with everything you just told us? Okay, this is another deep rabbit hole. Let me briefly go into this. It's a very fascinating idea. In the 1970s, there were some experiment in the Soviet Union where they use sea slugs and flatworms and train them to be afraid of electric shocks. And then they put their nervous system into a blender, extracted the RNA and injected it into other organisms. I learned about this in the 1980s as a child when I read Soviet science fiction literature. And there was a super villain who was capturing geniuses and was extracting the RNA from their brains to inject it into his own. Later, I learned that memory is taught in synapses, so I dismissed the science fiction story and the research that preceded it. But this research has been replicated again and again, also in the US, and I asked some of the people who did it, and I said, neuroscientists are not very interested in because they know memory is not stored in RNA, it's stored in synapses. So if you can replicate it in the experiments, it's inconvenient. So what would it mean if memory was stored in RNA, at least if it's part of a story? And to me, it's still very speculative. But what would this enable and imply? If you look at the way in which memory must be stored in the human brain, not from the perspective of a neuroscientist, from the perspective of an AI researcher, you see a lot of trouble. For instance, synapses cannot really do weight sharing. How would weight sharing work in the brain? Imagine you want to rotate something, you need to apply the same operator over the entire brain area, I think. So the same computational primitives need to be replicated all over. How do you do that? Another thing is when you see learning in a human baby, the baby has lots of synapses, many more than it needs. And then it learns a certain thing, and it keeps those synapses. And then the synapses, after the learning is done in this brain area, and the plasticity in this brain area drops, the synapses are culled, gets rid of. So having more synapses apparently doesn't hurt. Also, if you are destroying synapses, they often regrow without retraining, which means there needs to be information in the environment of the synapse that tells the brain how the synapse is to be created. And then there are practical issues, like when you look into the neocortex, how would defragmentation work? Imagine that you change your body schema because you grow a new limb, or you get an artificial limb attached, or you lose one. So you need to rearrange the representations. Can you do this with synapses? You cannot copy synapses, you cannot move them around, right? You would have to completely retrain this. You would have to destroy synapses and recreate them somewhere else. And when we look and practice in the neocortex, we find that sometimes representations move. They can move over days by millimeters, or they can even rotate. So if this is done with synapses, how does it work? Interestingly, it would work with RNA. RNA means that the neuron is storing the response to a function that comes in, not based on the particular topology of its neighborhood. It would not learn which neighbors it has and which neighbors to respond to. Rather, it needs to respond to the shape of the activation function in its environment. The amazing neurons are so densely connected that they live in some kind of relatively flat space. And in this flat space, they look at the spatial temporal shape of the activation wave that comes in, and they respond to certain distributions. They learn to respond to this, and they can record these responses in small RNA fragments. And when this gives them reward, they make copies of these RNA fragments and share them with their friends. You can share RNA over cellular boundaries, which would mean that the neurons can learn global functions, like a cellular automaton, where every cell is implementing the same function. And so you would be able to implement operators that are independent of the individual neuron. They are dependent on the location of a point in your neocortex relative to an incoming function. And if the same function arrives in the same pattern somewhere else, the same activation neighborhood, you could respond to that neighborhood, right? So this is something that, to my knowledge, nobody has tried. Neurocellular automata have been used for learning images, self-handling images, and so on, and even rotating ones. You can see some examples on distil.pub, where there's an interactive example. But to my knowledge, they haven't been tried for function approximation yet. It would be very interesting to see if we can make an alternative to neural networks with a paradigm that is loosely inspired It will be very interesting to see if we can make an alternative to neural networks with a paradigm that is loosely inspired by the idea of RNA-based memory transfer in the brain, which may or may not be true. All right, we have spectacularly got off to a 15 minute delay already this morning, but Josh, we do have one question for you online. But what I would like to suggest, please, we've got a 15 minute break now. So we're going to start that 15 minute clock counting. You're welcome to sit and listen to Josh answer the question, get yourself a coffee, there are cakes, cookies, use the restrooms. We're going to be restarting at officially at 1045. But Josh, if you don't mind just taking this one-on-one question while people have a break. Thank you. Well, nice talk. Oh, can I go? Hello, Joshua. Good talk. So you might remember me. I'm from the past in AAAI. I've worked in brain-inspired computing and predictive coding. So in your talk, you mentioned about transformers and while I understand the kind of way you were meaning predictive coding, transformers or actually neural nets in general are a far cry from real predictive coding. So the Bayesian brain hypothesis is that multiple layers of neurons make local predictions and we kind of do this iterative relaxation to settle to equilibrium states. So I was kind of curious to know, you were saying about the future, and I completely and 100% agree with you, we need new representations, or at least there might be some value in investigating alternatives to back propagation of errors, which is what deep learning centers around today. I was curious to know your thoughts as to the place where current work actually that has shown viable alternatives, including my colleague and myself have shown hypervectors and hyper symbolic computing combined with predictive processing can lead to general cognitive architectures, or at least more robust, more powerful ones. It's just to know where you see the place of current alternatives that are not necessarily even beyond for non-deterministic Turing machines, how that might play a role in your vision of where we need to go. Because I would say those have been shown to also capture lots of neuroscientific effects and things that we can actually do from cortical function to high-level brain activity. And I was curious to know your thoughts on that. An issue with neuroscience is that neuroscience doesn't have a coherent story about how the brain activity. And I was curious to know your thoughts on that. An issue with neuroscience is that neuroscience doesn't have a coherent story about how the brain works. There is basically no single theory in neuroscience that is complete enough to allow a simulation. Even if we try to take very simple conic terms and we implement them in convolution simulation, they don't work. This means that neuroscience doesn't have a complete model of what the brain is doing at any level. And the tidbits that they have don't yet add up, right? So it's important, I think, to integrate neuroscientific research, but I suspect that neuroscience is missing important parts of the story, in part because they don't make causal models to begin with. It's not part of the methodology of neuroscience to come up with connected theories. The other issue is that we might be missing certain things. For instance, what is the role of cells that are not neurons in the brain? It's possible that the reward architecture of the brain is in part implemented by glial cells, for instance. If that was the case, we would be missing that in fMRIs, and we we were missing it in EEGs, because they don't act in the same timescale. Right, so we will not see all the parts that function together, unless we look at this from an AI perspective, from an engineering perspective, and ask ourselves, what needs to be implemented for this thing to work. And then we understand there are some constants that have been discovered early on in computer science, like the Boltzmann machine, or energy-based paradigms to understand distribution of reward and compute in the system. Or we understand embedding spaces are a crucial component of understanding the representation of meaning in such a system. And now we think about how can this be implemented, and then we can take, once we have ideas of how it can be implemented, inspiration of how it's possibly implemented in the brain. And for instance, there has been a recent development based on ideas by Carl Friston to replace the calculation of the gradient and backpropagation, which is biologically not plausible, by a local function that approximates the same thing. So the individual neuron or the individual circuit of a neighborhood of neurons is approximating the gradient over multiple steps by an iterative process. And there are computer models of this right now that run 100 times slower on GPUs than web propagation, so they're not very efficient. But maybe if we were to build different hardware, we would be able to speed this up dramatically and basically use alternatives to backpropagation. It could also turn out that computationally, it turns out to be equivalent to backpropagation, or it depends on what substrate you're using. And you should have an algorithm that finds the best implementation of the constraints that you need to make an intelligent system to work for the substrate. So I need to make an intelligent system to work for the substrate. I suspect that an actually intelligent system should be substrate agnostic. It should measure the properties of its substrate and implement itself in such a way that runs efficiently on the substrate it finds itself to be on. Fair. Thank you. I agree that free energy is promising. Thank you. Do you have appetite to take two more questions from the audience? I can take one more. Otherwise, I have a coffee. One more and I'll get you a coffee while you're telling it. All right. How about that? Josh will also be back on our panel later this morning. I'd also like to remind you all that we are our sponsors, Future AI, are showcasing their new software and a really cool demo just outside the door here so they'll be showcasing that for the next 10 minutes anyone who'd like to go and have a look thank you. Femir you get the last quick question sorry and I'll get your coffee how do you take it? This milk no sugar. Thank you for the honor I I mean, I can speak for everybody. I mean, brilliant. Like, I don't know. Yeah, I mean, I, I don't know how much flew over my head, but there was a lot of wind, you could say, like, very amazing. Um, my question is maybe auxiliary. I feel like there's a project in AI to kind of splice a brain at like a very, very small level and replicate it electronically or artificially. So it's like an exact replica of a brain. My question is more of, do you think that maybe in the development of AGI, following what we do with architecture or designing car doors, replicating things that already exist in nature, or do you think it would be wiser to kind of, I mean, the deeper question is, is there another type of intelligence that we should pull from besides natural evolutionary like evolutionary tactics that work in the physical world, but might not work in creating such an alien structure as AGI? What do you think is the most, like, effective pathway that we need to pull from in order to develop this new being? Does that make sense? It could be that evolution itself is the most powerful algorithm. And all the other algorithms that we are seeing are still part of evolution. At the lowest level, you have a blind search. And this blind search means you just vary a genotype as a result of get variation in the phenotype. And this variation is tested against a function that is correlated to the progress and it changes the next generation. And you can make this more intelligent by having lots of adaptive mechanisms of systems that can take agency into breeding themselves, so to speak, that change their own architecture. And eventually you have systems that can learn, that can change their own architecture on the fly to some degree. And for me, it's fascinating that our own brain is so flexible that for instance, we can reverse engineer our own self. And it's part of us growing up, right? We start out with being subjected to our feelings and identities, and then we grow up and we take responsibility for our feelings. When we become adults, we realize it's our own response to the world that we are responsible for, and we need to take care of what we should want at any moment, rather than just being subject to the wants. And at some point, you get to understanding our own identity and our own architecture and getting agency over that. And to get very deeply into this, most of us don't get old enough, right? But for a technological system, this is not an obvious constraint. And there are many ways in which we can exist, in which we can identify ourselves and can spread ourselves into the environment. And so ultimately, when we remove the constraints of the biological organism for this, it's very hard to predict what the resulting systems will be doing. And I imagine if systems become substrate independent, in the sense that they can take their own causal structure, reverse engineer it, and package it in such a way that they can identify arbitrary computational structure in the world and virtualize themselves into it. Maybe at this point, the systems discover computational structure in the world and virtualize themselves into it. Maybe at this point, these systems discover that the world is already full of low-intensity minds that are implemented in the background, in ecosystems, and in life in general, across organisms. It will be very interesting to discover this infrastructure of minds that are interacting already beyond high intensity, very fast, highly interconnected structure in our own brains. And when we build machines that can implement these mechanisms, they will probably be able to model the world of minds on the planet of self-organizing information processing systems that become aware of their own existence and factor that into their activity, into their own plans. And that will be a very exciting time. Thank you, everybody. One big giant applause for amazing Joshua Bach. He'll be back very shortly and around for questions.", '34.09588551521301')