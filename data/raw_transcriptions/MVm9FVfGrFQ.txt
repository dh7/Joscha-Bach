('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/MVm9FVfGrFQ"> </iframe> </center>', " Yeah, I'm very happy to have you here. Can you introduce yourself, Joscha? Hi, I'm Joscha Bach. I'm an AI researcher currently working for Interlabs in California. I am interested in things that happen beyond deep learning. I joined artificial intelligence mostly to understand how our own minds work and how they relate to reality. And I've worked in the past on understanding emotion and motivation and cognitive architectures. And I'm generally very interested in the entire space of intelligent systems and how they interact with the world and society. Can you tell us what excites you about AI? AI is basically two things. On one side, it's a discipline that is the pioneer battalion of computer science and pushed it along into the future since its beginning. And for the most part, it's dealing with the automation of data processing. And on the other hand, AI is a philosophical project. It's the attempt to basically marry philosophy and mathematics by automating the mind, by closing the gap between them. Our own minds are so small that we have difficulty understanding the world at scale. And at a certain level, in order to get deeper than our individual minds can, it is in a single lifetime. And over many generations, with teaching each other, we need to scale up the principles of intelligent information processing by teaching machines how to think. And this premise of artificial intelligence is what interests me most. So if you think about AI, what in your opinion is actually intelligence? In simple words, intelligence is the ability to make models. It's not necessarily the intelligence, the ability to do the right thing or to make the right choices or to develop power. But intelligent people typically are often intelligent because they focus on becoming more intelligent. And that often is a sign of their normal regulation not working. So often in practice, intelligence is a compensation for other things that are not set up in the right way. Maybe you are in a very different environment, so you need to become smarter. But in a general case, the ability to make models happens in the service of control. And control means that you are looking at a deviation between how things are and how they should be. And you trigger an action that reduces this difference. And a simple controller controller like a thermostat only does this in the present frame. And if you combine controller with a modeling system, and this modeling system starts to integrate the expected deviations of how things should be from how they are over the future, and factors in the possible choices the system can make, that is the different actions that it can perform and how that is going to affect these can make, that is, the different actions that it can perform and how that is going to affect these different futures, you have an agent. An agent is a controller with the ability to control future states. And to control future states, you need to make models of the future based on the observations that you make in the past, and that requires that you combine the controller with a somewhat universal learning system. And this ability to universally learn to take the decisions that you're making in the present based on the observations that you make in the present and to take all these decisions over time and compress them into a policy. This learning is basically what is facilitated by intelligence. This means my perspective that intelligence is not only about making models, it's also about abilities to solve problems, right? Well, to solve a problem, you need to make a model, right? So I think that in the general case, the intelligence is the ability to make models is not a bad definition. But to understand how this is happening in the universe, you have to realize that it's mostly happening in the context of solving a control problem. But there can be situations where a system is not actually solving a control problem and still is intelligent. That's why I disentangle both of these definitions a little bit. So I don't think that intelligence is the ability to be effective in the universe. It is the component of that ability that is involved with making models. So I mean, if you think about the extreme progress AI had been made in making in the last years, or maybe in the last decade, did this surprise you? Yes and no. There were a few developments that happened a couple years earlier than I thought they would. And this depends on particular kinds of technologies, but by and large I have not updated my timelines very much, because I think that if you zoom out very far, we are still using pretty much the same algorithms as we discovered them in the 60s and 70s, with small incremental changes as was to be expected. And many of the progress beyond this is given by progress and hardware. And this progress and hardware has been largely predicted by Gordon Moore. And this basically this doubling of the ability to compute. And is this roughly within these predictions, what was happening, right? So it has been a little bit of a flattening in these curves, but the technology that we built has become much more valuable. So we can spend a little bit more money on compute when necessary. And if you look into more detail, you look at the individual algorithms, then I think one of the major breakthroughs was the ability to train neural networks at scale with unsupervised learning. There was this paper by Andrew Ng in 2012, and it was a collaboration with Google where they took random frames from YouTube image videos and then tried to find structure in them, and it was a deep network. And this succeeded to the point that they basically got an automatic emergence of face neurons and cat neurons and so on. So this thing was discovering categories just on statistical regularities. It was a far cry for what happened much later. But it was an existence proof at scale that could no longer be ignored. And in many ways, I think, for the public consciousness, this was what triggered the deep learning revolution, even though the algorithm for convolutional neural networks and the training had existed a while before that. And the next step is the transformer 2017. This idea that we should probably make statistics over what you should make statistics over was a little bit older. But this was one of the first implementations that worked at scale and was deployed in language modeling, because for image modeling, the existing convolutional neural network algorithms were already good enough and created something like a small bubble, but the same algorithms didn't work on language because in images, adjacent pixels are typically semantically related. So just by giving the networks a bias to look at the environment from the spatial environment for related information and build this up into hierarchies was already good enough as a heuristic to train the neural network. But for languages, it doesn't work because adjacent words are not necessarily always related. So it can be in between a sequence of words. For instance, you can want to have the relationship between the last and the first word in a book chapter, right? And so the convolutional network has difficulty to discover this and people needed a more intelligent algorithm that was the transformer. And it turns out that the transformer can also be used to find structure in audio files or to find structure in videos or in images. So there is somewhat general learning algorithms that can be used for all sorts of modalities. And that is now being scaled up to the point where you suddenly have several families of new products that are changing the way in which people interact with machines. When I remember like it was around 2015 when I learned about this theory of exponential growth and I listened to several lectures from Ray Kurzweil and JÃ¼rgen Schmidhuber, and it really flashed me. So, like, at first I thought, I cannot believe this is true. And I think I still like, Ray is very bold and making very clear forecasts with very clear dates. And I'm not so sure about actual dates or so, but can you maybe give the people who are listening right now who have not heard this, give a quick summary about the main idea of this exponential growth and the beginning of an S-curve. There is nothing that can grow exponentially indefinitely because everything that exists in the physical universe is at some point reaching saturation. And the beginning of an S-curve is typically also the first half of a hump, because everything that goes up also must come down over long time frames. At some point, the atmosphere on this planet runs out and the number of organisms is going to go down again. And in the same way, basically what we are looking for is a part of a trend. And it's sometimes difficult to see how the trend is going to pan out. Very often, we have these famous hype cycles. And a lot of people feel that at the moment, deep learning is overhyped. And at the same time, the same people that feel that AI is overhyped are also typically admitting that they did not predict Dali from happening or a stable diffusion from happening. And if you are underestimating the progress in actual products and developments, then you probably have an under hype situation still, right. And so we could also see that in 2000, most people felt that internet companies were overhyped. This idea that you could have internet companies that are more valuable than General Electric or Walmart seemed to be ridiculous after the bubble burst. But now we have had to regrow and what we see is that Amazon is there and Google is there and it's not going anywhere for the foreseeable future. And there has been this transition of what is the most important part of our economy and this most important part of our economy and innovation at the moment is indeed digital technologies. The world of bits has become in many domains more important source of progress and automation and wealth than the world of atoms. So at first, like when I heard about this, these ideas of Moore's law and exponential growth, and so I was also very skeptical and I couldn't really believe it. And I completely agree that there has to be an S curve, so it cannot go forever. The question is when, and I've been thinking about possibilities, like physical possibilities, where it could be a limit and how to overcome this limit. And I mean, you cannot make the current architectures smaller and smaller and smaller indefinitely, because at some point you will simply get too slow for like quantum effects to bump in. But I mean, there are so many things like maybe in the future 3D processors that don't only build in the width, but also in the height, or maybe stuff like optical processors, stuff like neuromorphic architectures, where you like make a decentralized processing happening. And what's it? And you can make estimates over the upper bounds, right? processing happening and what's it... We can make estimates over the upper bounds, right? So we know when we make relatively flat circuitry, we look at the present technologies, we can make an estimate of when are we going to go down to the level where the individual transistors are just a few atoms large, and we need a massive error correction to get additional compute out of them. Then we can look at alternative materials like switching from silicium to germanium and so on, or we think about different principles going to optical circuits, or also using the existing technologies in new ways. For instance, membrane stores where we store the memory next to the compute elements and have more distributed computation. Or we take best effort computing seriously and we think about self-organizing systems that are stable against indeterminism and the hard variability in similar ways as our brain are able to stack the probabilities until they are probabilistic enough, reliable enough, and probably would enable us to build computers that can be driven by body heat, right? An Apple Watch that is doing all the same things as an Apple Watch does right now, which means it's a small iPhone. And that because it runs on a suitable hardware with novel implementation techniques is going to run forever. Just by using the temperature differential between your skin and the environment in principle, these things might become possible one day in the not too distant future. There's also this question, how long are we able to sustain this progress? I expect that at some point there will be something like an internet FDA. And when you are trying to start a new website, you have to get a permission to do so. And maybe it takes a few million dollars to do the testing and building something like a new social media website will be so revolutionary and dangerous that you cannot get it through the process. Imagine that somebody would want to implement the internet today and would ask the government and the media for permission. There's probably no way that this could be acceptable because it's way too unsafe. Imagine people could talk freely about everything and send information about absolutely anything to each other. What this would mean for society. It's mind blowing. This should probably not happen. And yet it did. And we all agree that the effects are mostly good. And they have many problems that emerge with it. It's far from being without problems. But if you would shut it down and take it away from people, it would destroy much, much more than it would reduce harm. And this is a big discussion that is happening still to this day, right? There are many efforts, for instance, to, in Germany, we have the chat controller, the idea that the government should be able to decrypt every chat that happens privately on the internet, or digital channels, to protect children against abuse and grooming and from suicidal tendencies and so on. And this will have, of course, as a side effect that also all the communications between adults will be monitored by governments. There's a lot of side effects. And it looks like it's difficult to balance these things against each other. But if you look at it in detail, I think if you try to measure the harm, it's probably clear that we have to accept some harm to get the tremendous benefit that is given by the ability for people to communicate without government interference. And similar things are happening on the scale, if I think about myself, when I was young and I had access to a growing online community, in many ways, it saved my life, because I was able to find like-minded people and connect to them and be not alone in the world. And we have, for instance, to balance all the kids that are being harmed by the ability to access the internet and to communicate with strangers against all the kids that are being saved from suicide because they are able to link up to the internet and communicate with strangers. And so this is a very difficult balance that we are in, in a world that is constantly changing, where there are no simple answers and the trade-offs are extremely difficult. And we have such an internet moment where new technology is being created that later on would no longer be permitted when the regulation institutions are in place. It's generative artificial intelligence, the field in which you are working yourself. This idea that you can scrape off all the images and text from the internet that is not expressively protected against doing so, and you can train an AI doing this and then give everybody the outcomes of that training to build new things with it is extremely powerful. Like we, Lion, is a German research institution and by German law we have checked this really extensively. Like, looking at what is publicly in the internet and using it for research is by German research law, it's fine. And the thing is, we are also thinking about like if we provide like these links to all the internet images there that researchers and companies and whatever, like, could use, like Meta had been using and Google and Stability AI, and so they have been training, for example, these text-to-image models. And from what I have been seeing, there are several benefits and there are several threats that people are talking about. And it's a really broad discussion. And I personally think it's really good that there are debates right now about both things, about the potential benefits and also maybe even better about the threats, because from what I am aware of and what I'm convinced of, like Google and Meta and nation states already like investing heavily in exploring these technologies. And if people would not talk about this, imagine for example these technologies. And if people would not talk about this, imagine for example these technologies would be developed behind closed doors, no one would talk about them and the ordinary people on the internet or who watch like the general news or so, they would not realize it at all. And five years later something something leaks out. And it's so powerful that you could generate instantly like, like anything like video, audio, whatever, and suddenly, like the compute complete society would be completely overwhelmed with it. So I think like, for me, AI is not exactly the same, but like you could say it converges towards the ability to solve problems. Because right now my kids, they are 6 and 10 and they know YouTube very well, they know smartphones very well. When I was a kid I was watching German private TV like in the 90s and the early 90s and I was dreaming about having a super device where I could just choose the cartoon episode of my favorite series that I would like to view and it would simply appear on the television and right now my kids are seeing exactly this and yeah I think it's good to have like a debate about these accelerating technologies right now and to become aware of them and to take the best out of them. Because I think, in general, there are many benefits and there are threats. And in the end, if you are thinking outweighing threats and benefits, then it goes down to your general like worldview, about like axioms that you personally have about how the world is. And if you view the world as something that is like dominated by threats, and even if there are many beautiful things, you have to avoid these threats at all cost, because they could have terrible effects and destroy all the positive aspects in the world. Therefore you need to put your focus on avoiding these dangers. If you have these thoughts, then you will probably be very concerned about potential threats. And if you are more like an optimistic view, maybe not naively optimistic, but like Groundedly optimistic you think okay. Yeah, basically the world is pretty good And if we make mistakes this is feedback and even if there are serious hardships We can like learn from them and overcome them if you have such an open learning and change welcoming attitude then basically like not always, but most of the times, the opportunities seem far more attractive. And I think we are right now in a very exciting phase of human history, where the change is so fast that people who tend to have this worldview, oh, change is a potential threat, therefore it is potentially bad and threatening, and I want to stick to the good old times. And the other people will think, oh, change is like potentially benefit, and even if it's a little bit frightening and threatening, it could provide new opportunities and help me to learn about mistakes and mistakes are feedback. And so, and this is of course an extreme and in reality, we are all in between of these extremes. But like, this fuels a huge divide in our societies. And I think the earlier societies wake up and realize that like, one main reason for these like, that one main reason for these divides in the society is not that politically something has instantly changed, it's more that the world itself changes with increasing pace, and automation, the internet, Twitter, spread spread of information, like forming of sub-information bubbles and groups, all of this, this would not have been possible like 50 years ago. What do you think is going to happen next in your view? When you are working for Lion, what are your next steps? What do you expect the future to be? I think independent of certain organizations in text to image, which is currently like very hyped with stable diffusion and ImageGen and DALI, and so this will improve dramatically. I mean, we will come to a point where we can basically generate anything that we can describe with words. Maybe not anything at the beginning, but in a few years. And it will incrementally improve. And I think that things that people are now concerned about lots of stuff, so, I mean, related to Lion. So we get on Twitter lots of positive feedback and we get concerned feedback, for example from artists who say, oh, I'm concerned, like there was this Greg Rudkowski MIT article where this one artist who made awesome fantasy art is a little bit concerned, or who is deeply concerned actually now, because whenever he looks for art related with his name he mostly finds AI generated art, because he's so popular and these models can copy his style, that actually like, like there are thousands of like AI art pieces out there that look similar to him, but that are not from him. And people cannot find the original true artworks anymore. Or they can, but it's a little bit harder now and he's concerned about the future. So I think now people are saying, okay, these data sets are bad or these models are bad because they have too many images of certain artists like Rekordakovsky or whoever like feels like a little bit hurt or scared by this not a little bit but deeply scared deeply scared so the thing is from my perspective what I know about the technologies like Dreambooth, retrieval augmented generation and so on this is just just really a small preview of what is going to happen anyway. And these technologies, it's not like that there's like an evil Einstein sitting and oh now I finally found the one solution to this centuries-old equation. No, it's more like these solutions, they are pretty obvious and you don't need to be an Einstein. You can be like maybe a little bit smarter than average machine learning engineer and sooner or later you come to the idea, yeah, this should be possible right now. Because it's more like that the whole scientific field as a whole converges to these ideas and at some point they are simply obvious. And some people have these ideas maybe six months earlier, but if they wouldn't, someone else would. Imagine that you are building teapots. My parents have a pottery and they make ceramics. Imagine that you are working very hard to make a very good teapot. It's not easy because it needs to have the right shape and distribution and it needs to work in such a way that it's not dripping and that the tea is not coming out when the water is expanding, the hot air is expanding in it and so on. Many things that go into making a good teacup, a lot of experience and art. And somebody takes the output of that progress and looks at all the teacups that have been made by many artisans and tea comes up as an industrial process to build porcelain teapots and producing many many of these industrialized porcelain teapots which put them into many homes and in a more reliability quality lower price than the ones that were made by the artisans. This is going to create a big change. It will not lead to the complete disappearance of bespoke artisanal teapots, right? There will still be people who make them and there will still be some people who buy them. But the everyday use of standard teapots in most households will be changed by the availability of the modern industrialized teapots. And if we were saying we want to impose a tax on all the existing, on all the industrialized teapot productions, and want to make sure that we feed all those people who designed handmade teapots in the past, that would seem to be unfair. Because we realize that the purpose of the production of teapots is not to create an employment program for inefficient teapot producers. Instead, it exists to produce the best possible teapots for everybody at the lowest possible cost. And a similar thing is now happening with illustration. We have on one hand we have art. Art is capturing of conscious states. We care about art because it allows us to see certain things which we could not see otherwise. What we value about art is that there is a particular conscious being on the other side of the creative process that is talking to us through a medium. That is not the case at the moment for stable diffusion. Stable diffusion is producing something that looks very good and that is very interesting and is very decorative or informative and that makes one excellent scientific and that is very interesting and it's very decorative or informative and that makes for an excellent scientific illustration or a fascinating ornament. But it's art only to the degree that it expresses something that lets us see something that we could not see otherwise. And it happens that Greg Rutkowski is an extremely prolific and somewhat generic fantasy artist who has created stuff that people find very useful as ornaments. And now some very complicated and beautiful ornament that is created with an enormous amount of craft and talent that went into it. And now somebody has found a way to automate this. And that is, of course, very bad news for Grant Rutkowski. But it's very good for people who like this type of generic ornament. And I do not want to belittle in any way the harm and injustice that is happening to Greg if a machine is copying his output. But also think that the purpose of digital illustration is not to create employment programs for digital artists. It's to create digital illustrations, right? So if you have a machine that is able to produce digital illustrations, I don't think it's fair to impose a tax on every user of such a system that is feeding the people who did this work in the past. There is a revolution that is happening. That revolution is going to put people out of business in the same way as many typesetters got put out of business due to desktop publishing. Do we want to impose a tax on desktop publishing so the typesetters of the previous generations can be fed in eternity? No, right? This makes no sense at all. But we need to find a way of course to be fair and so on. And we need to find a way to make sure that cultural production is not diminished. But it's not. It's not like the number of fonts has gone down, the number of font designers has gone down, the quality of typography has gone down due to desktop publishing. In the same way, the quality of illustration is not going to go down. It's going to dramatically increase. Whenever we give people a new tool, it didn't mean that the art got worse. It meant that the artists got to the next level. And sometimes these were new artists. Often it had to be new artists. But there were people who were able to use the new technology in new ways to do things that could not happen before. When photography was invented, it was not just putting all the painters out of business or displaced painting completely. Painting still exists. But the craft part of painting, the purpose of producing family portraits or something, disappeared. Instead, the family portrait became affordable to absolutely anyone. And a new medium emerged. And this new medium was displacing some of painting, but the art itself remained. And a new thing emerged that was magnitudes larger than the previous thing. And in the same way, I believe that digital art and more generally, generative AI is going to create an industry that is multiple magnitudes larger than Hollywood. And it's going to enable people to do things that were unthinkable a generation ago. And it's going to enable people to do things that were unthinkable a generation ago. And it's most people do not even glimpse what this means, right? At the moment, the biggest news in my own timeline is chat GPT. chat GPT is is not a big old. And until today, it was completely ignored by the New York Times. Today are the first articles about it. And for by the New York Times. Today are the first articles about it. And for almost a week, that was the main content in my timeline on Twitter. Everybody was mind blown by this and created stuff. And my own kids did this. My son asked it how to make a platformer game and it explained to it how to structure this project of making a platform a game. And then he asked it how to write, start with writing the event loop in Python, and it would print out a solution for the event loop in Python and explain it in detail. And he spent hours with this thing and copied the output into Repl.it and wrote a game. And my daughter, who's much younger, got this thing to tell her a story about her coming home from school and meeting a horse and becoming best friends with that horse and how that played out. And then she modified the prompt to get different outcomes for the story. And once she had the story she liked best, she translated the entire story into a poem with perfect catchy rhymes. And so my kids completely loved this thing. And if Amazon had built chat GPT in time, they probably wouldn't have laid off so many of the Alexa team, because I think the Everything Explainer and Everything Creator is such an amazing, mind-blowing application, a killer application for digital interactive assistance that didn't exist before. It's amazing. So this thing, AI, is changing the world as we speak. This has been completely capturing the attention of my own timeline, whereas the attention of the New York Times is captured by the narrative of tech is taking away our ability to control the public narrative. And that is what we are terrified of. That's why we are mostly busy with fighting Elon Musk. And at the same time, the New York Times had four articles, at least, that I saw against Elon Musk. And I think that more hit pieces are going to come, because this idea that there is some tech nerd who takes away their medium of controlling public opinion and gives this to the public and some random person can talk to another random person until their opinions converge on something the New York Times did not agree on in their back rooms with their other journalist friends. This is terrifying. And it's going to have bad effects, but I also expect that if this is done right, we will see that in the future Twitter is going to become a societal sentient consciousness that is converging much faster on the truth in its mainstream narratives than the press is currently converging. Right. February 2020, the mainstream said COVID is not airborne. There is no evidence that it was airborne. evidence that it was airborne. The nerds on Twitter were talking about the Diamond Princess, this cruise ship, which landed in Japan at this time, early February, and observed how Japanese quarantine officials who tried to keep their distance from all the passengers and not touch anything got sick. So in the eyes of this public consciousness with the self-organizing, there was a convergence on what was true, whereas the media did not converge on this because they relied on institutional processes that had some bad incentives and that were slow. And similar thing happened with do masks help or not? And we saw a lot of these things, the divergence of the discourse on social media and in the mainstream media. We also saw the opposite, right? A lot of people on social media got to opinions like vaccines don't help and reinforce them in their eco bubbles. But overall, I suspect that there is a way to get this right to make this work in a way that works better, both than traditional social media and traditional media. And what we see when we zoom out over human history, we find that since the beginning of the technological revolution, things seem to be getting monotonously better. This is not always going to be the case, but probably going to be environmental disasters in our future that are difficult to mitigate and so on. But so far, all the things that we did with technology, we built new technology, where we were building machines that put people out of business and created hardship because people had to find new jobs, ended up with these jobs being better, and more satisfying and more creative and yielding better outcomes, producing more goods that people could use creating better standards of living for an ever larger number of people. So I could not see this for most of my life. When I was young, I was much more terrified and was more concerned about the negative effects of progress. And at some level I still am, but I also currently think that the story is much more optimistic in many ways. Technology is not primarily about the harm that it does. I want to say that I am as a person, I completely empathetically understand that many artists are in this situation right now. They see stable diffusion and maybe they didn't take it seriously six months ago when there was Dali Mini or so, but now they begin to realize that this is a threat. And I completely understand that they are terrified or scared or at least deeply skeptical and don't really know what to think about it. I want to say, so I'm here speaking as a private person, I'm like one of the people in Lion, like one of the founders. But actually I'm not like, Lion is not like a big corporate tech that is going to take away whatever. We are just a small non-profit. We're really small. We are all people from Discord and I'm like, in real life, I'm a teacher. And I'm just doing this, I'm not making any money with this and I'm like, in real life, I'm a teacher. So, and I'm just doing this, I'm not making any money with this, and I'm just doing this as a hobby because I personally believe that if we will have some super abilities, like generating images, generating music, like whatever, then I really do not want to live in a world where only nation states and a few big companies have access to these superpowers and can control whether I may access this website where I can eventually generate an image or do whatever. There will be much bigger powers coming through AI in the next five and ten years I think is my opinion. So I completely understand that people are feeling terrified by this. I completely understand this. But you told us that there are many, many, many very positive side effects. And to come back, what I think think what will happen with image generation. We will soon see image generation models by big techno companies and a little bit later probably by small labs that you could in principle train on completely safe data, but we have removed every artist who does not want to be in it and where we have removed every image that is not licensed and and where we have removed every image that is not licensed. And then the user can provide, during generation, after training, when he downloads it, can provide one or two or three or five sample images. And even though it has not seen this style or this particular, like, motive, it will be able to understand from these few 5, 10, 3 sample images, during inference, understand what identity and what style we have there and generate them in photorealistic quality. This will be in a few years, like pretty soon. Rather, maybe at the beginning not photorealistically, but I can already see this for artistic style. There's this GitHub repository in the paper called Aesthetic Radiance, where basically you can take a few images of a certain artist that the model had never seen before, and you get the clip image embeddings and you average them. So basically you do some simple machine learning magic, and then it can generate images and styles it had never seen before. Just during training. The user may be the teenager at home who downloaded Stable Diffusion or whatever. And this is just, really just the beginning. So even though people may be really concerned about losing their job, I think this technology is really powerful and I have this metaphor that I sometimes share with people. Let's say we have a big information tsunami. It's like an AI and misinformation and information and super power, all this together. And it is approaching our societies. And we as Lion, we are a group of researchers, like most of us, like free, part-time, like just hobbyists like me, sitting there and analyzing this, getting a little bit some shiny artifacts out of this tsunami and showing it to the people and show, hey, I found this artifact, I found this data, like for example our Lion5B dataset. It is all publicly available and like Common Crawl, the big resource where we took it from. We didn't go to all the websites and scrape them. No, we just got to something like a publicly available Internet archive, downloaded what a non-profit in Seattle scraped, like just the meta information, not the images, just the links to the images, and analyzed them. And this first 400 million image taxpayer data that we made, we did completely without funding, just by donations from a few people having a GPU at home or having 50 small Swan CPU virtual machines from the company and just throwing it around, here's a password, just do it. And this is not about us being very smart or geniuses or whatever or being rich or so. It is, we are living in a time where like a few hobbyist programmers or maybe a few master students like can do these things. And we were simply the first to do it openly, publicly, and this does not change the fact, like even if we would completely shut down, the knowledge is out there and it had been out there before we started this. It would have happened a little bit later and I want everyone, everyone to know that no matter what we are doing or what OpenAI is doing or what Google is doing, in a few years we have any way AI where you can like basically generate anything or basically if it's completely saved and I completely removed every sample every artist that doesn't want to be there explicitly, we remove it, you can simply take Greg Rutkowski images that it never had seen before, like the teenager at home, and generate perfect images in the style of Greg Rutkowski. And this is nothing that single people are responsible for, this is simply the fact, like how the different papers and all the scientific institutions converge upon these technologies, because it's eminent, it's there, and it's not restricted to a text-to-image. It will, like text-to-audio will be coming, and text-to-video is already like, make a video from Facebook and so like there are some proof of concepts that still look funny at the moment, not really scary, but you can still see, oh yeah, there are artifacts. But the pace with how this is increasing, I've been talking recently, like, chatting a little bit with Nina Schick, she's a political advisor, like, she wrote a book about the Infocapolips, the coming, like, it's about, like, deepfakes and the coming misinformation tsunami that will approach us. And she told me something that I have been thinking like five years ago when I saw the rise of the first StyleGans. And so I thought, wow, in 10 or 15 years, teenagers at home will be able to generate special effects like in Lord of the Rings. Just like by telling Alexa do this or just like clicking a little bit. The bottleneck will not be having a camera team or like special effects teams. The bottleneck will be only like the creative capacity of the teenagers sitting there and have the patience to describe the tools like the AI exactly what he wants and how the emotions should look and which actor should look where. This will, for the first time at least, be the absolute bottleneck. I think this is something eminent that is right there. And when people right now are talking about the potential misuses of text-to-image or whatever, it is good that they are talking about this right now. It will come. And it will not come for text-to-image. So my prediction, you asked me about my time length, I think within the next 5 years, plus, minus, maybe it's 3 years more or 3 years less, we will see that you can generate images, audio and video in an almost photorealistic or probably photorealistic way, quality, just at home by describing it. And when you think about how will society deal with this, like with this Twitter, like the New York Times is concerned about Elon Musk allowing free speech and no one is regulating it. I think in a world where everything will be full of really good looking fake media that your eyes cannot distinguish from truth or true camera footage or whatever. There's in my opinion, from my humble understanding of how I see it, only really one really good solution, or like there are two solutions. There's one scary solution. This is a solution of like doing mass surveillance and surveilling every computer, every GPU, and making sure that no teenager uses it in a bad way, and making sure to control everything that a teenager or a journalist or YouTube content creator is like doing and that it will be like coherence with what the society wants. This is scary. This is deeply scary. This is like far more scary than like AI generating like annoying fake media. What I think is a better solution, in my humble opinion, is educating the public and making authenticity really clear and valuable and easy to track. So I can imagine that in the future, when everything will be full, there will be something like a bookkeeping system. It could be a blockchain, it could be whatever, like something like some system where authorities from the United States, from Germany, maybe from Google, from even from Russia, whatever, like whoever wants, can certify a content creator like a journalist or a YouTuber or a blogger or an artist that in the opinion of this institution from the States or from Russia or from Germany this content creator seems to be trustworthy. And this certificate will expire after a time and this content creator will have to jump through some hoops and maybe prove to the authorities that he has an ID, that he maybe has some education, that he has not been spreading something that the current society like he lives in, like for example if you live in the states, that the state authority doesn't regard as misinformation, whatever this is. And if you do this and you jump through the hoops of this certification authority, they give you like a certificate. And if I, or like if someone else is looking YouTube and he wants to educate himself about the Ukraine war for example, by Russia, by China, and he can then click on a button and see several news from several standpoints. And he will be accustomed to do this, and it will be obvious for him that he has to choose one perspective upon this problem, because if he doesn't choose anything, he will see all kinds of like a tsunami of crap that looks really good. So like in the future people like will not believe anything they see on camera. And like already today like people are already thinking, oh, this is probably special effects, this is something like, but it will be obvious to the small, to the five-year-old who can understand this with his brain, like it will be obvious to doubt this. No one will take anything from granted without like several layers of proof of trustworthiness. I think that's correct. I suspect that in the future we will no longer need to rely on watermarks on digitally created content, because we can't. Everybody can create such content without watermarks. Instead, we will have to build networks of trust where we have a closed chain of evidence that is being marked from the moment of the creation of a digital document to the consumer or to the recipient. And that means that basically we have to have some kind of watermarking for authenticity. And this can be a personal certificate, it can be an institutional certificate, as you mentioned. But I think this is going to happen, it's almost inevitable. And the first people that I met, the first people who are trying to work on such certificate authorities and these change because they anticipate these developments. I think this is an inevitable thing. I'm not sure about the deluge of deepfakes because this has been predicted for an extremely long time and so far it just didn't happen. We have the technology to compromise society on all sorts of levels and yet people are not always doing it. And for the same reason when we discovered that there were attacks possible on the most important processor designs, that would allow us to make side-channel attacks to compromise cryptography on computers. It turned out that basically, there were now possible exploits that could bring down all the secure transactions or most of them at scale. And yet this didn't happen. And why is it that it didn't happen? Why did society not crash? And it's because society at the end of the day is still made from people. And the important defections against working society are not technological defections, they are social defections, and social problems usually have social solutions, which means there are contracts that we have, there is a legal system that we have, there are moral directions that we have and that we establish, and while we need to harden them against bad actors, the number of bad actors, the percentage of bad actors in society is finite and limited. And we can disincentivize them and we can take care of them. So there is only so much harm that is being done if technology is possible at scale, because there's only so much that a single individual can do to bring things down. And we have to worry mostly about systemic effects that we can actually prove to exist, and we will have to then deal with them at a systemic level. So I fully agree with this point, but with respect to the creation of digital content with AI from digital artists, I think it's slightly more complicated. At the moment, we have only one algorithm that works at scale. And this single algorithm that works at scale depends on having enormous amounts of training data. And so BioV in the future will probably have systems that work more similar to human minds, which means use very little data to extrapolate a lot that is coherent and consistent. The present systems approach coherence and consistency only when you give them more and more data, data. You put in 10 times more data and you get a better result. It's not like you use exactly the right data and you get the perfect result. At least so far. So this is going to change. Maybe at some point you can have systems which are going to explore the entire space of possible art creation in the same way as right now AIs are able to explore this space of Go games without human input, because they just play Go against themselves. You can also conceivably play art against yourself until you discover an extremely large space of artistic styles and when you're being confronted with an example, like I show you a photograph of a painting by a particular artist, it understands where it is in that space and it's able to reproduce the correct set of parameters to give you more in that style. Or it's even able to discern the state of the mind of the artist at this point when the artist created this. And it's able to create art that would be created by a person with similar traits, education, biography under similar circumstances. Who knows, we'll get there maybe. Or probably. Stable diffusion for example learns to generate a certain style because it had seen like 100 images or 100,000 images in this style and then it learns how like this stroke is made and how this pixel is made. It's more like that these technologies, like for example clip, or whatever, like clip embeddings. These are like small compressions of what an image is about. They do not encode how pixels are painted. They encode something like a general style, the idea of a style. Like if I could write down in a few paragraphs, if I would be like an art specialist and I would sit down and write maybe a few paragraphs about how the style is defined so that an artist could understand it from just reading the text. So this information is in these representations that these models use. And if you look, for example, at this aesthetics gradients paper, then you can see that just by providing the right clip embeddings, a model that had not been seeing a certain style can suddenly learn it. And from what I've been hearing and seeing from papers and from people on the internet community, I think even if you train it on an immense amount of data of course, so that it learns the concepts of what a style is and how to represent a style, then you couldn't go to this model and show it a few images of a style that it had never seen before, but it understands them not from looking at the pixels and understanding how the stroke is made and how the style is made, but more it understands, like, this is not like in a philosophical, like, this is not in a spiritual way, it's more like in an intellectual way it understands the essence. The intellectual essence if you would like explain as an art specialist to like another art specialist the essence about how like which techniques are applied and which emotions are conveyed to which colors and so like these features it can understand from images during inference, not during training, like with these new techniques that are like in the air, it's like emerging, it's like the time when I was saying, oh yeah, let's scrape the first image text from the internet and no one was doing it, saying, oh you're crazy and I just started it on a few small, really tiny computers. And after a few proof of concept, the community, more and more people who are programmers than me, they jumped on the train and supported it. And without any funding, we had 400 million image taxpayers. At this point, when I started it, it was emergent, it was in the air somehow that this would happen. And now I have the same feeling again, that even the perfectly filtered text sets, they will not prevent appliers, applicants of these models in the near future, may it be a year or maybe two years or maybe half a year, I can't tell. They will be able to generate it. And maybe to come back to something that you have been talking about, like with pottery, when your father had been making pottery and then, or whoever was making pottery, and now he got replaced through this industrialized pottery production. So actually I think people will always, always do art because they want to express themselves, because you can never automate it. Even if you have like a very good algorithm that knows your preferences or whatever, you're not doing art because you want to have an image that reflects your animal's thought. You want to do art because the process of painting this picture and expressing it, like feels so liberating to you and gain some insight in between, like while you're doing it. So by the way, before I studied computer science and physics, I studied acting for five years. Yes, my father would be bored doing things that he can do automatically or that can be done automatically. So over the course of his life, he switched a lot of the things that he was doing, not so much because of industrialization of production processes, but mostly because he was interested in doing the new things. So at some point he focused less on ceramics and more on sculpture and at some point less on sculpture and more on computer graphics. And for him this is an exploration that is happening and the new media that become available for him were always a new source of inspiration. And there will be people for whom this will be less the case, of course. And this does create also individual problems and injustices that are real, just that we need to put them into comparison or in relation to all the goods that will be prevented if in order to prevent the harms, we take the technology away. Basically, if you have a red team in our company, that is looking at the ethical effects and possible harms that are done to society by releasing technology, we also need to have a green team that looks at the harm that is being done by taking a potential benefit away from society by not realizing technology. And that is something that we really need to put into balance at the moment, this green teaming is mostly only done with respect to the benefits of the company itself. It's very little happening in terms of our company should be doing this regardless of the bottom line, because it would prevent a benefit. It's mostly that we think our company should not be doing this regardless of the bottom line, because it does a harm. So ethics is very often seen at the moment only as preventative. So if somebody is working in a responsible AI department as a large business consultancy, they're going to get paid by the government to tell certain people in the government what they want to hear. You're not going to get this government contract properly if you tell the government, based on complicated simulations of complex social processes, what society is going to look like. These are decisions that are being made for the benefit of the current stakeholders. And the current stakeholders that exist in the space that generative AI wants to move in are extremely large media industries. And these large media industries find that they have more to lose by outsiders taking over their consumers than they have to win by adopting these new technologies. And this is, I think, something that we have to be aware of. And it's something that is somewhat orthogonal to what's good or bad for society. And this question of what's good or bad for society and cultural production and economic production, social inequity and so on, is very, very important. And we have to ask these questions. We just have to make sure that a lot of the people who ask these questions actually serve different stakeholders than us. And so I'm saying us because in some sense I feel that we are in the same industry. We are trying to advance artificial intelligence for social good. And this social good also entails making things that make people more creative and allow them to do things that they couldn't do before. And that is not just in an artistic way or entertainment way. It's also generally in the production of thoughts, in the interaction of thoughts between people, in the creation of complicated models of reality and all levels. And artistic generative AI is, as you pointed out, just the beginning. There is going to be a very big development in the future. Right now, JED-GPT is not completely coherent yet, right? It is not able to prove what's true and what's false. And that's a big problem. It's still not there yet, but it's able to do amazing things in this regard. It's very close to an everything explainer. And in many domains, this everything explanation works, especially if you are an expert in this domain and you need support by an assistant right in the same way, the thing is not good as an artist. Stable diffusion is not an artist yet, but if you are an art director and you use it as a craftsman, then this thing can be super useful, right? It's able to automate a lot of the crafting process of an art project, if you have a very clear direction of what you want to do. And you also see this in the Nessun genre of AI movies, which is just starting, right? The technology isn't barely there yet, but there are some people which don't adjust technologies and play around with it to make stuff that looks good. But there are people which have an artistic vision that they feel they can realize for the first time with these new tools. And so it's happening in the same way as it did with CGI in the beginning. When CGI became available, a lot of the stuff that emerged was just digital crap that looked good. available, a lot of the stuff that emerged was just digital crap that looked good. And the development of digital art that was using CGI to make something that's novel and was really good took time. And it happened. Now we have CGI-reliant art that is an entirely new genre. And that's also going to happen for generative AI. After the Everything Explainer, we will have Everything Entertainers. We will have AIs that we ask compose this music that is going to have the following mood, the following purpose. I just want to dance right now. I want to have really good techno. No, make it hotter, make it warmer, make it more juicy, make it like a parfait. And it's going to do it while you yell it out. And then you're going to put up 20 projectors in your apartment with cameras and the cameras are going to get in tune with the projectors and account for the color of your furniture. And you are asking, I want to be in the middle of a Zelda game right now. And it should be in motion. And, oh, no, I also want to have an ocean there over there, that wall. And oh, no, it should snow now. And I want to be on a mountaintop. It is going to track where you are in the room. And it's going to look where your eyes are in the room and it's adapting all the generated images from all the projectors in real time to make that happen and you're going to be in the space that you want to be. The invasion of creative potential that is going to unlock the way of interacting between us and our immediate environment that is going to unlock. And this is just the beginning. Not right now, but maybe in five years or so, we will see general purpose robots that you can show simple tasks, like sort the bananas in this bin and the oranges in this bin, and then go over and clean up this kitchen or so. And these things will begin to work. They already begin to work in laboratory environments. They are a little bit brittle right now, and it's not really there, but it's eminent, it's in the air and this will like, like the discourse in the society that will emerge when people see these things will be much greater than what like right now is going on with the divide between the AI art community who is happy and the artist community who feels deeply scared. So I would have to ask you, like as a last question, if you were to talk to decision makers, like politicians, like the US president and maybe like the EU parliament, about the potential benefits and the dangers of these AI trends? What would you tell them? I would want to point out that we need to try to quantify the cultural and economic effects of the present technologies and how to build independent institutions that are not going to benefit from an outcome this way or the other. So the institutions that are somewhat independent of the stakeholders and how to measure the performance of these institutions with respect to such a mission and make sure that they are self-improving. I think that what we probably need is a larger legal framework. In my view, the most important law that we could pass would be a regulation that requires us to add a clause into every law that says what its intended effects are. And if these effects do not materialize, to automatically repeal the law. So every change to the law that we are making, every new law that we pass, every existing law that we amend, every existing law that we repeal, every new law that we pass, every existing law that we amend, every existing law that we repeal, is a new law. And every of these changes should instantly automatically be revoked if the intended effects do not materialize and the intended effects have to be formulated at the time of passing the law in such a way that they can be tested and quantified. So basically, if we implement a law that is regulating the internet, or if we implementing a law that is regulating social media, or a law that is regulating financial markets, or homelessness or anything else, we say this is the intended outcome of the law. If this intended outcome, it doesn't materialize like this in one year from now, and this outcome is not there in two years from now, or this outcome is not there in five years from now or this outcome is not there in five years from now, then in one year from now, two years from now, five years from now, the law is going to be automatically repealed. And if we had such a prevention, a position into our laws, we would have error correcting governance. Right? You can be as bad as you want, as long as you see what your error was and you always commit to correct it. And you make sure that this happens and you always commit to correct it. And you make sure that this happens and you are accountable to this. And so to me, this accountability is the most important thing. It's much more important than the individual technological development. So to me, it's, we clearly see that people which are sent to benefit from the new developments and the same thing with crypto. So for instance, cryptocurrency, in my view, have no possible beneficial use. It's a big pirate ship. There are many nice villages and churches even being built on the pirate ship. There are good communities with good people that produce good things with the money they made on crypto. But I think that the net benefit of crypto society is negative. There is no future in which cryptocurrencies and a healthy financial market can coexist. So in the long run, I think that crypto is going to get regulated away. And but we see that on both sides of the curtain, there are stakeholders that have less than pure incentives. There are people who benefit from crypto and crypto scams. And there are people who benefit from removing competition to the existing banks, which are inefficient and parasitic. people who benefit from removing competition to the existing banks, which are inefficient and parasitic. And they are duking it out among themselves. And it's on the average, we eventually still get good results, but it's not quite the direction that we want. And so to me, error correcting governance is more important than the individual effect of a technology. Yeah. So thank you very much. This was very inspiring and I've learned a lot. I would love to continue this later because there are so much more topics I could talk to you about. But for now, thank you, Joscha. you", '34.09493446350098')