('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/K78hH4IfXF8"> </iframe> </center>', ' Good morning, everyone. I would like to make a good day for you, wherever you are in the world, or good evening. I would like to welcome you to today\'s panel about representations for the next generation of artificial intelligence systems. This is, we are planning to make this the first panel in a series on vectors for cognitive artificial intelligence. This session today is sponsored by the Cognitive Systems Research Group at Intel Labs. And we are very happy to have been able to bring a few very interesting and influential, I think, and to me, very inspiring speakers together that are going to discuss a number of questions. And these speakers originate in psychology and philosophy, neuroscience, artificial intelligence, cognitive science, robotics. What brought us together is the insight that the representations that we have in current AI systems fall short of what we think has to be done. Human representations are different from the representations that we use in neural networks. Our representations are unified. That is, they have a model of the entire universe that you can relate everything to. They are universal. You can represent everything that we want to in the same paradigm. They are coherent, that is the world that we are representing makes sense together, the parts are not necessarily in contradiction to each other, and we optimize for this coherence. And they are real time, that is, they are connected to an environment in which you are situated and entangled with it, and always give us a result and they\'re online, we constantly update them and learn from them instead situated and entangled with it and always give us a result. And they\'re online. We constantly update them and learn from them instead of processing in batches. And so there are a number of things that our minds can do that current artificial intelligence systems cannot do or cannot do well. And we want to discuss some of the conditions for such systems. And every speaker is going to give a short position statement of about 10 to 15 minutes. If we run into timing issues, Tanya will give a little pointer or reminder to the speaker. And so we have some time left at the end to exchange ideas or to build on each other or to criticize each other and see where this discussion leads. Our first speaker today is Mark Bickert. Mark Bickert is the Henry R. Lewis Professor in Computer Robotics and Philosophy of Knowledge at the Yale University. And he is affiliated with the departments of philosophy and psychology. And he has work in metaphysics and the emergence of consciousness, cognition, language, functional models of brain processes, including persons and social ontologies. And he\'s going to talk about cognition and truth value. Mark, could you please share your screen? Okay, is that working? It is working. Okay, if you have the slides, then we put them on. Otherwise we just see you. I do have a few slides. Yes. And we don\'t see them yet. It\'s not working yet. You have to press on the share screen button below your screen. That\'s weird because I did have share screen. Okay, there we go. Not yet. yes, no. How about now? Yes, it\'s working. Okay. Okay. My concern, my interest here is with a very small little bit of what has turned out to be a kind of a systematic model and theory. The small little bit has to do with the nature of representing. And the issue here that I\'m trying to address is the nature of representing in minds or for minds, not representing for minds like from the outside world, like with pictures and symbols and so on. minds like from the outside world, like with pictures and symbols and so on and so on. So there is a little abstract. Classically representing has always been approached in terms of some sort of postulated correspondence. We go back to the Greeks, we have the signet ring in the wax, which leaves a correspondent impression in the wax. One of the Greek slogans was like represents like. So the signet ring actually exemplifies that because it\'s the structure in the ring that is like the structure in the wax. Skipping ahead a few years, Locke overturned this way of thinking about representing and substituted things like cause. So representing it as correspondence is no longer a structural correspondence like with the signet ring, but a causal correspondence. This actually is weaker than the signet ring model, but nevertheless, it\'s still a correspondence. Today we talk about information as correlation and having informational correspondences with things. We talk about lawful correspondences. There are many different proposals over the last millennia for what sorts of correspondences are the special ones, the ones that actually give us representing. There have been many problems with these proposals, some of which go all the way back to the Greeks. One that I take to be very central, simply because it seems to apply to just about every proposal in the literature, is how to account for the possibility of error. If you\'ve got a factual correspondence, whether it be causal, lawful, informational, no matter what it is, factual correspondences, factual relations only exist if both ends of the relationship exist. But representational quote, correspondences unquote, can be with things that don\'t exist at all or are false. And the Greeks actually knew this. They talked about thinking about something as being akin to pointing to it. And how do you point to something that isn\'t there? Or how do you point to something that\'s\'t there, or how do you point to something that\'s false? We have technologically more advanced ways of talking about this, but we still haven\'t solved the problem. It became a minor industry in the 80s and into the 90s to try to account for error. None of them really worked, but even if they had worked, they were all from the perspective of an outside observer of the animal, of the organism, of the agent. The real issue shows up with organism detectable error. And that has rarely been addressed at all. So the problem is that of error. The problem is that of error and truth, of truth and falsity. Truth value has always been the fundamental problem for correspondence models. Notice this is a normativity issue. Truth and falsity, functional, dysfunctional, rational, irrational, good, bad, these are all normative kinds of phenomena. And here we have right at the center of the nature of representing a normative issue, how to represent non-existence and falsehoods. Incidentally, it was Brentano who pointed out rather strongly a little over a century ago that the factual relations require that both relata exist, but intentional relations do not. One of the reasons that I take organism detectable error as so important is that it\'s necessary for error-guided behavior and for learning. If we can\'t detect error, we can\'t guide our behavior in terms of those detections of error. If we can\'t detect error, we can\'t try to modify things. We can\'t try to learn in response to that error. So even if we came up with some consensual model of observer dependent attributions of error, it wouldn\'t suffice, because the organism itself has to be able to detect its own error. Just to give you a feel for the seriousness of this problem. The radical skeptical argument that goes all the way back to the Greeks, that most people just ignore has the following general form. We can\'t check the other end of our representational correspondences to see if they\'re true or false, because to do so would require that we step outside of ourselves, have independent access to what\'s out there in the world, know what our minds think we are representing, be able to compare them and determine whether the mental representing fits whatever is actually out there in the world. We can\'t step outside of ourselves and therefore we cannot check. There\'s gotta be something wrong with this, because if its conclusion were true, as I point out, we couldn\'t have error-guided behavior and we couldn\'t have learning. So something\'s got to be wrong there, but it\'s an argument and a stance that has been around for a very long time and has not been defeated. One angle on or one perspective on the proposal that I make is to give up correspondence, at least initially, and ask what other than correspondence could have truth value. I\'m only going to be able to give some kind of intuitions here. The basic intuition is that future-oriented anticipation of potential interaction can be true or false. So if I, as a frog, anticipate that I could flick my tongue a certain way and eat, maybe that\'s true. Maybe there\'s a fly over there, and I could grab it and eat it. But maybe it\'s just a speck, or maybe it\'s a pebble, or who knows what it might be. And that anticipation, if I tried it, would turn out to be false. Notice that at least in principle, and there\'s lots of details that need to be filled out here, this is a truth or falsity that could be functionally detectable. If I flick my tongue and it doesn\'t work, there\'s nothing to eat, then it\'s falsified. The indication of potential interaction is falsified. This requires some sort of a normative functional notion of anticipating interaction possibilities. In order to fill this out, I would have to provide a model of normative function. I don\'t have time to do that. I\'m simply pointing to that direction. So the basic intuition of what I\'m proposing is that there\'s some kind of setup or microgenesis or preparation in the brain to engage in some kind of an interaction. And that preparation, if you actually engage in it, might turn out to be wrong. It might be a setup for the wrong sorts of interaction. If, on the other hand, if it works, then the anticipation is correct. And in reverse, if it doesn\'t work, the anticipation is not correct. There\'s lots of other issues. What about content? I\'ve talked about truth and falsity, but what about content of representing? I\'ve talked about normative functionality. Normative functionality either exists in its own metaphysical realm, which is sort of a problem, or it\'s emergent. But does emergence make any metaphysical sense at all? Is it possible? I argue it is, but it requires a subsidiary model to account for that. I\'ve used this word microgenesis for brain processes. How does that work? What\'s involved in it? I have some thoughts about that. Learning. What about learning? Notice, if we have a microgenetic setup process, and it succeeds, and success in that anticipation is followed by some sort of consolidation of the microgenic process, or if it fails, there\'s a destabilization of that microgenetic process, then we have a local variation and selection learning process. The overall system will tend to stabilize on microgenetic setup anticipations that are correct. And then, of course, there\'s many other mental phenomena that would need to get addressed within this kind of a framework. And I don\'t know how much time I\'ve taken, but that\'s the slice that I wanted to address. Thank you very much, Mark. That was a very interesting start in the day. And also, thank you for being so succinct. And I hope we can revisit some of the questions that you started to ask. Our next speaker is going to be a personal hero of mine. Steven Grossberg is a professor emeritus at Boston University and has a long list of things that he has achieved in his life. I\'m not going to go into all of them in detail, but I would like to focus especially on the fact that he is responsible for the adaptive resonance theory, which is one of the first big cognitive architectures that has been spanning the entire width of phenomena that the brain is producing. And it\'s a theory of how neural activity is able to produce representation and cognition and the facilitation of an organism that is connected with its environment and embedded in it. And I would like to give the floor to Steven Gossberg. Thanks. Can you hear me? We can hear you, Val. I hope you can. So I\'ve taken very literally what we were asked to do. We had a series of questions. And I\'m going to read the questions and then discuss them one after the other in less than 15 minutes. So the first question is, how is information represented in the brain and in other systems that can make sense of the world. Well, essentially all brain processes that are devoted to perception, learning, cognition, emotion, and action are represented by distributed patterns of excitation and inhibition across an entire network of neurons. Individual neurons rarely encode predictive information about the world, much as the individual pixels in a painting are meaningless. The patterns of activation across pixels encode context-sensitive information about the meaning of the painting. More generally, our brains are self-organizing pattern processing devices. These patterns are moreover organized according to at least two different computational paradigms that are needed to explain how brains make minds. And I call these paradigms complementary computing and laminate computing. Complementary computing explains the nature of brain specialization. Many scientists have proposed that our brains process independent modules, as in a digital computer. The brain\'s organization into distinct anatomical areas and processing streams indeed shows that brain processing is specialized. However, independent modules should fully compute their particular processes on their own, and much behavioral data shows this is simply not true. For example, during vision, interactions occur between brightness and depth, motion and color, motion and depth, texture and depth, texture and motion, and on and off. Complementary computing asserts, in contrast, that pairs of parallel cortical processing streams compute complementary properties in the brain. Each stream has complementary computational strengths and weaknesses, and several processing stages of interactions within and between cortical streams are needed to overcome these complementary deficiencies. These processing stages thus realize a hierarchical resolution of uncertainty. Complementary computing shows how properties that control observable behaviors arise from interactions between pairs of complementary cortical processing streams with multiple processing stages. Lamina computing explains how and why all higher forms of biological intelligence, including vision, speech, language, and cognition, are generated by specializations of a single canonical cortical circuit. My new book, Conscious Mind, Resonant Brain, How Each Brain Makes the mind, provides unified and principled explanations of how this happens. Well, the second question is what is the relationship between perception and reasoning, analytic thought, symbolic cognition, and how are these different modes implemented? Well, perception is general purpose because all perceptual experiences, whether familiar or unfamiliar, can be registered by our sense organs and process. However, recognition and more generally cognition requires that objects and events in the world, whether familiar or unfamiliar, can be, I\'m sorry, objects and events in the world are first learned and thus familiar. Learning occurs in all stages of brain processing, both perceptual and cognitive, where the earliest stages learn simpler things like the orientational tuning of simple cells in the visual cortex, and the higher stages learn more complicated things like invariant recognition categories in the anterior infertile cortex and cognitive plans in prefrontal cortex. Recognition categories function much like symbols. A single category can selectively respond to a large number of similar distributed patterns of activation across feature detectors. of activation across feature detectors. In order to use symbols and thoughts, cognition or reasoning, sequences of them needed to be stored temporarily in a working memory. And my book explains why all working memories in the brain, whether linguistic, spatial or motor, obey the same kind of brain circuit. It\'s a circuit that allows the sequences that are stored in working memory to be learned and stably remembered as cognitive plans that can be used to trigger context-appropriate actions. All of these processing stages have both bottom-up and top-down excitatory pathways, ensuring that learning doesn\'t experience catastrophic forgetting. They solve what I call a stability-plasticity dilemma. In particular, adaptive resonance theory, or ART, that was just mentioned, explains how multiple processing stages in our brain use these interactions to learn to attend, recognize, and predict objects and events in a changing world. All the foundational hypotheses of art have been supported by subsequent experiments and art has, to my great delight, provided principles and unifying explanations of hundreds of additional experiments. These explanatory and predictive successes may be because I was able to derive it already back in 1980 from a thought experiment about how any learning system can autonomously correct predictive errors in a changing world that Mark mentioned earlier. And the hypotheses that go into the thought experiment are facts that are familiar to us all from daily life are facts that are familiar to us all from daily life because they represent ubiquitous environmental constraints on the evolution of our brains. We\'re over nowhere in the thought experiment of the words mind or brain mentioned. So art is thus the universal solution to this learning problem, which I call the stability plus dissony dilemma, because it is how any system can learn quickly without experiences catastrophic forgetting. It\'s therefore, I think, remarkable that art dynamics also support conscious states of seeing, hearing, feeling, and knowing. These conscious states arise from resonances which occur when excitatory feedback signals between two or more brain regions match their signals well enough to cause the active cells to synchronize and sustain their firing long enough to trigger consciousness and learning. That\'s why I call the theory adaptive resonance theory. Because of the universality of the thought experiments, they provide a blueprint for designing autonomous adaptive intelligent algorithms and mobile robots for engineering technology and AI. Next question, are there efficient general representation paradigms or do different tasks require different approaches? Well, each modality of intelligence such as vision, audition, cognition, emotion, and action, and models are built up from a small series of shared equations and just a somewhat larger set of microcircuits. Usually just three types of equations suffice for short-term memory or cell activation, medium-term memory or activity dependent habituation, and long-term memory or learning and memory. Specializations of these equations and microcircuits are then assembled in modal architectures that carry out different modalities of intelligence. Again, look at my book if you want an extensive and self-contained discussion of this. Well, what is current AI research and deep learning missing in the treatment of representations? of representations. Well, thank you for that flash there. Deep learning, as you know, is just a feed forward adaptive filter. And it\'s untrustworthy because it\'s not explainable. And it\'s unreliable because it experiences catastrophic forgetting. You can\'t really use it in a life or death situation because if it fails, you\'ll be sued for everything you\'re worth. Or it is not just the feed forward adaptive filter, it\'s a principle theory of biological intelligence that\'s a self-organizing, explainable production system that carries out hypothesis testing. So it\'s all about self-organized error correction, fair self-stabilized learning, classification and prediction, and it loves to live in a changing world. Next question, what\'s the status of qualia in a general representational paradigm? To discuss qualia, you need to understand how, where in our brains and why, from an evolutionary perspective, nature was driven to discover conscious states whereby we can consciously see, hear, feel, and know things about the world. I believe that in brief the answer is that multiple processing stages are needed to convert incomplete and noisy sensory data to convert incomplete and noisy sensory data into sufficiently complete representations where our brains can effectively plan and act to realize valued goals. But if you believe that, then you ask, well, how do our brains know which of these processing stages compute sufficiently complete representations to control successful action. And my claim is that consciousness lights up such a processing stage with a resonant state. Again, my new book goes into that in detail, but let me just say a few words about why evolution may have been driven to discover visual consciousness. If you look at the retina where light\'s registered before light signals are bundled into the optic nerve and sent to our brains, you may know there\'s a big blind spot where the optic nerve forms, and no light\'s registered there. And it\'s as big as the fovea, which is the high acuity part of our retina. And we move our eyes with saccadic eye movements several times every second to point the fovea at regions of interest. If no further processing occurred, to point the fovea at regions of interest. If no further processing occurred, you couldn\'t look at or reach for things that hit the blind spot. Well, but we\'re not aware of that because we complete the representation over the blind spot so that we can look for and reach objects in these positions. And I would tell you a little more about that, but I want to stay within my 15 minutes. But as to how the brain knows at what stage of all this pre-processing representation is complete enough, the claim is a resonance with the completed surface representation and the next processing stage lights it up and renders it consciously visible. And we use that to look and reach. I call that a surface shroud resonance. I predict that it occurs between visual cortical area of the fore and posterior parietal cortex. By now, Art has classified six different kinds of resonances to support conscious states of seeing, hearing, feeling, and knowing. My book explains why although all conscious states are resonant states, it\'s not true that all resonant states are conscious states. The book also clarifies why lots of brain processing is never resonant. In any case, one example of complementary computing is that our processes for perception and cognition are complementary to spatial and motor processes, but I\'ll stop there. And if there are questions later, I\'ll be happy to try to answer them. Thanks for your attention. Thank you so much, Steve. Our next speaker is Yuya Sendimirskaya. Yuyaendemirskaya. Yulia Zendemirskaya is, since I joined Intel this year, a colleague now. Yulia is the leader of the applications research team of the neuromorphic computing lab at Intel. And in the past, she has led the neuromorphic cognitive robots group at the Institute of Neuroinformatics at the University of Zurich and the EDH and the Autonomous Learning Group at the Institute for Neural Computation at the Universit√§t Bochum. And she\'s going to talk about memory, intentionality and autonomy enabled by neural attractor dynamics. Thanks Jascha for the introduction. And thanks also for the invitation to this panel. I\'m really humbled, I must say, to talk after both Mark and Steve. Let me see, let me first check whether you see my slides. Are they there? Yes, they\'re not full screen. Yes. Perfect. Okay, great. Yes, I\'m humbled to talk after Mark and Steve because most of my research work over the last 15 years has got a lot of inspiration in their work. And I can guarantee you that in this presentation, this 10 minutes, there won\'t be any ideas that you won\'t find in either Steve\'s or Mark\'s work. However, what I tried to add is the sincere desire and work to show that these ideas can be operational or to make them operational and to actually build systems based on these ideas that can produce behavior in the real world and can show that these ideas are both needed and to show what they can achieve in behaving systems. So my personal interest is in what I would call embodied AI. And the citation that I want to bring up here is from Rodney Brooks, who had this beautiful article, "\'Elephants don\'t play chess." And indeed they don\'t. And moreover, most humans also don\'t play chess, or at least some humans don\'t play chess, and indeed they don\'t. And moreover, most humans also don\'t play chess, or at least some humans don\'t. And those who do, playing chess and doing some high-level cognitive stuff is not the only thing that they do during their lives and days and weeks. Most of the time, what both animals and humans do are other things. And most of it is about moving around, sometimes in very fascinating and ingenious ways. It also about doing such mundane things as washing dishes or cleaning up your child\'s room. And most of the cognitive or intelligent processes, they really unfold in these very practical, physical interaction with the physical world that amounts to planning, generating and controlling movement around the world, in the world in an environment. And if we understand in very deep, but also very practical way how that is done by biological neuronal systems, that could help us to actually make smart robots a reality. Humanity has been dreaming of those machines that will do things that humans do for millennia, I guess, as well, at least for hundreds of years. And we\'re still lacking machines that could do what humans or animals can do in real world environments, real and structured environments, where they are bombarded by unexpected events, where they cannot assume that they know everything about the immediate environment. So the big question, how can we get to know such machines that can interact with real world can behave in real world that can learn in real world in interacting with this world. So for this panel discussion, I have three key statements that I will know bring forward and then kind of substantiate them a little bit with a couple of examples and then hope we can have some discussion at the end. So the first statement is that representation requires memory, kind of maybe an obvious thing. And the next thought on top of that is that memory requires recurrence in whatever representation we build, especially if this memory needs to be sustained for any meaningful amount of time, and by meaningful I mean anything beyond a couple of milliseconds, the temporal timescale of neuronal systems, under noise that is present either in your sensory system or in your computing system itself and the structures. And it is true that today we have computers that have very little noise in their computing substrate that are very reliable, but the question might arise at what cost we achieve this absence of noise. And the cost is pretty high, like literal energy cost to keep our computers very, very reliable, their elementary elements. And now I have zillions of those elementary computing elements, those zeros and ones in the computer. And at some point the question arises, how much do we pay for having that reliable computing substrate? If we don\'t have a reliable computing elements, those zeros and ones in the computer. And at some point the question arises, how much do we pay for having that reliable computing substrate? If we don\'t have a reliable computing substrate, then in order to keep memory around, we need recurrence. And then, of course, this recurrent representation needs to happen in some suitable substrate and the substrate needs to be suitable for the content of what you want to represent. Are these no sequences of sounds? Are there some spatial patterns, temporal patterns, some water patterns? And depending on the nature of this representation, the nature of the substrate might differ. And you might also need to represent some additional things like for instance, value or utility of different no states or representations, and maybe also your certainty in these or that representation. And in this aspect, I will kind of disclaim that I don\'t have a very good answer yet what is the best representation for all the different things that we want to represent in our system. The second statement is that if we want our representations to be about some entities in the environment, And this is something that Mark was talking about. So if we want to make sure that whatever we represent is actually related to some object out there, to an apple that you\'re looking at, or that the representation of the motor plan that you want to execute actually results in some physical movement, then you need some additional structural components. You need some additional structure to your elementary computing unit, I will call it intentional structure. And I will show you that this has some very concrete consequences for the neuronal networks that we should build if you want our systems to have these type of representations that can be relatable to entities out there in the world. So these representations need some some notion and some representation of intention. So what we want to represent, or what we perceive or what we want to execute as a model action, they need the notion of condition of satisfaction that will monitor whether that representation has been achieved in the outer world. They might also require a condition of failure, some preconditions, moment to moment expectations, what will happen next. And this is something that Mark has talked about, that you need to anticipate something from your interaction in the world in order to be able to derive some normative signals that will help you to know when you make an error and know when you have to learn or adjust your actions. So this intentional structure is required for to build the systems that are aware of the environment, that can build representations So this intentional structure is required for, to build the systems that are aware of the environment, that can build representations about the environment, that can use those representations to act in the environment. And finally, if we have such units that have memory and they can form memory and they have this intentional structure, then we can start building networks that will be structured. They can be structured just by us, so maybe by some evolutionary or high level learning process or they can be structured by experience of our agents with the environment. And in this experience, temporal or spatial contingencies and patterns can be reflected in our neuronal system, can be memorized there. And importantly, they can be learned autonomously. So all the decisions about when to learn, where to learn, so which part of the neuronal system should be updated, and how shall this part of the neuronal system be updated, can be taken by the system itself autonomously, if it has this intentional structure in place. All right, so now a couple of concrete examples. So the first, memory. So why do we need memory? A very practical kind of view on that. So if you want to build some robotic agent that can behave in the real world environment, what is a robotic agent? It usually would have a number of sensors and it can be different sensors. It can be a camera that delivers images 30 times per second. It can be some neuronally inspired event-based camera that delivers events million times per second. It can be an audio sensor, tactile sensor, IMU. All the sensory signals, they bombard our neuronal system at different timescales with different delays and different temporal structure, also with different spatial structure. Now our cognitive neuronal system here in the middle needs to somehow build some representation of its environment that is sustained long enough for the system to plan some movements or movement sequences and execute them. So to link to the motor system and to different kinds of movements. Some may be very quick, like the blink of an eye or saccadic eye movements. Some might take quite some time if I need to move my arm or to move around in the environment to find a certain object. So in a way, the task of the neuronal system is similar to the task that one would have to do in order to represent an elephant that is, you know, sensed by a number of blind people who have access to different aspects of this elephant. And they also sample this input with different temporal scale. And our neural system needs to create that representation. So in order to do that, we need to create those memory states that can sustain themselves over some periods of time that go beyond just intrinsic neuronal dynamics. And one way to do that or to formalize those dynamics is in terms of attractive dynamics. So when we design our dynamics in such a way that it has those stable states that are stabilized by interactions that we create in our system. So that if there\'s some noise or distractors to try to bring our system outside of that attractor state, the forces in the system will bring it back to the attractor state. And that will help us sustain that memory state as long as needed for the agent to engage in some behavior. Now, in neuronal terms, what does it mean to create such attractor or how can we create that? And this is something that Steve has mentioned and has basically developed or suggested this type of an architecture is when we define a population of neurons and we interconnect them among themselves recurrently in such a way that they create stable activity patterns. And the simplest such activity pattern could be just a localized attractor bump. And you can create a network that will create these bumps just by a certain pattern of local excitation and global inhibition. So if we have such attractor networks that create stable states that can sustain their, their value, their activation over those periods of time, then we can start building architectures, neuronal architectures that could solve different tasks. And in our work, we have shown how you can build such architectures with these properties. What do these Snow Attractor networks enable? So first they enable memory to maintain that those representations long enough to do action planning, also to do continual learning with credit assignment or context away decision making. When you make your decision, not just based on your immediate context that you see a fly in the corner, but also sometime in the past. It also performs attentional filtering, so when you create a stable attractor state, then the distractors will be inhibited away so that you don\'t switch to them just because at some point this distractor maybe had stronger saliency than your current state. It also creates stable states that are required to control motion, to control movement, and also to control activation flow in your cognitive system. And it also allows you to have building blocks out of which you can build a neuronal architecture that will be transparent, where you will know and have understanding of what each module is is working, addressing the issue that Steve has mentioned, that we want to avoid architectures where we don\'t know what the architecture is doing. We have obtained the architecture that does the correct behavior, but we don\'t know why, how, and when it will break. All right, the second element was intentional structure. And there, we have suggested this very concrete module that allowed us to build architectures that we can instantiate in robotic agents and then make them organize their behavior in time. And this intentional structure, we call this unit an elementary behavior, has basically two neuronal nodes. One represent an intention, and this can be either perceptual intention. So if I\'m looking for object with particular properties, it might bias my perceptual stream to pay more attention to objects with these particular features. Or it can be a motor intention, where I generate some motor goal, and again, it will bias both my perceptual and motor system so that they engage in a certain action. This neuronal node will have some self-recurrent connection so that it creates a stable state. If I generate such an intention, I will stabilize it and will just stick at it as long as needed for me to achieve whatever goal of that intention was. And this achievement of the goal is monitored by another node that is connected to the sensory surface and it\'s called condition of satisfaction node. And it detects the state when I\'m done with whatever my intention was. If my intention was to find object of certain properties then my condition satisfaction will be active when I have found it. I have created a representation in my perceptual system that corresponds to that object. If that was some more intention that, again, some perceptual input that tells me I\'m done with that action. So we have postulated this little module as an elementary behavior, and we have shown like early on in work of Matisse-Richter in Bochum, that you can now build architectures that you can connect to sensory surface and to motor system of a robotic agent. And you can put this agent in an environment and can now let it execute different tasks, meaning that result in a different sequence of actions that will be automatically controlled to have some kind of executive control monitored and corrected by the system. And at any time when the system controls the robot it will stay connected to its environment and to its input. So if something changes in the environment the system will recompute the plan and continue staying engaged with its task. Now, so that was the system that was able to generate behaviors. Now, what if we want to be able to perceive behaviors? What if we want to be able to parse action sequences? We want the same system to observe someone being engaged in a sequence of actions and be able to parse and understand which actions are being performed. We can use the same notion of elementary behavior for that. We\'ve discovered that we need to augment it with another node or concept, condition of failure. So that if I build a hypothesis that I\'m currently perceiving a reaching action, but then this hypothesis is not confirmed later, I need the concept of failure. So I have to discard this hypothesis because in this framework, I I have to discard this hypothesis because in this framework, I have to actively discard the hypothesis. I have to actively inhibit the intention that was activated by me building the hypothesis that I\'m probably perceiving this reaching action now. The next element that we might want to do with the system, if we have the system of elementary behaviors, this network of elementary behaviors, we might want to be with the system, if we have this system of elementary behaviours, this network of elementary behaviours, we might want to be able to plan sequences of actions that unfold to a certain goal. So if I have a goal state, I might need to be able to plan a sequence of actions that would lead me from my current state to that goal state. And again, we can do it with the same elementary behaviour, we needed to augment it with another node, the motivation node, that keeps track of possible transitions or transition that are required to bring me to the goal. When I start my planning from the goal state, I have to go over the behaviors, the mental behaviors that will lead me to the goal without activating them. And in order to have the representation of the behavior that won\'t activate the motor action, I need another node, motivation node. And again, we have shown in a couple of examples that this works. And finally, when I have the system of elementary behaviors, this network, I should be able to also learn sequences of the sections in different ways. I could probably learn it from imitation learning, when I just observe the sequence and I just form a memory of the sequence with a mechanism that Steve has suggested, for instance, for forming serially ordered memories, sequence memories. Or I could use something like reinforcement learning to learn those sequences. So just trying out different sequences, observing rewards once in a while, and then kind of propagating this reward to neighboring actions in time by updating the value of different transitions, state action transitions, so that they can learn sequences of actions. And again, we have shown you can actually do it on a behaving agent, autonomously behaving agent. And finally, when we have these modules, we can indeed build all kinds of architectures. And here I won\'t go over the whole list, but we have tried different types of architectures for different tasks. This is one recent example from our work on object learning. So here we have a scenario where we have a robotic agent, can be simulated. We also did it with a real robot, iCub robot. It observes an environment and we wanted to learn a couple of objects that it needs to interact with the user. It doesn\'t need to distinguish any car from any cat. So it doesn\'t help if we train it with an image app and all possible images in this world. We wanted to distinguish these three particular objects that we will experience with this user. And in order to do that, so we can design an architecture that would do that. I will only highlight one part of this architecture that we call neuronal state machine, because in order to learn these objects autonomously, we need this system of, again, elementary behaviors that will detect different states in which the system currently finds itself, which is being a state that sees an object but doesn\'t recognize it. So it\'s a state when, no, I need to learn this object. I need maybe to ask the user for a state when I need to learn this object. I need maybe to ask the user for a label and then try to learn this object. If I hear a label, I can be in a state when I have heard this label before, or this is a new label. And these two different states will trigger different types of learning. So either I update representation of an already known object, or I create a representation for a new object that they haven\'t heard before. objects, or I create a representation for a new object that they haven\'t heard before. Now this is not the end of the story. If we really want our robotic agent to learn these objects autonomously in interaction with the user, we need more structures around. For instance, if we want it to learn efficiently, we might want it to learn one object at a time. And then we need some kind of attention dynamics that will direct the gaze of the robot and its attention, avert or covert attention to individual objects. So we need some attentional dynamics that will run now according to its own temporal, temporal scales and inputs. We might need different elementary behaviors or those neural states that will just trigger different states or behaviors of the robot. Now, looking at an object could be one behavior. Fixating on an object, like doing little microcyclic eye movements, could be another behavior. Trying to learn an object, it\'s another behavior. Trying to recognize an object, yet another behavior. All of them need to be instantiated as an intention, be executed and be monitored when they\'re executed. Another component that we have introduced is a spatial memory, so that you not only recognize objects, but you also build a spatial memory of where you have seen those objects. And again, there\'s a whole story or structure around the spatial memory. I just flash the different components of that. So we kind of call it SLAM, Simultaneous Localization and Mapping. So something, usually an agent that moves around in an environment has to do it, has to estimate its position, and it needs to build a map representation of the environment. One could see it as a very general task that the cognitive agent needs to do. Both these agent at the tabletop needs to do it, needs to build a map of its immediate environment with objects on the table, and some robot that moves around in the environment as well. An important component here, coming back to Mark\'s talk, is this error monitoring and correction. So we need to instantiate the whole module that will constantly, or at least once in a while, be able to estimate whether it has some error in its estimation of its own velocity, estimation of its own position, or expectation of what to see in different positions. When it\'s mechanism to detect the error to estimate this error, maybe magnitude, maybe direction, and then use this error to update our system to learn. I think we are out of time a little bit. But if you have a closing sentence that you want to say. I have a closing slide. So, one note. So these architectures, they\'re really great and we have been building them for years. Why haven\'t you heard about them? Most probably. And I think the answer is that these architectures, because we\'ve tried to build practical systems for robots, they\'re just running too slow on conventional hardware. And there, I do believe that this novel neuromorphic systems that we now develop at Intel and those other people develop all over the world, they really help because those neuromorphic hardware systems that follow the structure of the brain network, biological networks, they help us to run these networks, even large architectures in real time, what we need for practical robotic applications. Okay, so yeah, so this asynchronous parallel and event-driven computing hardware helps to make these type of networks usable and practical. Computing model with stability, intentionality and normativity, so ability to detect errors is required. And if you have these modules, we can build large scale neuronal architectures and learning should be an inherent part of the systems autonomous dynamics. So it\'s not some extra source that we add at some point. It\'s really just part of the systems dynamics. And the final statement is that instantiating models in real world behaving systems reveals the demands on artificial intelligence systems and controllers that might be overlooked otherwise. I\'m alluding to Feynman\'s statement that if I cannot build something, I probably don\'t understand it properly. And I think we have experienced it many times in our work that it\'s useful to build the systems. All right, done. Thank you so much. Sorry for taking a bit longer. Thank you, Julia. Our next speaker is Jerome Bussemeyer. And he is now a distinguished professor at the Psychological and Brain Sciences and Cognitive Science and Statistics at Indiana University Bloomington. And he is the father of decision field theory. And he has a very interesting perspective on the way in which mental representations work, which is not just using probability theory, but actually quantum probability theory. And without further ado, Jerome, the stage is yours. Hello? Yes, I can hear you. Okay, you can hear me. Okay. Yes. Okay, so what is the goal? Anyway, I\'ve been working on a topic the past 10 years called quantum cognition. What is the goal of this program? Well, it\'s not a physical or neurobiological theory of the brain, and it\'s not really a theory of consciousness. I mean, it could be implemented by maybe some of Steve\'s work. Instead, it\'s a mathematical theory about human behavior, especially judgment and decision making. So but why do we want to use quantum theory? Well, quantum theory, although it was developed in physics, it\'s a general axiomatic theory of probability, and human judgments and decisions are probabilistic. And the thing is that these probabilities that human judgments make are, you know, form, they don\'t obey the Kolmogorov axioms. And so quantum theory provides a viable alternative. But, you know, there\'s many alternatives, you know, why quantum theory? Now, one of the things that\'s, you know, useful about quantum theory that matches human judgments is non-commutativity of measurements. you know, why quantum theory? Now, one of the things that\'s, you know, useful about quantum theory that matches human judgments is non-communitivity of measurements. So measurements, they change the psychological process. They produce context effects. So the principle of complementarity, which was suggested by Neil Bohr, the principle of complementarity was actually suggested to Neil Bohr by Edgar Rubin, based upon work by William James. And so we want to put complementarity back actually suggested to Neil Bohr by Edgar Rubin, based upon work by William James. And so we want to put complementarity back into psychology. Steve talked about complementarity too in his neural models. And the last point is cognitive scientists like to work with vector spaces. And so quantum theory provides, let\'s say, maybe the optimal way to compute probabilities in vector spaces, as we\'re going to see in a minute. So how do we use quantum theory? Well, probability theory has been around for a long time, but it was first axiomatized by Kamagura from about 1932. Now, quantum theory was developed by a number of physicists in the 20s and the 30s, but it wasn\'t axiomatized until about 1932 by Vein-Neumann. So let\'s compare the axioms of these two systems. So first of all, classical theory, is based upon idea of sets. So we have like a sample space of possible outcomes. So each outcome is a point in a sample space. Like if you had a rating scale and somebody is making a rating from zero to 100, we\'d have 100 points in the sample space. Now the corresponding idea in quantum theory is a vector space. So each outcome in the vector space is an orthonormal vector. And if you had 101 possible rating scale values, then you\'d have to use a 101 dimensional vector space. Each axis or each orthogonal basis vector would represent one of the outcomes. So we\'re moving from a kind of a set sample space to a vector space. Now secondly, and this is I think one of the most important parts that distinguishes classical from quantum theory, when we talk about an event, let\'s say the event giving a rating greater than or equal to 75, you know, that\'s, it\'s going to be a subset of the sample space in the classical theory. Now, subsets, you know, the, the, the algebra of subsets is a, is a Boolean algebra. And so you have to buy all the Boolean axioms, like you have to buy closure. If A is an event, B is an event, then A and B is an event also. Also you have to buy a commutativity. A and B is the same as B and A. And you also have to buy distributivity. A and parentheses B or not B is A and B or A and not B. So all of those axioms have to be adopted when you buy into the subset representation of events, like an event getting greater than 75, greater than 75. But those axioms of Boolean algebra are really strict and human judgements don\'t always obey those axioms. In fact, we have evidence that they violate each one of those axioms. Now in quantum theory, and this is where I think it\'s really key, quantum theory and event, like getting a rating greater than or equal to 75, is a subspace of the vector space. So instead of having subsets of a set, you have subspaces of a vector space. And so now you\'re working with an event algebra that\'s based upon the logic of subspaces. That logic is not the same logic as Boolean. In fact, it\'s not Boolean. In fact, von Neumann and Birkhoff started developing a whole new generalized program of logic based upon quantum logic. And so in quantum logic, you might have an event A in the vector space. You might have an event B in the vector space, but there is no A and B, no conjunction. There\'s an A and then B, and that\'s because things don\'t commute. A and then B is not the same as B and then A. And distributive axiom also breaks down. So in producing what we call interference effects in quantum theory. So the event structure is different and we\'re changing from subsets to subspaces. Now in classical probability theory, what we do is we form a state, like I have a state. I believe certain things are true. Steve has a state. He believes certain things are true. And sometimes we agree, but anyway, he\'s got a state. Now in classical probability theory, we define that as a probability function P, that P is defined on the subsets of the sample space. And quantum theory works a little bit differently. The state is a vector, sitting in a unit length vector, sitting in your vector space. And the way you compute probability, is you take your vector in that vector space and you project it on the subspace corresponding to the event. So, and then you get the squared length of the projection. And so that\'s how we compute probabilities in quantum theory. Now, the basic, one of the key things is that separates these two theories is when we look at conditional probabilities. So in classical theory, the probability of event B given A, we take our original probability function B and then A, and we renormalize it by the probability of event A. But here we have the commutative conjunction in the numerator. Now in quantum theory, we do something that looks similar, but it\'s quite different. So the probability of B given A in the quantum theory, we start with our state, we first project on the subspace for A, and then we project on the subspace for B, and we get the squared length, and then we normalize. So they both normalize. But the thing is, these projectors may not commute. In fact, the non-commutativity is the key part of quantum theory. If things commuted, then quantum theory would reduce the classical theory. So in some sense, quantum theory is a generalization of classical probability theory whenever we have non-commuting projectors. And that\'s where all the action comes in in quantum theory. So let me give you an example. You know, why do we use this theory? And we have many, many examples, but I\'ll just give you one. This is kind of a famous example of what we call the conjunction-disjunction probability error. Human beings exhibit this kind of phenomena. This was originally studied by Tversky and Kahneman. We have a paper in Psych Review that kind of described our explanation for this. So, you know, this is an example story. Now there\'s many, many, many examples, but this is just one example. Suppose Linda was a philosophy major as a student at UC Berkeley, and she was an activist in a social welfare movement. So you\'re given this prior information. So you\'re told this information. Now you\'re asked to rate the probability of different kinds of events. Well, what\'s the probability that Linda\'s a feminist? Well, that seems pretty likely given this story. So you might give it a high probability, 0.83. There\'s nothing right or wrong about that. Then you\'re asked, well, is Linda a bank teller? Well, that seems unlikely given the story. And so people tend to give it a lower probability 0.26. These are actual numbers from an experiment, some averages across subjects from an experiment. But the key thing is when they\'re asked, is Linda a feminist and a bank teller? Well, they give it a probability rating of 0.36. Now, 0.36 is greater than 0.26. So we get that this is where we get a conjunction error. Because the conjunction now is rated higher than a single event. And when they\'re asked, is Linda a feminist or a bank teller, we get a disjunction error. They give it a rating of 0.6, but the disjunction is lower than the single event by itself. So we get these, this doesn\'t always6, but the disjunction is lower than the single event by itself. So we get these, this doesn\'t always happen, but there\'s many different situations where people will make these disjunction and conjunction errors. Well, we call them errors, but maybe it\'s an error with respect to Kama-Gorov theory. But in terms of quantum theory, it\'s not necessarily an error because as we\'ll see in a second, quantum theory provides a simple explanation from this. So these kinds of findings can follow directly from a quantum kind of axiomatic representation of reasoning. Now, first of all, I just want to point out the conjunction error is a violation of the law of total probability. So the probability of bank teller is the probability that she\'s a feminist and bank teller given a feminist plus the probability she\'s not a bank teller, probably she\'s not a feminist times the probability of bank teller given not a feminist. So that\'s the law of total probability. And of course, this event here, bank teller has to be greater than this event here because this is just one of these two that are summed up and this has to be greater than or equal to zero. So the conjunction, so this conjunction fallacy violates this law of total probability. And so that\'s kind of a signal for quantum theory. So quantum theory is kind of famous in physics for explaining violations of law of total probability. Like if you read Feynman\'s book, the double slit experiment is basically a violation of total probability. So here\'s a toy model. I hope I have some time to go through this. Now, this is just a toy, but it gives us an example of how the theory works. So in quantum theory, then we have events. Yes to feminine, you say yes to feminist, that\'s one event. You say no to feminist, that\'s another event. So I\'m representing the event yes to feminist by this horizontal subspace, this ray. I\'m representing no to feminist by this vertical ray. And they\'re orthogonal because you, this ray, representing no to feminists by this vertical ray. And they\'re orthogonal because you, you know, yes is usually exclusive from no. Now, the key thing is in quantum theory, when you have incompatible events, basically you\'re changing the basis for the vector space. So this is a two-dimensional vector space, but we have many different bases that we could choose. And so now here we got a basis for the bank teller. The bank teller is rotated with respect to feminism. Here\'s the subspace for saying yes to bank teller. It\'s this orientated ray with the positive slope. And here\'s the vector, the subspace representing not bank teller, it\'s a negatively related line. So we got these two subspaces that are incompatible for these events. Now here\'s our state vector. So this is after the story, you get the state vector. Now, if we ask, given the state vector, well, what\'s the probability that Linda\'s a bank teller? Or yeah, what\'s the probability that she\'s a bank teller? What\'s the probability, Linda\'s a bank teller? Or yeah, what\'s the probability that she\'s a bank teller? What\'s the probability yes to bank teller? We take our state, we project it on yes to bank teller, we get this tiny projection right here. The projection is like 0.16. But now if we take our state, or if we look at the conjunction, we take our state and project on yes to feminist first, because we answer feminist first, and then project yes on the bank teller, we get a larger projection, it\'s 0.3. So the projection from the state, original state to feminist, and then the bank teller exceeds the projection directly from our state to the bank teller. And so that\'s a simple toy model. Now we can derive it more generally from the axiomatic principles using this algebra that relies on the vector spaces for the quantum model. I\'m not going to go through the algebra because it\'ll take some time. But this model not only explains the conjunction and disjunction fallacies, but it makes a number of other additional predictions that we can test and that have been supported by the model. So in conclusion, then, quantum theory provides an alternative framework for developing probabilistic and dynamic models of decision-making. I haven\'t talked about the dynamics, but because I, with limited time. And it provides a coherent account for puzzling violations of classic probability finding, found in a variety of judgment decision-making studies. and it forms a new foundation for understanding widely different phenomena decision-making using a common set of axiomatic principles. What I didn\'t get time to show you is there\'s some really puzzling findings in psychology that never have been connected together before, and by using the quantum axioms, we provide a coherent axiomatic explanation for widely different phenomena using a common set of principles. So if you\'re interested in these ideas, then you can take a look at our book, my book with Peter Bruza on quantum models of cognition and decision. So then I guess I\'ll stop here then. Thank you so much, Jerome. That was a very clear talk, given the short amount of time that you had available. I hope that people got the core insight. Our next speaker is going to be Steve Rogers, affectionately also known as Captain America. And Steven Rogers is senior scientist for autonomy at the Air Force Research Laboratory at the Wright-Patterson Air Force Base in Ohio. And he is a thinker in cognitive architectures and he\'s very interested in machine consciousness as well. Steve, the floor is yours. Okay, let me try to share my screen here. I can go ahead and start the discussion. Yes, please go ahead. Yeah, yeah. So, you know, the fundamental research question that we\'ve been addressing is, you know, what are the principles of representations, think qualia, that enable flexibility? And so I enjoyed the talks this morning so far, representations, think qualia, that enable flexibility. And so I enjoyed the talks this morning so far, but let me weave a story for you that sort of uses the words of that question to sort of guide you through the way we\'re thinking about how to achieve flexible AI. So I start with, you know, addressing, you know, what are the principles? So, you know, I\'m reminded of the 1700s, there was a French philosopher who suggested that, you know, if I might paraphrase, principles, you know, compensate for a lack of facts. And when it comes to consciousness or predicting where AI is going, we certainly have a lack of facts. So we focus on principles and then we let those principles guide our implementations of AI. With respect to representations, our view of representations are representations are a combination of how an agent structures its knowledge in the processes it uses to use that knowledge to create meaning, and of course, meaning is then what it uses to take actions or update its knowledge, okay? So given that background, what are the principles of representations qualia, I need to sort of talk about qualia. And so to get there, I just want to back up briefly. It\'s been mentioned a couple times, things like Kahneman and Tversky and Evans and Stanovich, et cetera. There\'s a lot of people who believe that we have two cognitive systems. Sometimes they\'re characterized as CIS-1 and CIS-2, subconscious and conscious. But the idea of that way of looking at, there you go, great. If you just go down, once you get it to full screen, if you go to that second slide there, that\'d be perfect. So, you know, there, you know, some people think of this as one, a lot of people call it your gut response and they characterize like in the Evans and Stanovich case, this, these sort of functional characteristics over here on the right, you know, and I find that that\'s a fine way to look at this. You can get a lot of mileage by looking at this as a dual cognitive systems. But one of the challenges I\'ve run into is a lot of AI people, and I admit even myself on occasion, have suggested that our current AI approaches like in deep learning sort of replicate what goes on in CIS one. And there\'s some truth to that, I suspect, but there\'s all sorts of examples in the literature. I gave you this picture down here on the bottom of a person with blindsight. And what that means is they\'ve had bilateral damage to the visual cortex in both hemispheres. And so they have no conscious access to the visual world. Their eyes are fine. Their optic nerves are fine. Their brainstems are fine. This is particularly meaningful for me today because I was losing vision in my right eye earlier today. And so I\'ve been at a doctor all day. But this guy has no access. If you hold up something and say, what is that? He\'ll say, I can\'t see anything. The world is black, I\'m blind. But yet you put him in a cluttered hallway and he\'ll navigate his way through the hallway. He can approximate an envelope at the right angle to put it in a slot. So he clearly has access without consciousness being involved with the visual world. And I find that those examples interesting. There\'s others that are even more profound. There are examples where you can show that without consciousness being involved, you can determine whether an image is congruent or incongruent. And what I mean by that is, for example, if an image is of a grandma pulling out something out of the oven, if the image is of her pulling out a chessboard, that\'s an incongruent image versus her pulling out a cookie sheet. And it turns out that the calculation of incongruence is made without conscious deliberation because you in fact get those things brought to your consciousness sooner if they\'re incongruent. So I just wanna sort of set the stage and suggest that I\'m very focused these days on qualia and cis2, but I don\'t have delusions in that it\'s the only really cool cognitive processing going on. It\'s far more than just what we normally term as perception. You can broaden perception and maybe cover cis1, but let\'s move on to qualia. Let\'s go to the next slide, please, Tanya. To talk about qualia, the metaphor, and it is a metaphor, it isn\'t reality, that sort of cartoon in the middle of this slide gives it to you. You have this illusory, and I do mean illusory, there\'s no place in your brain where all the information is brought together. We have this like a little homunculus sitting there, absorbing the visions and the sound and the smells and the taste and having thoughts on his own, you know, but it\'s an interesting metaphor. But what I want to talk about with respect to consciousness, if you look at the little patch of red there on the right, what I mean is what you\'re actually seeing there, the mental image that your mind created for you is fascinating. And as Steve Grothward was talking about earlier, you know, the photon stopped at your retina. But this thing you\'re perceiving in your consciousness is created for you in your mind. And if in fact the square just to its right, if I change the hue saturation of brightness of it, you actually get a different perception. So that\'s where I use the quali word. The quali of what you see is evoked in your mind from that redness, a slightly different quali with a square right next to it. It\'s the quali that you experience in your consciousness when somebody plays a piece of music for you, or you taste a piece of pizza. Most importantly, on the bottom left here, it\'s what I see and feel and experience when I hold my granddaughter, Belle, there. Or even when Belle and I walked down to the local beaver pond with Napoleon, our favorite dog, and we\'re overwhelmed by sensations and emotions or the emotions I had when I buried my father at Arlington some years ago. So that sensory motor world model is what I\'m talking about when I talk about qualia. They\'re the elementary elements by which you construct this experience. It\'s the phenomenological experience you have, which we call consciousness. Okay. Now, next slide. So what we\'ve been after is, and I have to admit, before I just dive into this, the details of the S3Q theory of consciousness is, you know, I\'m a fan of Tversky and Kahneman and Gary Klein, etc. And Tversky once, when asked about artificial intelligence said he studies natural stupidity. And what that drives home is, I have no delusions that even if I could replicate some of the characteristics of that sensory motor world model that has moods and emotion, it\'s not going to solve all my problems. That being said, I do believe it\'s a source of great flexibility that could be had for artificial intelligence. So what we were looking for is just a small, simple set of fundamental laws, which capture the what it is like. And, you know, the what it is like from a Thomas Nagel perspective is key. But from an AI perspective, we\'re also after trying to answer the question, why is it like that? So as I describe these characteristics of the conscious experience, you know, from an AI perspective, we\'re trying to suggest if we instantiate these characteristics in our AI representations, it might show us certain cognitive capabilities that we didn\'t have without these particular tenets, without these particular laws, if you will. And and thus, we could probably get some insights into why consciousness is constructed the way it is. So the first is that, that the world is situated. I\'ll start with number two here. And by that, I mean the concepts, the qualia, as the plural, the singular is quali. The qualia, in fact, only have meaning with respect to how they\'re related to and how they can interact with other qualia. So the red I showed you, the red squared I showed you there, its only value is with respect to other colors, with respect to where it\'s at, with respect to when it occurred. So all concepts, all qualia are situated. The fundamental unit of conscious cognition are situations. The third one down here, I\'ll go to it next, simulation, is it\'s a simulation. And again, I\'m going to have to read Stevens new book, but but, you know, the idea that there\'s there is a decoupling between the world and in fact, even from your sensory data, in this conscious experience, this representation is decoupled from that. And in fact, we like to think of it as a as a simulation composed of qualia, but simulation, and thus, even the things that I perceive my perception, is an imagined present. It\'s not reality, what is out in the world. It is a simulation which creates a representation, which for me is stable, consistent, and useful. And in fact, when I recall something in the past and bring it to my consciousness, it is, in fact, creating a simulation of what might have happened to me, not only in terms of the experience, but in terms of everything that\'s happened to me, not only in terms of the experience, but in terms of everything that\'s happened to me in between. So I have imagined past. It\'s not a replay of an AVI file. And most people can buy off on the fact that when your consciousness is evoked, and you\'re trying to imagine something in the future, that you imagine that because you clearly haven\'t experienced it yet. So simulation is that tenet. Other people have talked about the idea of simulations, Larry Barcelow, for example, down at Emory is a great source. And then the third S in the S3Q is structural coherence. And by that, what we\'re trying to say is when I reach over and I grab this glass because I\'m thirsty, I consciously perceive its presence and there\'s enough mutual information between reality and my conscious representation to allow me to interact with it in a coherent manner. But it isn\'t just in terms of physical interactions, you know, my representations of color. If you look at what things are perceptually similar on a conscious sense, colors, and you can create a color wheel which captures actually some physical characteristics of wavelengths, etc. How the colors can interact. Those can all be captured. They\'re certainly represented in your conscious experience, and also in this structurally coherent world that\'s out there. And then the last Q, the Q in S3Q, is qualia. We believe the vocabulary of the conscious experience is made up of this bottlenecked reduced order view of the world. It forces nature to create a simplified model of the world to allow you to have this stable, consistent and useful representation. And it situates these things into these narratives composed of the qualia. So that\'s sort of the way we define consciousness. Now I\'ll just take a couple of minutes more to explain to you why we\'re focused on this. You go to the next slide, Tanya. It\'s all about, remember, I defined representations as how an agent structures its knowledge. So the things we\'ve been trying to seek is, where does the knowledge come from? How is it structured that causes the creation of this quality experience, this consciousness? So if you look at Peter Dominguez\'s book, he suggests that knowledge historically has either come from evolution, experience or culture. And in AI, we sort of follow the same route. That is, initially, all the AIs that we\'ve been building, you know, we just put all the knowledge in it from the beginning. It\'s not unlike, you know, providing it in your DNA. What we found more recently is, though, by allowing our AI bots to interact with the environment, them to have experiences, capture those experiences and store them as knowledge, that\'s much faster and more flexible, in fact, than just pre-storing everything. And then what we figured out is now that by letting machines interact with not only the data world, but the real world, both with autonomous cars and with aircraft drones and spacecraft, that in fact, most of the knowledge going forward from this time forward in the future of the Earth will be created by machines. Now, the problem I have with that is the knowledge that\'s being created by those machines are currently, it\'s only usable for that machine to create the meaning that we programmed it to do. Or if we\'re slightly creative, it\'s some variation thereof, but it\'s very limited applicability. Where we\'re headed is as we make these, Stephen brought it up earlier about explainable AI and deep learning issues. I\'m not as much a fan of explainable AI as alignable AI, AI that can align with not only the human, but with the other agents it\'s working with, whether those are robots or drones or cyberbots or my little microsats. Where I\'m headed is when these machines can share their knowledge in a form that those other bots can use to help them solve their challenges, that\'s machine culture, and that will be even faster than what we\'ve done so far. And then my last slide, Tanya, if you go to that, this is my driver problem, if you will. So as we instrument the world more and more with our bots, the amount of available knowledge is growing exponentially, that sort of orangeish-yellow, whatever color you see there, curve is. And the problem is, I have a great team. We\'re building great AI to solve real problems in a range of diverse tasks. But the problem is, each one of those applications is on the green curve. It\'s very narrow. It solves one problem. We structure the knowledge, or we structure its experiences to do it. And so therefore I have this gap. Jeff Jonas, who I got this idea from, he\'s an IBM fellow, calls this enterprise amnesia. That is on a given challenge I have, all the knowledge I should be able to bring to bear to make that decision better, I can\'t because I\'m in some sense amnesic to it. I haven\'t created the knowledge in a form that I can exploit it at the moment I need it. And it\'s because of our approach to this very narrow AI. Now, I\'m not, I contend that I quoted the Melanie Mitchell on the bottom of this slide. You know, I\'m not at all convinced our current approaches. And I think that\'s the purposes of Yosha\'s panel here is that these current approaches, I\'m not at all convinced they\'re on the path to general AI, if there even is such a thing as general AI. If you go down the Francois Chollet approach, maybe you don\'t believe there is such a thing. But certainly to make AI more flexible than it is, I think we\'ve got to break off of that curve. And so I\'ll stop there. Thank you, Tanya, for projecting for me. Thank you very much for this presentation, Steve. Let me share my own screen. I hope this is working. I\'m Joscha Bach. I\'m a cognitive scientist. I\'ve worked in the area of cognitive architectures in the past, trying to understand the relationship between cognition, perception, and motivation. And I\'m currently working at Intel Labs as a principal research scientist in the group of Gadi Singer. And I\'m leading an effort in understanding cognitive systems. The representations that we are currently using in the majority of AI applications can arguably has been started with Frank Rosenblatt\'s perceptron architecture. And they are almost unchanged in many ways to his original work. And Frank Rosenblatt\'s work has suffered a setback when Marvin Minsky pushed against it because he thought it\'s too simplistic, and we need many, many more structures and layers and so on to understand them, and even brought a book together with Seymour Papert arguing that neural networks cannot learn even many simple logical operations because he didn\'t see yet how backpropagation would be able to enable this. And now we are at the point where we feel that even backpropagation is probably not sufficient and we need new approaches to representation in neural networks. And there are, in some sense, three perspectives that I would like to present. One is the idea of the neural circuit. And indeed, we can argue that the neurons in our brain form circuitry, and also that our neural networks, in some sense, emulate functionality and functional division of such circuits. There\'s, for instance, excellent work by OpenAI, especially Chris Ola\'s group, that has been analyzing a number of computer vision networks and shows how the superposition of features leads to the representations and image net representations in existing neural network classes. And he argues that features are the fundamental unit of neural networks and correspond to directions in an embedding space, and the features are connected by weights forming circuits. And there is a universality phenomenon that we can observe that analogous features and circuits will form across different models. So if you take different neural networks and put them to the same task, and they have flexibility in forming their structures well enough, their architecture well enough, they\'re going to model analogous features. And this perspective of neural circuits corresponds to state machines in computer science. And there is another one that is the one that we can also think of the brain as something forming like a neural ether, so which waves of activation propagate. And the state that we are interested in is not the state of the individual neurons, but rather it\'s the state of the wave of activation that is being propagated along the neurons. And a perspective that we would be taking is that our brain is representing things very much like a synthesizer would do. So you start out with patterns at your systemic interface and your brain is trying to make sense of them by making them predictable. And if you know how a synthesizer works, you have a bunch of oscillators with potential meters that change the oscillations and you connect them in a certain way. It\'s a little bit of experience. You figure out how to make almost arbitrary sounds and basically every sound that you want. And you can parameterize the generation of the sound by changing the oscillators. And this principle does not just work for the auditory domain, but you can also use it for spatial frequencies and for stationary frequencies, that is, colors. And you can make then meta-oscillators that basically try to find the patterns within the oscillators. And when you work with synthesizers, you know how that works. You can basically create combinations of the different patterns and abstract, for instance, the pitch out of a number of sounds as its own parameter and so on, and find principal components at higher levels of abstraction. And then you can basically merge these percepts until you get to dynamic simulations of environment and all the modalities meet. And you can abstract this even more until you understand the generality of all these simulations and get to conceptual abstractions to an address space of all the objects in your simulations to make sense of the world. And this perspective of the neural ether is best described from a dynamical systems perspective. And a third perspective that I want us to look at today is exemplified by Jerome\'s work, which takes the cognitive representations as something akin to what we are describing in quantum mechanics. This does not necessarily mean that the brain is a quantum computer, but that the mathematical formulisms of quantum probability are the most suitable to describe the way in which representations work out in the brain at a state unnecessarily in a definite state, but they can be in an ambiguous superposition because they\'re constrained in a particular way and you will have to collapse these ambiguities when you want to reason, but otherwise perceptual representations are very often in a superpositional state. So if you have this attentional collapse of representations, you will have to have a bipartite mind. You have generative perception, which is distributed and geometric and ambiguous, and it\'s aiming for coherence. And you have an integrated attention system that in some sense is looking at this perceptual representation. And this integrated attention will have to be localist and disambiguating and aiming for correctness. And the purpose of this integrated attention system is to fix perception where it doesn\'t work and to interact with it. So we can see attention inside of the cognitive system acting as an agent similar to a conductor in an orchestra, that is interacting with the perceptual system. It is parametrizing the states of the perceptual system to collapse them into useful states, and so certain operations like reasoning, planning, and so on, can be performed on them. So this quantum probabilistic brain perspective that Jerome has presented basically gives us an insight into a dual process mind, that you have an observer inside of the mind and a phase space. How can we create a unified coherent model of the universe? And I think this question of how we can get to unified representations that are total, that are universal, that are fully connected, where all the domains are connected into a cohesive model, that is the biggest and most important unsolved problem in artificial intelligence today. And when we think about how our presentations work in general, how our model works in general, we start out with patterns. And these patterns are don\'t have meaning yet, right? They\'re the patterns that we perceive from the environment that don\'t contain sounds and colors and so on. Sounds and colors are functions that your mind is generating to make these patterns intelligible. They are hidden states, they\'re variables inside of your models, and a variable is a set of possible values, and these feature variables are related to each other with computable functions that constrain some of the values depending on the values of the other variables. And they also constrain the next future states that we can observe. They reduce the uncertainty in the next set of observables. And these relationships are not probabilistic, they are possible-istic. They represent which features are compatible to each other. So when you are representing a nose, and it has a certain pose, that is a certain direction and alignment in space, then there must be a face nearby. This is a certain pose, that is a certain direction and alignment in space, then there must be a face nearby. And this is one of the constraints that you learn that has the same direction in space and that is directly adjacent to the nose. And if you do not discover such a thing, then you will have a constraint violation. Maybe you\'re not looking at a nose. Maybe you need to change your models. Maybe the features that you took to be a nose are actually not a nose, but something else. And you have to come to a different parameterization of your model. And to find the right parameterization of your model, you need probability. So given the current set of constraint violations in the model and the current state of the model, there is a certain direction in which you should move the current model of what is the case. And this is probabilistic model. and these probabilistic relationships that we learn bias the model towards convergence. This is the reason why we perceive many optical illusions, for instance. The optical illusions are the result of such biases. And we also need to have valence, because not all representations, not all uncertainty that we reduce has the same value. The reduction of uncertainty is expensive, and we need to have a way to evaluate what we should be doing first. So these representations that we represent in our mind are connected to motivation, to a setpoint generator that tells us which things are important at every moment. So the goal of the model is to predict the next state from the previous state in each step. We try to find a configuration that minimizes constraint violations. This leads to a more coherent model. And we try to minimize the uncertainties to maximize the ability of the system to control its environment. At every moment, we have to do the most valuable thing. So we need to have an estimated reward. And this estimated reward needs to get distributed throughout the system. So all the components of the system, all the individual neurons are organized and self-organizing in such a way that they globally approximate doing the most valuable thing. Once we take this perspective, we can go to developmental psychology. And we now notice assimilation and accommodation notions brought up by Jean Piaget. Assimilation is the modification of the model state to make consistent with the sensory input. This means we don\'t change the way in which we understand the world, but we change our assumption of what\'s currently the case. And accommodation means that you have to modify the model structure, so we can allow the assimilation of all sensory data. And this means you have to extend the model, you have to exchange our understanding of the world. And now if we look at our cognitive system, we notice that there\'s basically a perception agent. This perception agent is richly entangled with the environment and situated in it. And this perception agent is going to create a set of dynamic functions that predict the next set of patterns in your environment, in its own environment. Part of your mind is also the environment of that perception agent. And we have a motivation agent that is trying to evaluate what the system is doing in the world and generate impulses that tell you what signals anticipated are important, what you should be doing. And you have an attention agent that is interacting with the perception agent, itself not providing a motive force, but it\'s directing the motive force in such a way that the perception agent is finding a representation of the world that makes sense at any given moment. And I think that this attention agent will have to have pointers to perceptual objects, the relationship between them, and it will have to have a mode of attention that is, are we looking at the counterfactual representation at a memory, at an exepulation, or something that is currently sensory? So you have the content awareness, and you have the attentional awareness, and you have reflexive attention. That is, the system is also ensuring that it knows that it is indeed the attention agent and the self-organizing structure of the mind. And this attention agent will have to maintain an index memory, where the perceptual system is just converging to states, the attention agent will have to have a memory of what it has tried before, because it cannot just follow a gradient and it\'s discrete state of interpretations. And this gradient, this memory is going to be accessible as a stream of consciousness. And I think that this attention agent is responsible for the funerality of consciousness in the human mind. So to sum up, what we have seen today are several perspectives, the perspective of neural circuitry, which corresponds to state machines, and the perspective of a dynamical oscillating system that you can describe as dynamical systems. And we have seen the perspective of a quantum probabilistic brain in which you have the mind divided into a dual process system, where you have a space, phase space of possible things that could be the case and a conditional observer, or the observer that is conditionally collapsing these representations into concrete interpretations of what is currently happening. And these perspectives are not necessarily in conflict with each other. There are different projections of this whole uncertain thing that happens in our mind when you represent ideas and knowledge. And with this, I would like to open up the floor to a discussion for the last remaining 20 minutes. So we\'ve seen several positions. We have seen Mark starting out with the problem of reference. How can representations point to two facts in the world? And I think that a possible answer that we might agree on is in the direction of we don\'t point to facts in the world. What we point to is a unified model inside of the brain, right? We don\'t live in the physical world out there, we live in a simulation that is generated by our own brains. And this simulation contains the coherent model of the world. I don\'t know if this is something that Mark would agree with, and maybe you want to address this. Mark, what do you agree with? And maybe you want to address this. OK. I don\'t think pointers work. Pointers can simulate some properties of reference and representation. But if you ask what is pointed to and how does the system know what is pointed to, and how could it ever detect that the pointing is in error, you run into the same sorts of problems. One of the questions that I left with had to do with content. And I think that\'s what\'s relevant here. If there\'s an indication in the organism that a certain interaction might be possible, in general, that will be correct under certain circumstances in the environment and incorrect under other circumstances in the environment. To have an indication that it\'s possible presupposes that this is one of those environments that would be supportive. And in that sense, you involve a presupposition about the environment in the indication that some interaction is possible. Presuppositions of that sort are what I propose as a model of content. It is those presuppositions that can end up being true or false. This might or might not be one of those kinds of environments that support. It differs from standard encoding ways of thinking about this, including pointers, in that there is no representation per se. There is no explicit content about what it is about those environments that\'s going to be supported. The presuppositions are implicit, not explicit. And they turn on a notion of implicit definition, of dynamic implicit definition, where implicit definition really only got introduced into formal logic in the late 19th century and generalizations of implicit definition to dynamic frameworks is even more recent than that. So what I\'m claiming is that, yeah, content is what we wanna be talking about. And the sense in which content is presupposition gives a dynamic way of doing that. But it gives us content that\'s implicit rather than explicit. It gives us content in the sense of an indication of a potential interaction, thereby presupposes that the environment will, in fact, support that interaction. So you can see a bit of intuitive convergence there with pointers. But pointers have to be explicit. They have to point to something. And that explicitness encapsulates all of the basic epistemological problems, or so I would claim. Thank you, Mark. Steve, do you find that our current discussion is missing something important? And when you are trying to look at representations as they\'re currently done in AI systems, is there something that people should be looking at when they are building representations of the world and of the actions of the agent or of the agent itself. You\'d like me to respond to that? Yes, please. Well, in my brief summary, I spoke about thought experiments. You know, when you can do a thought experiment, it means you\'ve been lucky enough to get sort of to the foundations of a subject. Einstein, as you know, derived both special in general relativity from thought experiments. My thought experiments are about how any system can autonomously correct predictive errors predictive errors in a world that\'s changing and not under control. I didn\'t start by thinking in this generality, but, and I didn\'t start with thought experiments, I always start with data. If I have any talent, it\'s seeing to the heart of data, the meaning of data, and being able to take hundreds of data curves and converting them into dynamics that can then explain and predict those data and many more data as emergent properties of the dynamics. With these thought experiments, the first is cognitive, how a cognitive system can autonomously learn to correct predictive errors. And it leads to adaptive resonance theory, which I think has successfully explained how our brains learn to attend, recognize, and predict objects and events in a changing world. There\'s also a thought experiment which leads to cognitive emotional models where emotion and motivation play the role of constraining your predictions based on what\'s valuable to the individual in a given context and at a given time. So that all the foundations of cognition and cognitive-emotional dynamics can now be explained as necessary consequences of universal thought experiments where you don\'t need fancy data to derive them. The hypotheses are trivial facts that everyone accepts from their daily life, but they act together and are environmental pressures or constraints on the evolution of our minds. pressures or constraints on the evolution of our minds. So like, you know, to derive cognitive-emotional dynamics that, among other things, years ago explained and went beyond explaining Kahneman and Tversky\'s data and had nothing to do with decision-making under risk. It was foundational cognitive-emotional dynamic. The hypotheses are more or less that I\'m trying to learn that A predicts B, A predicts B. I could do it within a range of time delays. No one would doubt that. And secondly, that learning is possible. And that leads to foundational models that can explain cognitive diversity data as well as a wealth of other data about normal and abnormal cognitive emotional learning. More recently, what goes wrong in our brains leading to behavioral symptoms of Alzheimer\'s disease, autism, amnesia, post-traumatic stress disorder, attention deficit hyperactivity disorder, visual and auditory agnosia, problems with flow-wave sleep. And I say these two as a challenge. If you don\'t know this and you are believing that you\'re saying something about how our minds embody intelligence, then you\'re simply not competitive anymore. I grew up in a traditional scientific mathematical environment. And the people who could explain and predict the most stuff, those are the people you have to study. I still believe that. And while I\'m alive, you might want to exploit me, because just the remarks I\'ve made now, you can\'t get around them. If you believe these trivial hypotheses, you have to believe the theories or give up your faith in logic and reason and the scientific method therefore. How many of you are willing to do that? Thank you, Stephen. Let me say one more thing. My new book, you know, I\'ve been working like a dog for 64 years and to write compelling articles in peer reviewed journals, you have to throw in all the technical details that reviewers will accept. But my new book is over 700 pages written in a self-contained, not technical, conversational style as a series of stories for the general public. So if you were ever curious about these things, now is the time to find out. And I also made it possible to sell this big book for less than $30 in hard copy on Amazon and $19 in Kindle. So it\'s affordable, it\'s accessible, and you can\'t get around things like thought experiments, no matter how hard you may wish to. I agree. People should read your book, and there are many pointers that are in it. I think that researchers are, many are still neglecting, especially for students in cognitive science, I think it\'s important to get the next generation into these ideas. Julia, do you want to respond to this or do you want to tell us why it\'s necessary to have recurrence of memory is not reliable? I didn\'t understand that particular point, but please feel free to respond to anything that you think is pertinent. Okay, so maybe I will take this idea of the thought experiments. And I would like to maybe ask Steve back, but I will just state my statement, maybe Steve can get an answer. So we see value, I saw value in my research of not just thought experiments, but the actual experiments of trying to first simulate the models that were built to see whether the behavior is what we expect. But then at the end to actually instantiate them in this embodied agents, and to see whether I can actually generate behavior that unfolds autonomously and can detect those errors. What does it mean? What does it take with real sensors that might have certain temporal characteristics or certain certainty and uncertainty characteristics that I might have not thought about when just thinking about my model as a thought experiment. So I just wonder whether Steve sees that there is value in such real world experiments experiments, or whether we can stay only in thought experiments? Well, our models started several generations of mobile robots. And I\'m all for that. But I also feel very much, I don\'t know if you remember the famous story when, I think the first time the astronauts came down to Earth in their capsule and they landed safely, the first thing, was it Neil Armstrong said when he got out, was the bump was there. The bump in their descent. And that was a scientific mathematical prediction. And everything that I\'ve done with over a hundred gifted collaborators is rigorous. And we have simulated many hundreds of experiments in quantitative detail showing that things work the way we say they do. And in fact, before computers were able to simulate data that may include the interactions of millions of heterogeneously organized brain circuits, I had to face my fears and learn how to prove global limit and oscillation theorems. And in fact, in adaptive resonance theory, Gail Carpenter and I and a number of our students have proved theorems about how arc models, arc 1, arc 2, arc 2a, arc 3. These are incrementally fuzzy art, fuzzy art map, distributor art map. How these things actually learn, classify, and predict in unpredictable environments. So there\'s a rigorous foundation here. The bump will be there if you want to build your own robot. But others have preceded you by maybe longer than you\'ve been alive. I\'m not sure what you are. I don\'t like to cut this short, but we\'re almost upon the hour. I know that- The thing about robots, funding needs to be sustained for a while to generate an effective robotics program. What we found, given our university funding, was that, oh yeah, in the university, the grant would run out before you could really deploy the robot in an industrial setting. That\'s why we were so glad that MIT Lincoln Lab ran with the banner, because they didn\'t have to work in the university, although unfortunately a lot of their grants only lasted for three to five years too. So the key to this development is getting sustained funding that\'s not a victim of every change in political administration. Before we end, I would like to ask Steve Rogers, please, do you think that computers will have qualia soon and that we can make models now already or is there something missing before make models now already or just something missing before we are able to build a system that has the equivalent of phenomenal experience? Well, in my own work... Sorry, there was a question to Steve Rogers. Oh, Steve Rogers. Yes, because he\'s about to drop out. I know that he has another meeting coming up in a minute from now. So. Yeah, Steve Grosberg, it\'s great to see you. I haven\'t seen you since years ago I was in your house. I had the pleasure of hearing Gail. And Jerry Bozmaier, it\'s great to see you too. To answer your question, Yosha, as you know, you\'ve been interacting with us recently. I believe strongly we are already seeing benefits from taking engineering characteristics of the quality of representation and putting them into AI solutions. I do believe we will come up with what I\'ll call quest consistent phenomenological experience. That is, it will not feel the way you and I feel or the way my puppy dog feels or anything else, but it will extract an engineering advantage from replicating the phenomenology. Because I do believe strongly that consciousness is not an epiphenomena. It\'s in fact in the decision cycle. You\'re right, I have a commitment with NASA right now. I\'m going to sign off, but I will listen to the video later for anything I miss. And it was great to see you guys. Thank you very much. I would like to thank all of our speakers today for participating. I would also like to thank Intel Labs and Gadi Singer for making this event possible. I\'m very grateful that we could make it happen. I would like to ask Tatjana Greenberg for helping with the organization. And I am very, very thankful that this happened. We have recorded today\'s session. I hope that the recording has worked out. And we are aiming to make it available soon on the panel website. With this, I wish everyone a wonderful day, evening, night, afternoon, wherever you are in the world, and we hope to see many of you again in the next series of this panel. Thank you very much. Thank you, Joscha, for organizing this all. Thanks. It was a great pleasure. this panel. Thank you very much. Thank you, Yoshua, for organizing this all. Thanks. It was a great pleasure.', '38.59989953041077')