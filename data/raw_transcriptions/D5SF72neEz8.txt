('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/D5SF72neEz8"> </iframe> </center>', " The first paper is by Joshua Bach and it's 7 Principles of Synthetic Intelligence. Okay, good morning. So, my name is Joshua. I come from a computer science background. Initially I came from computer science and philosophy and I'm mostly interested in looking at architectures. When we do HDI, what we are basically up to is we are trying to understand the mind as a machine, something which already, what we call enlightenment, has stated that if we want to look at the mind scientifically, as an engineer or as a natural scientist, we capture it as something like an iniquity, scientifically, as an engineer or as a natural scientist, we capture it as something like an iniquitism that brings forth thoughts, experiences, and perception. And if we enlarge this, so to speak, if we can see it as an iniquitism, something like a huge mill dent, right, we can see the individual parts, however, on each other. And this is, in a way, what we are trying to do. We are trying to understand the mind by constructing a working model of it, a formal theory that works. They copy very much the same way as physics does this, where they try to build a formal theory of the regularities of the universe and see if it really works out in their simulations. However, this is not the whole story, because Leibniz himself didn't believe in that proposal. The whole point is that perception and what depends on it, that is cognition, thinking, the mind, cannot be explained because otherwise it wouldn't be a machine if it would be explained by natural sciences. And this is not really conceivable that we would have something like parts working upon each other, which do bring forth cognition. And Leibniz did say this in the many, many centuries ago, and it's still something which is verified when he says, Roger Penrose basically says the same thing, when he reminds us that understanding and perception can not be understood and simulated computationally. It has to be something which goes beyond computation, beyond formal theories. And he thinks that maybe it's quantum mechanics because for some reason he believes that quantum mechanics itself goes beyond what we can capture mathematically. So it's a mystery. And since consciousness is the mind and so on, our mysteries themselves may be, hey, it's the same thing. John Searle also goes in a similar direction when he points out that computers can only do syntax. They only can do symbol manipulation, and understanding something which has to go intrinsically beyond symbol manipulation. So it cannot come from mere symbol manipulation, it has to come from something different, maybe from the intrinsic properties which only are inherent to biological neurons. And to put this more literally, the experience of the human being is not transferable into a machine. It's something which computers cannot have, and therefore computers cannot be creative. There are many, many more angles. This theme is variated, and basically we have a cultural consensus in our Western society more or less that computers can not and they should not and maybe they can not because they should not. And this vision of AI which we are witnessing is far from over. We are facing very strong cultural opposition and when we are doing general intelligence, general artificial intelligence, as a subject, a physicist, we find ourselves in a similar position as somebody who does genetics and tries to build a genome to create a cell or an organism. And the funding agency is populated by creationists. It's even worse in the room. Okay, but is this the only problem? Of course not. HDR and artificial intelligence itself has a lot of traps that it managed to fall into on its own. It starts with the fact that we are really not really discussing what we are up to, what we are really trying to build, what our living paradigms are, what kind of modules, techniques, mechanisms we want to implement. We have this general direction, artificial intelligence, and then we bend it in many ways to make it fit on the tasks that are at hand and we tend to diverge from the original question and we tend not to formulate this question in a very precise manner. And on the other hand we fall into methodologism, that is we develop a method, we find a method and then we find the intricacies of these methods between communities, sub-communities, that we publish in these communities, and eventually these methods do not add up again. And that's why we do not get unified architectures. We do not get whole pictures, but rather we get individual models. And these mechanisms that we build tend to be ungrounded. They tend to be very often symbolic. They do not scale up. They're confronted with real-world environments. We have too many of these approaches in AI. And on the other hand, we have too many of the dumb robotic approaches. We have lots and lots of interesting work with robots, which has a lot of value of its own, just as the symbolic processing has a value of its own, but not with respect, maybe, to get into the goal of general intelligence. And typically, for instance, we lack to integrate motivation with the representational structures in our AI programs. And also, and maybe most of all, AI suckers, not only from a lack of funding, but from a lack of prediction. Maybe these go hand in hand. Okay, what can we learn from that? First of all, we should aim at whole functionless architectures. Now what do I mean by functionalism? What you can see here is something like an MRI image of the combustion engine. It's not an MRI, which is an infrared image, but there's a big similarity, and the similarity doesn't stop with the colors. If we look at this in motion, we see lots and lots of very, very interesting things. Can you see up here to the right, there are the neuronal neurons. And we can see how they work, we can see if the thing goes at different speeds, how this image is going to change. We find very, very strong correlations. It's very descriptive. We also find that there are certain defects in the machine. If the combustion machine doesn't run the way it should do, we will find certain correlations which are very, very specific and typical in this image. But the bad thing about this image is it doesn't give an explanation how it works, because it doesn't identify the function of parts. And what we do in cognitive science very often these days, especially in neuroscience,, that we mistake such an image for an explanation of how it works. But what we want to have is something like this. We want to have such an explanation. And of course, there's a lot of value with the fMRI image, but only in the context of a functional explanation. We need to have a functional explanation to impose it upon the picture to the right and see what these individual parts, what we see in the description, actually mean. And in order to get there, we need to have a conceptual decomposition of the whole thing, into its parts and entities, and how they work upon each other. upon each other. Second, second lecture would be that this question that we have, how to achieve general intelligence, define the method and vice versa, it's not going to help us to have all these different methods and then try to accumulate them or to unify the different areas of AI. If we go to an AI conference these days, we will have papers on description logics, on agent technologies, on robotics, on game theory, on semantic web, and many, many different areas, and it's not as if we are someday going to merge all these things into artificial development divisions. It's not going to happen. What we need is, we have to define our methodology according to our criteria. Pretty much the same thing as Glenn Gerstle just said. And he should, just as Glenn Gerstle said, aim for the big picture, not for narrow solutions. You need to have a conceptual decomposition. This is an example by Aaron Slowman. And we should have an understanding of the individual parts which we have to interact with such as the reactive processes, interaction with the environment, the development, meta-management, how a persona comes about, what the theory of mind really is with respect to other agents, how memory works, how perception and action are interlinked and so on. works, how perception and action are interlinked, and so on. We should also aim for grounded systems, that is, systems which do interact with some environment. But on the other hand, we shouldn't get entangled in the philosophical simple grounding problem. There is nothing mystical in reality which we have to touch in order to gain intelligence. We should also remember that many, many embodied systems which we find on our planet with the vast majority of them, is not capable of general intelligence. On the other hand, we have very, very intelligent beings like say, Stephen Hawking was a thought embodiment of a fridge who is very intelligent. So embodiment is important, but embodiment doesn't necessarily mean that we have to be in touch with the robotic environment. We need to have representations, though, which are adequate to represent the world and to tackle the combinatorial explosion that comes with that. We need to have representations which relate to how we interact with the world, not just abstract symbolic descriptions. And we should not wait for the rapture of robotic embodiment in order to get there. Robotic embodiment is very, very costly, not just because of the cost of robots, that has shrunk it down considerably, but in order to get the robot to do basic things, we are going to spend a lot, a lot of time and focus. I've done, for instance, robotic soccer for a few years myself, and I've learned that if we do this robotic soccer, we have a very close environment with a limited number of representations which we encounter, and it's not as if we just arrived there and we just conquered the field, and then the task was solved in the same way as chess was solved. It's a very interesting task in its own right, because we have a discrete environment with many unforeseen features and so on. But eventually, this is probably not going to end up in general intelligence, but in some kind of an insectary solution, which is very good at playing robotic soccer. And the environment which we have there is considerably less complex than many virtual environments I've been playing with. So, a big challenge which we have here is to find benchmark problems for AI. We want to have benchmark problems which need to be AI hard, which require intelligence in order to solve them, and still they should be incrementally which need to be AI hard, which require intelligence in order to solve them, and still they should be incrementally solved. It's a very difficult task, just looking for benchmark problems. We should also aim for autonomous systems. Intelligence is not something which is related to problem solving, it's about finding the goals in the first place, finding the goals in the first place, finding the problems in the first place. Intelligence is not the answer to some resource allocation problem, rather it's a very complex control task. And we are organisms in a world which poses many, many different demands. And the, how to satisfy these demands is not specified in the moment we are born, but rather it emerges later in the way our interaction with the environment shapes us. And from this arise goals and motives and this shapes the way our cognition works, the things which are relevant, the things which are relevant in our own mental representations and so on. So we need to integrate these mechanisms into our AI systems, and this is something which I strongly believe. And also I don't think that intelligence is something which will appear in our system by some mystical emergence process. Just if we amass enough computational complexity, then suddenly pops up, some kind of intelligence pops up and then it tries to achieve world dominance and tries to wage a war against mankind or at least capture some movement on its own and do all the things that we find in sci-fi movies. These are things which probably have to be built into the system at least at some level. We have to identify the components which are responsible for things like personhood, social relationships, for phenomenal experience, for what we define as being conscious. And we have to take all these things and decompose them. And then at some level we have to realize that in the system. Okay, our own group has taken or tried to take these lessons and come up with a relatively simple, start point architecture in the course of the past seven years, which we call Micro-Psy. It's based on a theory by a German psychologist, David Dietrich-Dörner. The theory is called Psy-Theory, so we call this because it embodies a subset of the goals by a German psychologist, David Dietrich-Dörner. The theory is called Psi-Theory, so we call this because it embodies a subset of the goals of the theory, micro-Psi. And it uses a unified, universal body of representation. It integrates motivation in this system, and it creates autonomous agents which navigate our lab floor or which navigate virtual environments in order to pursue their goals. And the whole system consists of neural network simulator for the level of perception, which is integrated with, it's a special case in the more general semantic network for realism, which is in which you can define agents and run them. And then we have component which gives us multi-agent systems, so we can have many agents interact, have them interact, and conform them with environmental tasks, which they can build representations and learn. And we have modules, so we can implement the same structures and use the same structures on robots. so we can implement the same structures and use the same structures on robots. The whole thing is built into a big architecture, they can define by the size of the room or less, and this theory asks, what are really the components of a mission? What needs for something to be human-level intelligent or beyond that, or intelligent per se, What models do we need? The same is based on a unified mode of numerous symbolic representations, which work by spreading activation and are hierarchical, and on the lowest level are connected to sensors and actuators, thereby concepts can encode the structure, or relatives, and patterns which are presented by the environment and thereby acquire the meaning, the relationships with respect to the environment. And the individual parts are, for instance, threshold elements or more complex nodes. And the whole thing is integrated in this motivational system which learns by different kinds of pleasure and displeasure, and has neural learning and reinforcement learning methods. And it works by starting out with a basic set of predefined demands that the system has. Each of these demands corresponds to a drive to satisfy this demand. So the goals of the system are not predefined, but the demands of the system are given by the inherent structure of the system are not predefined, but the demands of the system are given by the inherent structure of the system, very much as our demands are given by our relative makeup. These are, of course, physiological demands for nutrition and for physical integrity, for instance. Then we have social demands. These take our interaction with others and make us be interested in others as agents, not as objects, and make us conform to social norms, also with respect to our own conceptualization of our own personality and ourselves as a being. And we have cognitive demands, and these cognitive demands are, for instance, a desire for the reduction of uncertainty, and another one to be competent in solving problems in general. And these shape the exploration of the environment, not only of the physical environment, but also of our internal cognitive environment. And together, they create a dynamic system which constantly changes its goals in order to build more complex representations of environments to solve the tasks better and to maintain survival of the individual. And if we look at an example of representation in that system, we have hierarchical representations which relate directly to the patterns of the environment, build a record of representations over them, and use others as symbols to get access to them when you need them, for instance in communicative situations. And these are embedded in situational descriptions of the environment which are acquire through learning. And they gain the relevance by the connection to the individual demands of the system. For instance, by the order. In this example, he might be afraid of dogs because he might be bitten by one. And on the other hand, he might be liking dogs because they are able to satisfy our demand for affiliation. This together creates meaningful representations of the environment, which help us in a different context to only get those representations which are relevant or achieve a goal and have the potential to limit cognitive complexity explosion that you normally would have if you just have an arbitrary non-relevant and smart representation of the environment. So that's it.", '9.752386331558228')