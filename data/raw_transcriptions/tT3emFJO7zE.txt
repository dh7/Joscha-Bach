('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/tT3emFJO7zE"> </iframe> </center>', " Bach is a scientist and an AI researcher with a focus on how computational models of cognition uh oh sorry uh I have a typo in my bio sorry anyway uh Josje is looking into how humans and machines process information um very simplified but he will go into that, I'm sure. He has thought and worked in AI research at various universities, science centers, media labs, and he's currently a principal AI researcher at Intel Labs in California. So it's very early morning for him. We really appreciate that you're here with us today despite. Yusha is mainly interested in using the methods and perspective of artificial intelligence to understand the nature of minds and their relationship to reality. So you didn't hear it, but you will have a chance to listen to it later on. I think it's a very good follow-up presentation to Anna's lecture as well. And we're very excited to hear what you have for us. Thank you. And I share my screen or should I just not use slides? So hello to everyone from California. Does this work all right? Can you see my screen and me? We see the presentation slides on the left still, but we see the main ones as well. Oh. Let's see. This is a little better. Yes, but it's not what I want. This is not only the slides, you see the presentation mode? We see presentation. You see only my presentation? No, we see the presentation mode. Okay. So everything is good now. Well, we don't see the presentation in full screen. We still see... Okay, that's unfortunate. Let's do something else then. Let me just share my entire screen then. Maybe this works. This is perfect actually. Okay. So my perspective, philosophy is the land of all theories that exist. Philosophy is conducted in natural language and is based on human understanding. It's one of the big intellectual root nodes that we have and is based on human understanding. It's one of the big intellectual root notes that we have and we want to understand reality. Mathematics, on the other hand, is the domain of all languages. It starts out as the simple languages, which you can formalize. It starts out with logic and numbers and shape and space. It's very difficult to say something that is true and provable in philosophy, because the languages that we use to make sense of the world are so vague, and many of the words are not properly defined. And on the other hand, it is very difficult to say something very meaningful about the world in mathematics, because mathematical languages are so simple that it's hard to capture the reality that we are in and the relationships that we stand in with other people, with our own life and so on in mathematical languages. And to make progress in philosophy, we now have wrapped most of the low-hanging fruits. Some say that the last breakthroughs in philosophy happened almost 100 years ago. And so to put there that we can go beyond what we can express in philosophy, we probably need to close the gap between the languages in which you can prove things and philosophy. This project is very old and for instance, has been suggested by Leibniz, who came up with the idea that we build a calculus, that we can express all ideas in a way that we can calculate whether they're true or not. And of course, back then, when he started this entire project, there was no way in which it could be executed, because humans are not able to monitor the calculations that their brains are doing. And in some sense, we need to automate this process. Also, there turned out to be a problem with mathematics are doing and in some sense we need to automate this process. Also there turned out to be a problem with mathematics as GÃ¶del discovered in the last century. He discovered that you cannot build a computer, even in pure mathematics or in any kind of universe that runs all of the existing mathematics without crashing. There was a bug in the way in which mathematical languages are defined. And Alan Turing later discovered that you can make a computer that can maybe not do all of classical mathematics, but it can do all of constructive mathematics, which is computation. And it turns out that the mathematics that we can use to describe reality does not contain infinities, it doesn't contain infinite loops and some other concepts that have been introduced as artifacts of classical semantics, but computation. So basically we can describe the world using automata, using hypothetical mathematical abstract machines that describe things as state sequences and transitions between these states. And Turing also found out that all the computers that we can build in this universe have the same power. The only difference between them is how much memory and speed they have, and of course what kind of graphics card interface they have to the environment, but they all can compute the same function if you give them enough memory. This means that everything that is a machine is in this category, and our brain is also in this category. It turns out that our brain is, as far as we can tell, based on these theories, a computational machine. It runs on some level, some code, there is some software happening, and maybe we can build computer systems that are doing the same thing. And this idea started the project of artificial intelligence. And this artificial intelligence is multiple things. For many people, artificial intelligence is just the automation of data processing. And this is the majority of what the people in my field do. But it's also the most important and most risky project that there is in philosophy. It's the idea of building a mind that is able to think about reality, to bring the languages in which we understand what something means, and the languages in which we can compute truth together. To do this, we need to understand how our mind grounds our meaning in elementary operations. Wittgenstein came back from the war and in the trenches of the First World War he thought about how to make philosophy better and he wrote his ideas down when he came back in the Tractatus Logico- Philosophicus which is a very beautiful book, something like 75 pages. It's almost like a poem. It's a philosophical book without any footnotes or any argument to convince anyone. It's just a single connected thought. And this thought, he tries to express how to make a computer language for thinking, how to clean up the natural language in such a way that it becomes formalized. And this project is very similar to what Minsky and others tried to do more than 30 years later. And in the end of his life, Wittgenstein found that this project had failed, because he doesn't know how to describe meaning in this language. And it's mostly because he didn't know how to deal with images, with perception, and with the reality that we experience in the context of computer programs. Formal grammatical languages are somewhat easy, but extending this into the world of meaning that we experience is hard. A little bit later, one of his pupils, one of his students, Alan Turing, came up with the idea of a Turing test in the context of a paper that he wrote in 1950. A core of this paper, which is very readable and everybody should read it, it's quite beautiful, is, can we make a computer program that passes as human and how would we know that we succeed? Most people have heard about the Turing test or know it intimately. It's the idea that you will sit on some computer terminal as a human being and you test whether something talking to you is a human or some kind of program. If the program can successfully pass as human, you should grant it being intelligent. I don't think that this is itself a very good condition. I think you want to do an in-depth interrogation that really checks what kind of faculties it has in the mind. And part of that is that you check whether it plays Turing tests on you, right? We do Turing tests on other people all the time. We try to find out what they understand, what they are conscious of, what they are aware of, to which degree they understand that we are minds and they are minds, and which faculties we possess, because we don't have faculties in every domain, we don't have awareness in all domains. What is a mind? I think a mind is a system that has general intelligence and intelligence is the ability to make models. Models help us to make the right decisions and movements. And most of what we've done in artificial intelligence does not fall under the category of building minds or general intelligence. It's narrow AI. It has only one model. It can only do one thing using this model. And the general AI can make its own models. It's able to discover, to learn, how to learn. And so it can solve the questions that are important to us, what is going on, what are the things that I can know, what am I and what should I do? An important notion for me is the idea of sentience. A sentient observer has an inner world that explains what it is happening in reality right now and it has a model of itself, because that's part of its environment. As a result of understanding its relationship to the environment that it's part of, it is capable of knowing what it's doing. Humans are clearly sentient. Many of them know what they're doing, but are cats sentient? Let's look at this cat. Here you see, it's a video that went around on the Internet last year. You see a toddler, but you cannot see if the toddler is going to some stairs that are very, very steep and the toddler is about to crash down the stairs and hurt himself or herself very badly. The family cat sees that and catches the toddler and beats the toddler back into the room to make sure that the toddler doesn't fall down the stairs. I think this is remarkable because this cat knows what it is. The cat knows what kind of abilities it has. It knows its place in the family. It sees itself as some kind of family member and has some responsibility for a family member that would fall down the stairs. So it's able to understand the intentions of that agent, of this toddler, and the capacities of that agent. And it knows that there is something in the future of that agent that it needs to intervene with to prevent something really bad from happening. And so I would say that this cat is clearly sentient. This cat knows what it is and it knows what it's doing. But the cat is probably not generally intelligent. It's not capable of understanding what a mind does, for instance. It's not capable of learning a grammatical language in the way we do. A generally intelligent agent is sentient, but it can also explain how it works. If we are able to build a general artificial intelligence, this means we have created an artifact that demonstrates our understanding of how our own intelligence works. Before we have created this artifact, we don't really know if we are indeed generally intelligent, if we know that what intelligence is, what we are, what our deep relationship to reality is. How do we perceive reality? Typically, we look at the world and we have ideas about the world, and we describe this distinction about the world in some dualist conception as a mental domain and a physical domain. Right? Dualism proposes that there are two worlds, the world of the physical interactions and the world of the ideas. Descartes calls this physical world res extensa, the world of the extended things, and the mental world res coccitans, the domain or the substance of thought. And as opposed to dualism, we also have idealist traditions, which say that, hold that there is only a mental world, that we exist in some kind of dream world, and that reality as we perceive it is a dream that is dreamt by a mind on a higher plane of existence. And this would explain why magic is possible, why things are possible that are not compatible with physics, like conscious agents and so on, which we cannot explain in a straightforward way using physics. And there is another intellectual tradition in philosophy which is known as materialism which holds there's only a physical world and in this physical world there are machines like our brains and these machines are capable of producing models and some of these models are mental models that give rise to consciousness and experience and so on. And there is a way to bring these two perspectives together, because I think that they're both incomplete. I think that both the physical world that we experience and the mental world, in which we think and reflect about that experience, exist inside of our mind. They're both dreamt by a mind on higher plane of existence, and this mind on the higher plane of existence and this mind on the higher plane of existence happens to be in the skull of a primate in the physical world. The physical world is nothing that we can experience. It's just some weird quantum pattern generator that we don't have direct access to, but that projects patterns at our systemic interface and it's our mind that creates both the mental world and the physical world, is mental worlds. In this perspective, we understand that we live in a dream that is generated by our own neocortex. What's most interesting to me as an AI researcher is how we could build machines that generate dreams in which they have a model of the universe that dynamically changes according to the features of the world that they're entangled with, in the same way as we are entangled by our body surface and our retinas to the world. Our brain computes functions to predict what's going to happen next in our retina and our body surface and inside of our own models in our own brain, right? We also predict ourselves. and inside of our own models in our own brain, we also predict ourselves. In this stream, that's basically generated, very similar to music is generated. I think it's a good metaphor to use a synthesizer. Synthesizer is a machine that generates sounds by building a circuit of a lot of oscillators that you connect, and by fitting the dials of that oscillator, you can learn how to produce almost arbitrary sounds. With a little bit of experience, you can use the synthesizer to make the patterns and sound that you want. It's a good way to think of neurons being oscillators that are being tuned by changing the parameters of these neurons, by changing the synaptic rates between them in the way that they are addressed by other neurons and the chemical signals that they send to each other. You cannot only use this for the sound domain, but you can also use this to represent colors and spatial frequencies. Once you had extracted the low-level patterns, you can try to look for higher-level patterns, for patterns in the patterns. You might discover, and you can do this also in a synthesizer that you build up step-by-step, that there is a way in which you can combine the low-level patterns into higher-level patterns. For instance, you understand different sounds as being the same sound at a different pitch, or you understand different sound sequences as being the same sequence with a different rhythm. In this way, you can build up structure that is going in the direction of person. You go from low-level features to compounds of these features that produce perceptual content already, something like textures. Then you can combine these textures into dynamic scenes. These dynamic scenes are in some sense simulations that at a high level of abstraction represent what might be going on in the world around us. Then we can build conceptual abstractions over the scenes that we see, which basically note what is in common over the different scenes that we experience. Then we can linguistic abstractions, like words like cloud and wall and floor and so on, that we can use to label the concepts and that we can use to organize our own thoughts and to communicate our thoughts to others, and to develop thoughts that other people suggest us and build new concepts in mental simulations based on linguistic representations. It's all done via the biological neurons. I think that we sometimes don't give them enough credit. They are like little animals that try to survive in this dark confine of the skull. In order to survive, they have to link up and collaborate and move the organism around as a result. If they don't do this in the right way, they will die. It's the only chance that they have. They need to form an organization that processes information in the right way. But the individual neuron is not very smart and is not entangled with the world in a very rich way. It doesn't have that many signals. At best, it can receive electrical impulses from a few thousand other neurons. And there are trillions of neurons that it doesn't talk to, and there is no world that it can talk to. So it's basically like a submarine in pitch darkness that sometimes hears the ping, and then it has to decide whether to respond with its own ping. It lives in a pretty lonely existence in a very dark and lonely universe in a way. The only thing that it can do is learn how to fire based on which combination of pings it hits its environment. This then checks what kind of reward it is getting. The individual neuron is a little reinforcement learning agent that links up with the other ones in two units. The basic unit in the neocortex seems to be a cortical column. The cortical columns can connect to each other in patterns that they can assemble and disassemble like Lego bricks. Each of these cortical columns is made of a few 100 neurons, and we have something in the order of a 100 million mini columns that are, in some sense, combining a little function approximation, a thing that can learn functions, and some message-passing machinery that allows them to link up with the rest of the brain. They're organized into areas. Each of these areas is dedicated for processing certain classes of functions, and they connect up in some hierarchy which the different cortical columns have receptive fields in the other hierarchies and project into other hierarchies. And this forms some kind of architecture for computation. Or you could see this as an orchestra with something like approximately 50 brain areas or 50 instruments. And the instruments are listening to the instruments in their neighborhood, and they are taking up the themes and melodies that they hear in the neighborhood and modify them and pass them on. And this quartical orchestra is playing a symphony that we perceive as a model of reality and ourselves and the interaction between ourselves. And this orchestra is some kind of conductor. This conductor lives in the dorsolateral prefrontal cortex, as far as we know, mostly, or this know, most of its representations are being stored. This is an instrument like the others, that is not perceiving the music of the orchestra as a whole. It's singling out a few of the instruments and listens to them, and synchronizes the activity or fix the activity and gives feedback to them. This is the conscious attention that we have. This conductor stores what we learn in an attentional protocol, to which we can later have access and that we allow us to generate a conscious experience. This conductor is producing an integrated model of our attention. An important thing to understand is that we don't experience physical reality, but we experience as a trance. Whenever something appears real to you, it's a trance. It's because something in your brain trusts that something is the way that's another part of the brain says so. Neurons themselves don't experience reality. Brains cannot experience reality. They're just physical mechanisms that we model in our own mind as separate units that are separate from other units and that interact. And in order to experience a reality, the brain needs to make a model of reality and an agent that is experiencing it. The brain does know what it is like to be a person, but it needs to know it. It's not a person, right? It needs to have a model of what it would be like to be a person so it can move an organism through a world of other organisms. And for doing that, it creates an experience of that thing. This experience is virtual. Consciousness is not a physical property of things, it's a virtual property. There are some neuroscientists which think that simulations can never be conscious, so computer programs could never be conscious. Consciousness is only a feature that real things in the physical world can have under very, very specific conditions that we don't understand. They got it exactly backwards. Physical things cannot be conscious. Only virtual things can be conscious because consciousness is a virtual property of a simulated system. You can only be conscious in a dream and our brains are best understood as machines that dream. And what interests me is how can we build systems that we entangle with reality so they can create their own dreams? And then how can we talk to them? So this is my input for today. And I hope that we have some time left for questions. Thank you so much, Joshua. That was really, really interesting. If you have any questions, please put them in the chat or you can also unmute yourselves and ask Joshua. But maybe I'll start for now. I was really intrigued by that video of the sentient cat and how it kind of realized the dangerous situation and recognized that there's something that it needs to do. And I was wondering if you know of any implications of our science into how the brains of animals can understand this better, that can help us understand better how they perceive reality or is it just kind of like a dumbed down version of humans? I suspect that for the most part it is indeed you could say a dumbed down version of humans and it's dumbed down because the not just because of the size of the brain of the cat versus the human brain but because of the length of the childhood. We train our human brains in some sense layer by layer. If you have a child, you can observe how as an infant it changes in certain phases or periods, that is, after a few weeks it suddenly undergoes a dramatic shift in its behavior and experience of reality. You can see that the next layer is coming online. And the frequency at which these new layers come online is going lower and lower. So at some point, there's a big shift around 2, 2 and 1.5. Then there's another one around 5 or 6. Then you have one that is on the onset of puberty. There's one in later adolescence. There's one in your mid-30s and so on. And maybe there's another one in your 50s. And it's like there is no end to the layers that you can make to model reality. And the time that it takes for us to become an adult that is able to feed itself is very, very long compared to most other animals. A gorilla is able to move into their own home at about one and a half or two years old. And a human takes something like 15 years or longer, even in ancestral environments, before they can find more food than they consume. So it's very, very expensive to have this long childhood during which you are functionally insane because you don't have a proper model of your place in reality and how to deal with reality, even though your body would be capable of doing that. It's your mind that is not capable of making sense of reality when you're young. You don't notice this, because how would you, right? You don't have any checks and balances that tell you what you don't understand. And I suspect that becoming a full adult is something that even the majority of human beings don't manage. I suspect that becoming a full adult is something that even the majority of human beings don't manage to achieve, where they become aware of how they construct their own model of reality. Psychologist Robert Keegan suggested there are crucial phases of development and most people stay in level three, which is where you make your reality based on what other people in the world around you think. So you form your ideas based on observing what good people think about the world and you assimilate these ideas via empathy. And when you are in agreement with the people around you, especially high-status people like your boss and your priest or your scientist or your leader, then you are good. At stage four, you understand that what's true is completely independent of what other people think is true, and you discover that something should only have confidence to exactly the degree that it's supported by evidence. At this point, you get agency over your own beliefs. by evidence. At this point, you get agency over your own beliefs. At stage five, you understand how identity is constructed, what values are, that values are generated in your mind, and that they're instrumental to something that you are serving, and how you can make sense of what it is that you are serving, and how other people do it, and what a greater place in the world is construed to be for yourself. So at this point, you become transformative. You get freedom over what you are. And this is something that maybe only a percent of people get to at least according to Robert Keegan. And I think that we can have intuitions of this before we have abstract models of this. And the cat has much faster development than us because it has a much shorter childhood, and the layers of the cat come online after a relatively short amount of time. This means that the cat does have a theory of mind, it has a model of sociality and so on, but these models are based on priors that are wired into the brain of the cat. The cat has good intuitions of what it should explore. And because each of the faces is so much shorter, the cat sees less training data than we do. So the cat cannot form the same level of abstraction. And I think the same is probably true for the great apes. They have brains that are in a similar region already than as human brains. So gorilla brains are smaller than human brains, but there are some humans which have brains that are smaller than the brains of the largest gorillas. They are completely capable of participating in human society and be relatively smart. The difference between us and the great apes is probably mostly the length of the childhood. Could be that there are some biases that makes them more interested in learning grammatical language or to perform certain tasks in the world. But I suspect that most of the capacity that we have is only the result of us spending more time learning them. Thank you so much. We have some 15-year-olds in the audience as well, so it's definitely maybe a step forward for them to learn about all the stages. But I have some other questions that I want to read out loud so that the stream viewers can only hear them. Anna Henshall was asking, are androids dreaming of electric sheep or as differently when the AI is dreaming, will it be like something we are dreaming of? Thanks for your talk. It depends on how you are entangled with reality and it's unlikely that the AI is going to be connected to the world at the same level as we are in the same way. The way you perceive reality depends on what kind of reality you are bound to. The world that you notice is one of mostly interactions with photons on your retina and with atoms that push against your skin and your eardrums. These are electromagnetic interactions between the different atoms and molecules. All these electromagnetic interactions take place in a three-dimensional space in a certain range of time and space at which we can resolve the world. So we don't see molecules and we don't see nation states or organizations and so on because we are not directly connected to them. We have to infer their existence. And if we build an AI it could be for, connected to the stock market and all the data sources and the entire internet, or it could be connected to all the sensors in a factory. And it would be connected at a very different timing. It probably have a higher temporal resolution than we do. And this means that the world that it perceives is a very different one. We would not be living next to it, but maybe inside of it. We might be the gut flora of some of the AIs of the future. Such a system would be seeing the world from a very different perspective. By the way, also, the humans form next-level agents. If you are part of a group that is very synchronized, then the group has a coherent spirit in a way. And the spirit is, I think, just the old word for operating system for an autonomous robot. And this was invented before we built autonomous robots ourselves. So the only autonomous robots that were known for people and animals and plants and cities and nation states and ecosystems and so on. And then when they have coherent activity, you can describe them as having some kind of software that coordinates the parts of them. It's a pattern that you see in coordinated activity. And if this pattern has a direction, some agency, then we notice that there is some sliver that animates the things. It's a projection that we make to understand how something moves around in the world. The same thing is true for us. We see each other as spirits emerging over the activity of the neurons, and only the neurons are real. The spirit that we see in another person is a pattern that we recognize in the activity of other neurons, of other people and their interaction with the environment or in groups of people. Anna, I cannot hear you. Okay. I'm muted now. Good. Marion is saying that she really likes the idea you mentioned that we could in a way see human interaction also as humans doing a true test with each other, testing their being human, and it kind of connects to what Simone is saying here, if we stick with the cat example and imagine that the cat was a human but it did not intervene, it wouldn't do anything to save the baby. What would that have told us about the human? Yes, it might have told us that the human doesn't care or that the human doesn't understand the situation. So there are two elements that go into this. One is the relevance of the situation that tells you this is this network of relationships that you're part of and that you are serving. The other one is to which degree are you able to understand the consequences of your actions and the events in the environment? Thanks. I think that there are no other audience questions, I'll leave a bit of time because I think this is a topic that needs digesting and maybe ideas pop up a bit later. You can also criticize things that I said or say, oh, I don't get any of that or this is nonsense or you can ask something weird. Go ahead. You can also unmute yourselves if you want, you don't have to write in the chat. But I think you show you're super confident, difficult to criticize. But we have a question. I always wonder how realistic the thought of AI is transforming visual impressions into proper procedures or reactions. Yeah, it's a very good question. For many people, the idea that we could build a machine that has experience of the world in a way similar to us, even surpassing our abilities and skills, and maybe the depths of our conscious experience is preposterous, right? It's almost an insult to the way that we perceive ourselves. And that's maybe something that is specific to our own culture in the way that we perceive our own minds as some kind of supernatural miracle. And I wonder what it would mean if something is fully supernatural, everything is eventually natural, everything is nature. If you perceive a supernatural power, then something must implement and realize this supernatural power. Even if it would turn out that this universe that we are in is some kind of simulation, magic is possible in this simulation, something must run the simulation. There must be something underneath it. The big question why there is something rather than nothing is very puzzling. Maybe everything possible exists and we are in one of the corners of existence that might exist. But so from this perspective, the universe is some kind of machine and we don't know what has put it there and why it exists. And we also don't know what the lowest level of that machine works. So we don't have a completely foundational theory of physics that goes far beyond quantum mechanics or theories that would generate the phenomena that we describe in quantum mechanics. Maybe this is a puzzle for which we need to build machines because humans are just not smart enough to solve it. But I suspect that we can solve the puzzle of how our own brain builds models of reality by building something like this mental orchestra. When you perceive the world, there are blips on your retina. These blips are generated by photons exciting nerves in the retina. What your brain is trying to figure out is the relationship of these blips to other blips on your retina. This is the meaning of information. Information is discernible difference. The meaning of the information on your retina is the ability to predict other blips at different times, or at the same time that are in some connection to them. You put this together into functions in your brain, and you perceive these functions as, for instance, people moving around the three-dimensional room, or the sun shines on them, and they talk to each other about ideas. This is the construction that we need our machines to make. We need to be getting to the point where we connect the camera to information processing system and it's capable of finding an organization that allows us to make sense of the blips on the camera sensor in a similar way as we make sense of the blips on our retina. Talking about things that we don't know, what is the biggest question currently for you in your work or research that you would absolutely love to find the answer for? One of the questions that I find very interesting is how do perception and the symbolic mind relate? Is the symbolic mind something like a specific kind of perceptual thing that is entrained on the perceptual mind? Or are they somehow using separate circuits and work according to different functional principles. I suspect it's deformer, but this interface between perception and experience is a symbolic organization, is something that is deeply interesting to me. There are other puzzles, for instance, the puzzles of physics that I find very fascinating and I feel that I'm ill-equipped to solve them. There seems to be a limit to what our human brain is doing, how many levels of meaning we can connect to each other in one integrated model and not lose track of the different ideas that we pass around in our mental model. In some sense, our brain is making simulations of the world and and we do this, and we understand our own mind and we understand physics. At some point, these simulations become so intricate that we need to take them out of our minds and put them into computer models so we can track what's going on. To build these computer models is very hard, so we need to have computer programs that help us build in these models. We start to automate our own thinking. And this is also something that is deeply interesting to me. How can I extend my own mind into machinery that has a fluid connection to my own mind? So I feel that still my motivation that goes up is controlling all this, and it's still part of me, but it extends beyond my human brain, beyond my human capacity in a seamless way. Sorry, just the last question maybe if there's no other ones from the audience. There's a lot of talk about generation A's algorithm and generation COVID. I guess now we are experiencing the world through simulations, through these Zoom calls that we're in, and I'm connecting with someone in California right this second, and I would like to hear a bit more about whether you think this actually makes any difference in our perception. Do kids who grew up with these screens that channel in different sorts of realities or situations to the same place make a difference in what kind of perception they will have as grown-ups maybe. Definitely, I think that every generation builds their own model of reality. And a big problem is that our future is changing faster than our models of reality can track it. And this means that our civilizational hive mind has lost the plot. We don't have a very good model of the future anymore. Our projections mostly end in 2100 when we know that climate change has become so bad that we don't know how to deal with it and we don't make projections beyond that. And most of our institutions of society do not plan further ahead than a few decades. And the pandemic has shifted this time horizon to much, much shorter. It's basically in a way which we organize our economy and make plans of the future. It's mostly just a couple years at the moment. So this is terrifying. And the current generation is from our perspective, the generation X called the generation Z. We don't really know if something comes after that. It's not as if you have left a lot of letters after that. I think that in a way, the bar generation for generation of World War II, still had intellectual continuity from the previous generations and ways in which they made sense of reality were shared. And after modernism ended, this perspective that built the big societies of the 20th century, which mostly crashed and failed, the communist societies and the fascist societies and the capitalist societies, which are all now either crashing or muddling through. This is all over now, right? And ever since, every generation has lost the plot even more and has less of a plan what to do with the future. We might have very strong intuitions and moral movements, but we probably need to build a new idea of how to deal with reality if we want to stay around as a species and that task is on the next generations. Thanks, Susha. This is actually I think a perfect ending point and thank you very much for your presentation. MBC ë´ì¤ ê¹ì§ê²½ìëë¤.", '8.16073727607727')