('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/Znb35oibx0I"> </iframe> </center>', " So, this time we set aside basically for a brief discussion which is supposed to be highly participatory, on the road map to AGI, or the possibility of multiple road maps to AGI. And I guess this is a topic that we've all been bouncing around for a while, with mixed results so far. And it is still something I think that is quite important for the field. What I wanted to do now is have basically each of the three of us just say a few words of our own thoughts and visions on the need and nature of a road map for AGI and then open it up for comments and discussion. So this will be a break in the sequence of PowerPoints, at least I didn't have one I may as well consider it beneficial. So yeah I'll get started, so the first endowment I had with an attempt to make some kind of roadmap for AGI was a workshop that, I'll have the chair. A workshop that Pat Langley and John Laird, two of the titans of traditional masonry in the US and cultural architectures in AGI, and Yosha, you were there in that workshop as well. And it was interesting, I mean, it was a group of all quite good researchers and the goal was to come up with some set of milestones and some ideas about a common environment and common tasks and common goals. And the most notable thing about that workshop was at the end of it we had agreed on pretty much nothing whatsoever, except we each agreed that our own approach was obviously better than everybody else's approach. I mean, it was interesting, there were good conversations, but it clearly wasn't leading to a roadmap to AGI. Now, out of that, Laird and Langley and a couple of their closer collaborators wrote a paper giving a list of what they felt were requirements for AGI and for an AGI environment, which was in the proceedings of AGI 2010, which was a reasonable paper, but it was really their view. It wasn't even the consensus of everyone in that workshop. Now, following on that, Ichmar Arel, an AGI collaborator of mine, who's a professor at the University of Tennessee in Knoxville. He and I organized another workshop, the AGI Roadmap Workshop, 2009, University of Tennessee, and I tried to more carefully pick the people who would be there. So I picked only people who thought that either virtual or robotic embodiment were reasonable, worthwhile and very useful things to do for AGI development, and I picked only people who wanted to build learning systems, no one who thought you should load your AGI with knowledge and a bunch of hand-coded rules in the form of production systems or expert systems. So I was trying to boil things down to people with some semblance of a common perspective, hoping that that would let us have some agreement on things and that didn't really succeed, we still disagreed and each thought our own way, it was better than everyone else's, we couldn't quite concur. Now it worked a little better, I mean we managed to squeeze out a paper, it was an AI magazine called Mapping the Landscape of AGI, which actually had all the participants of the workshop as co-authors and we all agreed on the conclusions. I mean the paper was relatively toothless compared to what any of us would have written on our own, but it did represent some kind of consensus understanding of what we're trying to work toward. And we, I mean we discussed a bunch of ways to frame the goal of human level AGI, which I wouldn't call the end goal of AGI because we could go way beyond the human level in principle, but we talked about things besides the Turing test, like say a robot college student test, where a robot could go to college and get a degree, or a virtual college test, or the test of having an AI go through an online elementary school curriculum and so forth. So we talked about various ways that we agreed it would be interesting to validate elementary school curriculum and so forth. So we talked about various ways that we agreed would be interesting to validate that you had either a human level AI or something along the path there. Josh Hall kept talking about the Wozniak coffee test. You make a robot go into a random person's house and figure out how to make a cup of coffee. Where, you know, they may have many different ways of making coffee, they may be stored all over the place, you might have to use a coffee maker or not, the coffee beans might be in some jar where it's hard to take the lid off. I mean, we all kind of agreed on what kind of end goals would be important, and we basically agreed on what general competencies and capabilities an AGI should have to have if it's gonna be human level and roughly human-like, while also agreeing there could be other types of AGI's that are not that human-like and were very different. What we didn't seem to agree on was what made sense to work on first, what made sense to work on after. So, I mean, some people thought, you know, start with language, that's what makes humans really distinct from other animals. Once the AGI can understand some language, you can teach a lot of this stuff by talking to it. And Josh Hall basically figured if you're not using a robot, you're just fooling yourself. You want to start with perception and embodied action in a robot, get that down like a young child does, and then work up to more complex things. And if you work on language, before you've had this robotic grounding, then you're basically just playing with formal simple systems in a way that has nothing to do with what your ultimate embodied AGI is going to have to do. So this is why the paper came out being titled Mapping the Landscape of AGI. Because we had Sam Adams from IBM introduce the metaphor like, instead of just one path, I mean a road map, a map has many different roads on it, right? So if you view AGI as the peak of a mountain, maybe with many little smaller peaks at the top representing different ways of getting to human level, different forms of human level AGI, then there can be different roads up the mountain to the top. One guy can go up the mountain via the robotics path, one guy can go up the mountain via the natural language path and put in a robot only at the end and so forth. And I was sort of happy we arrived at some moderate level of agreement on what made sense. What we couldn't agree on, unfortunately, was a common environment or set of tasks for measuring early stage incremental progress for an AGI. This was a major issue. I mean, it's not too hard to agree on tests for when you have a human-level AGI. I mean, the Turing test, say, one-hour Turing test with educated people is one example. Having a robot that can get a degree from Oxford following the same procedures as a human student is another example. I mean there's plenty of things that you would commonsensically accept as having achieved a human level AGI. But how do you know when you're 25% of the way there? What does that actually mean? How can you make a test for you part way to AGI that can't just be gamed in some conceptually trivial way and string together a bunch of different things? He said the IQ test. Surely someone could hack a system just to take the IQ test. And even Wozniak's coffee test, I wonder if you gave a list of every possible type of coffee maker ever made, and fed in the geometry of 10,000 American kitchens, and trained it on all those, could you use a combination of machine learning and expert coding to get something that would go in and make coffee in 95% of people's houses? I mean, it's hard to say which of these intermediate tests you could gain and which ones you could not. And so we didn't really agree on a series of incremental milestones where, OK, we all agree, but we'll make our AI systems pass this, then we're 20% of the way there. And go a little further, and we're 40% of the way there, because we each want to take different routes. I mean, the final conclusion I came to is, you know, we're not going to get everyone to agree on anything in the AGI field. I mean, psychologically, those of us who go into AGI tend to be very strong-willed, idiosyncratic type of people. You have to be very stubborn-minded to want to push ahead with AGI when most of the world, and most of your profession doesn't want you to. So getting all these stubborn-minded, idiosyncratic people to agree on something is quite difficult. But I haven't quite given up on this effort. And what David's going to talk about when I finally shut up is we're working together to make a robotics platform with very easy APIs for AI developers to use, and hoping if you release a low-cost robot with APIs that are very easy to plug an AI system into, then you can get more and more of the AGI community to get the low-cost robot, plug the AI into that robot, and have it go through a series of milestones and attacks for greater and greater robotic intelligence. So that's going to be interesting to folks who want to play with robots, and that's going to be interesting to folks who want to play with robots, and that's only a subset. And maybe someone else could do something like that in other domains besides robotics. But that's a somewhat unfinished quest, really, to find a way to get more and more of the AGI field to cooperate together on common environments, tasks, and milestones. Thanks, Ben. I don't agree with your pessimism as regard to the roadmap at all. Best of all, I don't think that all of the AGI researchers are egomaniacal and stubborn. I, for one, am very reasonable, I'm a very shy, humble person, and the only thing I'm just trying to officially say is the truth. And so, this is really not the problem. Rather, I think that some of the issues you might have experienced which fuel your pessimism about the possibility to agree on a roadmap might be due to the fact that the particular set of people that we invited for the workshops were project leaders of existing projects, which of course tried to further their projects to a large extent and tried to find an opportunity to expand on their projects, which is a very natural thing to do. But if we look at this from a different perspective, if we find a good project and we offer this project, it's pretty likely, if it's a very good project, a benchmark project, that people flock to this, precisely because it is helpful for them. There are a few things I would like to get off my chest first, because we are already making some assumptions with respect to the nature of the kind of benchmark that we need. And of course, even in this conference, we have seen conflicting assumptions. For instance, there is this mathematical speccing idea that you treat the mind as a reward maximizer and the kind of benchmark you would want to have for this is quite different from the one that we have in mind. Because we are looking at possible implementations, not at abstract specifications. And if the distance between specification and possible implementations becomes very large, we as engineers don't know what to build. And there's the neuroscientific perspective, which has a tendency to treat the mind as a product of neural or sub-neural activity. And there is this problem that you look through the microscope and you look at all these intricate things that neurons do. It's a little bit like you try to understand flight and then you look at birds. And then you look at feathers. And then you realize, oh, it's not just the feathers. There's the stems between the feathers. Every feather is connected to its neighboring feathers via tens of thousands of intricate stems. And these stems actually do have an implication on the vortices that the air is going to have when you move the wing. So actually, maybe you have to go to the atomic level to get a good understanding of what flying is about. How about core? You're probably not happy if some engineer comes along and says, but maybe you can build something completely without feathers. And if you build something with lots of feathers, maybe it turns out to be a penguin. It doesn't fly, even though it has proper feathers. And let's not forget, most of the things with a brain are not super smart. There's really also a problem with this developmental perspective when you start with simple robotic bodies and then try to make them move in the environment and so on, because let's not forget that on all this planet there's two billion years of evolution and lots and lots of different species. There are only Homo sapiens above the age of about three and Betty the crow, who are intelligent. So this means that maybe it is a very, very long developmental trajectory that you are going to take if you are with growth fiber and building things which are basically just critters in an environment and move about and hope eventually you can treat language and thought and writing novels and coming up with AGI as an extension of that, some kind of scaling process. So we already do have some assumptions that are a little bit counter to that. We have a functionalist construction instead of ideas. We see the mind as a information processing architecture. And in the case of humans, it's also more than that. It's a solution of a particular kind of control problem. That is, how to navigate a social climate successfully in a very complex environment. And these assumptions shape, of course, how we approach these benchmark problems. And traditionally in artificial intelligence, we had this tendency to see intelligence as some kind of particular function, like for instance, the ability to play chess. We had this idea that we make this function, this seemingly impossible thing, real. That we have cracked the nut, that we have understood intelligence, but we realized soon that we end up with a chess playing toaster. And then we realized, OK, we need to look at different functions that are working in an open environment that have vague stuff in there, like playing soccer, which are maybe social. And then we build soccer-playing toasters. And then we are building car-driving toasters, and so on. And it seems some kind of a consensus by now, a whole, that just by building better and better toasters, we are building our AI. And what we want to build is something different. We want to build something which is way more general. Then there is a different perspective of one of the psychologists, the psychological cognitive architectures. And they kind of reinvented a lot of what we did in AI and revived it after we killed it off in AI, more or less. And they tried to generalize over human information processing and learning. Their paradigm was, because you cannot sell theories in psychology, to pair each of their ideas up with an experiment. This experiment takes a human into the lab, or a small group of humans, and then you take tiny results and then you compare this with the result of your cognitive architecture. Unfortunately, it turns out that isolated performance in mental rotation tasks or the Wason four-part experiment does not scale up to general intelligence either. So I'm not saying that the cognitive architecture paradigm of the psychologist is not productive, it is tremendously productive, but we need to augment this with a different way of doing our research. And then there's the embodiment perspective, of course. In the extreme case, it's really the embodimentalist perspective. Don't do representation so much. Treat the mind as something which emerges from the interaction between body and environment and the brain is just a funny feature of the body. And I personally doubt that it's very productive because for the time being the environment is just what it affords, what actions it affords. So for instance, for a robot, the world is only that what it can interact with and our robots cannot do many interesting things. Apologies. And I hope you will have different opinions than me. But I think that right now, it's very, very difficult for a robot to climb a tree and to break down a little bit of wood there and to turn it into a tool or be inventive about the environment. But robots live in a very, very impoverished environment, which is much more impoverished than any kind of virtual environment you can build for the next decade or so, I guess. At some point, there might be a turnover point. But until this, I don't think that, personally, I think that robots are not the most productive paradigm. I hope that you disagree in a productive way. OK, so on the other hand, what I do see here is some kind of convergence. I do see convergence between roboticists, also use virtual robots, make bridges between virtual environments and real world. I see a convergence between AI methods and cognitive architectures. And there's also a big convergence among the ideas that we have among the cognitive architecture teams. And cooperation, I think, is no longer just limited by the egos of people, but it's mainly also a function of funding. So I imagine that we get funding for bigger cognitive architecture projects, that we will also see bigger convergence there. And I'm actually quite optimistic. So I do think that what we need are productive benchmark paradigms. It should be clear that we should have a paradigm that doesn't allow us to progress only in 20 years from now. So the college robot is probably out of the question. I don't see the prerequisites for building a robot that can go successfully to college within the next couple of years. I want to have a benchmark that I can address right now, where I can start programming coding now and give you results a year from now and comparable results. So it needs to be incremental. And it needs to be AI complete. I want to have a benchmark that forces us not to build a toaster or a coffee maker. I want to have a benchmark that forces us to build something that is actually smart, that we can talk to, that makes sense of the world, that can make sense of other worlds too maybe, that can make sense possibly in other ways as humans do, but which can do this. And I think controlling a body is something, of course, given a body in AI should be able to make something with it, but I don't think that it's necessary that it has a body in order to do that. Body control is an important issue. I don't think that the mind can be conceived of as being unable to control the body. But I can also imagine a mind that is a world prosthetist, like a built-in world of warcraft, that keeps it busy for all eternity so it doesn't go insane, and gives it enough room to play with, to come up with new ideas forever. So I think philosophically speaking, a physical environment or an external world is not a necessity, but it can be a very productive research paradigm and there might be very practical reasons to go for it. One more thing which we should look at is, besides scalability, is we do not want to go for adult performance. We want to go for a developmental perspective. And we also need to look at the funding perspective. We need to make something that is obviously scalable that the public understand, that funding agencies understand, that makes sense, that doesn't look insane, that is productive. So you're about the genius machine. All right, great perspective, guys. Well, I'm going to say that as dedicated as I am to developing hardware and to the value of robotic hardware, I think that a generalist perspective that integrates simulation and pure math and theoretical approaches with practical applications is the way forward. This sort of integrative perspective, I believe, is the only way. And taking that integrative perspective also means considering that our culture in inventing these things is an evolutionary ecology, and that includes business and funding cycles, and it also includes the ecology of memes or emergent memes, eems we might say, that play throughout an event like this particular conference or when a group of people get together and invent something new. So in that sense an interplay of the evolution of AGI with the real world is absolutely key to all the progress we've made so far and will make in the future. Now, if we consider the fact that physically embodied robots can interact with the noisiness, the chaos of the world, and then extract information from it, then we can realize that's how babies develop. And you know, Skinner boxes don't work for children, right? I mean, children need rich, stimulated upbringing in order to become intelligent beings. You know, regardless of the DNA, regardless, you know, in addition to, I should say, the DNA and the two billion years of evolutionary heritage that go into a child. So, if we consider this, it may be, and it's just a proposition, it may be that developing platforms that can interact with the real world in this kind of non-linear resonation and then extract that information and then use that data to build a more elaborate model of the world, maybe one very productive path forward. Now, not every researcher needs to agree with this proposition. What needs to happen, however, is an integration across community where researchers who don't agree can still plug their software into a standards-based system so that you can have potentially hundreds of thousands of researchers working on the same project from multiple fields, fields that they don't think interact with each other. And yet, you can have people who take an interpretive perspective and make these systems interoperate as a whole. If this kind of common platform and common approach is adopted globally, then you have something like a Cambrian explosion of research in AGI. You start seeing many diversified examples of AGI instantiations. And I believe that you will see a rapid acceleration of progress in the field. And I believe that we should not exclude any tool set. So just because I have a bias for physical robots doesn't necessarily mean that anybody else needs to, but shouldn't be excluded. And neither should simulation and neither should other approaches to the field. And what would be interesting is that if you can define these kind of common standards and common platforms, then you can benchmark the performance of these developments in various fields, whether it's computational neuroscience or you can benchmark the performance of these developments in various fields, whether it's computational neuroscience or computational linguistics or physically embodied cognitive robotics and start to compare those performance metrics to each other as well as the actual performance of the platforms. So then the performance metrics themselves begin to evolve. So and then within that, you wind up with a powerful set of common tools for developing products, for developing practical applications, and then testing those practical applications. So I think that underlying, if you were going to devise a total integrated architecture like this, you know, the goal being to have some kind of AGI performance within some period of time, like an Apollo space program, grand challenge for AGI research, then the critical things to happen would be you would need to define the overall platform and the team to develop this based on the grandest ambitions that you could specify. And that's where the term genius machines comes out because we don't want to just develop something that is a little bit smart. We don't want to develop something that is, I certainly would like to see more generally capable toasters. I mean, I sure would like a smart kitchen that has my coffee waiting for me in the morning. That wouldn't hurt my feelings. But I would like to see us aim past the performance of a three-year-old child. And if we are aiming for the performance of a one-year-old or a three-year-old, then we won't get to the point where our machines take their letters from Oxford and they go on to win the Nobel Prize. I think we need to aim for machines that have the capabilities of the greatest of us, the capabilities of human genius. And that's not mere intelligence, it is also creativity, we have to understand the mechanisms of creativity, the mechanisms of social relationships and ethics. And in so doing, we also want to aim past the greatest of human genius and sort of say, well, of all human genius that has existed, that's only a minor fraction of the potential human genius that could have existed. So how do you benchmark incremental progress to a superhuman genius? Well, I think what you do is you lay a path of continual incremental advances. So you have a continuum from today's sort of baby insect type robots. I mean, we have robots that may be able to understand speech with college level vocabulary, and I say understand with a tongue in my cheek there, but what I mean is that you have, you know, these machines that have like, insect-like performance, they're not smart like a baby, but they're smarter than a lot of humans in very, very narrow regards. They're idiot savants, effectively. I mean, they have, you know, the word idiot, she's on the first line. So the thing is, in your view, starting with these idiot savants and then progressing is a good idea. In other people's point of view, that's the wrong place to start, and you want to start with something else, and only get to language understanding of any level much later along the path. And that's the tricky issue that comes up sometimes in discussions of these things. Absolutely. We all want to make genius machines, but we want to take different routes to get there. Well, the way that I see it, if we look at the evolutionary landscape of possible approaches to realizing AGI, genius machines, then you sort of see little plots of development, like here and there. Some very low level, some based on fundamental evolutionary drives, and this kind of like dopamine motivation cycle, and then you have some that are very high level performance and sort of top-down in their architecture. But you also see these kind of tenuous bridges happening between these sectors, which when formed and when tinkered with, that tinkering is perhaps undervalued. Because if you sort of tinker them into something, then you start seeing often more general capabilities come out of the machines. I mean, just basically by integrating across these multiple approaches, then you inch towards generality, and then you are able to test in cycles. So again, the inclusive nature of the field should be appreciated. I think as far as a road map is concerned, I think that we shouldn't just map one little county in Kentucky or something, we should try to create as global a map as possible. And just because I may live in this little county in Kentucky doesn't mean that that's the only one that should be mapped. We should figure out how to get to New York and how to fly to London. New York and the White and London. Yeah. I think we should let other people. How should we do it with the microphone? Right around there, you want to do it in the microphone? I'll pick whoever's cute and start. Rick is pretty cute. Hi, my name is Rick Schwal with Saving Humanity from Homo Sapiens. So first of all, I must plead to have nothing near the education or expertise in the field that you gentlemen do, but I'm going to throw out only semi-tongue-in-cheek. Your benchmark test for the next couple of years is the robot taxicab. Now here's the thing, we're close with autonomous cars that pretty much work, I understand, but to actually be a taxicab driver you not only have to interact with the complex environment of the road universe, but your customers, okay? And so I would think you could get within a year or two to be something that was functional enough that you could be competing with each other, test where 20 test customers try the different taxicabs. Like Grand Theft Auto taxis. You know, but to really be a good taxicab driver you need to be able to hold down a conversation, right? So that you've got the level of, from your competing vote to be acceptable and to be great. And you can also show them movies. The taxi cab driver is in the finance. There's a real problem here. I think it may be an interesting test. One practical issue is that buying every amount of taxis to experiment with and getting the researchers permission to drive these taxis around in the street is there are significant practical obstacles, right? I mean, what David and I are hoping to do is have a low cost robot that a researcher, even a hobbyist could buy and experiment with and plug the software into the API. And that's hard to do with a taxi, right? I think that the taxi idea would be a wonderful concept for a business plan. And you might actually get some funding to develop this robot taxi if you have the right large corporation that starts with a G, that's already got self-driving vehicles. So towards a sort of common platform, so Ben and I are working to integrate a constellation of existing AGI tools and components that relate to AGI into a cognitive robotics platform that would generalize. So we have this small robot platform that we're developing that we think would be a really beautiful common platform, very low cost and also provide commercial opportunities for deployment through apps with developers in the world, through one of my startups, Handsome RoboKind. Really beautiful platform. But because we've open sourced the software and worked very hard to generalize this, it should work on many other potential platforms. So you can start to explore what kind of small robots might work in the marketplace and develop applications that would be more general so that way it's a lot easier potentially if you have this generalizable architecture to develop to develop you know toasters that that really want out of the box. More questions? I'd like to frame the question in a couple of ways that I think are novel. First off, I'd like to make the observation that the fact that we see of AGI researchers being unable to agree on benchmark tasks is not a mere consequence of the fact that AGI researchers tend to be stubborn because we fundamentally don't know which paths to the top of the mountain are the longest or the shortest or are blocked. We really have no idea which way to go and thus anyone who is doing anything useful and concrete is necessarily massively overconfident about which is the best path. And so I think we need to take it as a premise that each person with their own approach and their own idea of what part of the space of tasks that humans can accomplish is the right part to start with. And say everyone has their own benchmarks and so what we need is not a single benchmark but rather a meta benchmark, some way of categorizing how general a particular researcher's chosen subset of human task space is. I think we can do that by sort of formalizing, not in a computer science standards-based let's use this particular API sort of way, but in a mathematical formalization, saying that every researcher's set of benchmarks should be should be representable as a environment generator and that the the AGI should then be able to survive in some high fraction of the environments generated by that environment and then we measure is the fraction and the generality of the environment generator. And it might generate environments that look like virtual worlds. It might generate environments that look like IQ tests. It might generate environments that look like question answering problems, or any number of different sorts of things that people believe capture the fundamental aspects of what's necessary to build an AGI. But what we can measure in some sense is the height on the mountain, how much generality you have gotten to, and such that if you go all the way and expand your task subset as at the rate you're currently going that you will eventually encompass everything. I mean it sounds nice in principle, but in practice it's hard for me to see how you apply the math of general intelligence or to compare say the complexity or the generality of making coffee in the kitchen with the generality of say going through an elementary school reading curriculum. I mean we don't have a formalization of the real world and I mean counting the pixels doesn't work so it seems hard in practice to really calculate and compare the generality of these very different tasks. And there's going to be many different ways to do that math, each of which will favor some kinds of tasks over others. And then each researcher would naturally favor the computational model or the specific generality measure that rated their own type of task as being more general. I think it should be possible to do this sort of thing. But there is a second proposal that I'd like to make, which is, Wait, just before you make that second proposal, I just need to point out to everybody that lunch is currently on and ends at 1 o'clock. You're welcome to stay here and continue the conversation if you want to. But, well, lunch is on. And I guess the following presentation, I think that we should give the audience the opportunity to talk more, because we already know what we're saying. I want to know what you're saying. And let's spend the next 10 minutes on the panel, and let's see if some lunch is their choice. Is that all right? Go for it. Okay, so very quickly. Very well. Yn ystod y gwaith? Iawn. Iawn, felly, yn gyflym. Mae'n ddangos. Iawn. Mae rhai bobl wedi sôn. Dw i am ddweud un peth yn ymddigell y neurwyr. Felly, dwi am ddweud y gwnaethoch chi ddangos ffisiwn da iawn o'r hell ar ôl-ddifrifol, sef y bydd yn ddweud y holl eithaf yn y byd warcraft.g, sy'n ddweud y bydd y byd yn ddweud y byd yn dweud y byd yn dweud y byd yn dweud y byd yn dweud y byd yn dweud y byd yn dweud y byd yn dweud y byd yn dweud y byd yn dweud y byd yn dweud y byd yn dweud y byd yn dweud y byd yn dweud y byd yn dweud y byd yn dweud y byd yn dweud y byd yn dweud y byd yn o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'r ychydig o'n ymwneud â mynd i'r lefel melegol, ond yn dod yn ôl i ddarn o'r prínciplau gwych o'r cofnisiwn sy'n cymryd yr ymdrechion sy'n eu hadeiladu drwy ddarlith mewnfeydd mewn perthynas. Nid ydw i'n ei weld yn dod yno drwy'r ffyrdd mewnfeydd, yn enwedig, nid yw'n ymwneud â mynd yn dda iawn. Dyna pam mae'n ddweud i mi y byddai'r petheth da i'w wneud yw mynd yn ôl i'r ddau a edrych ar ddau. Rwy'n cael sylwad o beth y bydd y principau dda yn ymwneud â nhw. Rwy'n credu bod y peth yn ymwneud â dynamigau dynol, sydd yn amlwg yn llawn mewn llawer o'r architecturaethau AI mwy cyffredinol sy'n cael eu troi yn y ddau. Gadewch i Adam. Iawn. Helo. in the brain. I think Adam's on the screen. Okay. Hello. Yeah. So actually, look, it looks as though people want to collaborate, even though they come from different backgrounds and have different, different sort of philosophies and how all this should be done. This is not really working anyway. So this microphone. So, if you imagine the landscape or the roadmaps of possible roads to AGI as a, you know, some sort of big tangled Gantt chart, is there any way that you can collaborate? Is there any way that people with different philosophies, totally neuro-inspired, totally mathematically inspired, or somewhere in between, can they share some of these roads? Are there certain milestones that you can all benefit from? Are there sort of critical paths that you can all agree on? What about three benchmarks, for instance? Let's agree not on one path, but let's find out that there are some major different approaches, and not try to build a spaceship that had everything in it, including the coffee maker, but try to build three spaceships and see how far they go. I think it would be extremely useful for us to map in a public way with perhaps a graph visualizer attached to a sort of Wikipedia-like representation of the field of AGI research and AGI-related research that would then also link to developer sites. And then within this, if it was a regulated commented representation, you could then have proposed challenges. And it seems like a group like AGI Society is perfect for organizing this kind of aspiration of public representation, which would also be extremely useful for the field because researchers wind up getting fragmented and isolated and not being fully aware. So if you were able to then cascade through yn ddefnyddiol i'r ffyrdd, oherwydd y bydd y rhesywyr yn cael eu rhannu a'u gwblhau ac yn ddim yn ymwneud â'u gweld. Felly os oes gennych chi'n gallu casglu drwy cysylltiad fel hyn, byddwch chi'n gallu darganfod part ddau arall yw'r gweithwyr. Rwy'n credu, yn ôl i'r hyn rydyn ni'n yma wedi'i ddweud, yw ein bod ni'n rhedeg y cyfnod cyffredinol, y pwysigrwydd y cyfanswm. Nid yw'n o ran y drifoddwr taxi a'i gallu cyfathrebu gyda'i cwmni, ond mae'n bwysig o ran dysgu bod, er mwyn mynd ymlaen, rhaid i chi ffwrdd o gydraddoldeb. Felly mae angen i chi i gyd gydraddoldeb. Mae angen i unrhyw AI gael debyg cyffredinol, mae angen i chi ddod allan i ffwrdd o gydraddoldeb ac mae angen i chi ddod allan i'w mhleidleu a'i gynllunio o'r gydraddoldeb, neu byddai'n mynd allan i wneud mwy o ddysgu a phethau eraill. Rwy'n credu bod y rhan o'r rhesymau rydyn ni'n ei chymryd ar y wyddoniaeth a'r cyfathrebu yn ôl i'r cyfathrebu, sy'n y rhan fwy emosiynol, yw oherwydd mae'n grŵp dynol iawn. Nid wyf yn gwybod sut rydych chi wedifnod i ddod â rhai o'r broblemau debygol yma. Oherwydd arall, byddwch chi'n creu mechanau sy'n ymwneud â'r cyfnodau cyfnodol, cyfnodol, cyfnodol a chyfnodol, ac nid yw'r debygol. Nid yw'n cyfnodol a chyfnodol. Mae'n wahanol iawn i hynny. It's not rational and cognitive. It's very different to that. I agree. I think in studying this apparent male, this Dave is doing a lot to foster this aspect of the idea. So far. My primary area of research so far has been motivation. And we haven't had any talks on that topic, almost none. We had a little bit about creativity. Face talk was on, by the way. Yeah, a little bit. So that was the title. Yeah, that's true. But I do think that we didn't shed much light on the topic this year, and I do agree, it's a very, very important topic. Also, I think it's a completely different issue, I think that gender diversity would be a good thing. Also diversity of perspectives in general. I'm curious for comments, give me your hand, please. Let's talk about... Okay, so one of the things that I've been thinking about is that, you know, we used to have a very naive idea of intelligence 50 or 100 years ago that we kind of had a very, you know, logical or, you know, being a master chess player was the definition of intelligence. And so we began to realize how silly that was, and that there is starting to look at the intelligence, say, for instance, of a toddler human being in some ways is so completely beyond what we get out of a computer today. But I guess one thing that's clearly starting to really happen now is that in the narrow AI areas, we really are starting to get some breakthroughs that are happening because of working things at scale. And the result is, in these vertical areas of intelligence, we're having something that is completely off the charts relative to human beings. Human beings aren't capable of consuming tens of millions of books. Now maybe what we have right now are things that aren't terribly good at doing the semantic processing part of it, but in terms of doing things like information retrieval, question and answering, they're really going well beyond what any particular human being is capable or any animal human being. So I guess my question is when you're talking about your benchmarks I understand the idea of how having something that's embodied and learns like a human being does that idea how that's important and I understand the idea of like well well, maybe the benchmark is that you can do things at a similar cognitive level, to say, for instance, that a young human being has a benchmark. But I guess the thing that doesn't quite make sense to me about that is that we're already in a place where we have systems that are performing acts of intelligence that are way beyond even an adult human being. And so, can we talk about breaking this down in a way that talks about the general intelligence characteristics? Like what are the general intelligence characteristics. Like, what are the general intelligence components? It's a good question, multiple pathways, right? So one pathway that someone could think is feasible, although I don't particularly, is to start with the Google search engine or some other big data thing and try to get more and more semantic understanding. Whereas my own personal view of the right pathway to get to AGI is that there's there's some core of common sense understanding that's missing from these big data applications and it's probably not going to come from incrementally improving them and what I want to get from building robot toddlers or virtual robot toddlers is a system that has that kind of basic common sense understanding of itself in the world, basic ability to generalize and create. I then think once you integrate that with the sort of software you're doing for this big data, then you'll get something that combines human-like generality of understanding and reflective capability with a transhuman capability to crunch massive amounts of data. And that would be awesome. But it's plausible to think of another path where you start by expanding the semantic web into general intelligence, and I'm not going to follow that path, and I'm not going to try to stop someone else from doing, trying that path. And that gets back to the idea that there's many different pathways. And we don't currently have the scientific knowledge to tell which one is the best. Each of us has our own intuition. I would like to humbly add to that, that if you take this kind of approach, often for these more generalized versions of narrow AI that you see with self-driving vehicles and the little garage work and IBM's Watson, well, they often are just like sort of kludges, if you will. You take all these chunks and pieces and you glue them together to make them do something often are just like sort of kludges, if you will. You take all these chunks and pieces and you glue them together to make them do something pretty well that's a little more general than it's been done before, but it doesn't generalize the artificial intelligence in any deep way. But if you were taking this kind of pragmatic approach to solving a problem with a more general framework, an attempted general framework, then it may start to move a little more towards generalization. And that's where it's exciting to see a platform like OpenCog applied to a controlled human-robot interaction for an educational context, like we're doing with the RoboKind project. But that's just one example. There are other examples out there with other AGI projects. I did want to mention very quickly that in this course of progress towards genius machines, we are seeking what we call the Genie Initiative, and that is to develop this open source global framework that allows for people to commercialize, to develop proprietary software, but with the goal of achieving these Genie genius. So, Milosz, you have the last word. So I think that your remark has triggered something very beneficial, that is we see one of the differences, what we believe is missing, and I think this is eventually the question that we need to unearth in building our roadmap. So for instance, we have seen new developments in how concept formation can work automatically by number crunching, and I think this is tremendously useful. We also agree that we are far from being there yet, and one of the questions is do we need additional modes of representation with respect to identity, space, time, interaction, and so on. And what's missing, is it a body to interface with the world, which as Ben has suggested, or is it motivation and the creation of relevance by a motivational system and sociality and personhood and so on, as I would tend to suggest, and do we need to find particular benchmarks that allow us to explore these perspectives. I think with that we can probably be happy about concluding the session and see if we find something to eat. And I do thank you very, very much for your good arguments and I also like to thank you for the neuroscience perspective. I would love if we had more time to go back and forth on the arguments about that. Yeah, thanks for coming guys. And see you next week. you", '30.139381885528564')