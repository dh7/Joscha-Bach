('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/AormvNiBmdw"> </iframe> </center>', " So, what you just heard is our more decentralised computing angle, and actually, Mark is our chair of the Intelligent Cooperation Group in which many of these seminars have been taking place. We are now moving on to the more computing-focused session, which has very much the same layering of the last session before, but now we are also adding different things like D-Web, AI, compute into the mix. Okay, so, perhaps we will start with whosoever's slides I'm picking out first to tell us a little bit about what it is that you are working in, and how it is that we're going on to the longer-term future. And I'm picking on Peter Norvig who has done a fantastic recent Egoric event with us in person. Who was there at the recent Egoric in-person event? That was one of the first ones really post-pandemic, and it was really, really lovely to see all of you in person. Thank you again for joining there, Peter. Peter, what is it that you're working on? How does it relate to the very long-term, and what is it that people can help solve here? to the very long term and what is it that people can help solve here? Okay. A decade ago, Sebastian Thrun and I did the first online class to reach 100,000 students, and we thought the time was right to say we can change the way learning is done by analyzing the data of how students interact with the system. So we started that and we were very naive. One of the things we thought going in is that our job was to convey information. We quickly realized that it was more important to convey motivation and that was harder to quantify. And we started looking at the data and we didn't get as far as we thought we could. And so I kind of put that away for a while and worked on other things. So why was it hard? And I think the reason is because we don't have the ability to simulate the learner's mind. And we look at other areas of AI, which we've seen great progress in recent years, areas like playing Go, which was thought was going to be decades before we could solve it completely. And we did that. One of the reasons we did it is because we have a perfect simulation of Go. The game is literally black and white, so we know exactly what the result is of every move. And you can experiment and run billions of games in simulation. Things like self-driving cars we're doing pretty well and there we can simulate to high degree of fidelity the physics of how a car moves but there's still some things we can't get quite right. But the learner's brain we just have no idea of how to go from simulation to real. And that's what stopped us, this idea of human psychology. It's just too difficult to crack. Now I'm taking another crack at it. And I've put a .edu at the end of my name, spending some time at Stanford. And I think the time is right to revisit this question for a couple of reasons. One is there's just an order of magnitude more data available now. So more of these online systems have come up. Different companies and organizations have a lot of experience with a lot of students. And in the last two years, everyone has gotten used to this idea of you're going to interact online rather than in person in the classroom. So there's more data available, and there's also better tools available. Things like these large language models like GPT-3 that are tantalizingly close to being able to carry on a conversation and maybe act like a tutor or a conversant. And they still do some things surprisingly wrong, so we've got a ways to go, but it seems like this is the right time to figure out how it works. That's what I'm doing now. Lovely. Thank you so much. Okay. And you also have a challenge. What do you want other people to solve here? I guess to me, you know, I started out my graduate school career in natural language processing, so it's really understanding how these language models work. Lovely, okay, so we have a challenge here for you guys to solve. Okay, next one up, we have Rosie. Rosie, what are you currently working on? challenge here for you guys to solve. OK, next one up, we have Rosie. Rosie, what are you currently working on? What is a long-term potential trajectory? And what is a challenge that you would like this community to solve? Thank you so much. And actually, that was a perfect segue. So thank you, Peter. I want to talk about the concept of truthful AI. So can I just check how many people have heard of GPT-3? Okay, pretty much everyone, that's what I would have thought. How many people have had a chance to actually interact with it, use it, play with it? Okay, oh wow, all right, that's pretty good as well. Just to bring everyone up to the same page, GPT-3 is a large language model, it can do all sorts of things from summarization and classification through to generation of text, so you give it some kind of prompt, a sentence, and it will continue writing in the style of that sentence. So as Peter said, you can have conversations with it, it can write articles, that kind of thing. And I am on the product safety team at OpenAI, so my job is to think a lot about what kind of policies we need in order to make sure that these models are being used safely and responsibly, and how the API, what kind of products people can build with the API. And one thing that really breaks my heart is how many amazing socially beneficial use cases there could be for these models in high-stakes domains like healthcare or politics, but because we just aren't confident in the model being able to produce accurate outputs, we are generally very hesitant to approve products in those fields. And so I'm really excited about the idea of truthful AI, so trying to get these models not to just sort of make up nonsense, which sounds very plausible and it seems human readable, but have things that are actually based in fact and reality. And so a few things that I think can be useful for that include truthfulness data sets and fine tuning. So can we develop data sets that teach models to give us confidence intervals or express uncertainty when they're not sure of things? Can we get them to just generally be more reliable and accurate in the information they are producing? Prompt engineering is a really interesting emerging field as well. So when I first started working with GPT-3, I assumed that there would be some kind of parameter I would need to set in order to get it to produce content in a certain style or in a certain way. And it turns out you can just literally write in your prompt, like, like, say I don't know if you don't know the answer to this question, or be a polite chatbot or something, and that is enough to nudge it in that direction, so I'm really excited about advances in prompt engineering. Another aspect here is regulatory technology. Gillian Hadfield, who I think is a friend of Foresight Institute, has done a lot of really interesting work in this area, and I think if we're going to be trying to ensure these models are producing accurate output, we're going to need ways to audit that, to benchmark it, all of those things. I'm very excited about lots of different capabilities this will unlock, from accurate comprehension and summarization, getting it to synthesize and explain complicated concepts to us, and trying to prevent malicious use and be robust against intentional attempts to produce disinformation. Yes? Okay, sorry. And then the last thing I will say is, I think this is all ultimately aiming to try to produce things that we can interact with that will actually enhance our decision-making capabilities rather than diminish it. And so that's what I'm really excited about. And that's a very lofty goal. As a more concrete challenge that I will put forward for you guys today, even if we can't immediately jump to AI that never tells a lie and is always accurate, I think one thing that would really help in the meantime is if we can at least just get it to cite its sources. So if I ask it a medical question or a political question and it gives me a response, it can kind of send me a link or at least tell me where it got that information from and I can use my own judgment then to decide whether to trust it. Wow, that would be a very nice first start. Okay, thanks a lot. So for those of you who don't know Jillian Hedwin, who Rosie just mentioned, Jillian's seminar summary is here, which was, I think, a really fantastic one. And we have Peter Norvig's presentation also here. So if you want to read up more on what has just been discussed, please go for it. It's all on our seminar summaries. Thank you so much, Rosie. Super excited to have you on as a fellow next year. Couldn't be more pleased. Very, very, very excited. Okay. Next one up, we're moving into dWebland Booster. Will you tell us a little bit about what it is that you're working on and how people here in this room can help? We need some help. I work here with the Internet Archive and welcome. Back there is about 15 petabytes of the 70 petabytes that's the primary copy of the Internet Archive, blinking away, which is kind of fun, built by a lot of the people that are represented in the little statues around the edges. But we're running into some troubles in the World Wide Web, and it's based on a lot of centralization that's been going on. running to some troubles in the World Wide Web and it's based on a lot of centralization that's been going on. Not only I would say just big tech but also big publishers. The people that control the book industry, the academic publishing industry is really consolidating and they're leveraging this to make it so there's never digital ownership. So you can't actually take things and own them independently of a license agreement. Every reading event, they want to be a licensed event. You have to have a license to read. Doesn't that sound bad? So this is sort of one of the trends that's going on now with the web. It makes it very difficult to be the internet archive as a library to go and make copies of these perpetually available in new and different ways, such as the open AI kinds of data mining fundamentals. So, as a Hail Mary, we started talking about the decentralized web, the idea of having a web that operates a little bit more like old-style publishing. The publishing goes and puts things out, it lives in multiple places, so that if any of those particular places go away, you don't lose the work. That's not how the web currently works. Can we go and make a peer-to-peer back end for the World Wide Web and solve some of these problems so that people could make money by going and publishing on the net without being plugged into a platform? That was kind of the idea of this. So can we have robust, private web? How do we get there? One, we need some of the tech and browser technologies needs to be able to support this upgrade, so that's going to be an interesting issue. But we also need government support, like we did for the first World Wide Web, also the ARPANET and the like. And we need people that think differently about how they're going to compete rather than join into a big publishing conglomerate and it was the only way that your words are going to get out there. So we need entrepreneurs to be able to make all of this come about. So I think if we're going to have these sort of third party participants, whether it's OpenAI or the Internet Archives, to be able to go and make an ecosystem work in new and different ways, we're going to have to go and have a decentralized web technology. So what's a challenge here, which Alison, I think, correctly pointed to? And I would say it's the ethics. Because when you have one of these new technologies, things can go really badly wrong if it's the ethics, because when you have one of these new technologies, things can go really badly wrong if it's steered by the wrong folks. I think Tim Berners-Lee was an amazing man to go and basically sit out the gold rush. He didn't make billions of dollars off of the World Wide Web. He went to be basically a statesman, a civil servant of this evolving World wide web. Who are going to be the next generation, Aaron Schwartz is, sorry, to be the statesman of this next generation to try to keep us kind of on board. So yeah, there's going to be, you know, this is a group of, has a lot of libertarians, so you know, and motivations of greed are sort of calculated in. Fine, but let's keep that at a small, such that the structure works that we end up with a game with many winners, that we have an ethics and a structure that makes it so that there's not higher and higher barriers to entry as this new technology weaves through. And we sort of tamp down some of the bad impulses that will be put in place that will basically fuel enemies of a decentralised web technology. So if you have ideas in this kind of area, we're trying to figure this out, let's build a better web. Great. His slide will also be added over there, and someone's already taking photos. I'm loving it. Okay, lovely. The slide will also be added over there. And someone's already taking photos. I'm loving it. Okay. Lovely. Mark, you have no slide, but nevertheless, I'm sure if I can hurl a few prompts at you, you'll be just as well. So what is something you work on? What's an exciting end goal for your field? And what do you need help with? What would you like other people here to solve for you as you're moving about? It's interesting. I actually sort of rehearsed in my head a different, to answer to a different question. Then go to, with your different rehearsal because I want to know that. Here we go. So, being on this panel, I'm going to be speaking in an AI-centric way, which is outside of my main area of expertise. But other people are doing a much better job than I would hope to do at actually building outward to make computers intelligent. I'm going to speak about the risks at many levels of abstraction, and our approaches towards building a world in which we coexist cooperatively with AI get past the risks and benefit from the wonderful benefits that AI has to improve our lives. So, the first thing is basic computer security. Below AI, this is part of what I'm working on. And it's surprising to me that so much of the discussion of AI risks starts at the AI level of abstraction, of assuming that the AI is actually working according to its code and the issues are what it's trying to do or how it might be misled or fed false signals to mislearn. But that AI code is running on some computational platform. If that computational platform is insecure, you can reach up through it and corrupt the AI without using AI technology to corrupt it. You can just use the capacity it has to lead it to different goals. At this point, the finally well-known example is, well, when I was at Google and I knew that Google was working on self-driving cars, I assumed for a long time that, my God, these are guided missiles we're putting on the street that have all the intelligence needed to recognize crowds and plow into them at high speed. Surely the smart people at Google would be running these on some underlying, highly secure computing platform. And that's what I thought until I asked. And they're running it on the same Linux style basis that all of our other insecureable systems are running. So that's at the first level of abstraction and threat. Another level, so let's say we solve that one. That one's necessary, but it's way far from sufficient. Next level of interesting. I just put up the SEO4 slide as well as well, which is something that you always bring up. I have no commercial affiliation with them, so I'm just a big fan. But I will say, I think this is the single most important software project in the world. They are the only ones that have built a really practical high-performance operating system that can be used in production without penalty other than legacy compatibility and has a formal end-to-end proof of correctness of the implementation, not of a model of the implementation. And this project is surviving on a shoestring while governments spend many billions of dollars allegedly addressing the computer security problem. Okay. Next level of abstraction up is epistemic threats. Is a lot of the shouting about fake news or algorithms by big tech or recommendation systems leading to, optimized for leading to clicks or whatever, the problem is that you've got these big AI systems targeting the behavior of individual defenseless humans. And what I would like to see and what I think is a practical thing to work on is epistemic assistance, where the human combined with his epistemic assistant is a much more defensive unit for engaging with a world that might be using AIs to try to mislead them. And then finally, projecting forward to a civilization in which most of the computation that's, most of the cognition is non-human cognition, is of cognitive architectures that we find incomprehensible. What does it mean to have a framework of rules, a law-like framework for interacting with them cooperatively and in a way that we can both benefit each other. And I think the basic neutral framework of voluntarism is a sufficiently universal concept for enabling various parties to pursue their goals without having to model their utility, taking some utilitarian framework that requires us to have some notion of what their utility is, the simple universal notion of revealed preferences and adequate approximation of utility such that they can be getting what they want while helping us get what we want. Lovely. I just put up the podcast in which you talk about civilization as relevant super intelligence actually the panel that Brewster and hosted us for here at intelligences and AGI's and And and corporations and I am putting out for those of you who are interested in finding out more Mark is leading our intelligent corporation group Which is a group that has seminar summaries and seminars on these types of topic and we are currently co-authoring a book on intelligent voluntary cooperation. If you want to know more about this, then let us know. But in one of our first sessions on this topic, Mark and Robin actually talked about this concept in a really lovely way, and I think this is still one of the, I think, more interesting slide presentations to actually explain the concept of intelligent volunteer cooperation. So if you're interested in this, then please take a look at the seminar summaries in this group. Okay, cool. Last but not least, we have Yosha. So Yosha, what's going on for you? What are you working on? What are long-term consequences and challenges along the way? I have a slide too, if you find it. It's not that important. I'm Joscha Bach, I am a principal research scientist at Intel Labs. I am part of a 100 people group that is working under the headline Emergent AI. And the topic that interests me personally is understanding the vectors of intelligence. What we currently see is I, the second phase of AI, or it's one of the many surfaces on which you can project the development. The first phase of AI were narrow task-based systems where you constructed systems and algorithms with respect to a single problem that had to be solved. And now we have flexible AI where we are constructing algorithms that learn how to solve a problem. And the same algorithm can be deployed across a wide range of problems. And the next phase of AI I think will be systems that are somewhat universal, that are able to tackle an extremely wide range of problems and learn how to learn. So meta-learning is going to be one of the topics. And when we think about how to assess such systems, it turns out that there is not a single benchmark that we can deploy, but there will be many different tasks and domains in which we will have to evaluate these systems along many dimensions. And some of these dimensions include things like the ability to deal with knowledge and the ability to represent the universe, the ability to interact in real time with the universe that you are coupled with and to make an integrated model of that universe, the ability to act on your own autonomously with the world and understand what you should be acting on. The ability to collaborate deeply, which includes what Peter mentioned, the ability to deeply model your interaction partners, their goals, and the shared goals that you should be having, the shared purposes. And this is something that eventually will enable ethical AI, I think. So you cannot have ethics without shared purposes above the level of the ego. And this means that you have to understand a larger aesthetic of the world. You have to have an idea of what the world is going to develop itself into. And systems that are capable of doing this need to be sentient. And sentience is not the same thing as consciousness. I think that consciousness is our solution to get to sentience. I call sentience the ability of a system to understand its own nature and its place in the universe, which means it has to build an integrated model of the universe that it's part of. And the creation of a coherent, integrated model of the entire universe, this big function that is able to track reality in real time, that is large and rich enough to relate everything that we understand or that we are considering to some kind of relationship, this is the problem of meaning. And it's the biggest unsolved problem in AI, how to make this big coherent model of the universe. What is coherence? I think that coherence is the global minimization of constraint violations in a system that represents the world in such a way that you minimize uncertainty based on the value of that uncertainty. It's expensive to reduce uncertainty, so you have to have valence, you have to have preferences about what you think is important. And this means that you have a system that is motivated by something. And ultimately, what every system that is alive, that is part of the universe, that is creating complexity, has to be motivated by, is depending on its place that it has in this bigger scheme of things. And these systems of the future will have to understand their place, and they will also have to help us to understand our own place in the bigger scheme of things. So I think that we are, in the long term, going to look, if everything pans out, in the best possible way, at systems that help us to collaborate with life on earth, and to collaborate between life on earth and to collaborate between people and machines. That's the best possible outcome and there are many, many risks on that way. But I think what's much more near term is that we will be able to automate many of the processes in which people are making sense of reality. And this is an old dream of philosophy, to be able to mathematise the way in which we think and perceive and make sense. And I think this is going to trigger a scientific revolution, a second scientific revolution. Sometimes wonder if the AIs of the future will love to get drunk so they are only able to integrate the universe over something like 12 layers and they will be as confused as human physicists when they look at the universe. Okay, well, lovely. Leaving it at a humorous note, are there any comments, questions that people, panelists want to make to each other? Any problem that you think you can already solve here from someone else or want to give a nod to someone else or,, like, a universal nod across the panel. All right, any questions from the audience? Okay, we have one who I think you haven't asked a question yet, so, please, let's go with you. And maybe say your name as well. I'm very sorry, I don't know your name. Yes. Hi, thank you. My name is Molly. Nice to meet everybody. My question Yep. Hi. Thank you. My name's Molly. Nice to meet everybody. My question's for Peter. I also work in the education tech space in the past, and I'm really curious about what things you've learned about how to motivate large communities of learners and kind of the work you're doing in that space, trying to apply language models to the motivation problem because that's really interesting. I guess a couple of things. One is trying to be at the right level of where they are now, and trying to figure that out is important. Another is meeting their goals. So in the early days of these online classes, there's all this criticism, oh, only a few percent of people are completing the classes. You know, if only 2% of high school students were completing, that would be a big problem. And so one of the things we did is ask them up front, what are you trying to accomplish? And we found most of these people who were dropping out, that's exactly what they wanted to do. They didn't want to take the class. They just wanted to come in, get a little bit of a feel for it, and then move on to something else. So I think making sure you know what they're trying to achieve and help them achieve that, I think is important. And then community is important, of having somebody else to motivate you. Because it's all too easy to say, oh, I don't feel like it today, I'm not going to work on this, but if you say, oh, well, there's a study group with my four buddies, I can't let them down, I'm going to show up and I'm going to do that. And so trying to find the right ways to do that, and one of the things we did sort of accidentally when we were launching the class, we said we were going to have a discussion forum, but the code for that's not ready, so we'll cancel it. And that was great, because what happened was students invented their own. So one group of students would say, well, we're going to make a Reddit forum. And another one said, we're going to do something on Quora, and we're going to do something someplace else. And that way, they felt like they owned it, rather than like it was imposed on them. And that actually worked out much better. So finding ways to give them ownership, I think, is important. And we let the programmers off the hook who didn't get it finished. All right, lovely. OK, everyone, we are now moving on to our lunch break. I do want to say thank you so, so much for this fantastic panel. I feel like we got individual dips that went really deep, and I really look forward for how we will be constructing that technology tree from the individual nodes throughout the afternoon. Your node is over there, so, in the afternoon, that's where you guys will be gathering. So, if you work in decentralised computing and AI, then this is the place where you will be gathering. I do also want to just let you know, the first two panels, the Neurotech panel and the Space panel were the panels where we are just in the next year kicking off the individual technical groups, one chaired by Krian Leavitt, who is the chair, and the other one by Randall Koerner, who is also here on stage. This one here already exists. It is Mark Miller, the chair, as I mentioned, and you can find all of the different seminar summaries here. We have the 2021 program seminar summaries of each of the individual seminars that we had in this group. If you want to know more about this group, talk to Mark. There's an application form you can apply to join it, and we do take application on a rolling basis. All right, everyone, I don't want to stand between you and the food. I do want to say, if you have grabbed your food, there is the NFT gallery happening, and it is down those stairs, and then right at the front, basically below this room, if you go down the stairs around the corner, there is a gallery happening. And for now, enjoy your lunch. We'll be meeting here again at 1.30 for the biotech and rejuvenation part of the day. So meet you here at 1.30 again and enjoy your lunch. It is down out there where you guys were mingling and getting coffee. Thank you all for joining, thanks. Thanks.", '14.611412286758423')