('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/WRdJCFEqFTU"> </iframe> </center>', " Our next talk is going to be about AI, and it's going to be about proper AI. It's not going to be about deep learning or buzzword bingo. It's going to be about actual psychology. It's going to be about computational meta psychology. And now please welcome Joshua. APPLAUSE Thank you. I'm interested in understanding how the mind works, and I believe that the most fruitful perspective of looking at minds is to understand that we are systems that if you throw patterns at them, you find meaning. And we find meaning in those in very particular ways, and this is what makes us who we are. So the way to study and understand who we are in my understanding is to build models of the information processing that constitutes our minds. Last year, about the same time, I answered the four big questions of philosophy. What's the nature of reality? What can we know? Who are we? What should we do? So, now, how can I top this? I'm going to give you the drama that divided a planet. Some of the very, very big events that happened in the course of last year, so I couldn't tell you about it before. What color is the dress? I mean, if you do not have any mental defects, you can clearly see it's white and gold, right? Turns out, most people seem to have mental defects and say that it's blue and black. I have no idea why. Well, okay, I have an idea why that is the case. I guess that you got two. It has to do with color renormalization and color renormalization happens differently apparently in different people so we have different wiring to renormalize the white balance. And it seems to work in real world situations in pretty much the same way but not necessarily for photographs which have only a very small fringe around them which gives you a hint about the lighting situation. And that's why you get these huge divergences, which is amazing. So what we see is that our minds cannot know objective truths in any way outside of mathematics. They can generate meaning, though. How does this work? I did robotic soccer for a while, and there you have the situation that you have a bunch of robots that are situated on a playing field, and they have a model of what goes on in the playing field. Physics generates data for their sensors, they read the bits of the sensors, and then they use them to update the world model. Sometimes we didn't want to take the whole playing field along and the physical robots because they are expensive and heavy and so on. Instead, if you just want to improve the learning and the gameplay of the robots, you can use a simulation. So we wrote a computer simulation of the playing field and the physics and so on that generates pretty much the same data and put the robot mind into a simulated robot body, and it works just as well. That is, if you are the robot, because you cannot know the difference if you are the robot. You cannot know what's out there. The only thing that you get to see is what is the structure of the data at your systemic interface. And then you can derive a model from this. And this is pretty much the situation that we are in. That is, we are minds that are somehow computational, that are able to find regularity in patterns, and that we seem to have access to something that is full of regularity so we can make sense of it. Now, if you discover that you are in the same situation as these robots, basically you discover you are some kind of apparently biological robot that doesn't have direct access to the world of concepts, that has never actually seen matter and energy and other people. All it got to see was little bits of information that were transmitted through nerves, and the brain had to make sense of them by counting them in elaborate ways. What's the best model of the world that you can have? What's really the state of affairs? What's the system that you are in? And what's the best algorithm that you should be using to fix your world model? And this question is pretty old, and I think it has been answered for the first time by Ray Solomonoff in the 1960s. He discovered an algorithm that you can apply when you discover you're a robot and all you got is data. What is the world like? His algorithm is basically a combination of Bayesian reasoning, induction, and Occam's razor. We can mathematically prove that we cannot do better than Solomonov induction. Unfortunately, Solomonov induction is not quite computable. But everything that we are going to do is going to be some approximation of Solomonov induction. So our concepts cannot really refer to facts in the world out there. We do not get the truth by referring to stuff out there in the world. We get meaning by suitably encoding the patterns in our systemic interface. AI has recently made huge progress in encoding data at perceptual interfaces. Deep learning is about using a stacked hierarchy of feature detectors. We use pattern detectors and build them into networks that are arranged in hundreds of layers, and then we adjust the links between these layers, usually using some kind of gradient descent. And you can use this to classify, for instance, images and parts of speech. So we get to features that are more and more complex. They start out as very, very simple patterns and then get more and more complex until we get to object categories. And now these systems are able, in image recognition tasks, to approach performance that's very similar to human performance. Also what is nice is that it seems to be somewhat similar to what the brain seems to be doing in visual processing. And if you take the activation in different levels of these networks and you improve this or enhance this activation a little bit, what you get is stuff that looks very psychedelic, which might be similar to what happens if you put certain illegal substances into people and enhance the activity on certain layers of their visual processing. So, we want to filter out all the invariances to get to the invariant, the invariant being that case being Anna. So, we want to filter out the pose that she has, the lighting, the dress that she has on, her facial expression, and so on, and then we go only to this thing that is left after we removed all the nuisance data. But what if we want to get to something else? For instance, if you want to understand poses. Could be, for instance, that we have several dancers and we want to understand what they have in common. So our best bet is not just to have a single classification with filtering, but instead what we want to have, we take the low level input and get a whole universe of features that is interrelated. So we have different levels of interrelations. At the lowest level, we have percepts. On a slightly higher level, we have simulations. And on the even higher level, we have a concept landscape. How does this representation by simulation work? Now, imagine you want to understand sound. If you are a brain and you want to understand sound, you need to model it. Unfortunately, we cannot really model sound with neurons because sound goes up to 20 kilohertz. Or if you are old like me, maybe to 12 kilohertz. 20 kilohertz is what babies do. And neurons do not want to do 20 kilohertz. That's way too fast for them. They like something like 20 hertz. So what do you do? You need to make a Fourier transform. The Fourier transform measures the amount of energy at different frequencies. And because you cannot do this with neurons, you need to do it in hardware. And it turns out this is exactly what we are doing. We have this cochlea, which is this snake, snail-like thing in our ears. And what this does, it transforms energy of sound, different frequency intervals, into energy measurements. And then gives you something like what you see here. And this is something that the brain can model. So we can get a neural simulator that tries to recreate these patterns, and then can predict the next input from the cochlea, then it understands the sound. Of course, if we want to understand music, we have to go beyond understanding sound. We have to understand the transformations that sound can have if you play at a different pitch. We have to arrange the sound in the sequencer that gives you rhythms and so on. And then we want to identify some kind of musical grammar that we can use to, again, control the sequencer. So we have stacked structures that simulate the world. And once you've learned this model of music, once you've learned the musical grammar, the sequencer and the sounds, you can get to the structure of the individual piece of music. So if you want to model a world of music, you need to have the lowest level of the precepts, then we have a higher level of mental simulations, and which give the sequences of the music and the grammars of music, and beyond this, you have a conceptual landscape that you can use to describe the different styles of music. If you go up in the hierarchy, you get to more and more abstract models, more and more conceptual models, and more and more analytic models. These are causal models at some point. These causal models can be weakly deterministic, basically associative models which tell you if this state happens, it's quite probable that this one comes afterwards. Or you can get to a strongly determined model. A strongly determined model is one which tells you if you are in this state, and this condition is met, you're going to go exactly in this state. If this condition is not met, or a different condition is met, you go into this state. This is what we call an algorithm. Now you're in the domain of computation. Computation is slightly different from mathematics. It's important to understand this. For a long time, people have thought that the universe is written in mathematics, or that minds are mathematical, or anything is mathematical. In fact, nothing is mathematical. Mathematics is just the domain of formal languages. It doesn't exist. Mathematics starts with a void. You throw in a few axioms, and if you've chosen nice axioms, then you get infinite complexity, most of which is not computable. In mathematics, you can express arbitrary statements because it's all about formal languages. Many of these statements will not make sense. Many of these statements will make sense in some way, but you cannot test whether they make sense because they're not computable. Computation is different. Computation can exist. It starts with an initial state, and then you have a transition function, you do the work, you apply the transition function, you get into the next state. Computation is always finite. Mathematics is the kingdom of specification, and computation is the kingdom of implementation. It's very important to understand this difference. All our access to mathematics, of course, is because we do computation. We can understand mathematics because our brain can compute some part of mathematics, very, very little of it, and to a very constrained complexity, but enough so we can map some of the infinite complexity, non-computability of mathematics, into computational patterns that we can explore. So computation is about doing the work. It's about executing a transition function. Now, we saw that mental representations is about percepts, mental simulations, conceptual representations, and these conceptual representations give us concept spaces. And the nice thing about these concept spaces is that they give us an interface to our mental representations. We can use to address and manipulate them, and we can share them in cultures. And these concepts are compositional. We can put them together to create new concepts. And they can be described using higher dimensional vector spaces. They don't do simulation and prediction and so on, but we can capture regularity in our concepts with them. With these vector space, you can do amazing things. For instance, if you take the vector from king to queen, it's pretty much the same vector as between men and women. And because of these properties, because it's really a high dimensional manifold, these concept spaces, we can do interesting things like machine translation without understanding what it means, that is, without doing any proper mental representation that predicts the world. So this is a type of mental representation that is somewhat incomplete, but it captures the landscape that we share in a culture. And then there is another type of mental representation that is linguistic protocols, which is basically a formal grammar and a vocabulary, and we need these linguistic protocols to transfer mental representations between people. And we do this basically by scanning our mental representations, disassembling them in some way, or disambiguating them, and then we use a discrete string of symbols to get this to somebody else, and he trains an assembler that reverses this process and builds something that is pretty similar to what we intended to convey. And if you look at the progression of AI models, it pretty much went the opposite direction. So AI started with linguistic protocols, which were expressed in formal grammars, and then it got to concept spaces, and now it's about to address percepts. And at some point in the near future now it's about to address percepts. And at some point in the near future, it's going to get better at mental simulations. And at some point after that, we get to attention-directed and motivationally-connected systems that make sense of the world, that are, in some sense, able to address meaning. This is the hardware that we have can do. What kind of hardware do we have? That's a very interesting question. It could start out with the question, how difficult is it to define a brain? We know that the brain must be somewhere hidden in the genome. The genome fits on a CD-ROM. It's not that complicated. It's easier than Microsoft Windows. And we also know that about 2% of the genome is coding for proteins. And maybe about 10% of the genome has some kind of stuff that tells you when to express which protein. And the remainder is mostly garbage. It's old viruses that are left over and have never been properly deleted and so on. Because there are no real code revisions in the genome. So how much of these 10% that is 75 meg, code for the brain, we don't really know. What we do know is we share almost all of this with mice. Genetically speaking, a human is a pretty big mouse with a few bits changed to fix some of the genetic expressions. And that is most of the stuff there is going to code for cells and metabolism and what your body looks like and so on. But if you look at how much is expressed in the brain and only in the brain in terms of proteins and so on, we find it's about, well, of the 2%, it's about 5% of the 2% that is only in the brain. And another 5% of the 2% is predominantly in the brain, that is more in the brain than anywhere else. Which gives you some kind of thing like a lower bound, which means to encode a brain genetically based on the hardware that we are using, we need something like at least 500 kilobytes of code. Actually, this we, a conservative low-bound, it's going to be a little more, I guess. It sounds surprisingly little, right? But in terms of scientific theories, this is a lot. I mean, the universe, according to the core theory of quantum mechanics and so on, is like so much of code. It's like half a page of code, that's it. That's all you need to generate a universe. If you want to understand evolution, it's like a paragraph, it's a couple lines really to understand an evolutionary process. And there's lots, lots of details that you get afterwards, because this process itself doesn't define what all the animals are going to look like, in a similar way the code of the universe doesn't tell you what this planet is going to look like and what you guys are going to look like. It's just defining the rule book. And in the same sense, the genome defines the rule book by which our brain is built. The brain boots itself in a developable process, and this booting takes some time. So some initial learning in which initial connections of watch and basic models are built of the world so we can operate in it. And how long does this booting take? I think it's about 80 mega seconds. connections of wards and basic models are built of the world so we can operate in it. How long does this booting take? I think it's about 80 mega seconds. That's the time that a child is awake until it's three and a half years old. By this age, you understand Star Wars, and I think everything after understanding Star Wars is cosmetics. You're going to be online if you get to a ripe old age for about 1.5 giga seconds. And in this time, I think you are going to get not too much more than 5 million concepts. Why? I don't know. If you look at this child, if a child would be able to form a concept, let's say, every five minutes, then by the time it's about four years old, it's going to have something like 250,000 concepts. And so a quarter million. And if we extrapolate this into our lifetime, at some point it slows down because we have enough concepts to describe the world. Maybe it's something, I think it's less than five million. How much storage capacity does the brain have? I think that the estimates are pretty divergent. The lower bound is something like 100 gigabytes, and the upper bound is something like 2.5 petabytes. There is even some higher outliers, if you, for instance, think that we need all the synaptic vesicles to store information, maybe even more fits into this. But the 2.5 petabytes is usually based on what you need to code the information that is in all the neurons. But maybe the neurons do not really matter so much, because if a neuron dies, it's not like your world is changing dramatically. The brain is very resilient against individual neurons failing. So the 100 gigabyte capacity is much more what you actually store in the neurons, if you look at all the redundancy that you need. And I think this is much closer to the actual ballpark figure. Also if you want to store 5 million concepts, and maybe 10 times or 100 times the number of percepts on top of this, this is roughly the ballpark figure that you're going to need. So our brain is a prediction machine. What it does is it reduces entropy of the environment to solve whatever problems you are encountering if you don't have a feedback loop to fix them. So normally, when something happens, we have some kind of feedback loop that regulates our temperature or that makes problems go away, and only when this is not working, we employ cognition. And then we start this arbitrary computational process that is facilitated by the neocortex. And this neocortex can really do arbitrary programs, but it can do so only with a very limited complexity, because really you just saw it's not that complex. The modeling of the world is very slow. It's also something that we see in our eye models. To learn the basic structure of the world takes a very long time. To learn basically that we are moving in 3D and how objects are moving and what they look like. Once we have this basic model, we can get to very, very quick understanding within this model. Basically, encoding based on the structure of the world that we've learned. And this is some kind of data compression that we are doing. We use this model, this grammar of the world, these simulation structures that we've learned, to encode the world very, very efficiently. How much data compression do we get? Well, if you look at the retina, the retina gets data in the order of about 10 gigabits per second. And the retina already compresses these data and puts them into optic nerve at the rate of about 1 megabits per second. This is what you get fed into the visual cortex. And the visual cortex does some additional compression, and by the time it gets to layer 4 of the first layer of vision, to V1, we are down to something like one kilobit per second. So if you extrapolate this and you live to the age of 80 years and you are awake for two-thirds of your lifetime, that is, you have your eyes open for two-thirds of your lifetime, the stuff that you get into your brain via your visual perception is going to be only two terabytes. Only two terabytes of visual data throughout all your lifetime. of the data you're going to get from your visual perception is going to be only two terabytes. Only two terabytes of visual data throughout all your lifetime. That's all you're going to get ever to see. Isn't this depressing? So, I would really like to tell you to choose wisely what you're going to look at. Okay, let's look at this problem of neural compositionality. Our brains have this amazing thing that they can put metal representations together very, very quickly. For instance, you read a page of code, you compile it in your mind into some kind of program that tells you what this page of code is going to do. Isn't that amazing? And then you can forget about this, disassemble it all, and use the building blocks for something else. It's like Legos. How can you do this with neurons? Legos can do this because they have a well-defined interface. They have all these slots that fit together in well-defined ways. How can neurons do this? Neurons could maybe learn the interface of other neurons, but that's difficult because every neuron looks slightly different after all this is some kind of biologically grown natural stuff. So what you want to do is you want to encapsulate this diversity of the neurons to make them predictable, to give them a well-defined interface. I think that nature's solution to this is cortical columns. Cortical column is a circuit of between 100 and 400 neurons, and this circuit has some kind of neural network that can learn stuff, and after it's learned a particular function, and in between, it's able to link up with other cortical columns. And we have about 100 million of those, depending on how many neurons you assume are in there. We guess it's something, at least 20 million, and maybe something like 100 million. And these cortical columns, what they can do is they can link up like LEGO bricks and then perform, by transmitting information between them, pretty much arbitrary computation. What kind of computation? Well, Solomonov induction. And they have some short-range links to their neighbors, which come almost for free because, well, they're connected to them, they're a direct neighborhood. And they have some long-range connectivity, so you can combine everything in your cortex with everything. So you need to have some kind of global switchboard, some grid-like architecture of long-range connections. They're going to be more expensive, they're going to be slower, but they're going to be there. So how can we optimize what these guys are doing? In some sense, it's like an economy. It's not an energy-based system, as we often use in machine learning. It's really an economy. The question is, you have a fixed number of elements. How can you do the most valuable stuff with that? Fixed resources, most valuable stuff, the problem is economy. So you have an economy of information brokers. Every one of these guys, of these little cort cortical columns is a very simplistic information broker. And they trade rewards against neg-entropy, against reducing entropy in the world. And to do this, we just saw that they need some kind of standardized interface. And internally, to use this interface, they're going to have some kind of state machine. And then they're going to pass messages between each other. And what are these messages? Well, it's going to be hard to discover these messages by looking at brains, because it's very difficult to see in brains what they're actually doing. You just see all these neurons. And if you would be waiting for neuroscience to discover anything, we wouldn't even have gradient descent learning or anything else. We wouldn't have neural learning. We wouldn't have all these advances in AI. Jürgen Schmidhuber said that the biggest, or the last contribution of neuroscience to artificial intelligence was about 50 years ago. That's depressing and it might be over-emphasizing the unimportance of neuroscience because neuroscience is very important once you know what you're looking for. You can actually often find this and see whether you're on the right track. But it's very difficult to take neuroscience to understand how the brain is working, because it's really like understanding flight by looking at birds through a microscope. So, what are these messages? You're going to need messages that tell these cortical columns to join themselves into a structure and to unlink again once they're done. You need ways that they can request each other to perform computations for them. You need ways they can inhibit each other when they're linked up, so they don't do conflicting computations. Then they need to tell you whether the computation, the result of the computation that they're asked to do is probably false, or whether it's probably true, but you still have to wait for others to tell you whether the details work out, or whether it's confirmed true, whether the concept that they stand for is actually the case. And then you want to have learning to tell you how well this worked. So you will have to announce a bounty that tells him to link up, a kind of reward signal that makes them do computation in the first place. And then you want to have some kind of reward signal when you got a result as an organism, when you reached your goal, if you made the disturbance go away or whatever, if you consumed the cake. And then you will have some kind of reward signal that you give everybody that was involved in this. This reward signal facilitates learning, so the difference between the announced reward and the assumed reward is the learning signal for these guys. So they can learn how to play together and how to do the Solomonov induction. Now I told you that Solomonov induction is not computable. And that's mostly because of two things. First of all, it needs infinite resources to compare all the possible models. And the other one is that we do not know the prior probability for our Bayesian model. We do not know how likely unknown stuff is in the world. So what we do instead is we set some kind of hyperparameter, some kind of default prior probability for concepts that are encoded by the cortical columns. If we set this parameter very low, then we are going to end up with inferences that are quite probable for unknown things, and then we can test for those. If we set this parameter higher, we are going to be very, very creative, but we end up with many, many theories that are difficult to test, because maybe there are too many theories to test. Basically, every of these cortical columns will now tell you, when you ask them if they are true, yes, I'm probably true, but I still have to ask others to work on the details. So these others are going to get active, and they're being asked by this asking element, are you going to be true? And they say, yeah, probably yes. I just have to work on the details, and they're going to ask even more. So your brain is going to be true? They say, yes, probably, I just have to work on the details, and they're going to ask even more. So your brain is going to light up like a Christmas tree and do all these amazing computations, and you see connections everywhere, and most of them are wrong. You're basically in a psychotic state if your hyperparameter is too high. Your brain invents more theories than it can disprove. Would it actually be sometimes good to be in this state? You bet. So I think every night, good to be in this state? You bet. So I think every night our brain goes in that state. We turn up this hyperparameter, we dream, we get all kinds of weird connections, and we get to see connections that otherwise we couldn't be seeing, because they're highly improbable. But sometimes they're whole, and we see, oh my God, the DNA is organized in a double helix. Wow. And this is what we remember in the morning. All the other stuff is deleted. So we usually don't form long-term memories and dreams if everything goes well. If you accidentally trip this up your modulators, for instance by consuming illicit substances, or because you just go randomly psychotic, you basically enter a dreaming state, I guess. You get to a state where the brain starts inventing more concepts that it can disprove. So you want to have a state where this is well balanced. And the difference between highly creative people and very literal people is probably a different setting of this hyperparameter. So I suspect that people that are genius-like, people like Einstein and so on, do not simply have better neurons than others. What they mostly have is a slightly hyperparameter that is very finely tuned, so they can get better balance than other people in finding theories that might be true, but can still be disproven. So inventiveness could be a hyperparameter in the brain. If you want to measure the quality of a belief that we have, we are going to have to have some kind of cost function which is based on the motivational system. And to identify if the belief is good or not, we can have structural criteria, for instance, how well does it predict the world, or how well does it reduce uncertainty in the world, or is it consistency and sparse? And then, of course, utility. How well does it help me to satisfy my needs? And the motivational system is going to evaluate all these things by giving a signal. And the first signal, kind of signal, is there are possible rewards if we are able to compute a task. This is probably done by dopamine. So we have a very small area in the brain, a substantia nigra, and then the ventral tegmental area, and they produce dopamine. This gets fed into the dorsolateral frontal cortex and the frontal lobe, which control attention and tell you what things to do. If we have successfully done what we wanted to do, we consume the rewards. And we do this with another signal, which is serotonin. It's also announced by the motivational systems with very small area, the raffinucleus, and it feeds into all the areas of the brain where learning is necessary. Your connections are strengthened once you get a result. And these two substances are emitted by the motivational system. The motivational system is a bunch of needs that are centrally regulated below the cortex. They're not part of your mental representations. They are part of something that is more primary than this. This is what makes us go. This is what makes us human. This is not our rationality. This is what we want. And the needs are physiological, they are social, and they are cognitive. And we are pretty much born with them. They cannot be totally adaptive, because if we were adaptive, we wouldn't be doing anything. The needs are resistive. They are pushing us against the world. If you wouldn't have all these needs, if you wouldn't have this motivational system, you would just be doing what's best for you, which means collapse on the ground, be a vegetable, rot, give in to gravity. Instead, you do all these unpleasant things, you get up in the morning, you eat, you have sex, you do all these crazy things. It's only because the motivational system forces you to. The motivational system takes this bunch of matter and makes us do all these strange things just so genomes get replicated and so on. And so to do this, we're going to build resistance against the world. And the motivational system is in this sense forcing us to do all these things by giving us needs and the need has some kind of target value and current value. If we have a differential between the target value and the current value, we perceive some urgency to do something about the need. And when the target value approaches the current value, we get a pleasure signal, which is a learning signal. If it goes away from it, we get a displeasure signal, which is also a learning signal. We can use this to structure our understanding of the world, to understand what goals are, and so on. Goals are learned. Needs are not. To learn, we need success and failure in the world. But to do things, we need anticipated reward. So it's dopamine that makes the brain go round. Dopamine makes you do things. But in order to do this in the right way, you have to make sure that the cells cannot produce dopamine themselves. If they do this, they can start to bribe others to work for them. You're going to have something like a bureaucracy in your neocortex, where different bosses try to subdue others to do their own bidding and pitch against other groups in the neocortex. It's going to be horrible. So you want to have some kind of central authority that makes sure that the brains don't produce, cells don't produce the dopamine themselves. It's only being produced in a very small area and then given out and passed through the system. And after you're done with it, it's going to be gone. So there is no hoarding of the dopamine. And in our society, the role of dopamine is played by money. Money is not reward in itself. It's, in some sense, a way that you can trade against a reward. You cannot eat money. You can take it later and get an arbitrary reward for it. And in some sense, money is the dopamine that makes organizations and society, companies and many individuals do things. They do stuff because of money. But money, if you compare it to dopamine, is pretty broken, because you can hoard it. So you're going to have these cortical columns in the real world, which are individual people or individual corporations. They're hoarding the dopamine. They sit on this very big pile of dopamine. They're starving the rest of the society of the dopamine. They don't give it away, and they can make it do its bidding. So, for instance, they can pitch a substantial part of society against understanding global warming because they're a prophet of global warming or a god of technology that leads to global warming, which is very bad for all of us. So, our societies have a nervous system that lies to itself. How can we overcome this? Actually, we don't know. To do this, we would need to have some kind of centralized, top-down reward motivational system. We have this, for instance, in the military. You have this system of military rewards that you get. And these are completely controlled from the top. Also, within working organizations you have this. In corporations you have centralized rewards. It's not like rewards flow bottom up. They always flow top down. And there was an attempt to model society in such a way that was in Chile in the early 1970s. The Allende government had the idea to redesign society or economy in a society using cybernetics. So Allende invited a bunch of cyberneticians to redesign the Chilean economy. And this was meant to be the control room where Allende and his chief economists would be sitting to look at what the economy is doing. We don't know how this would have worked out because we know how it ended. In 1973, there was this big push in Chile, and this experiment ended, among other things. Maybe it would have worked. Who knows? Nobody tried it. So there is something else that is going on in people, beyond the motivational system. That is, we have social criteria for learning. We also check if our ideas are normatively acceptable. And this is actually a good thing, because individuals may shortcut the learning through communication. Other people have learned stuff that we don't need to learn ourselves. And we can build on this. So we can accelerate learning by many orders of magnitude, which makes cultures possible, and which makes anything possible, because if you are on your own, you're not going to find out very much in your lifetime. You know how they say, everything you do, you do by standing on the shoulders of giants. Or in a big pile of dwarfs, it works either way! Social learning usually outperforms individual learning. You can test this. But in the case of conflict between different social truths, you need to have some way to decide who to believe. So you have some kind of reputation estimate for different authorities, and you use this to check whom you believe. And the problem, of course, is that in existing society, in real society, this reputation system is going to reflect power structures, which might distort your beliefs systematically. Social learning, therefore, leads groups to synchronize their opinions. And the opinions get another role. They become an important part of signaling which group you belong to. So opinions start to signal group loyalty in societies. And people in that world, they should optimise not for getting the best possible opinion in terms of truth, they should optimise for having the best possible opinion with respect to agreement with their peers. If you have the same opinion as your peers, you can signal them you're part of the in-group, they're going to like you. If you don't do this, chances are they're not going to like you. There's rarely any benefit in life to be in disagreement with your boss. So if you evolve an opinion-forming system in these circumstances, you should be ending up with an opinion-forming system that leaves you with the most useful opinion, which is the opinion in your environment. And it turns out most people are able to do this effortlessly. They have an instinct that makes them adopt the dominant opinion in their social environment. It's amazing, right? And if you are a nerd like me, you don't get this! You do. So in the world out there, explanations piggyback on your group allegiance. For instance, you will find there is a substantial group of people that believes the minimum wage is good for the economy and for you, and another one which believes that it's bad. And it's pretty much aligned with political parties. It's not aligned with different understandings of economy because nobody understands how the economy works. And if you are a nerd, you try to understand the world in terms of what's true and false. You try to prove everything by putting it on some kind of truth and false level. And if you are not a nerd, you try to get to right and wrong. You try to understand whether you're in alignment with what's objectively right in your society, right? So, I guess that nerds are people that have a defect in the opinion-forming system. Usually, that's maladaptive, and, under normal circumstances, nerds would mostly be filtered from the world because they don't reproduce so well, because people don't like them so much. And then something very strange happened, the computer revolution came along, and suddenly, if you argue with the computer, it doesn't help if you have the normatively correct opinion. You need to be able to understand things in terms of true and false, right? So now we have this strange situation that the weird people that have this offensive strange opinion and that really don't mix well with real normal people get all these high-paying jobs and we don't understand how is that happening. And it's because suddenly our maladaption is a benefit. But out there, there is this world of the social norms, and it's made of paper walls. There are all these things that are true and false in a society that make people behave. It's like these Japanese walls, they made palaces out of paper, basically. And these are walls by convention. They exist because people agree that this is a wall. And if you are a hypnotist like Donald Trump, you can see that these are paper walls and you can shift them. And if you are a nerd like me, you cannot see these paper walls. If you pay close attention, you see that people move, and then suddenly, in midair, they make a turn. Why do they do this? There must be something that they see there, and this is basically a normative agreement. And you can infer what this is, and then you can manipulate it and understand it. Of course, you can't fix this. You can debug yourself in this regard, but it's something that is hard to see for nerds. So in some sense, they have a superpower. They can think straight in the presence of others, but often they end up in their living room and people are upset. Learning in a complex domain cannot guarantee that you find the global maximum. You know that you cannot find truth because we cannot recognise whether we live on a playing field or in a simulated playing field, but what we can do is we can try to approach a global maximum, but we don't know if that is the global maximum. We will always move along some kind of belief gradient. We will take certain elements of our belief and then give them up for new elements of the belief based on thinking that this new element is better than the one that we give up. So we always move along some kind of gradient, and the truth does not matter, the gradient matters. If you think about teaching for a moment, when I started teaching I often thought, okay, I understand the truth of the subject, the students don't, so I have to give this to them. And at some point, I realized, oh, I've changed my mind so many times in the past, I'm probably not going to stop changing it in the future. I'm always moving along a gradient, and I will keep moving along a gradient. So I'm not moving to truth, I'm moving forward. And when we teach our kids, we should probably not think about how to give them truth. We should think about how to put them onto an interesting gradient that makes them explore the world, the world of possible beliefs. Applause And these possible beliefs lead us into local minima that's inevitable. These are like valleys, and sometimes these valleys are neighboring, and we don't understand what the people in the neighboring valley are doing unless we are willing to retrace the steps they've been taken. And if we want to get from one valley into the next, we will have to have some kind of energy that moves us over the hill. We have to have a trajectory where every step works by finding a reason to give up a bit of our current belief and adopt a new belief, because it's somehow more useful, more relevant, more consistent, and so on. Now, the problem is that this is not monotonous. We cannot guarantee that we are always climbing, because the problem is that the beliefs themselves can change our evaluation of the belief. It could be, for instance, that you start believing in a religion. And this religion could tell you, if you give up the belief in the religion, you're going to face eternal damnation and hell. As long as you believe in the religion, it's going to be very expensive for you to give up the religion, right? If you truly believe in it. You're now caught in some kind of attractor. Before you believe in religion, it's not very dangerous, but once you've got into the attractor, it's very, very hard to get out. So, these belief attractors are actually quite dangerous. You can get not only to chaotic behaviour where you cannot guarantee that your current belief is better than the last one, but you can also get into beliefs that are almost impossible to change. And that makes it possible to programme people to work in societies. Social domains are structured by values. Basically, a preference is what makes you do things because you anticipate pleasure or displeasure, and values make you do things even if you don't anticipate any pleasure. These are virtual rewards. They make you do things even if you don't anticipate any pleasure. These are virtual rewards. They make us do things because we believe that there is stuff that is more important than us. This is what values are about. And these values are the source of what we would call true meaning, deeper meaning. There is something that is more important than us, something that we can serve. This is what we usually perceive as meaningful life. It's one which is in the service of values that are more important than I myself, because after all, I'm not that important. I'm just this machine that runs around and tries to optimize its pleasure and pain, which is kind of boring. So my PI has puzzled me, my principal investigator in the Harvard department where I have my desk, Martin Novak. He said that meaning cannot exist without God. You're either religious or you are a nihilist. And this guy is the head of the department for evolutionary dynamics. Also he's a Catholic. So this this really puzzled me and I tried to understand what he meant by this. Typically, if you are a good atheist like me, you tend to attack gods that are structured like this, religious gods, that are institutional, they are personal, they are some kind of person, they do care about you. They prescribe norms. For instance, don't masturbate. It's bad for you. Many of these norms are very much aligned with societal institutions. For instance, don't question the authorities. God wants them to be ruling above you. And be monogamous, and so on, and so on. So they prescribe norms that do not make a lot of sense in terms of being that creates worlds every now and then, but they make sense in terms of what you should be doing to be a functioning member of society. And this God also does things, like they create worlds, they like to manifest as burning strawberry and so on. There are many books that describe stories that these gods have allegedly done. And it's very hard to test for all these features, which makes these gods very improbable for us, and makes atheists very dissatisfied with these gods. But then there's a different kind of god, that is what we call the spiritual god. This spiritual god is independent of institutions. It still does care about you, it's probably conscious. It might not be a person. There are not that many stories that you can consistently tell about it, but you might be able to connect to it spiritually. Then there is a God that is even less expensive. That is God as a transcendental principle. And this God is simply the reason why there is something rather than nothing. This God is the question that the universe is the answer to. This is the thing that gives meaning. Everything else about it is unknowable. This is the God of Thomas Aquinas. The God that Thomas Aquinas discovered is not the God of Abraham. This is not the religious God. It's a God that is basically a principle that asks the universe into existence. It's the one that gives the universe its purpose. And because every other property is unknowable about this, this God is not that expensive. Unfortunately, it doesn't really work. I mean, Thomas Aquinas tried to prove God. He tried to prove a necessary God, a God that has to be existing. And I think we can only prove a possible God. So if you try to prove a necessary God, this God cannot exist, which means your God proof is going to fail. You can only prove possible Gods. Then there is an even more impoverished God, and this is the God of Aristotle. And he said, if there is change in the universe, something is going to have to change it. There must be something that moves it along from one state to the next. So I would say this is the primary computational transition function of the universe. And Aristotle discovered it. It's amazing, isn't it? We have to have this because we cannot be conscious in a single state. We need to move between states to be conscious. We need to be processes. So we can take our gods and sort them by their metaphysical gods. The first degree god would be the first mover. Second degree god is the god of purpose and meaning. The third degree is the spiritual god, and the fourth degree god is bound to religious institutions, right? So if you take this statement from Martin Novak, you cannot have a meaning without God, I would say, yes, you need at least a second degree God to have meaning, right? So, objective meaning can only exist with a second degree God. And subjective meaning can exist as a function in a cognitive system, of course. We don't need objective meaning. So, we can subjectively feel that there is something that is more important to us, and this makes us work in society and makes us perceive that we have values and so on, but we don't need to believe that there is something outside of the universe to have this. So the fourth degree God is the one that is bound to religious institutions, it requires a belief attractor and it enables complex norm prescriptions. If my theory is right, then it should be much harder for nerds to believe in a fourth-degree God than for normal people. And what this God does, it allows you to have state-building mind viruses. Basically, a religion is a mind virus. The amazing thing about these mind viruses is that they structure behaviour in large groups. We have evolved to live in small groups of a few hundred individuals, maybe something like 150. This is roughly the level to which reputation works. We can keep track of about 150 people, and after this it gets much, much worse. So, in this system where you have reputation, people feel responsible for each other and they can keep track of their doings, and society kind of works. If you want to go beyond this, you have to write a software that controls people. Religions were the first software that did this on a very large scale. In order to keep stable, they had to be designed like operating systems in some sense. They give people different roles, like insects in a hive. They have even a part of these roles is to update the religion, but it has to be done very carefully and centrally, because otherwise the religion will split apart and fall together into new religions or overcome by new ones. So it's some kind of evolutionary dynamics that goes on with respect to religion. And if you look at the religions, there's actually a very simple evolution of religions. So we have this Israel tradition and the Mesopotamian mythology that gave rise to Judaism. It's kind of cool, right? Also history totally repeats itself. Yeah, it totally blew my mind when I discovered this. Of course, the real tree of programming languages is slightly more complicated, and the real tree of religion is slightly more complicated. But still, it's neat. So if you want to immunize yourself against mind viruses, first of all, you want to check yourself whether you are infected. You should check, can I let go of my current beliefs without feeling that meaning departs me and I feel very terrible if I let go of my current beliefs without feeling that meaning departs me, and I feel very terrible if I let go of my beliefs. Also you should check, are there other people around there that don't share my belief? Are they either stupid or crazy or evil? If you think this, chances are you are infected by some kind of mind virus because they are just part of the outgroup. And does your God have properties that you know, but you did not observe? So basically you have a God of a second or third degree or higher. In this case, you probably also got a mind virus. There's nothing wrong with having a mind virus. But if you want to immunize yourself against this, people have invented rationalism and enlightenment, basically to act as immunization against mind viruses. And in some sense, it's what the mind does by itself, because if you want to understand how you go wrong, you need to have a mechanism that discovers who you are, some kind of auto-debugging mechanism that makes the mind aware of itself. And this is actually the self. So according to Robert Keegan, the development of our self is a process in which we learn who we are by making things explicit, by making processes that are automatic visible to us, and to conceptualize them so we no longer identify with them. And it starts out with understanding that there's only pleasure and pain. If you're a baby, you only have pleasure and pain. You identify with this. And then you turn into a toddler, and the toddler understands that they're not their pleasure and pain, but they are their impulses. And in the next level, if you grow beyond the toddler age, you actually know that you have goals, and that your needs and impulses are there to serve goals, but it's very difficult to let go of the goals if you are a very young child. And at some point you realize, oh, the goals don't really matter because sometimes you cannot reach them. But we have preferences. We have things that we want to happen and things that we do not want to happen. And then at some point we realize that other people have preferences too. And then we start to model the world as a system where different people have different preferences and we have to navigate this landscape. And then we realize that these preferences also relate to values. And we start to identify with these values as members of society. And this is basically the stage, if you are an adult being, that you get into. And you can get to a stage beyond that, especially if you have people around this which have already done this. This means that you understand that people have different values. And what they do naturally flows out of them. And these values are not necessarily worse than yours, they're just different. And you learn that you can hold different sets of values in your mind at the same time, isn't that amazing? And understand other people, even if they're not part of your group. If you get that, this is really good. But I don't think it stops there. You can also learn that the stuff that you perceive is kind of incidental, that you can turn it off and that you can manipulate it. And then at some point you also can realize that yourself is only incidental stuff that you perceive is kind of incidental, that you can turn it off and that you can manipulate it. And then at some point you also can realize that yourself is only incidental, that you can manipulate it or turn it off, and that you're basically some kind of consciousness that happens to run on the brain of some kind of person that navigates the world in terms to get rewards or avoid displeasure and serve values and so on. But it doesn't really matter. There is just this consciousness that understands the world. And this is the state that we typically call enlightenment. In this state, you realize that you are not your brain, but you are a story that your brain tells itself. So, becoming self-aware is a process of reverse engineering your mind. It's a different set of stages in which you realize what goes on. So isn't that amazing? AI is a way to get more self-awareness? I think it's a good point to stop here. The first talk that I gave in this series was two years ago. It was about how to build a mind. Last year, I talked about how to get from basic computation to consciousness. This year, we have talked about finding meaning using AI. I wonder where it goes next! Thank you for this amazing talk. We now have some minutes for Q&A, so please line up at the microphones. As always, if you're unable to stand up for some reason, please very, very visibly raise your hand. We should be able to dispatch an audio angel to your location so you can have a question too. And also, if you're locationally disabled, if you're not actually in the room, if you're on the stream, you can use ISE or Twitter to also ask questions. We also have a person for that. We'll just start on microphone two. Wow, that's me. Just a guess, what would you guess, when can you discuss your talk with a machine, and how many years? I don't know. As a software engineer, many years? I don't know. As a software engineer, I know if I don't have a specification, all bets are off until I have the implementation. And so it can be of any order of magnitude. I have a gut feeling, but I also know as a software engineer that my gut feeling is usually wrong until I have the specification. So the question is if there are silver bullets. Right now, there are some things that are not solved yet. And it could be that they're easier to solve than we think, but it could be that they're harder to solve than we think. Before I stumbled on this cortical self-organization thing, I thought it's going to be something like maybe 60, 80 years, and now I think it's way less. But, again, this is a very subjective perspective. I don't know. Number one, please. Yes. I want to ask a little bit about metacognition. It seems that you kind of end your story saying that it's still reflecting on the input that you get and kind of working with your social norms and this and that. But Kohlberg, for instance, talks about what he calls post-conventional universal morality, for instance, which is thinking about moral laws without context, basically stating that there's something beyond the relative norms that we have towards each other, which would only be possible if you can do kind of metacognition, thinking about your own thinking and then modifying that thinking. So kind of feeding back your own ideas into your own mind and coming up with stuff that actually can't get thinking about while processing external inputs. I think it's very tricky. This project of defining morality without societies exists longer than Kant, of course, and Kant tried to give these eternal rules and others try to. I find this very difficult. From my perspective, we are just moving bits of rocks and these bits of rock, they are on some kind of dust mode in the galaxy, out of trillions of galaxies. And how can there be meaning? It's very hard for me to say that one chimpanzee species is better than another chimpanzee species or a particular monkey is better than another monkey. This only happens within a certain framework and we have to set this framework. And I don't think that we can define this framework outside of a context of social norms that we have to agree on. So, objectively, I'm not sure if we can get to ethics. I only think that's possible based on some kind of framework that people have to agree on, implicitly or explicitly. Microphone number four, please. Hi, thank you. It was a fascinating talk. I have two thoughts that went through my mind, and the first one is that it's so convincing the models that you present, but it's kind of like you present another metaphor of understanding the brain, which is still something that we try to grasp on different levels of science, basically. And the second one is that your definition of the nerd who walks around and doesn't see the walls is kind of a definition, or it reminded me of Richard Warchie's definition of the ironist, which is a person who knows that their vocabulary is finite and that there's other people who also have a finite vocabulary. And then that obviously opens up the whole question of meaning making, which has been discussed in so many other disciplines and fields. And I thought about Jerry Da's deconstruction of ideas and thoughts and Butler and then down the rabbit hole to Nietzsche. And I was just wondering if you could maybe map out other connections where basically not AI helping us to understand the mind, but where already existing huge, huge fields of science into like cognitive processes coming from the other end could help us to understand AI. Thank you. The tradition that you mentioned, Rorty and Butler and so on, are part of a completely different belief attractor in my current perspective. They are mostly social constructionists. They believe that reality, at least in the domains of the mind and sociality, are social constructs. They are a part of social agreement. Personally, I don't think that is the case. I think that the patterns that we refer to are mostly independent of our mind. The norms are part of social constructs, but for instance, our motivational preferences that make us adopt or reject norms are something that builds up resistance to the environment. So they're probably not part of a social agreement. And the only thing that I can invite you to is try to retrace both of the different belief attractors, try to retrace the different paths on the landscape. All the things that I tell you, a lot of this is, of course, very speculative. These are things that seem to be logical to me at this point in my life. And I try to give you the arguments why I think that they are plausible, but don't believe in them. Question them, challenge them, see if they work for you. I'm not giving you any truth. I'm just going to give you suitable encodings according to my current perspective. Thank you. The internet, please. So someone's asking, if in this belief space you were talking about How do we how is it possible to get out of local minima and Very related question as well Should our should we teach some sort of met momentum method to our children so they don't get stuck in local minima. I Believe at some level it's not possible to get out of the logical minima in an absolute sense because you only get to get into some kind of meta-minimum. But what you can do is to retrace the path that you took whenever you discover that somebody else has a fundamentally different set of beliefs. And if you realize that this person is basically a smart person that is not completely insane but has reasons to believe in their beliefs and that they seem to be internally consistent, it's usually worth to retrace what they have been thinking and why. And this means you have to understand what their starting point was and how they moved from their current point to their starting point. You might not be able to do this accurately. And the important thing is also afterwards you discovered a second valley. You haven't discovered the landscape in between. But the only way that we can get an idea of the lay of the land is that we try to retrace as many paths as possible. And if we try to teach our children what I think what we should be doing is to tell them how to explore this world on their own. It's not that we tell them this is the valley. This is basically, it's given. It's the truth. But instead we have to tell them this is basically, it's given, it's the truth, but instead we have to tell them this is the path that we took. And these are the things that we saw in between, and it's important to not be completely naive when we go into this landscape, but we also have to understand that it's always an exploration that never stops, and that might change everything that you believe now at a later point. So for me, it's about teaching my own children how to be explorers, how to understand that knowledge is always changing and it's always a moving frontier. We are unfortunately out of time so please once again thank Josje. Thank you. Thank you.", '29.220659017562866')