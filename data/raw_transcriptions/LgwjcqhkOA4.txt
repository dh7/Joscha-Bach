('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/LgwjcqhkOA4"> </iframe> </center>', " Welcome back to Street Talk. Just a little bit of housekeeping before we kick off today. Polina Solividov is one of the organizers for a charity AI conference called AI Helps Ukraine. Now, their main goal is to raise funds for Ukraine, both from the folks attending the conference and also from companies sponsoring the conference. And it's not too late to sponsor the conference and support it. So, please do if you possibly can. Now, all of the conference and support it. So please do if you possibly can. Now all of the funds that they raise will go to Ukraine Medical Support, which is a Canadian non-profit organization which is specializing in humanitarian aid for Ukraine. Now they have some of the world's leading AI experts keynoting at this event. So Yoshua Bengio, Timnit Gebru, Max Welling, Regina Barzilay, Alexey Efros, and also one of our own personal favorites here on MLST, Professor Michael Bronstein, the one and only. Now the conference is online pretty much from now until the 6th of December, and it's being hosted from Miele in Montreal. Their goal is to raise $100,000, and they really, really need the support of the AI community to club together and to just donate anything that you can So we'll link to the conference in the video and the podcast description Please donate if you can and also share the links on on your socials Now I'm at NeurIPS this week in New Orleans, so I'll be walking around with a camera Please just bump, you. Please just bump into me and if you wanna record any spicy takes on artificial general intelligence, then let's do it. Today is a conversation with Joshua Barke, who's one of our most requested guests ever. We recorded the conversation back in April, which gives you a bit of an indication of our backlog. I can only apologize about the backlog. Keith and I recently started a new venture called X-Ray. And as you can imagine, we've been working around the clock coding, just trying to get that business off the ground. But when we've made our millions, we'll devote all of our time to producing amazing content on MLST. So yeah, please bear with us. Loads and loads of cool content coming your way soon I hope you enjoy the show today peace out. Dr. Yoshua Bach is our most requested guest ever. Yoshua Bach is a cognitive scientist focusing on cognitive architectures, models of mental representation, emotion, motivation and sociality. Yoshua's interview on Lex's podcast, he did two interviews on Lex's podcast, have been watched over two million times so far which is just absolutely unreal. Now Yoshua, I've watched many of your interviews and I still don't feel that I have a firm grasp on some of your views so today if you don't mind I hope we can do a tour de force over some of your most important views in our shared space, to the extent that we can keep up with you of course. Now, for example, we'd like to discuss Godel and computation, consciousness, digital physics, free will and determinism, large statistical models, and indeed whether they're AGI or parlatric or something more esoteric. Now, when people talk about God or consciousness or any other complex phenomena, it relates to everyone and it means something different to everyone. It's ineffable and every conversation sounds like a typical post-ketamine discussion, which is to say extremely low information content. Now, the topics we're discussing today are very complex and often we're reaching for the best language to use to conduct the conversation so I hope we do well today. Anyway, Dr. Yoshua Barker, it's an absolute honor to finally welcome you to MLST. Thank you very much, I'm glad to be on the show. Amazing. Well, when I started doing computer science many years ago, interestingly, the theory of computation wasn't even on the curricula. And I was wondering whether you thought it should be. I mean, presumably you think it's extremely relevant for AGI. Now we want this to be as pedagogical as possible. So please explain everything like with five. What does computation mean to you? I think that computation is easier than most people think. It means that you have a causal structure for every transition can be decomposed into individual steps. And when we talk about computational models, we decompose the world into states and transitions between the states. And then it turns out that there is a certain minimal system that is able to execute everything. And this can be described in many ways. The most famous one is probably the Turing machine and many other ways in which you can describe the Turing machine. For instance, you can just do the Turing machine by doing search and replace on strings. And this is how the lambda calculus is defined. And all the programming languages and the lambda calculus and the Turing machine turn out to have the same power. That is, if you can compute something with one of these paradigms, you can compute it with the others, as long as you don't run into resource constraints. So as long as it still fits into memory and you don't care about speed, they all have the same power. But in practice, of course, every system is limited. So we don't run things forever. We want them to give us a result after a certain time. So what matters is what can be efficiently computed, not what is reachable at all. Awesome. And there are, and we're going to get into this a bit, but there are some possible loopholes, at least in, let's say, whether or not the universe is limited in certain ways that the definitions of like Turing machines are. And one I wanted to ask you about specifically is Penrose's claims. And so he claims that what Gödel's work in fact proves is that the human mind can understand truths that are not provable. So specifically one can show that, you know, a given Godel sentence is necessarily true given the mathematical analysis, even though it can't be proven within the formal system that it's that it's defined. And Penrose claims that this capability to understand, if you will, to mathematically understand, is in fact non-computational, at least in part. And so if he's right, then our brains might be what Turing referred to as Oracle machines. These are computers that have access to a non-computable Oracle or function that they can then utilize those Oracles in order to perform hypercomputation essentially. So I'm asking, I'm curious, are you open to this possibility? And if not, what is your response to Penrose's arguments? I suspect that Godel has been misunderstood by a lot of philosophers. Godel was a truth realist. That is, he thought the truth really exists out there, that it's a thing that is eternal in some sense. He had this very strong intuition. And mathematics classically is also formalized in this way. The difference between mathematics and computation, at least in the standard sense in which we normally teach mathematics at school, is that mathematics has no states. Everything in mathematics just is eternally. It's a single state. And if you want to go through a sequence of states, you put an index into the formula. But still, everything is there at the same time. The index is just a way to access this thing. And this way of having mathematics stateless is very elegant because it allows us to define functions that have infinitely many arguments. If you would have a state machine that tries to consume infinitely many arguments, it would never finish before it goes to the next step. And the same thing in the middle of the function, if you would compute something, if it's stateless, you can just compute all the indices all at once. Even if it's infinitely many in classical mathematics, in a computational system, you would have to do this maybe one after the other. And if you do it in parallel, you will have lots of CPUs running in parallel, right? So you run into limits. And the same thing with the output. So in the classical mathematics, you can chain infinitely many steps and functions and exchange infinitely many arguments. But of course, mathematicians never did this in practice. It's just a specification. This is how they like to write things down. And they want to calculate it, they still have to go down and do it sequentially, step by step. It's just mathematics is defined in such a way as if you could offload this to some supernatural being or some grad student who is going to do the infinitely many calculations. And Gödel took this specification of mathematics and he found out that when you have this stateless mathematics you can for instance define self-referential statements that change their truth value depending on the statement itself. And this recurrence can lead to a contradiction in the state itself. So you basically get two statements which say I am wrong and by referring to itself it changes its own truth value. Right. So if MathMens is stateless, you will now run into a conflict. In a computational system, that's not a big problem. Your computer is not going to crash. If you write it down the right way, it just happens is that your truth value fluctuates in every execution step. It's not going to converge. But this is not the real truth, right? Truth is something that doesn't change when you call it, call the function again. Right? So this, what's going on here? And I think what Godel has discovered is that classical mathematics doesn't work. What you cannot build is any kind of mathematics doesn't allow you to build a machine, a hypothetical abstract machine, any kind of universe that runs the semantics of the classical mathematics without crashing. Yeah, but it kind of seems like, okay, we're going to believe Gödel's use of mathematics to prove that mathematics is flawed. There seems to be almost an inherent contradiction in there. You either believe mathematics and thus you believe Gödel's proof of, you know, some specific limitations on computational systems, right? Or you believe that somehow mathematics is flawed, in which case you can't trust the proof that mathematics is flawed. I think Gödel's conclusion was that there is something fundamentally going wrong, that there might be an inability of mathematics to describe reality. And if you believe that truth is real and it exists independently of the procedure by which you calculate it, then this seems to be plausible. And it was also the conclusions a lot of philosophers have drawn from this, which basically read Gödel's proof and concluded that mathematicians have admitted that their arcane techniques are impotent to describe reality and therefore philosophers who don't understand mathematics have a clear advantage. Of course this is not the conclusion. Instead what turns out is that if you just skip or if you drop the original classical notation as the understanding of mathematics and replace it by computation. Basically we say truth is what you calculate with the following procedure and you can define any kind of procedure that you want. You just have to make sure that it converges to some kind of value in the way that you want. Then you resolve your problem. It's just that you lose your notion that truth is independent of that procedure. And so in some sense the classical mathematics is a specification that cannot be computed from the perspective of computer scientists. This happens all the time. Some customer wants you to build something that cannot be built and you have just proven that it cannot be built. Right. It doesn't mean that you cannot build something useful. And I think that Penrose believed that our brain is actually doing these infinite things. And it's not. When we reason about infinity, we are not actually reasoning about infinitely many steps. What we do is we create a symbol and then we do very finite computations over that symbol. But we cannot construct infinity, we cannot build it, we cannot go there from scratch and write down some clever automaton that produces an infinity for you. I was recently browsing Penrose's book, The Road to Reality, and I would say that, I mean, I don't know that much about physics, but the chapters were really interesting. You know, they're talking about surfaces and manifolds and symmetries and fiber bundles and gauges and wave functions, calculus, matrix theory, and even computation. I mean, almost all of the discussion was on mathematical modeling at different levels of description or emergence, if you will. And in machine learning and AI, we are forever challenged by trying to get machines to model physical reality at different levels of description using an interoperable set of tools. So it seems increasingly true that we need machines that can learn descriptions and concepts at multiple levels if we're ever going to have AGI capable of understanding the world and learning novel semantic models. All of machine learning models today work by chopping up a Euclidean space into what is effectively a locality sensitive look up table, a very big one. And we need AGIs that can go far beyond this. It's got to be able to learn novel geometries beyond even what humans could have come up with and the ability to reason topologically and algebraically over those geometries. Something which I think you would agree is not happening with the current deep learning systems. Let's start out with the notion of geometry first. If you read Penrose's book, what you find is that his entire universe is geometric, which means it's made of continuous spaces in which things are happening. And if we actually look into the world deeply, quantum mechanics is not a geometric theory. The geometry only emerges approximately at the level of the space-time description. And it seems that geometry is actually the domain of too many parts to count. In reality, all the objects that we describe as surfaces, if you zoom in, are made of discrete parts, like atoms and particles and so on. And these in turn are made out of things that have a finite resolution. And if we look into our computer programs, you can create stuff that looks continuous to us, but there's nothing continuous look into our computer programs, you can create stuff that looks continuous to us, but there's nothing continuous inside of our computer programs. And it turns out that the assumption of continuity requires that we partition the space into infinitely many parts. Right, so now we are again running against that thing which GURL has shown gives us difficulty. And it's not a big problem in practice, because in practice we never need to do these infinitely many things to produce a computer game with an arbitrary fidelity. We can make something that looks like space, but the space that we think in and so on is an approximation that our brain has discovered. It's a set of operators that converge in the limit. But the limit doesn't exist. It just happens that when you live in a world that is made of too many parts to count, almost everywhere where you look, you need to find these operators that converge in the limit. And the set of operators that happens to converge in the limit is still computable. This is what we call geometry. And to use these uncomputable geometric approximations for macroscopic physics like Newtonian mechanics is completely fine. You're just going to compute it up to a certain digit and then this is good. But it's a problem for foundational physics. Because if it turns out that you cannot take a language that actually computes infinities, if you cannot construct just your language, then you cannot write a universe in it. Right? So our universe is not written in continuous language, but Penrose universe is. This doesn't mean that geometry is full, we need this to describe the world of too many parts to count, but we do this via computational approximations, our brain does the same. So let me ask you this then, because we've come across kind of the infinities a couple of times, and I know that you place an emphasis on constructive mathematics. So of course, you and all of us, accept, let's say, the existence of potential infinities, algorithms that you can sit there and just keep calculating for as long as you want and get kind of more digits. But it's really around actual infinities that we seem to be running into problems. So let me ask this first question here, really leading up to some computational questions, which is, can the universe, can our actual universe that we're in right now, be actually infinite in spatial extent? The problem is that it can have unboundedness, in the sense that you have a computation that doesn't stop giving you results. But you cannot take the last result of such a computation and go to the next step. You cannot have a computation that relies on knowing the last digit of pi before it goes to the next step. In the sense that you don't have an infinity. But infinities are about the conclusion of such a function. It means that you actually run this function to the end and then do something with the result. Unboundedness is different in the sense that you will always get something new that you didn't expect, that you cannot predict, but it's just going on and on without this end. And I think it's completely conceivable that our universe is in this class of systems in the sense that it doesn't end. But it doesn't mean that there is anything that gives you the result of an infinite computation. Because if that was the case, then it could not be expressed in any language. It also means if something cannot be expressed in any language, that you cannot actually properly think about it. Because when you think, you need to think in some kind of language, not in English, but in some kind of language of sort or in a mathematical language that doesn't have contradictions. And what Gödel has shown is that the language that he hoped to reason in about infinities breaks, that it has contradictions in it, that at some point it blows itself apart. So the languages that we can build are only those in which we have to assume that infinities cannot be built. So infinity in this sense is meaningless because we cannot make it in any kind of language. So the thing is though, I'm not limiting what the universe is capable of based on human, you know, mental and linguistic based on human, you know, mental and linguistic limitations or even mathematical limitations. Like, I'm asking you if it's possible for this universe that we're in to ontically be, right now, actually infinite in spatial extent. The thing is that you try to make a reference to something that you cannot observe, that cannot conceive of, other than making a model in some kind of language. And to have that model make sense, the language needs to work, right? Otherwise you are just maybe in some kind of delusional thing. And we can construct delusional things, we can construct languages that have bugs that we cannot see. But if we use a language that has bugs in it that we cannot see and we cannot repair them, then this means that the stuff that we express in the language is not meaningful. We have to use a different language that has maybe the same expressive power but doesn't have these bugs. But now if you try to think about the universe in a language that allows you to imagine that the universe is literally infinite rather than very very very big and much bigger than you can imagine and not ending which is for all means and purposes almost the same thing right then if you do this other thing then your thought doesn't mean anything so it's basically you cannot properly express the idea in your own mind without running into contradictions that the universe is infinite in the sense that such a universe could exist. Okay, so you're basically telling me… That's the issue. I cannot think that the universe is infinite. I cannot express this. That's my issue. Okay, fine. So you're basically saying that the English that I used just a minute or so ago just is not coherent or not conceivable. There's just, you know, it's not something. The underlying thing we had the English, right? English is not designed to be coherent. It's designed to be disambiguating. It's designed to be unprincipled, to allow us to express things vaguely and not break. But if you think really, really deeply and really exactly, then the question is, what kind of model is your mind building? At which point is there just some kind of noisy nebula that you're pointing at without actually decomposing it in anything that would make sense? Okay. And so the lack of really the ability to conceive or for actual amphendones to ontically exist in some sense, you know, if we just deny all that, so we're really just stuck with, all right, we've got finite everything, discrete everything, there's no such thing as a continuum, you know, there's no such thing as actual infinite spatial extent, etc. That's really the world that you're proposing here, right? That everything is constructed from, at the end of the day, finite, discrete, you know, kind of elements. So if we... So again, imagine your mind is a library of functions in a way, and these functions are doing jobs. And on the box, you write down what these functions are doing. And you construct a box that this, in this box, there is an infinity between, for instance, a continuum between two points. And then you open up the box and look at what's actually inside of the box and you realize it's just a lot of small steps. And it's designed in such a way that you can if you want to have more steps it's going to give you more steps if you zoom in right. And it's totally doing apparently what's written down on the box. But if you look very closely realize oh no the thing that is written down on the box that you have written down on the book cannot actually be in there. You can prove that it cannot be in there. It must be something else that's in there that is doing most of the work of what you've written down. So what you should actually be doing, I think, if you are interested in how things actually work, write on the box, what it's actually doing, which means it's going to subdivide or any interval with any any resolution you want as long as you can afford it. Okay. One mystery, if you will, for me, and I'm hoping you can help me understand this, is that all of the standard models for physics that we have today, they do have in them these continuous, for example, for example symmetries that, rotational symmetry or things like that, they're built off of positing continuums with continuous waves, you know, lots of continuities and infinities, at least in the mathematical descriptions. And I think you would based on what... Except for quantum mechanics, right? Right, yeah, and I think based on what you've been saying, you would say are those are artifacts or properties of our of our mathematical descriptions of reality, but they're not actually extant, you know in reality and my my mystery there is why do those those continuous and And mathematical maybe flawed and inconsistent with infinities all over the place descriptions work so well for describing phenomenon at different levels. If everything at the end of the day, you know, if we just looked at high enough energy and small enough resolution we'd see kind of the grid and you know all the discrete effects and the grid and you know all the discrete effects and rotation happening kind of in little tiny tiny infinite you know tiny very small but not infinitesimal degrees. You know why does all this continuous infinity based mathematics work so well? What is the explanation for the unreasonable effectiveness of that kind of mathematics? The easiest answer is that The easiest answer is that the world in which we live in is made of extremely small parts and we could not exist if that world was not made of that many small parts. So for instance, you want to have a momentum for particles that are almost continuous. So you can address the space with high resolution because the momentum is what tells you where information comes from in the universe, basically the direction from which information reaches you and so on. If that would be very coarse, then the complexity that you could build would probably be far lower and we consist of so many parts that when you look down, it's uncountably many for all practical purposes. So the mathematics that we need to describe the world that we are in, that we need to model are mostly not in the realm of countable numbers. The countable numbers only play a role when we are looking at very few macroscopic things. As soon as we leave this domain of a few apples on our table, we almost instantly drop in this realm where we just need to switch to a continuous description of things. And this is completely fine for most of our history. When we did physics, we never zoomed in that hard. And even now, when we really need to zoom at the level where the Planck length matters and the resolution of the universe becomes visible. And it's of course not some Euclidean lattice, some grid that you can see. It's just that at this level, you no longer have space. I wanted to move matters back over to some of the happenings in the world of large language models and deep learning and so on. And first quickfire question. I honestly, you're a bit of an enigma to me, Joshua, because obviously I've read some of your research and you seem like a hybrid guy to me. You know Ben Goetzel very well, for example, but you're also hugely into the hype train on the connectionism and you, you know, for example, you criticized Gary Marcus's article. So the first question is, are you a symbolist or a connectionist? I'm neither. The thing is that I hate deep learning. It's the best of us. It's just deep learning is ugly. It's brutalist. It's a few very simple algorithms that are blown up to the max. But I cannot prove that these algorithms do not converge to what we want them to converge to. It's maybe not elegant, but it works. And the solution to problems with deep learning so far has always been to use more deep learning, not less. So what upsets me about Gary Markey's argument is not that I'm not sympathetic to what he's trying to push at. I'd like to build models that are more elegant, more sparse, and so on. But in the past, all these elegant, sparse models have been left in the dust by just using more deep learning. And we can also see when we zoom out a little bit that there is not an obvious limit to deep learning itself, because deep learning is not just the algorithms. Deep learning is a programming paradigm. It's differentiable programming. It basically means that you express everything with approximately continuous numbers and you use algorithms that converge within certain ranges. And when it doesn't converge, then you just tweak it and you introduce a different architecture, which is some kind of discrete operations that you do on these continuous numbers and so on. You just patch it, unless you write your program slightly differently. And you can automate the search for the program. And the people who do deep learning are not orthodox in the sense that they say, oh my god, symbolic structures are not allowed. I cannot use a Python script in here rather than just a TensorFlow. This is not what's happening. It's also not that they are constrained to any kind of thing. They will use whatever is working. And what we see is that the end-to-end train systems are growing more and more powerful. And rather than sitting there by hand and tinkering and finding a solution, we can just use a system that is tinkering automatically to a dramatically larger space than we would ever be able to explore by trying all sorts of algorithms. So when we look at Gary Marker's articles like his deep learning is hitting a wall and so on, and you look what he's actually giving as arguments, the arguments are not very good. He gives us an example, the NetHack challenge. NetHack is a game which has a very large horizon because you basically have only one life. You need to explore very deep labyrinths, you need to plan pretty far ahead with what you're doing. And so it's something that is difficult to discover this right solution with a deep learning model that has no prior ideas about what it's doing. Because it takes us very, very long until you get the necessary feedback to learn about your actions. And people are relatively good at learning this because they have so many ideas about what the situation is that they're in. There's so many priors from our world interaction and from other games that we have played that we can bring to the tasks. So the current winner of this is a symbolic solution. And this symbolic solution that Gary Marcus gives as proof that symbolic methods are still ahead of deep learning things. In a single case, right? Not like he has a big array of tasks where they are superior. It's just two students who have written a program that is made of lots and lots of if- of events. This is just a big hack. This is not some symbolic learning algorithm that does something novel, hybrid or whatever. No, this is just a script. And deep, is Gary Marcus seriously proposing, oh my God, deep learning models are limited and we need to replace them with more scripts? This is not a good argument. Yeah, so I think maybe, and look, I get that there are these kind of two competing camps and they maybe go after each other with some hyperbole. No, they don't. This is only on Twitter. There are no competing camps. Yande Kun is not orthodox in the sense that he believes you need to use this argument and all the other arguments are impure and flawed. His brand is to build systems that work. And if one of his people comes up with something that works better than what he came up with, he'll probably praise him for that and let him go on. Yeah, sure. But there's absolutely, however, there is, you know, there is, let's say momentum and hardware lotteries and paradigms that kind of reinforce themselves. And to an extent, they can strain the system. However, there is, you know, there is, let's say, momentum and hardware lotteries and paradigms that kind of reinforce themselves and to an extent they can strangle off, you know, resources that maybe we, like we shouldn't be investing all our eggs in one basket. We shouldn't be pouring, you know, the 99% of research funding necessarily down deep learning. And I think that's kind of the problem that these paradigms cause. But I want to get back to something you said, which is that you can't prove that … No, wait, it's a good point. I think that's an important point. I think that in absolute terms the other approaches get more money than they did before. It's not that we have a funding stop, as we had at some point, a research funding stop for neural networks. And Marvin Minsky wrote a book where he thought he had proven that the neural networks cannot converge over multiple layers, perceptrons cannot do an XOR and so on. Right, Minsky was wrong. People found a way around this, but at this time there was so little funding that this cutoff mattered. And at the moment, if you want to do something that has AI and headline, the chance that you get it funded, no matter what paradigm you're doing is greater than ever. So the absolute amount of funds that goes into any kind of paradigm that you want to work on is greater than ever. And the reason why the majority of funds goes into very few paradigms is because these are the things that work in industrial applications. There is no other algorithm that is able to learn from scratch how to translate between arbitrary languages and generate stories and draw pretty pictures for you. This is the only game in town at the moment, the only class of algorithm that converges over all these many domains. And people are looking for better alternatives. And yes, we are in a bubble because of course they are looking mostly where things already were. You have hardware that works, you have libraries that work and so on. It's hard to get out of that butthole, that is true. And it's always good to push for alternatives and so on. But I don't think that we should be in a panic and say, oh my God, there is something politically wrong. I suspect that by and large, the forces of the markets and the forces of the academic researchers that want to explore alternative are pushing in the right direction already. Yeah, I mean, fair enough. And you know, you could be right and there may not be that much of an imbalance. But I want to get back to one technical thing you said. Yes, it seems apparent that, let's say, what deep learning is doing is this this differentiable program search, if you will. And a question I have about that is if we imagine the space of all possible programs, requiring that we're doing a differentiable search is certainly gonna skew that sample space. It may even cut off programs in that space that can't be discovered easily by differentiable search. So I'm wondering, doesn't that leave open the possibility that other algorithms that are more discrete in nature, say evolutionary algorithms or discrete program search or whatever, they may have access to a different subspace of the space of all programs that aren't easily accessible by differentiable paradigms. Is that true? The question is how do you find it, how you do find these algorithms to manipulate the discrete things. I agree that when you have a perceptual model that is modeling everything with chains or sums over real numbers and if if you know an artist and he gets characteristic artifacts, for instance, in the generative models, you often have the problem when you try to model a person with glasses or without glasses, that because the model thinks that these features are somewhat continuous, you often run into the situation that you get areas in the generative model where the glasses are half materialized and it looks always very weird. And you have these strange things where reality has a discontinuity, but your model has permissible states where you are smack in the middle of the discontinuity and you try to generate something that cannot exist. You want your model to be structured in such a way, ideally, that every model configuration corresponds to a world configuration. And this is not necessarily the case with many of the deep learning models. And what the deep learning models as you train them harder typically tend to do is that they squeeze these impermeable areas until you are very unlikely to end up in them. And it's probably possible to get them to implement filters and all sorts of tricks. But what you can also do is you can combine this with some kind of discrete machine. And then what you do is you learn how to use this. So this deep learning network is not interacting with the world directly, but it learns how to use an architecture that does that. So, for instance, instead of training a neural network to do numerical calculations, you can train it to use a numerical calculator. And in this way, it can become very sparse again. Right, so there's not an obvious limit to that I can see where I can prove to the deep learning people, oh, here's where you should stop deep learning, because they can just combine the deep learning approach with other approaches and use the deep learning system to remote control this. And it turns out when we reason and so on, even when we do discrete reasoning, that the their deep learning approach with other approaches and use the deep learning system to remote control this. And it turns out when we reason and so on, even when we do discrete reasoning, that the steps that we assemble to each other are heuristics that require some kind of probabilistic element. So when we form a sort, and the sort is made of very discrete elements, the search for that sort is some kind of deep learning process that is happening, right? And when we make the pool, we do this, we emulate a discrete reasoning. But of course, we can combine this and we can get a neural network to learn how to perform the discrete operations. There's a certain thing that I would like to see, which is something like a more sparse language of thought. When we are looking at deep learning models, there's a phenomenon that people are sometimes observing which they call groking. That is, you train the model and the model gets better and better and then it overfits. Which means it gets very good at the training data, but it gets very bad on the real world. It thinks that it hasn't seen before. Like a person in psychedelics was able to explain everything in the past, but it gets very bad on the real world. It thinks that it hasn't seen before. Like a person in psychedelics who's able to explain everything in the past, but is no longer able to perform well in the future because they're overfitting. They basically fit the curve too closely to the data that they've seen. And there are many tricks in deep learning to go around this overfitting to make sure that this doesn't happen. And people try to avoid it. And then what they discovered is when you take this overfit model, you train it more and more and more and more. At some point, it sometimes clicks and it gets much better than ever before. And there is a question if there's something that we're doing wrong in deep learning. For instance, when you think about how people learn, they learn very different from GPT-3. People first learn by pointing at stuff, at things that are relevant to them, that they can eat, that can hurt them, or that they find pleasant and so on. That they can feel, that they have contrast on it that are salient to them. And so you start out with learning these semantics based on the saliency and relevance that you have. And then when you learn language, you learn basic syntax, how to put things together, and in the long tail of the syntax, you learn language, you learn basic syntax, how to put things together. And in the long tail of the syntax, you learn style, how to express things with nuance and so on. And with GPT-3, it's the opposite. You first learn style, right? And then you learn syntax as the regularities in the style and the semantics is the long tail of that. And to make that happen, you need to learn much, much more. You need to have more training data and so on. Maybe there's a way in which we can reverse the order and basically get it to start out with relevance, to build a curriculum that you first get very sparse regularities, where it clicks into place and you always make sure that you can handle it with very limited resources and only see the style and the niceties and the nuances as far extensions of of these very sparse concise models that have very big predictive power. Yeah, I mean on that, I mean the Grocking paper was very interesting and a lot of these large language model fans always cite that very, very quickly when you have a conversation with them. But there is a problem with machine learning in general, which is that there is, as you said, there's a spectrum of correlations and almost all of them are spurious. And on one side of that spectrum, you have the idealized features you actually want it to learn, which will generalize after distribution. And then, of course, if you go down that spectrum, you pick up on all sorts of very spurious correlations that just happen to generalize very well. And if you tell the models not to of very spurious correlations that just happen to generalize very well. If you tell the models not to use those spurious correlations, the performance of the model will go down. But I want to just move a little bit over to Yazaman Rizegi's paper. I don't know whether you saw that, but she showed that the performance of large language models for arithmetic tasks are linearly correlated to the term frequency in the training corpus, suggesting that they are memorizing the data set, which presumably you would agree with. And Google has recently released this 540 billion parameter language model called Palm, which interestingly does extremely well on, for example, some of the Google Big Bench tasks such as the conceptual combinations task, which is one of them, which tests for compositionality, which we'll talk about in a minute. But you know, compositionality is when you can take constituents from the prompt and compose them together to form the answer. Now it's tempting to jump to the conclusion that these models are starting to magically reason at scale along the lines that you were just discussing. But I still think there's plenty of opportunities for shortcut learning, you know, by which I mean these spurious correlations, given the brittle interface of an autoregressive GPT style language model with these human designed benchmarks. Would you agree with that? Yeah. When I started my own career in computer science in the nineties. I was in New Zealand and the prof, Jan Witten, realized that I was bored in class. So he took me out of the class and in his lab and he gave me the task to discover grammatical structure in an unknown language from scratch and left me pretty much to my own devices on how to do this. So the unknown language I picked was English. It was just unknown to the computer, but it was the easiest one to get a corpus for. And they gave me the largest computer they had. It has two gigabytes of RAM. And I did in-memory compression with C and so on, and tried to do statistics, and I quickly realized nRAM statistics don't work because of too many words in between. So unlike VisionTask, where ConfNets have useful prior by thinking that adjacent pixels also relate to semantically related information. So adjacency in images is a very good predictor for semantic relatedness. It doesn't really work in NLP. So the transformer was discovered in natural language processing for that reason, because you cannot use direct adjacency very well. And so I realized I cannot use n-grams, which depend on direct adjacency between words, and so I first of all use ordered pairs of words and try to find correlations between pairs and then find a mutual information tree that would give me the best prediction over the structure of the sentence for all the sentences that I would have in my corpus. And indeed this correlated to structure and I realized this is going to not just give me grammar but it's also going to give me semantics if I can more deep statistics. But I will need something not just ordered pairs but I need to have something like fourth order models. But to do the statistics even in memory was clever in memory compression and many tricks that I did. I could not do full statistics on this. So what I realized I had to do was that I do multiple passes. And at first I discard almost all the information. I only pick out the most salient things. And then my time was over in this lab and I went back to Germany and never revisit this area of research again. But what I had realized is to make progress, I need to make statistics over what I need to make the statistics over. And the very principled base, I need to learn what I have to learn. And I didn't pay attention to this domain at all, and I also missed the 2017 transformer paper and its relevance. It was only when GPT-2 came out that I realized, oh my God, they did this. They did statistics over the statistics. And it's still not the right solution, I think. It's not the way in which our brain is doing it. It's some brute force shortcut, where, for instance, the individual attention heads are not correlated with each other, but in reality they are. In reality, our attention heads are integrated into one model of what's going on. And it's not that we have an attention net on every layer that just pays attention to what's happening in the lower layer, right? It's much more clever in our own mind. And this thing is active. We single out things in reality. We search which book we need to take out of the shelf to update our working memory context so we are able to interpret the current sentence that we don't understand. And so we always go for saliency when we read something that doesn't make sense. At least my mind works like this. I discard it. I will not stop until it makes sense or I will have to go to some preliminary. I will not accept some kind of vague statistical approximation of what I read. Keep this as an intermediary stage in my mind until with the hope that eventually converges. Right? So it's a completely different learning paradigm. When we teach our children arithmetic it's not that we show them lots of very long math textbooks and hope that initially that will not make any sense to them but as they re-read them again and again with many samples, eventually it will click and they will converge on arithmetic. No this is not how it works. You start with giving them extremely simple things and say, in these extremely simple things there is structure that you can fully understand. Now go and find the structure that you fully understand. Once you've done it, we make this a little bit more complicated for you. This is probably the paradigm that we could be exploring. I know, but the problem is it's incredibly deceptive when you have something which appears intelligence and of course the boundary of our perception of intelligence is a receding one. But I wanted to just get on to, there are some incredible generative visual models like Dali and the disco diffusion. So these models, I think are going to revolutionize the creative profession. I've been playing with disco diffusion all day today. I've already ordered some prints to go on my wall. It's incredible. So yeah, the two obvious settings where large language models might be successful are coding and information retrieval, in my opinion. But let's take pause for thought. So I've played with Codex, and I'm resolutely sure that I wouldn't want to use it. I think code and knowledge are a different ballgame to art, which I think will be amazing. With Codex, there's an impedance mismatch between the process of generating the code and then debugging and running the code which has euphemistically been framed as prompt engineering or another term which I've just invented, retrospective development. I think it's easier to start again from scratch than fix broken code from a large language model. I mean this is quite interesting at Google it already takes months to get any code checked into their mono-repo because it's basically a bureaucracy because they needed to have gatekeeping after they decided to use a mono-repo. So could you imagine how much bureaucracy there'd be if they allowed people to start checking in code which was generated from an algorithm. So anyway, I think there's an exciting possible future for using these systems for information retrieval, rather than the way that we go through and prune the results on a Google search. These models might just answer directly, but I hasten to think what that search UI would look like. Would its output be sclerotic or unadaptable? Would it be relevant to the query that I put in? Would its output even be true? Perhaps it will ask you to select what kind of truth you are looking for. So, do you think these models would vitiate or spoil our society, or do you think they would actually enrich it? It's very hard to say. I think that from some perspective, our society is already maximally spoiled. Humans, as they live today, are basically locusts with opposable thumbs. This is not going to go on forever, this technological society. We are, it seems to me, on some kind of Titanic that is going to hit the iceberg no matter what. And what basically should make us content is that the Titanic was the only place in the universe that has internet. And we are born on it and we wouldn't have been born if there was no Titanic. We would not have been born in a sustainable ancestral society. Right, so in some sense our society needs to reinvent itself. It's not really working right now and we don't know what the future is going to look like and if it's going to be very technological or if limit certain things. No idea what's going to happen. But if you think about how our current approaches work, if you want just to make programming better, I suspect that these tools can help. But they will be much more useful if you do not have to have this battle between a machine that doesn't really understand what you want. And instead you have something that is working next to you. It's like, imagine you were working for some corporation and the corporation introduces some kind of planning tool that requires to do, to jump through all sorts of hoops. And it turns out that the planning tool itself makes you more productive, but it makes work much less fun. It's still rational to use it, and everybody will hate it, but by and large it will be used if it makes people slightly more productive. And everybody will feel there must be a better solution, something that feels more organic. And so it could be that Codex is in this category, that it makes mediocre programmers much more productive at producing boilerplate. But it's not just this. It's often able to find solutions very quickly, but you need to use a lot of Stack Overflow before you understand the new language or before you tease this new algorithm apart that you want to understand and so on. It's just when it doesn't probably turn you into a better programmer, if that is your goal. But for your employer, maybe they don't care whether you're a better programmer. They just want you to churn out these pages of code and then they run this against the verifier and against the unit test and then are done, go to the next thing. So maybe it's not that important. But the systems, if you would want, what would they look like? I think they need to know what they're doing. You want to have a program that is not just able to reproduce something very well in a given context. You want to have a program that is not just able to reproduce something very well in a given context. You want to understand the context as deeply as you do or better. So it needs to understand what kind of world it's operating in in the moment and what itself is, what is it that it can do, what is it that it needs to learn still. So in some sense, you want systems that are sentient. And this sounds like, oh my God, that just means you have a learning system that's general enough to model in principle the entire universe. And this is not as outrageous as it sounds, because Dali is already dealing with two modalities, language and images. And we will get to video and we will get to audio, all connected, and you see them early steps in this direction with the Socratic model paper for instance. So I think that's almost inevitable that this generality will happen and you will have to let the system to work in real time so it can discover itself. I think there's something really magic though about the creative process here and also the prompt engineering is another thing we can talk about. But Kenneth Stanley once made this thing called PicBreeder. And you could essentially distribute the selection of these images created with CPPNs, Compositional Pattern Producing Networks. And you would just get these beautiful images. They were so incredibly diverse and interesting. So it's not that the algorithms were intelligent, there was something magic about the externalized process. And what's really interesting about these models like Dali, for example, is that creativity has been distilled down to a raw idea in your head, right? So for example, I might decide to mix the style of two artists and combine them with a new subject. I want a black cat in front of Royal Holloway University in the style of cyberpunk. I've been doing that all day. And the technical process is now done for you. The only limit is your imagination. So just like Kenneth Stanley's pickbreeder, creativity itself has now become this uber-efficient and externalized process. I think it's unreal, but the thing is, like the reason I never thought GPT-3 was intelligent is because it can't be used non-interactively. The magic must happen when it's used by humans interactively. Well, you can basically build a machine that is generating prompts for GPT-3. So in principle, you can build a robot that has a vision to text module and that is used to prompt GPT-3 into generating a story about a robot who sees these things and interacts with them. And then you take the output of the generative model and translate this using text to motor module. And in this way, you close the loop. And while this, I just used it as a thought experiment to think about the limitations of embodiment for such systems. Seikan is essentially doing that. Right? So somebody has made this happen and it, even with the language model, it works to some degree. And we know that we don't want to do this with natural language because natural language is a crutch. The systems make up for this by you just using more natural language pasta than people could use it. But there is some language of thought that we are using that is not learned but discovered by our own mind that we converge on. That is much more efficient. And this language of thought seems to be able to bottom out and perceptual distributed representations that are unprincipled like these neural networks are in a sense unprincipled, so they don't break. Then there is something that is vague and ambiguous and has small contradictions in it. But at some level, it also is able to emulate very principled logic very well and becomes very sparse and very powerful and expressing things concisely. And this very concise out that there is an issue with compositionality in this. Right. So, for example, if you have a very large number of elements, you can have a very large number of elements. And you can have a very large number of elements. And you can have a very large number of elements. And you can have a very large number of elements. And you can have a very large number of elements. And you can have a very large number of elements. And you can have a very large number of elements. And you can have a very large number of elements. And you can have a very large number of elements. And you can have a very large number of elements. And you can have a very large number of elements. And Gary Marcus points out that there is an issue with compositionality in this. Right? So you need to find the semantic structure of a sentence that is made of a hierarchy of concepts. And this is easy to do with the grammar. And it's much harder to do this with a deep learning system that needs to discover this in a way and structure the space in the right way. It's not impossible. So when Gary Marcus says these models cannot do this and cannot learn it, deep learning system that needs to discover this in a way and structure this space in the right way. It's not impossible. So when Gary Marcus says these models cannot do this and cannot learn it, he is probably wrong. But I think he is right in the sense that this is something that is much, much harder for the current approaches. They need dramatically more training data than a human being, and the algorithms are not doing this naturally. So there are probably ways in which we could make this happen much more elegantly and quickly and converge, for instance, on models for arithmetic. That's right. I mean, I remember I read a really good Twitter thread, I think it was by Raphael Miliere, you know, about compositionality of these large generative vision models, because usually compositionality is referred to in respect of language models. I think Raphael said that the assessment of the claim is complicated by the fact that people differ in their understanding of what compositionality means. But if language is compositional, as you say, and thought is language-like, as argued by the proponents of this language of thought hypothesis, I think Raphael said that he thought language itself should be compositional in a similar sense and perhaps by extension visual imagery should be compositional. So I think Gary was arguing in a nutshell that it's hard to go from the image or let's say the utterance if it's NLP to the structure or the grammar or the constituents. It's much easier to go the other way around, would you agree? The issue is that language of thought is executable and natural language is not. We execute natural language by translating it into our mind in something that we can execute. When you reason about code, you might use natural language to support your reasoning, but the code that you build in your mind is built in some kind of abstract syntax tree that you can actually execute in your mind to some degree. And then you get a sense of the output. So you entrain your own brain with an executable structure. And this executable structure has properties that are quite similar to the ones that the compiler has in your computer. So you can anticipate what the compiler is going to do with your code. It's not going to do this with all the depths that your compiler do it. You might still have to run your code, but you will find when you're an experienced programmer, your stuff will usually run. So our language of thought can do this. It can execute stuff. And it's not just some mushy neural network that guesses what the outcome is going to be and is right some of the time. But it gets pretty good at figuring this out. And this means that it has to build this compositional structure that has some verifiable properties. And we observe ourselves operating on this verification process, right? When we do introspection for every program, we observe ourselves, how we direct our attention on making proofs. And this attentional algorithm that works in real time, that is making changes on your mental models and then predicts the outcome of these changes and compares this with what your mental computations give you and then fixes your models of how your own thinking process in this domain works and so on. You can observe yourself doing that. And it's nothing where I would say a given approach or the given approaches that we have will never get there. But there seem to be ways in which we have just barely scratched the surface in what you need to be doing to make these models sample efficient and sparse and more adequate to model domains you're interested in. bars and more adequate to model domains you're interested in. Cool. Okay. Well, I mean, just to finish like the discussion about the open AI stuff. I mean, I agree with the prognosticators and I do think that these large language models and these visual generative models will be revolutionary for some domains, but, you know, I think you really need to have a human guiding the creative process, which is a huge limitation, but I think you really need to have a human guiding the creative process, which is a huge limitation. But I think it could also potentially hint at what intelligence actually is. I think intelligence might be this externalized process in a cybernetic sense, if you like. This idea of intelligence being fully embedded in an algorithm in a single agent might be the wrong way to think about it. I think that humans by and large are very confused. Very often you need a human to guide a human, right? And then you ask yourself, if you do this recursively, does the society know where it's going? Or is this at some level confusion at all levels that is balancing each other? So there seem to be very few people with a plan right now. And it's quite apparent that we see this in the sciences, we see this in politics, we see this in art and literature. It is that humans have a higher degree of sentience, but by and large, very few people have a principal plan on how to build a sustainable, harmonic world. And if you set an AI system to this task, it might make more progress on it. It's just that DALI is not operating in real time on a universe that is entangled with, and neither is GPT-3. Both of them are in some sense fancy autocomplete algorithms. But this fancy autocomplete is able to do autocompletion that are far beyond the autocompletion abilities of humans in almost every context. And so I don't see DALI yet as art. It's a very strange sense when someone at OpenAI let me throw prompts at DALI 2 and I got images back. I had a sense of ownership. I had the sense that I was doing that, even though it was clearly drawing skills using skills that I didn't have. And I suppose that you had the same impression when you were generating things with your diffusion model that you're going to put up on your walls, right? You did that using this amazing tool that was empowering you to do things that you otherwise never could do. But you are the creative nexus. And to make an artist, a digital artist, you would need to create an autonomous creative nexus in a way, a creative entity, something that reflects on the world, because art is about capturing conscious states. So we would need to build a system that has a story about itself and that is reacting towards own interactions with the world. And the thing wouldn't need to be human. It would need to be consistent, Something that is an intelligent entity that is creatively interacting with the world. I think we could totally build an AI artist franchise right now that would have a huge following, but what it needs to have is an identity that is not fake, that is actually built from its interactions with the world in real time. Well, I think we've got a lovely segue there because you said that art is about representing our conscious states. In a way, I disagree with you because you could say, well, it's very reductionist. I've just put a prompt in there and I've created art. Well, I think it is art, but how much of a representation of my conscious state is it? I think Douglas Hofstadter would say it wasn't. But over to the matter of consciousness because we're a bit low on time. You said actually that you've spent much of your life thinking about what consciousness is and you said that you thought it was very mysterious but you now think that it's a riddle that can be solved, right? So on your recent Theory of Everything interview with Donald Hoffman actually, you said that it was virtual, not a physical thing, that brains are mechanistic and that the elements of consciousness are magical somehow, but you said it had an acausal structure, but not the way physics is built, but it was a story which the physical system tells to itself. You said that the organism is a coherent and consistent pattern which is state-building at least at some level of analysis and that consciousness allows organisms to coordinate their cells to succeed in their niches and then you spoke of information processing over cells. Now what model of, I should say, like what measure of consciousness do you think you most align with? There is only one should say, what measure of consciousness do you think you most align with? There is only one theory that offers a measure of consciousness, and that is the integrated information theory, whereas you actually put a number on it, and it's not clear what that number means. It's not that there is some kind of scalar that measures this. And people, we think of consciousness as something that is more qualitative than quantitative. Either somebody is conscious or somebody is unconscious. And when you are conscious, you can have a lack of acuity. You can be adult in your brain and you can be drifting in and out of consciousness. But it's still a qualitative thing of whether you have that or not. And this qualitative thing seems to be simple. Probably much simpler than people expect it to be. The hard thing might be perception. And consciousness is on top of perception. It's a certain way to deal with our attention. So I think a very important aspect of consciousness is reflexive attention. That we notice ourselves attending to something and we reflect on that and integrate this in our model. The conundrum with understanding consciousness, if you go right into the history of everything starting with Leibniz and many others, Leibniz has this idea of, imagine you could have a mill and this is your mind, you could have a mill and this is your mind. And the mill is made of lots of mechanical parts and somehow the thing is feeling and perceiving things. And we blow the mill up so large that we can walk into it or if we would today, zoom into it until you see all the parts. And we just see these pushing and pulling parts and nothing of them can ever explain a perception or a feeling. And it's this very strong intuition that also drives the Chinese rule and many other thinkers will get attracted to this and it seems pretty obvious that these mechanical phenomena are insufficient to explain what's going on. It's not an obvious connection. So people become dualist. There somehow are two completely separate domains. And I think in a way this dualism is correct but not in the sense that the mental states are ontologically existing. They exist as if. There is no organism. There is only this connection of cells and this collection of cells is acting in a coherent way which means we can compress it, we can model it using a very low dimensional, much simpler function than look at all the cells in general. And the organism is only approximating this function but what makes the organism more powerful than a collection of cells is exactly that function, this structure that we project into it. And the interesting thing is that by the information processing within the organism, the organism can discover that function by itself and use it to drive its own behavior. So while the organism is not a person, it's not even an organism, it is very useful for the organism to behave like it was an organism and also to have an idea of what it would be like to be a person that interacts for instance with the social world. So it creates a simulation of that. It's often not even a simulation, it's often just a simulacrum. That's what makes it acausal. The difference between a simulation and the reality is that the simulation is modeling some aspects of the dynamics of a domain on a different causal substrate, on a different causal footing. So you have a computer game in which you can shoot a gun, but there is no proper physics in the game that would recreate what's happened in the real world when you shoot a gun. Instead it is using a different causal structure of your software program to give you something that gives you good enough dynamics so you can interact with the world and experience these causal structures. You can make a different decision, you make different moves in the game, and as a result, the game behaves as if you would expect it, because it's imitating the same causal structure using this different substrate. In a simulacrum, you don't have the causal structure. Like a movie doesn't have causal structure. It only gives you a sequence of observables. And our own mental model of ourselves is a mixture of simulation and simulacrum. So we sometimes create a sequence without causal structure. It looks like it does this thing magically. And sometimes we have a causal model, but this causal model is not the real deal. It's just this simplified geometric simulation of how the world works. It's a game engine that our brain is producing to anticipate what happens in the physical world. Yeah, so that very strongly resonates with me. And another person I very much respect whose opinion resonates with me regarding consciousness is Carl Friston. And I'm not sure how much you're familiar with his free energy principle and his thoughts on consciousness, but I'd like to put forward to you one of his more recent definitions, if you will, or proposals to explain consciousness, and get your opinion on it. This is from his 2018 article titled Am I Self-Conscious, or Does Self-Organization Entail Self-Conscious or Does Self-Organization Entail Self-Consciousness? And what he says here is, the proposal on offer here is that the mind comes into being when self-evidencing has a temporal thickness or counterfactual depth which grounds inferences about the consequences of my action. On this view, consciousness is nothing more than inference about my future, namely the self-evidencing consequences of what I could do. Does that align pretty closely with your view? No, I don't think it's sufficient and I also don't think it's necessary. I like Friston's idea, but most of the free energy principle comes down to predictive coding, which is in some sense radically tested with GPT-3. GPT-3 is trained in some sense entirely on predictive coding. It's only trying to predict the future from the past. And the future is the next token, based on the tokens that it has seen so far. And GPT-3 radically tries how far you can go with this. And you can go very far. But you need far more samples than an organism does. So there are players in us that go beyond predictive coding. Maybe we converge towards this over many generations in the evolutionary process. So I don't think it's a stupid idea that Carl Friston proposes, but we are born with additional loss functions that let us converge much, much faster on something that is useful to the organism. And if you think about consciousness, he has a point about agency in there. Agency means that you have a controller that is able to control the future. It took me some while to understand this, but when I grew up, we talked about BDI agents see means that you have a controller that is able to control the future. Took me some while to understand this, but when I grew up, we talked about BDI agents and they seem to be quite complicated and convoluted. You need to put a lot of code there to make a BDI agent, but there's beliefs, desires, and intentions and so on. But if we think about what actually is a minimal agent, a thermostat is not a minimal agent, a thermostat doesn't have agency, it doesn't want anything. It just acts on the present frame by doing the obvious thing. But imagine that you give the thermostat the ability to integrate the expected temperature differences over the future when it does x now or y now or does it a moment later, right? So suddenly you have a branching reality and in thising reality, you can make decisions and you will have preferences based on this integrated expected reward. So just by giving the thermostat the ability to model the future, you turn it into an agent. This is sufficient. And if you make this model deeper and deeper, it's going to get better and better at it. And at a certain depth, the thermostat is going to discover itself. It's going to discover itself. It's going to discover the idiosyncrasies of its sensors and notice that the sensor operates differently when it's closer to the heating element and so on and so on. Right. So it becomes aware of how it functions. It might even become aware of the way in which its modeling and reasoning process works and to improve it or to account for its inefficiencies in certain ways. And this is also what we do with our own self. But this model of the self is not identical to our consciousness. Our consciousness is a feeling of what it's like in the moment. It's the experience of a now. There is an experience of a perspective that we are having. And this is what's absent in the description of Friston. He is missing the core point of what it means for some to be conscious. It doesn't mean that it has a self. It doesn't even just mean it as a first person perspective. It means that it experiences a reality. And that's not described in this Friston quote. We explicitly asked him this question actually when we talked to him last time and his response there was that this concept of feel like is really something that would need to be coded into the generative model that this agent has about the world. Number one, it has to have a generative model as we've just been discussing. It has to be able to entertain counterfactual possibilities for the predictive coding, right? And he's saying that these feel-like concepts would literally be encoded in that generative model as hypotheses that we recognize. So things like I'm feeling pain, for example, would be a concept within that model. And he says there's actually evidence from you know treating patients with chronic pain and this sort of thing that that's actually exactly what's happening in the mind that that the feeling of pain is actually a concept that's built in as a slot if you will into this generative model. I mean, what do you think about that proposal? The semantics of pain are given by the avoidance. That you don't want to experience the pain usually. And it could be that you cultivate the pain and use it to make something happen on the next level, but it requires that you are then building a multi-level control structure if you want to use pain productively, some artists are maybe doing. But you cannot have pain, I think, without an action tendency, without something that modulates what you are doing. So your cognition is embedded into this engine. And to build such an engine that does it, that causally changes how you operate, is not that hard. But when you live inside of such an engine, it feels very strange that there is something that is happening that somehow depends on what you are thinking, but you cannot control it. It controls you. It's upstream from you. You are downstream from it. And when you get upstream of your own pain, the pain stops being pain. It's something that is a representation that you can now control. And we are able to get there, but it's not easy and you're not meant to get there because it means that we can immunize ourselves to pain and sacrifice the organism to our intellectual interests. What's crucial about feelings when you look at them introspectively is that feelings are essentially geometric. I don't know if you noticed that. So for instance we notice feelings typically in our body. And that's because I think that the feelings play out in a space. And the only space that we have always instantiated in our mind is the body map. So they are being projected into the space to make them distinct. And when we look at the semantics of the feeling, we notice that they are contracting or expanding or they are light or they're heavy and so on. This is all movement of stuff in space, right? It's all geometry. Plus valence, the stuff that is going to push your behaviors in a certain direction. So these are basically the interactions of some deep learning system that is producing continuous geometric representations as perceived from an analytic engine. It's an interface between two parts of your mind, between the analytic attention control that is reflecting on the operations that your mind is doing while it's optimizing its attention and the underlying system that represents the state of the organism and tells where you should be going and makes this visible to you. A system that is not able to speak to you uses geometry. And these geometrical features, this is what we call feelings. So that's a very interesting connection and I think Jeff Hawkins of Numenta would be quite interested in that as well because some of what he discussed with us was that in his view the evolution of let's say abstract thinking and what not actually came from systems that evolved to operate in just simple three dimensional kind of motion and that eventually those were reutilized by the evolutionary process to start engaging in abstract thinking, which he views as movement through an abstract space. And so I think there's a lot of connection here to what you're saying about feeling, which is that again, in a sense, our mind has reutilized this, this three, three plus one D, you know, movement mapping capability that it needed in order to survive in a three plus one D, um, you know, environment, physical environment, and it's reutilized those for mapping feelings. It's reutilized them for mapping to abstract thinking is like a form of motion and in an abstract space. Is that a fair connection? Yeah, yes, but I don't think that it's because it's borrowed from the world in which we interact, but because of the, it's the only game in town. It's the only mathematics that can deal with multi-dimensional numbers. Right, so when we talk about spaces, we actually talk about multi-dimensional numbers, about things that are not just a scalar in a single dimension, but features that are related. And sometimes you can take these features that you measure continuously because they have too many steps to meaningfully discretize them. Right. So what you do is you sometimes discover that you can rotate something. And this is when you get a space in the sense as we have a space to which we are moving. And these spaces which you can rotate things only exist in 2D and 4D and 8D. And so the geometry that we're talking about is constrained to certain mathematical paradigms which you can derive from number theory for first principles. And our brain is basically discovering a useful set of functions to model anything, a set of useful computational primitives. And we can probably give our deep learning systems a library of predefined primitives to speed up their convergence. That's also the reason why there is useful transfer learning between different domains. You can train a vision model and use it as a pre-trained thing for audio. And it's not because it's the same thing, but because it has learned useful computational primitives that it can apply across domains. But there is geometry in the audio signal. So this is very interesting territory. I hope you'll come back to dive into this a bit more deeply when we have more time and a better connection because I agree with you. I think there's some very fascinating math here. Fantastic. Well, Dr. Joshua Buck, as's, as I said, you by far the most requested guest that we've had right from the very beginning. So it's an honor to finally get you on the show and I hope we can get you back soon for a longer conversation. Thank you so much. Likewise. I enjoyed this very much. Let's meet again soon.", '38.96499490737915')