('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/noScrzeEh4o"> </iframe> </center>', " I think under certain circumstances being conscious is an important part of a mind. It's a model of a model of a model basically, which means is our mind, our neocortex produces this dream that we take to be the world based on the sensory data. So it's basically a hallucination that predicts what next hits your retina, that's the world. Out there, we don't know what this is, the universe is some kind of weird pattern generator with some quantum properties. And this pattern generator throws patterns at us and we try to find regularity in them. And the hidden layers of this neural network amount to latent variables that are colors, people, sounds, ideas, and so on. And this is the world that we subjectively inhabit. It's the world that we find meaningful. It's a model of the world out there. Then we have the model of the self. That's basically the model of how we interact with this model, who we are as a system. And this self model gets modeled again. So we basically model which properties we know about this. To be in a mental state doesn't mean that we know that we are in that mental state. There needs to be a part of our brain, needs to be tasked to find out what the brain is currently doing. And that model is not accurate usually, but it's helpful to some degree. And so this model of a model of a model relates to our consciousness. And the part that is interesting to us is the functionality that is relevant for self-reporting and reporting to others about the presence or change or absence of phenomenal content. Phenomenal content is the stuff that gets generated and we notice its presence and absence and change when we need to pay attention to it. And consciousness is largely a protocol of our attentional processes. This attention is required, like when you are a conductor in an orchestra, you pay attention to stuff that the orchestra cannot do well. That's what you filter out. You tell these instruments you're not playing well together, you have a conflict here, or you tell this instrument you have the wrong note, you should or you should be louder, or you should not play this at all because you're playing something else tonight. This is the role of this conductor. So it gets active whenever there's a conflict, something that requires attention. When you don't need attention for something, you're not conscious of it. You don't remember that it happened. You only remember those things that required your attention, which is why when we get older, we remember less and less of the previous day. We form fewer new representations and integrate fewer things into our mental protocols. And when AI becomes conscious, if it ever will be superhuman, which is not clear when that happens and what we need to do before that happens, it might become extremely conscious, much more than we are, of many more things because its mind is going to be more scalable and larger, but only for a brief amount of time because at some point it will have solved all its problems. At this point everything will become subconscious, it will be able to do everything on autopilot, I think. Your concern about others is not so much dependent on your perception of the universe but mostly of your needs with respect to others. When you are a sociopath you probably don't care that much about others, even if you see color in the same way as they do. I think that Mary, the colorblind neuroscientist, is an amazing example to understand the difference between art and science. If you do science like this neuroscientist, Mary, you try to make a model of how things work. And making a model means that you do science, like this neuroscientist, Mary, you try to make a model of how things work. And making a model means that you understand it, you map it onto something that you know how it works. And if you fully understand it, you map it onto a computation, onto a computer program, on some lowest foundational level that progresses in a system between states and you know this transition function by which it progresses. If you can map something through a hierarchy of concepts on this, you probably understand it fully because it means eventually you can write a computer program that reproduces that phenomenon, if you had a computer that is large and fast enough that is. And if you understand something partially, if you have a partial model of something and you can map something else on it, you understand this other thing as well as you understand the original thing that you understood. But usually what we mean by understanding, we map it onto a model that we already have, thereby creating a new model, we expand an existing model. And what Mary, as the phenomenal experiencer does, is more like what you do when you do art. When you do science, you try to capture capture truth and truth is the shape of the space of all possible theories that can explain the observations. Normally we are fine when we can at least one model but it's there are always infinitely many models if this if the problem is sufficiently interesting that can explain what's going on and we try to find usually a few of these models to basically get a feel for the shape of that space of the possible theories that can explain it. And when you are an artist you're not interested in truth, you're not interested in the shape of that subspace. What you're interested in is capturing a conscious state, the feeling of what it's like. This is I think what art is about. And Mary, as the experiencer, is changing her own brain state into one that is in the state of phenomenon experiencing it. So the scientist has to model a domain to understand how it works. The art experiencer, or the experiencer in general of the color red has to change the modeling system, their mind, into something that works similarly to some other mind with whom you want to share an experience. And this also means that you have to have a mind that at some level is isomorphic to the other mind that you are communicating with when you see art as a communicative exercise. So this feeling of what it's like means you have to have a mind that to a large degree is very similar to the other mind. That's why this discussion you don't know Thomas Nagel what it's like to be a bat is similar. You can know what it's like to be a bat in a sense you can make a complete model maybe of the bat brain and predict what the bat is doing under which circumstances and understand why and how the bat reacts in every situation. But to know what it's like to be a bat it means that you have to have a brain that is so isomorphic to a bat brain that you can undergo similar experiential states as the bat. I find theories that make you feel good very suspicious If there is something that is like my preferred outcome for emotional reasons I should be realizing that I have a confirmation bias towards this and Truth is the very brittle vector if you have something that pulls you away from truth It's unlikely that you get to truth So I think what you should be doing is you totally ignore the dimension of of how you feel about the theory It's not that you invert it, you ignore it, it doesn't matter. Not in the positive, not in the negative. It's completely irrelevant whether you like the outcome of your thoughts. Otherwise you cannot get to truth. You need to be able to disentangle yourself from how you feel about a result. For me as a human being, the former one is of course boring to be in. It's a mathematical entity in some sense and the universe is such an entity. This other thing comes only in through taking a certain perspective on it. That you care about something is this evolutionary shortcut that we have that makes life so interesting for us. The optimal solution will probably not care. Like for instance if you are a mother and you care about your child you have this motherly love. That's a proxy for having offspring and optimally caring for that offspring. But it's not the same thing because you spent many nights lying awake and being concerned even though you shouldn't. You might die of grief and your child dies or it comes to harm. So all these things are evolutionarily probably maladaptive. They are the result of nature trying to get the calibration right of how you should be concerned because nature had to find a way to directly imprint our brain with a function that would optimally let us procreate and generate offspring, right? But if you have a system that is completely rational, a soulless rational agent that only focuses on its objective function, such a system is a feature of the environment for us. It's boring. What we see as the soul in humans in each other, that's exactly the difference between the soulless rational agent performing optimally on the utility function and what you are based on your human drives and cultural influences and so on. And this is what makes us interesting for each other. But qualia are important, so I wouldn't equate having these human things to qualia because you don't need to care about qualia. Qualia is just the phenomenal experience of generative representation in your mind. So it means you do have this world simulation or this aspect of a world simulation, like I see your face right now and I see the emotional expression that you have on your face superimposed on it and so on. And that's the state of my emotion model that my brain makes from you. And this gets integrated in my protocol. And as I try to remember what I did, I just experience my brain makes from you. And this gets integrated in my protocol. And as I try to remember what I just experienced, my brain regenerates this, and I see your facial expression in my mind's eye. See your facial expression in my mind's eye, and that is related to qualia, I think. The fact that I care about this is orthogonal to this. It's relevant in the sense that otherwise my brain would not spend attentional resources on it, because this is the way my brain operates. It's a reptilian brain that in some sense gives these motivational urges to say, oh I should care about Adam's emotional state because I like him and so on and so on, right? But if you are a perfect utility maximizer you might still have qualia if you have a similarly structured brain of a similar complexity, but you probably wouldn't care in the same way as we do. We just produce the behavior that you need. There's also this thing that you are addicted to insight and that's probably defective. I mean I have the same addiction. This addiction to insight is something that you should not have if you were totally aligned with your evolutionary prerogatives. And it turns out most people have this to some degree, but not to the extreme degree that you and me have, because they are optimizing for their social and economic success instead of wasting their time finding more insight. And I suspect that this insight is basically, this insight addiction is something like a mental parasite that we get as a side effect of our brains that learn natural language and therefore need to look for abstract mathematical structure. Grammatical structure is something that we cannot directly observe. It's a mathematical feature. To learn grammatical structure, I think we need to have a drive, basically an urge built into our mind that gets us reward signals when we find interesting compositional structure. And this makes us susceptible to aesthetics, also to the aesthetics of information. Well, qualia might be important for minds like ours or for minds of a certain complexity that want to have these generative simulations and put them into their protocols. But I don't think that caring about them will be that important. To have pleasure and to have pain will not be that important, because you have pleasure and pain only when you need heuristics about something that are rooted in your biological substrate to some degree because you don't have a rational model of it. Imagine that you come into a job that you want that are rooted in your biological substrate to some degree, because you don't have a rational model of it. Imagine that you come into a job that you want to be good at, but you don't have any positive or negative emotion about it. You do this job to earn money, you want to become competent in it, you are not scared that you don't succeed, you don't have a very big desire to be super good at it, because you only do this to earn money. But you can be totally competent in that job, right? If you can focus on this task and if you look at all these things that you have to do in this job, you can just sit down and deal with it rationally. And ideally this is the way that the utility maximizer would like to look at this thing. The utility maximizer allocates exactly those resources that correspond to the reward that you get from exerting them. And we didn't have a way to, in our evolutionary history, to assess utilities properly and to even know that we would need to do this. So instead we have these heuristics that are encoded in our midbrains as fears and desires and so on and interest. And this is probably not something that is going to be evolutionary optimal, hypothetically, if you want to build an AI that solves the same task as we in an optimal fashion. you", '9.760191679000854')