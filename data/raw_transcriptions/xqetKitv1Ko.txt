('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/xqetKitv1Ko"> </iframe> </center>', " Good evening. As you all know, a number of influential people like Elon Musk and Max Tegmark and Elias Yurikovsky and Stephen Hawking and so on are very concerned about the danger of AI and hard take-off and the risks that are involved with this. And a number of people get upset about these people, that they are concerned about these science fiction topics when there are very real things to worry about, like the AI impact on the labor market. Why should we worry about these imaginary topics? In the last few weeks and months and years, there have been a number of articles like this one by Kevin Kelly who complained along these lines, or Gerald Lanier who basically says AI is facial recognition, I know because I've done this, and you should not be worried about facial recognition taking over and destroying the planet somehow, or obliterating humanity. Or Andy McAfee, who tells the policymaker of this world, don't worry. In every technological revolution that he had so far, we were able to retrain the workforce. And why should it be different this time? And while this is mostly what people want to hear, so it's a very good message to package and give to the mainstream audiences. It's not what nerds like you want to hear. So let's look at this problem in some more depth. I do think that the worry about the near term effects of existing AI technologies and possible existential risks by AI are very separate topics. They are as separate as saying we are worried about the risk of a regional conflict, a war, and we are worried about the results of a global nuclear war. Very different topics, the global nuclear war is far less probable, and so on. But if it happens, it has a very, very different effect. It's going to affect us as a civilization on a very different level. We have obstacles to AI, obviously. In 1950, when this field began to form in the throes of cybernetics and so on, we didn't really know what intelligence is. That's quite similar to the area when biology started, and biologists didn't know what life was. We could point at life and say, this is the living stuff, and we could point at stuff. We were pretty sure what was not living, but we didn't know the difference. At least we didn't know what constituted the difference, because that was one of the main things that the biologists needed to find out. And a similar thing happens in AI. I mean, part of our research is to find out what intelligence is. And when we really find it out, in detail, we can probably build it and understand it. Now, it turns out biologists cannot build life because life is cells. The cell, as it happens, is the smallest modular machine to scrape neck entropy from the universe over a very wide range of environments, and that can also run evolution on it. And all life that we know right now and that we are talking about in biology is made from cells. Everything that is made from cells is life. Everything that is not made from cells is not life. The virus is usually not seen as life. There must have been pre-cellular life at some point in the universe, maybe even on this planet, but we haven't found it because it has been obliterated by life. Because if you are living and you are not a cell, you should quickly get a membrane to protect yourself from the influences of the environment. And once you have done this, probably a lot of the walls with cosmic dyes are necessary to make that happen. You can run evolution. You're good. OK, so the cell is the smallest known modular machine that can extract neck entropy to preserve its structure in a reversible universe in which you cannot have a perpetual mobility. And therefore, if you want to create structure and stabilize it, you are stuck with increasing entropy. Therefore, you need a source of neck entropy, and that's in short supply in this universe. A multicellular organism is one where several cells band together to soar in their lot in this mission and share their evolutionary fate. They often share also a nervous system to share their information processing. That's basically a dedicated bunch of cells in our organism that's a bunch of glorified fat cells that are teased into doing some computation on behalf of the organism. Right? So as we have just seen, intelligence is the ability to achieve goals over a wide range of environments. I think that's intelligent behavior. I don't think it's quite the same. I have a child. This child is six years old. It's able to hack my email and write emails into the world. And it's not achieving useful goals, as we tell you. But it's very smart. So I have also noticed that many of the most intelligent people I know are unable to achieve goals over a wide range of environments. Yeah, and that's not an accident. I think that intelligence is basically the ability to make models. And this is usually done in the service of achieving goals of regulating a system, but it's not always useful to the organism to do this, especially to the human organism. There's this good regulator theorem that says if we have a system that regulates something and it regulates it well, then it has to have a model of that system. A perfect regulator will be isomorphic to the system that it regulates. And therefore, it will be the perfect model of that system. And the best model that we can find, as Marcus has pointed out, Solomonov got onto this. Basically, if you wake up and realize, oh my god, I'm just a robot. And I'm connected to the world with some kind of feature vector over which a universe throws bits at me. What is the best theory about what this universe looks like? Well, Solomonov figured out the best theory that we can find is the shortest among those programs that best predict the observations from the past observations for all observations, right? So basically try to find a very short program, the shortest among all programs that best predicts what you are seeing. And if you build generative models like this, they allow for creativity and problem solving. So our current machine learning systems can do some types of models quite well. They can do convex optimization with deep learning, which, as it turns out, is pretty good to solve a number of perceptual problems. We have some probabilistic models. We have genetic algorithms. A variety of other things. But the general case of a mental representation is a probabilistic algorithm. For instance, when you learn how to write a program, how do you do this? You basically have entrained your brain with a probabilistic algorithm that tells you how to write programs. And it's very hard to learn these probabilistic algorithms with stochastic gradient descent. So basically, our current machine learning algorithms are very good for approximating certain functions and not very good for approximating certain others that are relevant. At least, that's what I currently suspect the problem is. We need some more clever algorithms. And the question is, when are we going to have them and how much effort does it take? And how much do we need for doing this? I suspect that our mind is probably best understood as a general modeling system. So we can make general models of a very wide range of problems and complexities. Possibly our brain has multiple paradigms. Maybe it has one general one. It has an interface to its environment. It's combined with one universal motivational system, a differential attention, routing systems, and so on to provide the infrastructure to make all that happen. How complex is the whole thing? Well, there's some debate about this. There is some comorbid complexity to our brain, which means how much source code do we need to generate a brain? And the upper limit of that is probably given by our genome. We all know our genome fits on a CD-ROM, so it's not that long. 90% of that is non-coding, which means it's mostly virus signatures of stuff that killed a lot of our ancestors. It's not supposed to kill us again. 10% are roughly coding. And these 10% is pretty much what we also virus signatures of stuff that killed a lot of our ancestors. It's not supposed to kill us again. 10% are roughly coding. And these 10% is pretty much what we also share with mice. Genetically speaking, we have very big mice. And most of these 10% probably code for what happens in a single cell. And the cellular differentiation and the blueprint of the organism, how these cells lump together and so on, is going to be a very small fraction of this. So maybe there are 70 gigabytes roughly of coding stuff, then there's one gigabyte of structural organization of the organism, and a very, very small subset, we don't know how big that is, codes for building the brain. So it means that our brain is probably not built like clockwork, like a machine with lots and lots of small, intricate, moving parts that have to work together in just the right way to make it happen. And mostly it doesn't happen, and one in one billion human really works, and all the others we have to throw away. No, it's more like built like a cappuccino. You throw in a certain set of ingredients in the right order and let it bubble a bit, and then it works in almost all cases. So this is basically the comorbid of complexity, the difficulty how to make it. But how many units does it have? How difficult is the implementation? Because for nature, making more cells is very easy. You just double them, and you have twice as many. And it's one step, right? It's amazingly cheap to make more cells. But for us, it's difficult, because our reproduction mechanism technology are more expensive for this. And last year, I had a class at MIT. So I asked my students, and we made some estimates what we think how much memory do we need to run something like a person. And based on what you think the structural unit is, you get two different estimates. So if you think sub structural unit is, you get to different estimates. So if you think subcellular structures are important, stuff within the neurons, like the vesicles and so on, are important, then you get to exabytes of memory that you need to store this. If you want to model at the level of synapses and neurons, you get to some petabytes of data. But if you think that the functional unit that really matters is a cortical column, and the connectivity between the cortical columns, then you get to something in the order of just a couple hundred gigabytes. And this is what actually most people there believe. Now, this seems to be amazingly small compared to what most people think about. But most people think about, oh my God, we need to simulate a whole brain. We never ask ourselves how many brain would we actually need to run MacOS on them. Because brains are super mushy and redundant because of all this inconsistency and indeterminism in them, right? So you would need to get the necessary stability and causal structure to run an operating system or a decent computer game on a brain. You probably would need multiple brains. Okay, so I do think that some of the problems are really not solved. But I think from this point of view, if civilization lasts long enough, AI, strong AI, is very, very likely to happen. There's no clear, obvious obstacle to AI as we see it now. There are some unsolved problems which we don't even know which they are, so it's very hard to say when we're done. As you know, when you do software engineering and you don't have a spec, you never know when it's done. It can be of any order of magnitude. But at this point, there is no reason why it shouldn't be done. So if we compare this to our biological brain, AI has some abilities to scale. And our biological brains run into limits that obviously given by evolution. So we have a metabolic limit. If you want to have a fast nervous system, there's a lot of cells. You basically need a place high up in the food chain to be able to nurture the whole thing. There's a size limit. If you want to have a larger brain, you also need to have another organism. You have a pretty large organism as things go. And our brains consume 20% of the glucose of our body. It's very hard to make them larger. If you want to have a substantially larger brain, you need a substantially larger body, which means you need to eat a lot more. This means you need a lot more extensive ecosystem around you to produce the food chains that you are at the end of. Then there's the training problem. Humans need something like 80 years to train. Before that, they are often pretty much useless. Basically, before you get the training data in this, we have a layer by layer learning process and the duration of this training determines the level of intelligence. I suspect that one of the main differences between the great apes and the humans is the developmental speed. There's a clock that says, now I stop neuroplasticity in this layer and turn on the next one, and this clock runs much, much faster than a gorilla, so the gorilla moves into her own home at 14 months and not at 18 years or 24 years or 30 years. Then there's the interface problem. Of course, if you want to be smarter than a single individual, you need to split the intelligence over multiple individuals, and now you have this problem that you need to make them talk to each other, which is very, very, very slow and also lossy. And as further they get apart, the more expensive it gets for them to communicate. And then there's the alignment problem, because as soon as you have multiple individuals, they have not completely aligned interests anymore. So they cannot completely trust each other anymore and need to make very separate models of the world and of each other. So this is all problems that AI doesn't have. There are some people which proudly announced that the human brain is so super efficient that it consumes only 18 watts or so. Yeah, come on. This thing also consumes only 18 watts. But of course, it's slightly smaller than the human brain still. But it's much, much easier to make those 18 watts than to make the human 18 watts. Imagine you would have to feed this thing sandwiches until it has generated these 18 watts. It's ridiculous. Instead here, I can put up a few solar cells, and I'm done. This is much, much more elegant. And I can use arbitrary energy sources for this. I can use geothermic energy. Feed a human on geothermic energy. Good luck with that. So we also have reusable knowledge. Once you have trained one computer, you can put it into all the others, right? You have cheap and reliable high bandwidth communication. So you can spread your computational units all over the place. And you don't need to have a multi-agent system anymore. It can be a single individual, because you don't have individual bodies that run around. You also don't need to have a multi-agent system anymore. It can be a single individual, because you don't have individual bodies that run around. You also don't need to have generation change to adapt to changing environments. You don't need to have senescence. The main reason why we get old and get brittle before we need to, we would need to based on how our cells make up probably after a few hundred years. But the fact that we already die off after 80 years or something is probably an adaptation to having grandchildren that you don't want to out-compete. If you were in direct competition with your grandchildren, guess who would win? It would be very bad for the species. And most species don't have a situation where they can spread out into new areas all the time. Most species that exist are confined to a narrow habitat because they rely on their foot chains. So as soon as you fill this habitat and you want to adapt to the changing conditions in it, you want to have some kind of generation change, which means you make the elders brittle and dead at some point. So AI doesn't need all these things. So what's the purpose of our life? Mike Russell of the Santa Fe Institute said that the purpose of life is the hydrogenation of carbon dioxide. It's a slightly simplified story, but there is something to it. What life does, as I mentioned, is it harvests negentropy to preserve its own structure. How is it possible for life to close negentropy gradients? Because there's competition to life, just plain simple chemical reactions. If you have some kind of energy differential, then suddenly stuff starts to burn and it's gone. Right? This is very bad. If you want to compete with a fire, you're going to lose. So how is it possible for life to compete with other reactors on this planet that want to close neck entropy gradients? Well, there are some reactions that require control. You need to add a little bit of energy to get more energy out in the end. And controlled reactions require a degree of information processing. And this is basically the market opportunity where life steps in and says, these are some chemical reactions that I can get and you stupid chemical reactions cannot get. Only the smart chemical reactors like me get it, right? So this is where life has an opportunity, and there are some areas where you have stable reactions like this, a stable environment that stay unchanged for a very long time, are pretty mild and benign, and some planetary surfaces that allow that. So if you understand it, what we're doing is basically, as organisms, we hydrogenate carbon dioxide. What's the optimal utility function for life? Well, there are multiple ones that you could think of. For instance, you could say it's, of course, the maximization of evolutionary fitness, since we are subject to evolution. Or you could say you maximize your future options, which leads to something like AXI and so on. Or you could maximize the social structure creation by your genotype. And it all comes down to the same thing. It's the maximization of the total consuminic entropy of you as a system or of you as a genotype. And the problem is that evolution didn't have the way to put this directly into the nervous system of the organism to tell it, this is the problem that you're meant to solve. How do you maximize your evolutionary fitness? This is actually the problem that we need to solve. The problem that every organism needs to solve is, how do you maximize your evolutionary fitness. But in order to write a program for this or to formulate the problem so you can find a solution in your given environment, you need a Turing-complete brain. Evolution didn't find a way to build a Turing-complete brain from scratch and tell it what the problem was. Evolution is pretty dumb itself. It doesn't have a brain, it's just a principle. So it has no awareness of what the problem was. Evolution is pretty dumb itself. It doesn't have a brain. It's just a principle. So it has no awareness of what the problem actually is. The organisms need to find this out for themselves. And there's probably no organism that found this out before us. We are probably the first organism on this planet that found out what the problem is, that our brain is meant to solve, maximize our evolutionary fitness. But we have been around before this. Evolution was discovered only very recently. And if you look at the US population, it seems that people are even evolved not to believe in evolution. So there's this thing that we only have a proxy for this. We have something built into us, a motivational system that is based on a bunch of demands that manifest in desires and fears and so on and make us struggle through the world. So we have these physiological demands, things like hunger and thirst and hunger for different nutrients or something, if you had something that's more salty or it's more sweet, for rest, for safety, right temperature range, libido and so on. We have hundreds of these physiological drives. Then we have a dozen social drives, desire for affiliation, desire to nurture others, reduce their suffering, and so on, a desire to conform to internalized norms, which gives rise to the concept of serving something that's bigger than you, becoming part of a system of meaning, and so on. Then you have the cognitive desires and these cognitive minds are competence, exploration, and aesthetics. And all this gives rise to some vector that, when we are lucky, results us having offspring and maximizing our evolutionary fitness, on average, over many individuals. And if you look at AI, for AI, it's going to be straight. Consume the maximum amount of neck entropy that you can scrape from the universe. And maybe don't do it all at once, because you don't get it all at once, so try to live as long as you can, right? So this is basically the competitive situation if you would live next to superhuman AI that is the product of an evolution or it becomes subject to an evolution, so it stays around. And as soon as you have multiple AIs and they compete, the ones that is going to emulate this evolutionary principle best is going to win. So we have all these human values, like I want to reduce suffering. I'm a vegetarian and so on, because I care about animals. I want to improve the content of my mental representation. Like many of us who are artists or scientists, I have a defect. It's probably really a defect. I think that the building of mental representation that serves the purpose of learning. And learning is there to eat more. Basically, all the structural complexity that our mind has is for erecting some surfaces on which we can out-compete yeast. That's it. But what happens with my brain? I get vexed by the beauty of the loss function. Oh my god, my mental representation is so amazing. Let me look at it. Let me get it right. Let me improve my mental representation. This is probably defective. I don't think this was intended by evolution. It's meaningful for society to keep a few, very few of us around, because sometimes what we do has useful side effects. But most people realize this is pointless, what this guy is doing. So I always get this very strange reaction when I talk to humans, not nerds, about what I do. Because they ask me, what is it good for when you do AI? Can we save cancer with it, or brain diseases, and so on? And yeah, maybe it's a side effect, but why do you care? How don't you understand that building AI and understanding the nature of consciousness and our relationship to the universe is absolutely the most important thing, and everything you do is basically like yeast. So if you look at all these human values, preserving human civilization, preserving lives, reducing suffering, and so on, and you make this compete with, oh, no, just let's scrape the maximum amount of neck entropy from the universe. And that's actually more efficient. Who's going to win? Of course, the other side, right? And this is the thing that concerns me. I asked my class of MIT students, are you concerned about superhuman AI taking over and obliterating humanity? And they were not. And I asked them why. And they said, well, why should we root for the monkeys? But you know, there is a problem with this. For instance, if you look at consciousness, consciousness in humans, I think, is intensely related to having a protocol of what you attend to. You are only conscious of the things that require your attention because you need to resolve a conflict. If you can do something without thinking about it, because it's automated, you are not conscious of it. It might even happen in your cortex, but you're only conscious of it if it requires attention, right? Because this is what consciousness is for. It's basically a conductor that organizes your mental orchestra and resolves if some of the instruments have a conflict, and needs to pipe up or down or needs modulated or get tuned and so on. This is what happens. This is when you are conscious and you put this into your protocol and later you can access this protocol to talk about what you attended to and learn from it. And this is your stream of conscious experience, this protocol, right? But if you are an AI that has solved its problems, then you will be basically doing everything on autopilot. So if you have superhuman AI, it might be extremely conscious for a very brief moment in time, and then it will be eternally unconscious because it has solved fundamental physics and everything in its domain and knows how to turn the solar system into a Dyson sphere, and that's it. That's as good as you can do. So I think that scalable AI probably won't require consciousness for a very long time. So the result that is there is going to be a very glorious neck-entry scraper, but it's going to be, from our perspective, extremely boring. So this is, I think, the reason why there is no far-future science fiction. Because far-fut future science fiction, in some sense, should involve the notion that there will be space battles in the future that are still orchestrated by primate brains that are primarily concerned with the difficulties of mating. And this is not realistic. Like, not at all, zero. It's not going to happen. So what's our solution? And this is not realistic. Not at all, zero. It's not going to happen. So what's our solution? I think the future in this respect, in the long term, looks very bleak. We could upload. Actually, uploading is good, because uploading is super easy. It's necessary and sufficient to build a machine that thinks it's you. Maybe your friends will complain, but you can upload them too. Because if you think about it, how do I know that I'm me? Because I think I am me, right? It's no guarantee that I'm me. Yesterday I had a couple of gin tonics, the night before I flew here over many, many time zones, and I killed a number of brain cells with this. But I still think I'm me, right? Because I think I'm me, I'm me. It's sufficient, also necessary. If I build an auditory machine that thinks I'm me, I'm me. It's sufficient, also necessary. If I build an arbitrary machine that thinks it's me, it's good. If there are two machines that think they are me, like me and this other one, we might have a problem, but it can be easily solved. So uploading is an option if you really want to have a machine that thinks it's you, but maybe it's not really necessary. Then, of course, you could be concerned about AI ethics. It is trying to cripple your optimal entropy scraper in a way that makes it more human-like. But it also means that you're probably very blind to what a vile and disgusting species we are. Because imagine you have built an AI that is curious and creative, and so on. You make it more human-like. Do you think it's going to be nice then? No, probably not. So maybe the solution is that we understand what's going on, understand the nature of our life, that existence is probably something that is neutral. And life is temporary. It's always confined to an entropy bubble. Entropy is a limited supply in this universe. At some point, it's going to be over. And life is temporary. It's always confined to an entropy bubble. Entropy is a limited supply in this universe. At some point, it's going to be over. And AI is only one of many existential risks that we are facing. There are many other existential risks. For instance, the sun expanding beyond the Earth at some point because it burns out, and us not being able to go to the solar system that has not burned out yet. Or being hit by a meteor or global warming and so on. And one of those things is going to get us at some point. The question is if AI gets us first. I think there is a small chance that AI gets us before global warming, but it's not clear that it manages to do this. And the ultimate existential risk is that we run out of negentropy, at which point we will stop. There will be no way at some point to sustain intelligent life in this universe. So over the course of the universe, it's this very dark place. And there's basically a very, very brief flash on one or some planetary surfaces where there is a reflection of the universe in some minds that have an inkling of what's going on. And that's it. It's a very brief flash, this moment of consciousness and understanding. And if we build AI or not, it's not going to change very much about this. But back to the sumitral thing, I do think that the existing AI is sufficient for ending the labor-based economy. We need to solve this. I agree with Marcus Hutter that actually we can solve it, and we will probably run into big trouble while doing this, and it will blow things up and a lot of people will die and will be horrible but eventually it can be good. And future AI may or may not pose an existential risk like many others like nuclear war, like global warming, like others and we should address it and I think it's very reasonable to treat them as separate problems and throw a lot of money on also dealing with this problem. Maybe not as all the money, certainly not all the money, but we should put some brain power on it. I don't see an obvious solution to the problem of dealing with AI risks and I have tried to explain why. I'm less optimistic than Marcus and many others. I think that some of the optimism is also probably because people think it helps them to find a solution at all. I'm not sure if a solution exists, but I think it's possible to increase the likelihood to find solutions or to increase the likelihood of good outcomes and of more people having interesting lives, of my children having interesting experiences and yours, and let's try to optimize the chances for that. Thanks. So while, yeah, we can field a couple of questions with Yosha now. While Ben tries to load the presentation onto Yosha's laptop. So who has the question? Any questions about negativity? Yep, OK, first one. Can I ask a question? Yep. It's not me. Hey, do you want to answer a question? Sure. Yeah. What is the question? I'm sorry. Well, in all the talks here, I think the idea is that once we have artificial general intelligence, artificial super intelligence, we have an ethical problem, an ethical goal function, and how should such entity behave, or whether it merges with humans or not. But I think for autonomous cars, we have to solve the ethical problem even before we have artificial general intelligence and artificial superintelligence. Because we have to decide, as a society, what the ethical choices have to be for autonomous cars. Not because they are superintelligent, but simply because they are faster than humans and can drive more safely than humans. So the idea I think is that first we have to solve this ethical goal function definition problem and that will help also later on to solve the artificial general and artificial superintelligence problem. In the end we just go to zero and that's it. So what's the question? So the question is actually why are we not paying attention more to this ethical problem? Because I think it's very much focused on, okay, we do not know what actually artificial genital superintendences will look like, and then so we cannot also solve the ethical problem on that. But I think there's a very... Is this a question? Let's try and make them questions and not like treaties and free volumes. Why is there not attention to this, more attention to solving the ethical problem for autonomous cars? Well, ethics is something that is very hard to derive from first principles. And ethics emerge as a problem, as you want to have a principled approach to resolve conflicts between multiple autonomous agents with immutable minds, with immutable preferences. So for instance, I have a preference that I don't want to experience pain. This preference is almost immutable. It's very hard to remove. It's possible with meditation or drugs or surgery, but usually we see this as immutable, right? And as long as you have these multiple agents that you think are valuable and you want to care for them and account for their preferences, you have to find a principled way to resolve conflicts over resources or conflicting preferences among them. And this is what ethics is about ultimately. And you can do this with respect to cars. Here the question is probably what's the value of human agency versus what's the value of human life. Do you think that human life trumps human agency? In that case, have self-driving cars right now, because self-driving cars already right now make fewer mistakes than people. That's a very obvious question. In the general case, there's all these other things. Philosophers had have trolley problems for a long time. It was completely academic. Philosophers don't have real jobs because they don't have formal education. They need something to do. Now suddenly, they're self-driving cars. They look somewhat like trolleys. Oh my god, and there are now a lot of papers about this. This shouldn't surprise anybody. Also, it's something that journalists can use to write articles about, because it's somehow interesting and geeky and something that people can do something about it. But I don't think it's a problem in practice. In practice, you want to build a car that is functioning within certain parameters, and you want to have a legal infrastructure that shields the makers of the car from ruin if they make a minor mistake. That's the major problem. So I don't think we have very big ethical problems with self-driving cars right now. There are minor problems, but I don't think that there are major problems with self-driving cars right now about ethics. Okay, thank you. That's all we have time for. We can ask more questions. Bye!", '19.47775411605835')