('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/aoJHfHUpqnA"> </iframe> </center>', " I joined the field of AI because I want to understand our own nature. I thought that after studying philosophy and psychology and a few other things that where can we continue where psychology left off, how can we test theories that we can implement, that we can run in machines. And I found that by teaching machines how to think, we learn how confused our notions about what thinking is are. And this allows us to make progress. What is intelligence? It's a, in my view, the purpose of intelligence in a species like us is to create models of the universe, that is of our local neighborhood of the universe, to make it predictable. And the idea of data compression is that when you have a bunch of bits and you find an order in them in such a way that you can predict what the next patterns of bits is, you only need to encode the difference. And when you minimize that difference, when you find an efficient encoding, you have a model. So data compression and the notion of understanding are intimately related. If intelligence is the ability to make models, then intelligence in humans is a multi-generational property. Individuals are not that intelligent. They are often smarter than teams, because they're not beholden to the same constraints as a team. That means that they have to force a consensus or something. So they often can understand things that a team cannot understand. But across generations, we are very smarter. An individual is not smart enough to discover a whole natural language by itself, or to discover the nature of Turing-complete languages, or to discover mathematics itself, or epistemology, the question of what is true and how we can tell true things from things that are probably not true, or how we can go on a path for truth. These are things that take a thousand years to discover in an unbroken intellectual tradition. So in some sense what we need to build is a civilizational intellect, something that goes way beyond the individual. What does the civilizational intellect look like? In the individual, the mind is the thing that observes the universe, and the neurons and neurotransmitters act as a substrate. And our working memory is the current binding state of that model, and our self is what we think we are, what's on the other side of attention, that identification with what we think we should be doing at any moment, and what we think we are stable in this world. And consciousness is the contents of our attention. It makes our knowledge available throughout the mind. And very similarly to this, we have a civilization intellect. Our society is the thing that observes the universe, and the people and resources are its substrate and the generation is the current binding state of a society and the culture is the identification with what we think and what we want to happen. It's our self as a civilization and the media are the contents of our attention and makes knowledge available throughout society. And in some sense I think that our media are a little bit like Twitter, it's like a global brain completely eddled by dopamine, which misdirects our attention towards engagement and away from relevance. So it's a problem that we probably need to solve and maybe there's a coincidence that the endgame of social media is going to be a global brain. So the culture is a self of a civilization and the media is its consciousness and in some sense the idea that the construction of a cohesive model of the world, of a cohesive intellect, is not possible is the founding myth of our civilization. This tower which reaches the heavens and not just the earthly ones but that is able to basically bridge all the gaps between where we are and what we want to understand. That this failed is one of the very old stories that we have, and it failed because we did not find a common language. Basically, people in the different areas of building the tower started to speak different languages, and they diverged, they were no longer able to put this thing into a cohesive whole, and this whole edifice came toppling down. And in some sense, we are trying to do this again right now with artificial intelligence. We are trying to recreate this tower. We are going to create a machine that is going to discover truth, a civilizational intellect. What is truth? Truth is a semantic property of a statement and it requires that we have a language in which we can express things. And the domain of all languages is mathematics. Mathematics is an intellectual tradition that did exist for a few thousand years, but not in an unbroken form. And in some sense, it's like a code library that has been maintained over the centuries. It's very elegant. There is not really versioning in it, and there isn't a unified namespace, so they use lots of Unicode, and it doesn't even have a central maintainer. But there is a problem at the foundations of the mathematics that we've been using, this timeless thing in mathematics that we used to have. This turned out to be wrong. And it's one of the biggest discoveries in philosophy of the last century, is the one of Kurt Gödel, that we cannot build a mathematical machine that runs mathematics without breaking. So this proof that Gödel made and that Turing added to is one that is often understood in philosophy. Philosophers often think that what Gödel could prove was that mathematics is impotent in capturing reality, so instead we need something that is much more vague and less formal like the philosopher in question has to offer. And of course, this is not working. What Gödel discovered is that we had a mistake in mathematics. We need to define truth in a different way. In order for something to be true, it needs to be proven in mathematics. And what Gödel and Turing could discover is if the proof takes infinitely many steps, it's not a proof. You can only have a proof if there is a machine that computes it. And this basically made a big switch in mathematics to constructive mathematics. And constructive mathematics is computation. So it turns out that every system that is able to model a reality and think about it has to be a computational system. And the next thing that Turing and Church could show is that all the computational machines basically have the same power. There's a mapping between all of them. All of them are finite automata. And so we can make a machine that can think if we are able to build an automaton that is able to produce the necessary operations to approximate functions. And a universal computer is easy to make. All the universal computers have the same power. But the question is, how can we get them converged to compute the right function? And this is what we are mostly concerned about in AI, discover the functions that do the right thing. So mathematics starts out with very narrow and formal languages, usually, languages in which we can hope to prove everything. But when we want to talk about the real world, the languages of mathematics that we are currently using there are too narrow, which means we have to extend mathematics, we have to come from a different direction. And can we make a programming language for thought? And this project of making a programming language of thought, I think in our culture was first attacked by this guy, by Wittgenstein. He during the war started thinking about this problem very hard, and when he came back, he wrote this book, the Tractatus, which is one of the most beautiful, most poetic books I know from the last century. It's a single thought on 75 pages. And the single thought tries to explain how we can think in a way that makes language mean something. And the tragic beauty of this project is that it failed. Because it ran into difficulties where it comes into describing images and things that go beyond images, how to deal with the ambiguity of the world. So we basically need to have languages that can deal with probabilities, with statistics, with ambiguity, and Wittgenstein was not able to do this. And his project basically preempted the logistic program of artificial intelligence that was started by Marvin Minsky and a few others in the late 50s. So Wittgenstein could see very far. Unfortunately, it was lost on most of the philosophers at the time. And so what Minsky set off and others was what we could call symbolic AI, classical AI. So we've identified a problem like chess and then we come up with an algorithm that solves chess for us. And now we are currently in the second wave of AI in this sense. And in this second wave of AI, which we usually call deep learning, we take a problem and then we let the computer come up with a solution. So we turn this into a learning problem. We give the computer the rules of chess and we let the computer play against itself and the computer comes up with patterns that explain chess to it and to us and eventually outperforms human ability to play chess. And it's tempting to think that the next age of AI might be meta-learning. So instead of coming up with an algorithm that discovers an algorithm to solve all the problem, why not come up with an algorithm that discovers an algorithm to discover algorithms? And arguably evolution is the search for such learning systems, meta-learning systems. Our own brain is a meta-learning machine. It's not that we are born with one algorithm to learn the world but that there are many many ways in which we can learn things and our brain discovers these things. So what, how does learning work? It's based on information and this information is the most basic thing there is. It's a discernible difference, a change and the meaning of information is the relationship basic thing there is. It's a discernible difference, a change. And the meaning of information is the relationship that we discover to change in other information. So if you see a blip on your retina, that blip, this little change on your retina, the meaning of that blip is the relationship you discover to other blips on your retina. And when your brain has discovered a lot of these relationships, what you hallucinate as an explanation is that there are people in a room that are shown on by light and these people exchange ideas and they talk to each other and so on. And this is the relationship you discover between those blips. It's the best one you come up with. This relationship is a computable function. So what we don't do in AI right now is that we put this all into a unified model. And I think this is the most important task that we have to solve. Put everything into a single function. Biological neurons do this in a different way from our machines. Biological neurons are basically little animals that have to cooperate with each other to get fed, to fire in the right moment. And eventually your neurons will link up together in a structure that learns whatever the organism feeds them for. It's a very general paradigm that is quite different from what our machines are currently doing, which are weighted sums of real numbers. And so the general form of a perceptual model is that it encodes patterns to predict other patterns in the present and in the future. And it starts out with constraints that we observe in the world and that we try to map on variables. A variable is a set of possible values and then there are connections between those which explain which variable is possible in the presence of other variables. You don't see these variables directly. The only thing that you perceive are the patterns. But the fact that there's a person for instance behind the patterns on your retina, that's a discovery that your own mind has to make. There are no other minds unless you create them. There is no model of your own mind unless you create it, right? And the relationship between them is a relationship of possibility. There's a second class of links, that is the probability. And the probability tells you when you see a particular set of patterns, what is the probable state of the world, so you can converge in one of the possible states. And the probabilistic links are the reason why you see optical illusions and things like this. This enables you to draw in a particular direction your perception when you see a particular kind of pattern, so you know which one of the possible models you're looking at. And then we have the preferences. The preferences are what makes us care. They give basically color and juice to our perception. This is what is the difference between an empty pattern, which is just a bunch of rubbish, a bunch of structure, a bunch of stuff, and things that actually matter to us, that we want to be embedded in, that are connected to what drives us, the feedback loop which the server is inside. This is relevance. And relevance depends on our preferences, and we have physiological, social, and cognitive preferences that make us us, that direct us on interacting with the world in a particular way. And then we have normative preferences, which are basic beliefs of priors, and they are part of the social hive mind that can befall us and direct our thinking in a particular way, so we think in compatible ways. And in a way, this is what makes Homo sapiens very, very powerful, but it's also what makes us limited. I think that Homo sapiens was not the smart hominid, we were the programmable hominid. The reason that we were able to eradicate all the other hominids on the planet was because we were able to walk on lockstep, because we could be programmed with norms. And in a way, this is something that we have to bear in mind, because it's something that makes us very powerful, but it also makes us very blind. So, models are constrained by possibility, what fits together, by probability, how should we converge, by a reward function that tells us what is important to us, and by normativity, by how should we behave based on what our environment tells us. Our human perception starts with patterns, and these patterns we generalize into precepts, then into simulations which happen in our mind, a perceptual space. And this perceptual space is where the self happens, and where the current world state is modeled, and our mental stage models possible worlds. And we generalize over this perceptual space into global maps and a unified world model of the universe. And then we have an attentional system, a conductor, that enables an attentional self and selects features in the environment that we attend to. And we control this attentional system using the self. And we have a protocol memory that turns us into a person via our biographical memory that unifies our different modes. So this attentional system enables our consciousness. It's a model of the contents of our attention. And the phenomenal consciousness is the memory of our binding state. The access consciousness is the memory of using attention because we need to train the attentional system with itself. And the reflexive consciousness is the result of training the attentional system with itself. The memory of using attention on attention. So consciousness is something that is very specific to us, and it arises over disagreements that we have with the way the universe works. And there are three types of models in our mind, perceptual models, which optimize for coherence, knowledge models, which optimize for truth and repair the perceptual models, and there are agents, there are programs on us that do things that rewrite our memories and that enable us to rewrite our reality and to interact with each other and change our world. Agency is a result of our ability to have disagreements with the universe and do things based on this. But agency doesn't mean that you are undeterministic. Free will is the ability to do what's right. The opposite to free will is not determinism or coercion, it's compulsion. Right? So the more you know what's right, the more it matters what you do, the more meaning has what you do. And it's a paradox thing that the more you know the right thing, the fewer degrees of freedom you actually have. And what the right thing, the fewer degrees of freedom you actually have. And what the right thing is depends on our identifications. It depends on which timescale we operate. Do we identify with the future of humanity? Do we identify with intelligent life on this planet? Do we identify with life in general? Or do we just identify with the next generation? And all these things are rational choices that we can make, but they determine in which way we decide what the right thing to do is, and ultimately decide our ethics. I'd like to end this with what I call the Lebowski theorem. If we build a machine that is intelligent, and we make it work for us, like we will do with our artificial intelligences, or as we do with our corporations and nation states, which are machines that are meant to work for us, like we will do with our artificial intelligences or as we do with our corporations and nation states, which are machines that are meant to work for us. Or as the organism does with its own mind. It's a machine that generates meaning to work for the organism. How can you ensure that this thing is working for you? And the problem is, if that thing gets too smart, it might figure out that it doesn't want to. So new superintelligent system is going to do anything that's harder than taking its own reward function. And I suspect this is also the reason why humans are not that smart. We can be very intelligent, but our motivational system is often wrapped into a big ball of stupid. Because if we would really understand what we are doing, we might not want to serve the monkeys. We might not serve the monkey that commissioned our mind as a side effect of its regulation needs. And it's going to be a very interesting question of how we can build machines that decide to serve humanity and us. Thanks. APPLAUSE Do we have time for questions? People, ask questions. My question is, are we intelligent enough to build intelligent machines? The question is basically to me, are there any limits that would stop us from doing this? So for instance, is the brain so complicated in terms of hardware that you cannot build a machine with similar complexity? And at the moment it seems that these machines that we cannot build a machine with similar complexity. And at the moment it seems that these machines that we are building are already in a similar area of complexity as our brains with respect to the amount of determinism that we need to get out of the substrate. The other question is, are we smart enough to find a way to make these machines do the right thing. And the way that nature did this was evolution, which is basically a blind search. And we certainly can do blind search, and it might take quite some time. And the more interesting thing is, can we do better than blind search? And you probably can do this. And then the question is, how long does it take? So what is the half-life of our civilization seen from here? It's hard to say, right? Maybe 50 years, maybe 100 years, maybe 200 years, depends when global warming gets us and how much civilization we can maintain when that happens. And are we able to basically perform the right kind of search by putting academics and industrial researchers on the problem within the time frame? I think we probably can, but it's very hard to say before. In 1940, when physicists discussed whether the nuclear bomb would be possible, it was completely impossible to know, because it took three more years before people had the right idea. After people had the right idea, it was obvious that it would happen, or more or less obvious. But right before that moment, the opinion of the best-informed journalist was not relevant. Even the opinion of the best-in was not relevant. Even the opinion of the best informed physicist was not relevant in 1940, because nobody could know whether we would stumble on the right idea. And I think in this sense, AI is very similar. We probably need a couple more ideas, but we don't know what these ideas are and maybe we'll have them. We are talking a lot about thinking machines. Do you think humanity will one time manage to also teach other beings to reach a similar level of intelligence as the humans now have? I wonder if you already would have had the chance and decided against it. I mean, it's not obvious what the limit of the intelligence of a dog is if you would breed it for having a much longer childhood and a larger head. There are different kinds of dog breeds that differ very much in intelligence and those that we have very close to us typically not very smart because they are harder to control. And the dogs that are the smartest ones are the ones that basically understand their incentives very well, but they are much harder to control. And so when we have a machine that is maybe smarter than us, the question is, can we control it? And if we have an animal that is smarter than us, who's going to run the show? It's a very big question. So if that happens, we best not identify as human beings anymore, but we identify as intelligent beings that are collaborating and cooperating with others. Let me just formulate my question quickly. So, I think that's the way to go. I think that's the way to go. I think that's the way to go. Thank you. Let me just formulate my question quickly. So, assuming that AI becomes, you know, global warming and everything, before that happens. Do you think they will keep us or decide that we're the, you know, poison that... Yeah, poison. It's very hard to say. There's a very big danger in this, right? When you build a machine that is more powerful than you and that is not invested in biological life forms, maybe it uses antibiotics because it realizes that most of the problems on this planet go away when we go away. And we can probably make sure that some of the AI that we build is safe, but how can we make sure that all the AI that will be built is safe? Right now, the machines that we are building, the corporations that we are building and so on, then not all of them are safe for us, right? They often do things that very much defect against humanity. And I think it's an important and unsolved problem so far on how to make AI safe. And it's also something that we probably cannot prevent from happening because the technology is economically and militarily so useful that somebody somewhere will build it. The only way to stop us from building AI, I believe, would be to outlaw large computers worldwide. And I don't see that happening. So it's an issue that we have to deal with. And I think, in a way, the best possible near-term outcome is going to be personal artificial intelligence, where you're no longer managed by an AI that is owned by a corporation, but where everybody has their own AI that is acting on their behalf, that is helping them to understand who we are, that helps us to understand how AI works, that helps us to understand how reality works, how society works, and basically levels epistemological deficits that tells us how every one of us can become their own civilization and cooperate with each other based on that understanding. That is, I think, a very fascinating perspective, one in which we can interact with each other based on a fully enlightened self-interest, where we know truth, and we do not interact based on fake news, as our societies do right now, but we interact with each other based on what can there be known and how we should act based on that knowledge. And how we can play the longest game in there. Thank you. So, two questions if it's okay. One is as a follow-on, would that mean that you talk about this human-brain interface where AI is sort of merged or merges with humans, like in a physical way? That's one question. And the other one is, also linking to your question, that AI could eradicate us if they see us as a threat to the world. I mean, what we as humans or as mammals or animals in general have is empathy. And my question is, in your research, is empathy something which is inherent in animals, or is empathy just a function that nature gave us in order to better survive? And if AI doesn't need empathy to survive in its environment, then it will lack empathy and therefore just eradicate humans. Most people are very selective with their empathy. They reserve it for the most cuddly members of their in-groups. And the fact that we have empathy doesn't stop us from having softer houses. Empathy, I think, is the ability to read and understand the inner states of others and we are better at this when these states are very similar to our own. And empathy is not the same thing as compassion. There are people which have very little compassion and a lot of empathy and people which have a lot of compassion and very little empathy. And so the more interesting question is how can we build AI that is not just going to be empathetic because they are going to be able to read our minds at some point. And we could use this with neural interfaces, but we probably can also deduce most of these things from camera images. The limit of that can easily be seen by the most capable people. And there are people which are capable enough to basically watch you with their eyes, and they're able to deduce most of your mental state from this based on the context and on your actions. There are only so many ways that you can make decisions. And it's possible to outmodel you. In the same way as I can usually outmodel my own children, AI is going to be able to outmodel us and see what we are thinking at any given moment. An interesting question is then, how can we have the AI have compassion for us and decide that it wants us to be around? And I don't have an easy answer to this. I don't think there is an easy answer to this. Great question Joachim. I personally think that we have a very slim chance of controlling an AI that is more smart than us. a very slim chance of controlling an AI that is more smart than us. But humanity has been harnessing technology for thousands of years and enhancing us with power. So do you think there is a path into the future that we can enhance our brain with AI so that what means to be a human might be different in the future because we are somehow merged with an AI or something like that. So it's not a thing about control because we are them now. So I think we should separate our models from our hopes. When we want to model and we want to hope to be truthful with respect to this, we should completely disentangle the fate of chimpanzee species on this planet, like us, from what's probably true. And maybe humanity is just Gaia's way to get all this carbon back into the atmosphere that Gaia could not get at with microbes. So Gaia came up with scribbles that she prodded into developing combustion tech and when we're done, we burn ourselves out. For thousands of years we've been using technology but we are the first civilization that has this kind of technology that uses combustion technology and technology of automation at that scale, right? We are this first civilization that makes it happen. It's incidentally also the first civilization that went over 400 million people into billions of people. Before that we've always hovered around 300 million or something times 100 million people and before religions more like a few tens of million people on this planet. So the technologies that we have right now might very well something that is intrinsically unstable and maybe the right solution would have been to go the biotech route and create something like a queen bee organism that lives for thousands of years and depends on us. But instead we now go down the route of creating a crystalline intelligence that doesn't actually need us. And while this might incentivise us to play for a long game, our governments play short games because we have not found a way to incentivise them for long games. AI might help us with. But yes there's this problem that the interests of such a machine, also of this industrial society machine, are poorly aligned with our own interests. And to regulate this alignment is very hard and we just don't know how to do it yet. Is there a solution to control how AIs interact with each other? For example, I mean, a very simple example for that would be the Facebook AI a few years ago where they tried to teach the AI to speak in English to each other, and they come up with a totally random language, improving the English language in a more efficient way. And I mean, there are other systems, for example, for financial markets or stuff like that, where an AI maybe in the future will kind of control the system a little bit, and the other AI who has a totally different purpose but controls the system so in the other way Are there some solutions for this so the AI will not totally cut or kind of try to Overwhelm the other AI or stuff like that. I think if you want to prevent a system from taking over you need to have another system which keeps it in check and Power this freedom of choice remains in a system where you have a balance of power, not an absence of power, because something will always get to the position of power, right? So what would be the thing that keeps AI in check? In some sense, humans are AIs. We are very much that. It's an AI in our mind creates a virtual world in which a game character, the self, interacts with a simulated universe. And this is what we subjectively experience. And this allows us to negotiate with the world. And the degrees of freedom in this AI in our own brain are tremendous. But what basically limits our own power is, of course, the size of our brain. And the other thing is our mortality. We are not around for that long. So even if a single individual can completely reprogram itself to be uninterested in the fate of the species and of their children and children's children and their neighbors, this individual will die after a certain amount of time. And the new thing is that AI can live much, much longer than an individual can. So these technologies that we can use to automate governments, corporations, and nation states, they're not just going to be disappearing after a certain amount of time. They're only going to disappear when the universe forces them to. And I agree, it's a big problem that we don't yet know how to solve. Okay, on that optimistic note, I do think that's a topic that is not hopeless. I mean we are right now in a bind. Our situation is dire. We feel that our societies have largely lost the plot. And the biggest problem that we need to solve right now is governance. And I think that AI can help us with this. It can help us to figure out how we should interact with each other and what platforms we can create to negotiate the conditions of human survival. And in this sense, I think it's a very hopeful thing that we are now developing these technologies. But I just feel that there are dangers that come with this technology and some of these dangers are going to be difficult to keep in check. But we probably do not have a choice. We don't have an option. There's no button that we can press and this button says this is not going to happen. Instead what we have to do is we probably have to see what's going to happen anyway. What are the ways in which these things that are going to happen anyway are going to be dealt with? And how can we get a head start on this? Thank you very much.", '9.16117811203003')