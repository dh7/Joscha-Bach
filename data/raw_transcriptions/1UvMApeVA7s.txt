('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/1UvMApeVA7s"> </iframe> </center>', " Alright folks, so we'll get started with the workshop part of our workshop. This is where I get to ask questions. So we have a set of four focused topics which are described at the highest level with a couple of two concepts. This one is object and situation and we've listed a few catalyzing questions to get the conversation started. These are also going to help us try to have a semi-focus to this interaction, because I think it easily could veer off path. So we have to have some sort of a direction, although I'm not sure right now how direct that direction will be. On the panel, we have Pei Wang, Dave Kramlberg, Yoshua Bach, and Tom Averitt. And this is also this first part. And in fact, in all of the sessions, four sessions, which will be each roughly about 20 minutes, you can ask the panelists questions about their own work. If they gave a talk earlier, you can ask them about that. But I ask the panelists to try to keep the answers short so that we can get in questions from the audience. There's another part to this, which is we will also open up the floor to the audience. There's another part to this which is we will also open up the floor to the audience answering questions from the audience. So it's not really thought of necessarily as a panel of experts who will now answer your questions, but rather as a dialogue. So that should make some people, I think, happy. Thank you. So let's get started. I just want to go through these questions first and maybe ask the panel to say a couple of things about them, any one of them or all of them. I don't care. And then we'll open the floor. So can I ask you, is it better that you give the microphone to the panelists and you use that microphone there? Yeah, that's a good idea, actually. So the questions we have listed for this session as a guiding light are as follows. Is it meaningful to say someone understands an object? How is that different from understanding a situation? So in other words, the process of understanding directed to an object versus a situation. Third, can a real world situation ever be fully understood? I think Pei actually touched on that a little bit. Are situations the same as arrangement of objects? Are situations basically composed of an arrangement of objects with some constraints or maybe rules? Are situations different from events? Or in other words, where does time factor into this? And lastly, do a context and a situation imply the same thing, or something different? If so, what? But now, let's go through really quickly, if you like any of these and want to comment, or if you don't like any of them and you want to comment on why, why not? You can take the mic. Yeah, quickly. I think clearly it's meaningful to say someone is an object. The difference between object situation in my model is, at a high level, they're the same. As I said, everything you want when it starts Eventually represent as a concept. But of course there are differences. Say object focuses on some entity, environment, maybe typically there are multiple entities with relations among them. But that difference to me is secondary. Can real world situation ever be fully understood? Of course not. I already explained that. Situation is the same as object, again it's the same thing. Higher level they're the same, but in details of course they're different. Situation is different from events. Similar question. They're both represented by concept, but the event, typically when we use that word we mean time matters. Situations sometimes time matters, sometimes it doesn't. To a context, a situation implies the same thing or something different. I usually use this in this context or situation I use them as a thing. There is some subtle difference, but I don't think it matters that much. Of course, these notions are used in common language, which means they are very ambiguous, and everybody understands something slightly different about them. And ultimately, this whole thing refers to very basic questions of epistemology. And the epistemology that I would use for all these things to describe them is roughly as follows. When we describe the world, we usually do so based on discernible differences that we see in any given moments. It is the element of an observation. And these discernible differences, the fancy name for that is information, can be measured in bits and so on. And if you have a bunch of these samples of these discernible differences, they describe a state or a subset of a state, because you usually cannot sample the whole state. And then we model any part of the world, we usually do so by identifying a transition function that explains the relationship of these states to each other. And if we can fully explain the transition function between the states that we use to model a part of an arbitrary system, we understand the system in some degree. And then we understand a new system, we usually do so by mapping it onto a system that we already understand. And there is one basic system, the most elementary system of all, and that's primary computation. So basically, something where you describe the world as a set of states and a computable transition function if you know how to compute to get from one state to another. And when I set this basic epistemology, I can start answering these questions. I can, for instance, map things saying that a situation is a state. An episode or a sequence is a succession of situations. Or you could also say that from a certain perspective, you collapse the time dimension, and you call the whole thing a situation. And an object is something that can be part of a situation, so you can excise it and use it in different situations again. And this also means that situation and object are, under certain circumstances, interchangeable, because a situation is a particular kind of object, and an object is a particular kind of situation. But an object has to implement a certain interface, which means you can excise it from that situation and use it in another one. I think we have to move on. Yeah. I don't know. Thanks. I think we have to move on. Thanks. That's pretty good. Okay, thanks. For now. Okay, then I'll stop at this. Before I go in it. Okay, just briefly, I think Pei-Heng's talk really touched on really well the idea that you can't, you know, you can't have a system that's not a system that's not a system that is a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's a system that's Just briefly, I think Pei-Heng's talk really touched on really well the idea that you can't complete understanding is kind of the skull that's never really reached that we kind of work incrementally to, you know, we're trying to understand something toward that direction. In terms of situations and objects, I mean my opinion there is that a collection of objects as a situation becomes somewhat more complex and maybe qualitatively different in that now you have an agent trying to navigate or something else, an environment. And I'll pass it because we're getting trying to navigate or something else, an environment. And I'll pass it because we're getting much more time. So I'm mainly up here to answer questions about my talk because I've thought extensively on understanding. But I think it's really important if you can test for understanding because it helps AI safety. That's about my point of view. All right, so please come up if you have questions. So maybe it's just a problem. Can you please stand behind the left turn? I can't see the video. That makes sense. You're still not behind the left turn. Sorry, I have to be correct about this. Otherwise the video, you can't see. So just on the first question, it says understanding an object. In English, I'm not sure that that's something you can say. I'm not sure I can say, I understand a chair. So I understand the chair. I do. Say anything. I understand how a chair is used. I understand what a chair is for. I understand how a chair is used, I understand what a chair is for, I understand how a chair is made, but... I don't understand understanding a chair. You stare at the chair until you become one with the softness of the chair. No, I think that's a good point, and I think by that question that's more what at least I'm thinking about is that it's not so much understanding the object in and of itself but for the purposes of utility what can you do with it, how does it work, how can it be modified or changed or used for something else is what I think is the important component of understanding objects in that sense. The problem with the chair is it doesn't do very much. There are not many interesting state transitions involved in a chair. So a chair is not really a process. A chair is something like a composition of features that are composed in a particular way. I think it's fair to say that I understand what defines a chair. I now understand what you mean by a chair, for instance, but it's not the same thing as saying I understand the for statement. The for statement is also an object, but it does something. There's some process involved, and once you are able to disentangle the process that's involved in there, you understand this particular kind of object. So what you allude to doesn't mean that objects in general cannot be parsed like this, but there is a difference in addressing an object that is simply a compound of features or understanding something that has to be mapped on underlying processes. Is it my turn now? Yeah. processes. Is it my turn now? Okay, thank you. Um, I wanted to say I really appreciated your presentation. I love the experiential grounding system. Totally eliminates any foolishness with art problem. Joshua, if you care to, you were doing an excellent job of throwing light on a mainly confused subject to be able to say, I appreciate your attention. So we don't need to answer. Good. Is that clear or no? No. It's not clear what the content of your question is. Oh. So I wasn't really asking a question. I was just inviting you to discuss more as you were discussing and you were interrupted when you were giving a very nice discourse on the nature of objects and situations. I thought that was very good because there was so much ambiguity in that and I thought you were doing a good job of articulating the differences. Thank you, but you were supposed to pretend that you have a question. I have a question which is related to these topics but maybe digs down a little deeper which is something that I've worried about a bunch in recent work with OpenCog, which is that I'm at a primitive practical level when you have an AI system that's just taking in pixel data. I mean, recognizing that such things as persistent objects actually exist is itself a somewhat difficult cognitive problem. And then related to that is recognizing that coherent events exist, right? So, like looking at me now, there's the object of this coffee cup, and now it's gone. Now you guys all think it's the same coffee cup, and I'm not a good enough magician to have tricked you. Waving my hand up and down, I mean there's a persistent event there, and you can say, well, okay, there's some surprisingness at the beginning and end of the event or something, so that's how we know there's an event there, but that's also can be a bit tricky. Some of the event may have disappeared and come back. And then situations, of course, they're also coherent in that same way as objects and events maybe, but they can persist. Like the situation of the panel discussion is one thing, the situation category panel discussion is another thing. And these get more and more complex, but at the very basic level even, like recognizing that this cup is an object, or Djosha is an object from the visualGI system, I mean, if you know how the AGI system recognizes that the object exists in the first place, that goes a long way to an understanding how that AGI system understands the object. Because, I mean, on the face of it, an AGI system just processing a stream of data, it wouldn't need to construct the concept of objects, right? Like if you have an AXE system processing the stream of data and choosing actions, is there the object of the cup implicit in its predictive model or isn't there, right? Or XITL or something. But in NARS, or OpenCog, or MicroPsy, I guess even just given a stream of data, it's going to construct the concept of the object. So I'm interested just to the extent you can reconstruct it in your mind. Within your respective AI systems, if the AI system is just given streams of data, what is the process by which it constructs an object as something that's worth representing or manipulating? Then after that, we could go on to events or situations, but objects is hard enough, actually. That's the whole question? That's several questions. Yeah, it's a several- I mean, Tom can answer it for AIX ITO, right? That's the whole question? That's several questions. I mean Tom can answer it for AIX ITO. I guess on my part you already know what I'm going to say. First, continue with what I said in the morning. To put it in extreme, I think all the objects actually exist in the mind, not in the world. The world provides the world. The world provides the material. So what we recognize, or what you said, it's basically identity of object, how to recognize that. It's basically the environs, or the relative stable patterns in the input stream or binary stream, whatever. And also, it's a matter of degree, once again. So there is usually some kind of threshold when this thing and this thing are similar enough. And also, there is probably a temporal spatial continuity, and then they recognize it as one. But when they change enough, they are going to stop there. Even sometimes they say someone, for example, like Ben doesn't look like Ben anymore today. Or is the caterpillar the same object as the butterfly? Oh yeah. So to me, it's kind of like something continuous. But sometimes for practical purpose, we have to cut somewhere to say this is the same thing and that is not the same thing at all anymore, but it's still related to the previous one. One question then to do with a symbolic system like NARS is to what extent does that cutting rely on sort of sub-symbolic number crunching from the perceptual data, and to what extent is the cutting a symbolic operation, and what kind of feedback is there between sub-symbolic and symbolic operations? Actually, currently I'm taking the very extreme position that I don't really distinguish symbolic and sub-symbolic anymore. In the sense that, for example, in ours, everything is a term. You can have atomic term, you can have compound term. If we really get low level input into it, then a pixel can be a term. A term is nothing but an internal ID of a pattern. So it can be an individual, it can not be a further divided signal, or it can be some kind of pattern. So a symbol in the sense of semiotics, say, is something that can be manipulated flexibly within a system of symbols. Yeah, you can still have that. You can still have that. The only difference, the major difference is what I said. The symbol is not something which is a label of an existing outside object anymore. As far as you gave up that, in terms of computer science, every data structure is a symbol. So you can do that. In terms of semiotics, if you like, Percy, semiotics, which you're familiar with, you can distinguish icons, indexes, and symbols as different types of signs. And in that sense, not everything is a symbol, right? Well, I'm not going to make that distinction anymore. Even that semiotic distinction? I think that this is still a little bit too much. I will just say that everything get equal to the system. As far as I have some kind of a handle within the system, which is consistent to a degree. And you can say that, or at least I will call that a term. I will be careful about the use of the word symbol, because that word have preload a lot of material. But I mean, if you go back to Perse or something, like a stop sign, spatiotemporally correlated with the thing that it's supposed to refer to. Whereas the word stop in an abstract discourse is not spatiotemporally correlated with the thing that it's supposed to refer to, whereas the word stop in an abstract discourse is not spatially correlated with the thing. So you would have called the stop sign an index. The word stop in every conversation is a symbol, right? For example, if I say the word chair, in NARS I don't call it a symbol anymore. I can say it's a term, but that term already has some grounding. But there are still pure symbols, like for example, the letter X, which can stand for a different thing, we can still call that a symbol. Anyway, so setting aside the terminology, then I guess I would rephrase my question, like to what extent is the partitioning off of some sensations or some classes of sensations into an object to what extent is that the result of reasoning about things that are fairly simply defined in terms of the percepts whereas to what extent does that require more abstract concepts in the network? I can't even allow both of them. I can't allow something to be both a single symbol or a single atom and also a structure of many things. My point is, we don't have to always draw a sharp line between what is an object and what is not an object. It's just something that's more pervasive. It's pretty clear to everyone here, like, this is an object. But if you drop it, it's broken. Do you have the same object broken, or do you have multiple objects each? There are many borderline cases. But yet there are also many extreme cases. Yes, the extreme cases. Yes, the extreme cases will be, you know, this thing is stable enough, and that we won't know. The same thing is true for the concepts in ours. You already may say it's the same concept, even though each inference actually changes a little bit. Yeah. And then we will say it's still the same. There's also more abstract objects, like the object of discussion. Yeah. To what extent is it the same criteria? Well, it's a matter of degree. For example, is this, our current discussion, the same discussion as the previous conversation, or is it two separate discussions? Yeah, yeah. So I guess what interests me from an engineering view is what specific criteria are used to make that distinction. Yeah, I don't think there is a sufficient, necessary condition for something to be taken to be an object. It's a blurry. It's fuzzy, the boundary. Sure. I will take that one. Yeah, what would be the fuzzy criteria for taking something as an object? Similarity, similar enough to the previous, like this. The image, again, the visual image is similar enough. But two identical twins may be more similar than you now and you think it's like this. No, no, no, but if they're in different location, I have another attribute to distinguish them. So some kinds of similarity count more than others? Not really. In that situation, it's not purely similar. If you have twins, one show up, then another one go out of the door and come back. Right. But you and yourself ten years from now are not that similar either. It depends on your discussion. For some discussion discussion I will say it's the same person. For some other discussion I will say they're two different entities. Even though they happen to be continuously in essence. What do you think, Joshua? Before we go to the next one, are there any questions from you guys on this particular discussion here? Because I think that was pretty interesting. Yeah, go ahead. So, in disembodied voice, I'm just saying, I don't want to have to keep reminding people, but if you want to be on the camera, you have to stay behind the lectern. If you don't, it's fine. He doesn't want to be here. You can't pose it to us. I'm trying to be non-observant. He doesn't want to be. You can't pose it to us. I have a few different questions, but I guess we're probably very tight on time, right? Go ahead. So the first is more, I don't know if I can quite frame it as a question, but it's more of an idea, which is sometimes it's we use physical metaphors to deal with conceptual ideas. Like that John grabbed the idea out of the air. So the idea is metaphorically represented as an object, so that it's easier for us to be able to almost see it in our minds, and perhaps not for Joshua. But to be able to spin it around. And with the idea of, say for instance, understanding the chair, which may be the role of something like a sitarnastan. So this is kind of the second part, which is the, I guess, sometimes if you take, I'm not quite sure, Pai-Wai-Wun, when you talk about a term, what you actually mean. Like if I say, for instance, if I have the symbol bank, the person can't understand it unless it's represented around with the related objects. You know, like I banked the plane, or I'm sitting on the river bank, or I went to the X to withdraw some money. You know, you can't, without the context of the objects around it, it doesn't make any, you can't disambiguate the sense. But there's also the sense itself has an identity which is independent of the term, the term is just a label. So I guess my question in that particular case is, are you, like I think sometimes people, they conflate the label with the underlying object, which has its own underlying meaning identity. I think I understand what you mean. First, but I need to explain. First, a term, well, technically you can just think of it as an ID. OK, it's an internal ID of something, of course it's an ID of something, typically a data structure. Second, what you said makes complete sense. The only difference, for example, when we say bank, there are two different images related to it. In our speaking, we still have that. The difference is, I'm not saying that a bank is a financial institution or a bank by the river is two objects out there. No, they are also ideas in your mind. But of course it's not unrelated to the outside because eventually they come from the interaction with the world. But I do not assume there is an object there, there and your name, it's just a label, you touch to this one, you touch to this one. Instead, I just say there's three concepts, so your bank may be related to this concept of a financial institute or to the concept of a river bank. But this will fit into the semantics I provide. Maybe on that note, Joshua. There's of course a distinction between conceptual representations and the underlying stuff. I think in our mind we have generative probabilistic algorithms that produce the mental simulations that we perceive as being the world. Basically, within this dream that is generated by the neoportex is the result of operators that we entrain our brain with that produce dynamic structure. And these sensory motor scripts describe different domains of the world that we want to model. And concepts are the address space of these sensory motor scripts. Concepts are a manifold that is shared between speakers because they get bound to linguistic symbols. Linguistic symbols exist in families of speakers. And for that reason, you can infer the structure of the conceptual manifold just by looking at the statistics of language, which is why these vector space translation models work so well for machine translation. They're not perfect, because they cannot do the mental simulations that are behind these things, but they can roughly get the relationship, the relative relationships in that conceptual space between the different concepts that exist. They don't get the content of these concepts. They cannot simulate. And you talk about king and queen, what the king and the queen look like, and how they actually interact, and so on, and so on. So they lack all this operational knowledge. Now, when we understand things, we make a model of things, and I think we have to understand this distinction between model and implementation. Whenever we talk about a domain, we assume that there is a ground truth in that domain, which means there is a particular way in which it is realized. There is a particular way in which the universe computes that particular thing. To compute might sound energetically specific, but it's not specific. It's just a clear way of talking about there is a set of discernible differences that changes in a particular way. This is somehow realized in the universe. However it is, this is what it is. It's necessary and sufficient that this happens. There is some computational thing that we are describing, even if that thing does not really exist, but it's what we want to model. And now our model can be one possible model that explains possible implementation. But there is, of course, as we can show, usually an infinite number of possible implementations, which means you have a very, very large space of models that are potentially valid. It doesn't mean that everything is valid. There's only a particular set of processes that could produce the phenomena that you observe in that domain. And a full understanding of the domain means that you understand the shape of this complete subspace of valid models. That's why Marvin Minsky said, you only understand a thing if you understand it in at least three ways. Of course, three ways is not sufficient. But it means that you get a feeling for the space of possible explanations for that space. And usually, we are content when we say we have one possible model. Then we understand it. Then we are very lucky. Right? I now understand, say, the lambda calculus, I understand computation. But if I understand the Turing machine and if I understand the type, the untyped lambda calculus and if I understand probability and many others, I get a much, much better understanding of computation. It turns out there's an infinite number of possible equivalent definitions of computation. And when you understand them all, you can understand this complete subspace. You have a full understanding. And now, if you want to translate this into a cognitive architecture, it gets tricky, right? How do we model, for instance, a relationship between a concept and a space of possible understandings instead of just one. How do we switch from them? How do we do model revision and so on? OK, let's stop here. Paul, do you remember Ben's question? Yeah, right. Yeah, I want to hear about AXE's internal conception of objects. So how does AXE understand the teacup? How would AXE's model of the teacup be computed? So I think a fully trained AXE would have a compact representation of the universe, like say some small set of natural laws, like the initial conditions of the Big Bang, for example. And then from there, everything would derive. So there would be no T-cap in that sense in the model that you could identify easily. But on the other hand, it would have a perfect understanding of the T-cap in the sense that it would perfectly predict, like if I put it upside down, the e-counter would go up and so on. And that's a perfect understanding of the T-code. Yeah, now probably AXETL for big T and L will have the same property. As you decrease T and L, eventually you would have to make a representation of the T-code. But probably then you made it so small that none of the nice theory is useful. Possibly. I guess. But I don't know. That's interesting. Yeah. Actually, TNL plus multi-NL could have a more sensible representation to get the anode from the bone. Yeah. All right. So I think we'll switch now to the next session. So you guys ready? Alright, so I think we'll switch now to the next session.", '20.392446041107178')