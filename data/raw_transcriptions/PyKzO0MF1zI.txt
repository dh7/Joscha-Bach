('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/PyKzO0MF1zI"> </iframe> </center>', " I'm Joschaba, I'm an AI researcher and cognitive scientist. I'm at the moment based in Berlin and that's about what there is to say. When I was young I studied philosophy and computer science because I wanted to find out how the mind works and I still do. So I went into cognitive science. I did my PhD at the University of Osnabr√ºck, building a cognitive architecture. And at the moment I'm working in a small company in Berlin where we build AI. What are the motivating factors for you to participate in the AGI conference? Well, one of the motivating factors was certainly that I was one of the chairs of the conference. I see the AGI conference as an attempt, or part of an attempt, to get back to the original idea of artificial intelligence that is to see computational systems as an avenue towards developing and understanding how the mind works. I think this is at its heart a philosophical enterprise but the methods that we need are some that we do not find in contemporary philosophy. So my idea is not to build applications or to develop formalisms that can be used to build applications, but it's about maybe a subset of analytical philosophy. It's in a way computational philosophy, where analytical philosophy allows you to express everything that can be or use everything that can be expressed in a formal language and principle. I want to narrow this even further down towards things that can be expressed in a computational system that are computable, that can run. This means you do exclude paradoxes. It includes not only those things which you cannot specify, but it also excludes those things that you can specify that are paradox that wouldn't run. So eventually the proof that you have understood something from the point of view for computer scientists is that it works. And I do think that we need such an approach, as it has pervaded most of physics, for instance, already, we need such an approach in philosophy of mind too. we need such an approach in the philosophy of mind too. I don't think that there is a mainstream approach to artificial general intelligence anymore since then. Back in the day, these were people which originally came from cybernetics and other fields and had the idea of taking the perspectives that they had in cybernetics and other fields and had the idea of taking the perspectives that I had in cybernetics and other disciplines together up into a new field to use the newly available computer languages and computing systems to bring the ideas together to build artificially intelligent systems and many of the ideas that we have now and that we sometimes call new AI were already alive at that point. And there were several detrimental factors. For instance, there was a very strong optimism because of ignorance of the problems that people would run into, but also because there were a lot of low-hanging fruits. AI made tremendous progress in the span of very, very few years. So suddenly we had some simple natural language processing, some simple planning, simple game playing and so on. And then there came a time when it was very, very hard to go beyond these relatively simple things. And this led to disenchantment of many people with the original goals of AI, but it also let the public catch up with this disliking of the idea of AGI, because the idea is basically telling people that there is no metaphysical domain in which their souls reside, that we are basically software. And for many people this is a very offensive idea. So our culture, especially our Western culture, is not exactly sympathetic to that idea. But nevertheless, AI as a discipline in academia has been always very productive. Almost everything that is now interesting in computer science has been AI at some point. So AI has been a moving frontier, a pioneer battalion of computer science all the time. There was never a time when this was not a fruitful paradigm. But now the people which work in AI have pretty much abandoned the original idea. They have abandoned the direction of building something that is like a human mind or beyond that. And this means that they continue with what they were just doing before. They went in the direction. They built, they dig holes. They dig holes in description logics, into agent-based communication protocols, semantic network formalisms and so on. They build isolated communities that are no longer united by this general vision. And I do think this is not good. It's not good for AI as a field that produces useful applications, and especially it's not good for the way we want to do the cognitive science that is building computational platforms that are unified models that take in account models from neuroscience, philosophy, ideas, knowledge that we have from linguistics and many many other disciplines into common platforms and test them, make them testable, improve on them. And I think we need to revive this. So this is what AGI is for and I think it's a rather recent development not only with the AGI conference but with other conferences like COXIS and BICA and some movements within triple AI and so on to get back on track with some of the original ideas of AI. Well I think the question how the mind works is probably one of the most interesting questions that a human can ask. And this is part of a discourse that's been going on for centuries. And to work in AGI now is a chance to participate in this discourse in my view. So it's one of the most exciting things I could possibly come up with in my life. Yeah, I think that there is a needless clash, if you look in the history of philosophy, between materialist and mechanist philosophies on one hand and phenomenologist traditions, for instance, on the other. And I do think that we need to take a slightly different perspective, one that focuses on information processing. Because for epistemological reasons, the only thing that we truly have available is information, a discernible difference at our systemic boundaries in some way. And things like matter and so on, energy, these are just possible encodings, these are possible interpretations of the data that we have, of the observations that we have. And our job now as AI people or as cognitive scientists is, I think, to find possible implementations for information processing systems that can bring forth all the phenomena that we are so interested about, like emotion, motivation, phenomenal experience, mental representation, imagination, agency, intentionality, sociality, personhood. All those things eventually are certain modes, I think, of information processing. So, with taking this neo-mechanist perspective, I'm not trying to abstract from all those phenomena and ignore them and say that they're not relevant, but what I'm trying to do is to find a productive way to give answers to them. This basically depends where those people are. So, a very, very simple and simplified narrative would be, if you take for instance our standard western background, which is informed to a large degree by Christian dualism, it looks like this. We have this idea that the world around us is actually not real. It's some kind of World of Warcraft. It's glorified World of Warcraft, of course. It's an amazing amount of detail, but still the things that we have in there, the values that we perceive, like money, the quality of our housing, beauty, fame, this is not real. This is just game currency. And our real souls are outside of this domain, our bodies, our experience. This is just the interface of an avatar into this world. And the world has been created of course in this traditional narrative by God who has root access and could program it all and there are also some other guys which might have root access, most notably the devil. He also has root access but he lacks God's vision, he is just a hacker, he corrupts the system at certain points. And since he has this outside perspective, he realizes what it's all about and he uses his knowledge to corrupt us out of our metaphysical currency, our souls, by giving us game money that he can, via being a hacker, easily create. Now, from this perspective, subtract God and the devil and you are where most educated people the West are right now, including many philosophers. And what AI is telling them is, you guys are NPCs, you are non-player characters, there is no one outside of this world of Warcraft which controls you, you are a character that is entirely implemented in this domain. And for many people this is a very strange perspective. But it resolves a lot of seeming puzzles. For instance, the big puzzle of quantum mechanics. I think the difficulty with the comprehension of quantum mechanics is not quantum mechanics itself, it's pretty simple compared to other theories. It's the difficulty to map the concepts of quantum mechanics to our macroscopic experience. So we are asking ourselves, where is the light actually when it travels from the sun to our eyes? Is it a particle or a wave while traveling there? But if you look at the screen, you see a game of World of Warcraft, and you see the game sun on the screen, and you see your armor on the screen, and the sun bounces off that armor, and hits eventually the screen and you see your armor on the screen and the sun bounces off that armor and hits eventually the screen and so on, you would never ask the question, where does the light happen to be during that journey? You ask, how is it implemented? And I think we need to take the same perspective towards the world and some very strange perspective for most people but a very natural perspective for computer scientists. Actually I do not have that many hopes for the future. For instance there's this 2 degree centigrade climate goal and we all by now know that 2 degrees centigrade will not leave us in a very comfortable future. But we also know that this means that we are only allowed to burn 20% of the known fossil resources. I think it's pretty unrealistic to expect that existing companies and so on will leave 80% of their assets in the ground. It's not going to happen. So we are looking towards something like 5 degrees centigrade global warming. This is just one of the many possible avenues and issues that might concern our future. And maybe this turns out not to be a big issue at all, maybe we are lucky and get away from this. But I mean that we are probably facing existential risks that at some point will make it very difficult to continue academic research. And I do not see my work as a part of creating a bigger, better future. I do think that our civilization as it is, is probably pretty much doomed. But this is not a reason not to do things that you find meaningful now. And the production of culture, the value of culture, the production of insights, the engaging in discourse between people. I don't think that it really depends on whether the earth will continue to exist forever or whether it's eventually consumed by the sun. It doesn't matter. I'm not working for the future. I'm working to get some insights. I'm interested in talking to people, to listening to their insights to learn something new. Well, I don't think that we probably have enough time, long enough to develop AGI. I don't think that it's so difficult, because nature has managed to come up with it and gets it stable in every brain. And there is a very bad signal-to-noise ratio in biological neurons. And so the computational complexity of the brain is not many orders of magnitude beyond what current computers can do. We don't know how the software should be written. We have some good pointers though. And I think eventually it will boil down to a few hundred organizational principles that can be encoded in the genome. And the other complexity boundary for this is probably the information content of the genome, which famously fits on a CD-ROM. So actually our blueprint for our body is simpler than the blueprint for current edition of Microsoft Windows. But it's not been built to be re-engineered. It's very difficult to find the organizing principles. And it's often very easy to see them with hindsight, like back propagation learning with hindsight is a very very simple phenomenon but it took mankind a very long time to hit upon the principle of back propagation learning and I guess we have a few hundred problems on the difficulty layer, a level of back propagation learning to solve before we get to AGI. So in my perspective, we have many, many PhD students which will need to get engaged with this and combine this into a common platform, and I don't see any easy shortcuts. So I'm one of those people within the field which are skeptical of very rapid progress. I think that AGI will be some decades in the future, if it happens. If you are referring to the question of AGI as an existential risk, the singularity and so on, I think that we need to change the perspective a little bit. Nick Bostrom has given a talk on existential risk and in his vision the AGI is something that needs to fight its way out of a box. Like there is this little Roomba robot and then you make it more smart and smarter and then it becomes sentient and eventually it finds its way out of the cabinet, out of the basement and starts to dominate the world. And something like a movie iRobot, if of the basement and starts to dominate the world. Something like in the movie I, Robot, which we shouldn't let come to pass. So some superhero will step in and prevent this thing from taking over and letting mankind continue on its God-given righteous course towards oblivion. And I don't think this is going to happen this way. I think that AGI is not going to escape from some basement, but AGI is going to be used as a decision-making tool in existing organizations. AGI is going to borrow its motivation from corporations, from governments, from whatever organization is using it as decision-making tools. And these organizations are already agents. They are already agents that are stronger than humans. So this singularity takeover is a thing of the past, it has already happened. This planet is no longer ruled by humans, it's ruled by organizations. If you want to change something on this planet, you better become an organization and make sure that you are very effective and that you can pull off those changes that you want to. But of course you will have to do this in the framework of the existing organizations. There is no vacuum left anymore. It's all in interaction already. And humans are kind of ephemeral for the most part because it's very hard to get to a bifurcation point in a large-scale existing organization where you can really make a difference, as Obama can surely tell everybody. I think one of the most important things, I guess, is to resist some of the pressures that are currently on academia. Academia has become as a job market very harsh and this creates a tendency for streamlining. People are forced into certain roles in order to become successful, for instance, as philosophers. Only a small fraction of the people which aspire to become philosophers eventually get tenure, for instance, and at some point they will be faced with the grave existential risk in their own careers. So they do tend to adopt methods over questions and I think this is a very dangerous thing to do because academia is there to answer questions. It's not there as some addition to commerce to produce products that can be commercially applied or to produce people that can be commercially applied or to produce people that can be commercially applied. Academia is there to produce ideas, to foster ideas, to create people, to create society. And I think this is, over all the disciplines, one of the most important things that we should do as academics and scientists, regardless of whether we are working in AI or philosophy or biology or economics or anything else. So I would say the most important thing for young scientists is to question yourself what questions do you want to answer and then go for it. I do think it's valuable if somebody is an AI skeptic because I do love if people have different opinions from mine, if they have good arguments to defend them. The most important thing is always the argument and the only thing that I have to offer are arguments and what I consider to be insights and which will probably be overcome by better insights. So I would encourage people to disagree with everything I say and question it and come to their own conclusions, naturally. On the other hand, I think it's important that people are not motivated by fear or by the idea that they need to conform to a mainstream opinion. The truth is rarely in the middle, rarely ever. The truth is typically where the good arguments are, where the facts are. And for instance, I find in the anti-AI movement, it's very popular to have arguments like AI is impossible for technical or mathematical or other obvious reasons that prevent us from ever realizing it. On the other hand, we shouldn't pursue it because it's so dangerous. These arguments are mutually exclusive. And if people use the same argument, both these arguments, like for instance Joseph Weizenbaum, who I personally greatly admire, did, I think there is something wrong with your argument. I think there are argument. I think there are different needs you need to satisfy. One is if you want to produce and foster an academic discipline you do not only need to be a very good scientist. What you need to be is also to be a good politician and politicians are not people that take the acute angle but that are good at pushing the envelopes and they're not good at I'm too much a scientist to do that. And I think that the field also needs politicians.", '9.745982885360718')