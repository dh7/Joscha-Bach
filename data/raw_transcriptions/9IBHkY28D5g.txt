('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/9IBHkY28D5g"> </iframe> </center>', " It's my great honor to give today's first keynote. And thank you very much for inviting me to this event. Today, I want to discuss the question of whether large language models can carry us all the way to artificial general intelligence, the big holy grail of AI, and whether they are going to run into limitations. First of all, to answer this question, or to begin answering it, we need to find out what it means for AI to be general. Classical AI basically work by identifying a problem like chess and figuring out the solution and then writing an algorithm that implements that solution. So a lot of the intelligence exists actually on the side of the coder that is then translated into a distilled skill. Whereas in machine learning, the skill that is being distilled is the ability to learn something. So the developer writes an algorithm or the researcher that is learning how to solve the problem. And the question is, what comes after that approach? Well, it's probably meta-learning, an algorithm that learns how to learn, right? And today, the answer is, it's the transformer. It's an algorithm that just learns to solve all problems by looking at all the data on the internet and then generalizing over those data and then figuring out by integrating over the data in all sorts of ways to statistically generate a function to which we can reflect a random number and put in a prompt that is going to give us the solution to all our questions. And it's very surprising. A lot of people made definitions of intelligence in the history of AI, and the next token completion idea of the transformer was not very high on the list, even though it's related to data compression and so on. But if you look deeper into what the transformer is doing, then we notice that it's indeed an algorithm that to a very large degree is learning how to learn, and does have a lot of generality. And transformer is not the only thing that is being used in the current generation of models, just one of the approaches to make efficient learning and neural networks feasible. But the scaling hypothesis says that using the current set of algorithms with some slight modifications and more data and modifications of the loss functions and so on, we will be able to solve all the problems of intelligence in such a way that we can get to a system that approaches generality. This is the scaling hypothesis. It's what open AI and a number of other people in the space are currently betting on. And this principle has led to an extremely rapid development in the last few years. And this leads us to the question, when is it that the language model becomes better at artificial intelligence research than humans? And I think that's an interesting criterion for generality, because it means that whatever generality is missing can now be left to the system. And because of this rapid progress, a lot of people now feel that the possibility of artificial general intelligence has changed dramatically. Many people still think that it's science fiction and no system that is not biological or just not a human being will ever be able to truly think and understand the world. But if you look at, for instance, the prediction at the online forecasting platform Metaculous, we find that within a relatively short amount of time, the predictions have reduced from 2,000 to 50,000 to 25,000. 2000, 20, 50, 5 to 25. Right, but a lot of people are expecting some big lead general AI system to be publicly known. So how can we identify what it means to be general? And my cognitive computing group at interlabs have been working until recently, we thought about this question, and we came up with a space of capabilities that has several dimensions, and we call them the big nine, the basic capabilities like perception, learning, representation, reasoning, knowledge, language, autonomy of the system, collaboration, its ability to be embodied, which is very relevant to robots. And these capability vectors define a space in which the lower strata are basically extremely narrow and task-specific, and the highest stratum is intelligent. And when we imagine the space as composed of regions, then we can take any intelligent or AI system, even the least capable chess programs or these general chess programs and locate them in the space. And we can also do the same thing for our language and vision models. And we find out that a lot of that space is currently uncovered. The systems don't seem to be autonomous, they're not really coupled to an environment in real time, they have difficulty with real time online learning and so on. So the question is, when do we reach a system that reaches human level or beyond in all capability dimensions? So is there something that large language models can do? This reminds us to the question, what computers can't do. In 1972, the philosopher Huber Dreyfus wrote a quite influential book that described what computers cannot do. In 1972, the philosopher Huber Dreyfus wrote a quite influential book that described what computers cannot do. And when I read this book, I found that even at that point, a lot of those things were already being actively worked on. And Huber Dreyfus then updated his book, What Computers Still Can't Do, and even had the third edition. But it was in some sense erased, because the question, what computers can't do do was not the answer that he gave. He basically looked at what early algorithms for representing and manipulating for positional logic struggled with, right? And so a computer is a far more general thing than what existed in the applications and implemented algorithms at the time when he wrote the book. And this led me to see the implementations that exist in the current system as the limit to what the system itself can do. What's a computer? A computer is a universal function executor. This means it can execute any function that can be formulated and expressed, and the difficulty is in finding and implementing that right function. So what we are, computers can in principle do everything. If the computer is large enough, you can put a universe inside of it and everything is going to happen. But finding something that is producing this universe is the hard thing. Of course, also the computer needs to be fast and large enough. So what we are looking for is a function approximator. And Gary Marcus is now basically making a similar thing as he, as Stryfus did when he is explaining that large language models are extremely limited in what they can do. And he thinks that it's a very short-sighted paradigm that needs to be combined with hybrid approaches. And it's not just the large language model that he's concerned about, but the entire field of deep learning. But what is deep learning? Deep learning, I could say, from the most general perspective, is making this function that is in the computer out of many, many sub-functions that can be put together automatically. So it's compositional function approximation. Or you could also say that instead of writing stuff by hand and putting the developer in charge of solving subproblems and so on, we go towards end-to-end trained functions. And the way in which we do this is differentiable computing, which means we express the entire program using linear algebra in such a way that we can shift the solution around in a search space of programs that is somewhat continuous until we click onto the right solution. And the way in which we are doing this is always some form of stochastic gradient descent. And so this is all definitions of deep learning, again, more and more specific. And the most specific one is to say that's the current set of state-of-the-art algorithms, something like the current GPTs that we are using for language models, vision models and so on. Or, in the most narrow sense, it's the current generation of these generative models, for instance, Chet-GPT. And if we look at Gary Rucker's criticism, it's probably not applying to compositional function approximation itself or even to differential computing, but it's possibly applying to a current set of SOTA algorithms or large language models. And when we look in detail, even that is a sketchy question because they are quite general and the abilities that they have are very hard to make a moat around and say this is what they cannot do. Because deep learning itself is something like a universal function approximator, which means it can, out of the space of functions, search through a space of programs and stumble on almost any program. It's just maybe not the most efficient way to do this with the current class of algorithms, but if you find the right approximation criterion and the right architecture, which is part of this process, then you will be able to often find a solution to the most complicated problems. Maybe even the solution to the problem of finding solutions to all problems itself. There are very salient limitations to the current language models. For instance, they're not reliable very often in what they do, they might not be truthful in their output, they might be incoherent, and they might have problems with creativity. But of course, there are solutions for many of these issues, these functional limitations. For instance, we can use better training algorithms and reinforcement learning with human feedback. Probably many of you saw this metaphor of unsupervised learning producing some eldritch horror that exists inside of the computer that has no semblance with what the structure of understanding of a human mind. But using supervised fine tuning, we get that thing to be closer to simulating a human mind. And with reinforcement learning with human feedback, we can put a mask on it that lets this thing deepfake being a human in a pretty plausible way, as long as you're not looking hard enough. In order to make the model more truthful, we can combine it with a prover. And maybe this is what Elon Musk is attempting to do when he builds his Tooth GPT, as he has called it. When we look at the lack of coherence, of course, we get more and more coherent as we add more and more data, but maybe we can add a different loss function. And when we look at how large language models learn currently, they basically learn everything by looking at patterns and completing these patterns based on statistics that are defined in the patterns. And if you give them more and more patterns, at some point they discover the syntax of human language, if the patterns that they're looking at are sequences of characters or words, and then they discover the style in which we write texts and can make texts that sound like texts that come from particular authors. And when you give it more and more, then it's also going to discover more and more semantic regularities until it can not only deepfake style but also deepfake meaning to a point where there is no difference between the deepfake and the actual semantics anymore. And you ask it, write me a computer program that does x and it produces a computer program that does this. And the way in which minds learn how to understand is quite different. We start with semantics of basic pleasure and pain, and we touch things, and this creates an initial meaning, and this influences on the organism that the organism gets feedback to the mind. And then we recognize patterns in the percepts that correlate to the semantics that allow us to discover deeper semantics, and then we discover syntax in language. And eventually we discover in the long tail of the syntax the style in which something is being written. So after we bootstrap the system to such a way that it can recognize patterns in text and the semantics behind them and how they correlate to them and we give this thing a textbook, it's first of all going to learn the semantics of the textbook, for instance, how to do simple arithmetic, before it understands the general syntax of language and the style in which the textbook was written. It's really the other way around than it happens in the large language models. And if we emulate this coherence-first principle into a loss function, maybe we get to systems that become coherent much, much earlier. So what about creativity? Well, it's very hard to say that language models are not creative, right? But there seems to be something missing. What is the thing that is missing? I think that creativity requires three things. The first is novelty, that this thing that is being generated was previously unknown. And indeed, large language models and vision models can create things that have not been seen in the training data. And if you give them enough training data, they can know so many latent dimensions that interpolation and extrapolation become the same thing. Another requirement is invention. It means that you're not just following a gradient and finding a solution, but you're bridging discontinuities in the search space. And it's something that large language models can also do. They're able to discover things that are not just between things, but they can confabulate. They can try new things. They might have difficulties to test that these things are true, but we can extend these systems by grounding and first principles reasoning and so on that allow to test this. So invention and novelty are not really the problem. I think a problem is authorship and not in the sense that you can legally acquire copyright on what you produce, but that what you're producing is modifying the author, what you, the system that is producing this thing. So you cannot do this twice because you now see the world from a different direction. And this is something that the improved models really struggle with, because they don't learn from their own output, they don't learn from their real-time interaction with the users and the world. And I think this would be required for a language or vision model, for a creative AI system to develop its own voice that becomes recognizable and turns it into a true artist. Not necessarily a human artist, but something that is artistic on its own voice that becomes recognizable and turns it into a true artist. Not necessarily a human artist, but something that is artistic on its own and has a recognizable voice. Beyond these functional limitations, we see capability limitations. For instance, the language model does not perceive the world around it. It cannot directly be connected to a camera and so on. It does not update itself in real time or turn something deeply into its structures. It struggles with things like symbolic mathematics to high level and it might have difficulty for simulating complex things. So if you ask it to imagine a very complicated scene and perform construction tasks in it, it's failing at this and it doesn't have agency of its own. But again, we can overcome this without leaving the paradigm. We can combine our large language model with a diffusion model that is guided by camera input, and that the diffusion model converges onto the partition of a scene and then correlates the states of the embedding space of the language model and the vision model, and thereby get a system that is able to report about what it's seeing and perform inferences on this. We can get around online learning by using existing algorithms, by just using a database to extend the prompt memory of the language model. So we can basically, if the prompt memory, the working memory of the system is full, we can use a database that is smartly swapping in and out memories from that working memory. And when that database is full or inference on the database becomes too slow, then we just turn the system off overnight and we retrain the system, fine-tuning the contents of the database into the system. So it's basically dreaming at night to update the data during the day. And maybe this is also somewhat similar to how humans learn because you find if you ask people to understand something that is counter to what they currently know, they often need to sleep over it one or multiple times to resolve all the dependencies of these new ideas that they can no longer hold in working memory. And when language model struggles with mathematics, of course, we can do similar things as humans are doing now. We can use a computer algebra system and the large language models can learn to use a computer algebra system, or it might even be able to write its own by using its ability to write programming code with an external compiler, and then test the programs that it writes against that compiler, and then uses the compiled functions as its own functionality and extend itself thereby in a similar way as a human being does this. Then Stephen Wolfram writes Mathematica to extend his own mental abilities. And of course, for mental simulation, we can in a similar way use a physics engine and write our own physics engine to describe reality. And maybe this is also in some sense what our own mind is doing. So what about agency? It's the different perspective that we get when we look at AI as a tool versus an agent. An agent is self-directed and the minimal agent is basically a controller for future states. The way in which you get from a thermostat to an agent is that you let the thermostat not control the present, but you let it control the entirety of the future deviations of the target temperature and the current temperature. And to do this, it needs to be able to model the future. So it needs to look at reality in such a way that it can have a different expectation horizons and then a branching world and its current decisions to turn the heating on and off are going to affect these trajectories in the future. And this allows the thermostat to become much, much more efficient by becoming an agent. And once it's modeling these different trajectories, it basically has beliefs about the agent. And once it's modeling these different trajectories, it basically has beliefs about the world. And by choosing the trajectory or desiring to choose the trajectory that minimizes a number of deviations, it has preferences now. And its own switching actions become decisions on the space of those preferences. And the system that we are, of course, is going to be much more complicated because we have such a complicated modeling system and so complicated sensors that they're able to discover ourselves in the world. So if an agent gets to the level of sophistication, where it discovers its own actions as a causal factor in the world, and the way in which it achieves on those actions, we get to a more and more complex agent that achieves more and more levels of self-awareness. And if you think about how such agents achieve their functionality, they don't just made up of a single agent, but of many, many sub-agents that form and that start to interact to form something like a society of mind. That's a concept that was invented by Marvin Minsky, was not the first one. It was very much influenced by Oliver Zellfridge. And Oliver Zellfridge came up with the pandemonium theory, an idea that there are a bunch of agents, demons, on the stage, that each of them have a different functionality. And they call each other up on stage to solve a certain task and get pushed off stage when they're no longer useful in a given context. So we can imagine that in our mind at any given time, there are a bunch of those agents on stage that are helping us to deal with the situation at hand, produce a model of the scene that we are in, switch out different parts of that scene and so on and change the scene as the world changes around us. Can we do something like this with large language models? I'm not sure I can do it like you, but we're kind of running out of time. It was a fascinating talk. Let me just wrap up the last sentence. I prepared for 20 minutes, we're almost there. So we can basically take the individual models of a cognitive architecture and each of them can be composed of a language model. So despite me not knowing whether large language models can get us all the way to AGI, there is no proof that they cannot. And I think the future remains extremely exciting using this single paradigm. Thank you. paradigm. Thank you.", '9.744000911712646')