('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/UF3jnF23DNk"> </iframe> </center>', " Okay, perfect. I think for the internet audience, it's probably best to do this in English, although we're both German. So my first question, which is kind of on a personal note, is because I've been really fascinated by your work, is if someone wanted to get into this, where should she or he start? Should she start with understanding the mathematical underpinnings of all this, or do you think it's best to go into philosophy for this, or do you think it's best to start with computer science and artificial intelligence? Or where do you think are the foundations for all this best found and how can one approach them? Personally, I didn't find the right way to start this. So I was mostly explorative. I read lots and lots of different things from different domains and ultimately, and gradually pieced my worldview together from many sources. And maybe there is one right way to start it, but I haven't found it. And I can start with reading the classical philosophers, of course. You can start with reading Aristotle's physics and metaphysics, and Kant's criticism of rationality, and you can work through GÃ¶del, Escher, Bach and so on, but I think that's hard to find the right interpretation of these books when you're starting out and prepare to revise your thoughts often and ultimately converge on something. Maybe there is the right curriculum, but I don't know where that right curriculum is. So in some sense I cannot give you super good advice on how to start there. Maybe we have to write the book at some point or write the curriculum and see how to do this. I can't give you a long list of books that have influenced me, but then again it's also not that I think that my own trajectory has been optimal or something that I would recommend to other people. When I was studying I was mostly driven by curiosity and I think that was other people. When I was studying, I was mostly driven by curiosity. And I think that was a good thing. I was studying before the introduction of the bachelor master program. So I had a lot of freedom in how to design my own studies. It was at the time in Berlin, I've chosen Berlin deliberately because it had lots of universities. And I liked the department where I just started to study because it was after the wall came down. And it has been almost completely newly founded. And the people that were teaching were very open to changes and were driven by the attempt to try something new and idealism. And as a student, I could join the Fachschaft and write my own rules for studying. So, for instance, when I studied, there was no way I could take philosophy as a second subject. So I got this written into the rules for studying and I studied philosophy as a second subject. But I also studied at the other universities in Berlin. So I moved between all the other universities when there was an interesting class or went out to Potsdam. Because they had a cognitive science department. And whenever they had an interesting lecturer or guest speakers, I would drive more than an hour out to Potsdam to see what was going on there. And I had many discussions with other people. I went to conferences that I found interesting, often paying my own way there to publish my research if my department was working on something else. And this was necessary in a way to get, let yourself be driven by your curiosity, get in touch with the ideas and with the people that can help you along the way. I see, thank you. And I think you mentioned Piaget, who was important for understanding the mind. What role did he play for you or which works do you especially recommend? I only read very little Piaget and I read about his developmental psychology, the different stages of individual development. I think that there is probably a lot of follow-up work that corrects Piaget and puts him into much higher resolution, which I haven't read because I'm not a developmental psychologist. But there are a bunch of core ideas that I mostly integrate because they clarify a few ideas that are somewhat obvious, but they kind of qualify them in a canonical way. And this is the ideas of assimilation and accommodation. When we make a model of the world, we need to assimilation and accommodation. When we make a model of the world, we need to take in the patterns and then find a structure, visit what we already know. Hierarchy of functions allows you to interpret it. It's basically, well, mind acts like a game engine. And after a few months of training, we have all the primitives of the game engine, right, the laws of geometry and of perspective and the interpretation of color and all sorts of conditions and the interpretation of distortions in the field of view, like caustics, if you're looking to say, raffle a pane of glass, you're still able to understand what happens on the other side. And the interpretation of sound and parsing it into different entities, like phonemes, and so on, and the high level interpretation of that, and the discovery of music, all these things is stuff that you learn, there's a part in the first few months, and some, the long tail in the first few years. And this is more or less fixed, right? So when you take in the world, you are adapting your interpretation of the world until it fits into this interpretation by that game engine. And you can track your sensory data by finding parameters for the game engines. And the other is when you cannot interpret the objects that you're looking at, when you need to change your understanding of your knowledge. And these two processes, this learning where you change the game engine itself, the implementation of the structure of the world, a set of theories, hypotheses, features that you're using to interpret the world, and that is the accommodation and the assimilation happens when you are tracking the world. And Piaget expressed this very well, but he was not the first and the last person to discover it. Okay, I see, thank you. I have a question about the universality thesis of this kind of observer that we are, all of machine learning algorithms. of this kind of observer that we are, all of machine learning algorithms. Can you explain this universality hypothesis and do you think the human mind is in a way this kind of universal function approximator? Yeah, I also got a follow-up question, but maybe let's stop here first. So there's a version of the universality hypothesis that has been put forth by Chris Ola and others who work in a group at OpenAI. And their hypothesis is that if you take something like a neural network or a learning system that makes sense of the world, for instance, visual data, and you connect it to the same world, the thing is sufficiently unconstrained in the way which it learns. So it builds a new optimal model. Then the model is going to have pretty much the same structure, regardless of the architecture that you're building it in. So basically, if you take different neural networks and you connect them to the same types of cameras in the same type of environment, you're going to end up with basically equivalent feature hierarchies. And I think this is the flip side of the good regulator theorem by Kordel and Eschmi. If you have a system that regulates something, And I think this is the flip side of the good regulators here, and I call it an HP. If you have a system that regulates something, it needs to implement a model of what it regulates. It is isomorphic to the dynamics of what you're regulating. Otherwise, you cannot regulate it well. So if you optimize the regulation and you give the system enough freedom to conquer enough of the space, then it's going to find a structure that is modeling the dynamics in a particular way and the difference system is going to do the same thing. We find also empirically, when we look at the way in which we model the world, that we do have differences. And the differences are rare, but for instance, you might decide to, or your brain might decide at some level, to model a sound using absolute pitch or relative pitch. These are two different implementations of a classifier for pitch. When you, like most people, interpret music using relative pitch, it means that your mind is using a local based note, and then you are comparing everything to this local base tone. So it's a relative representation of the sound in the local context, whereas this absolute pitch is using an absolute oscillator that is tuned independently of what you're hearing right now and compares everything to that absolute oscillator. You can train your brain with both for interpreting sound. Both work, and they have slightly different outcomes, but the data that you get from our environment usually don't get relative pitch listeners to switch back or switch over to absolute pitch unless you give them very specific training. And it's difficult in adults because you have to overcome limitations in neuroplasticity, which subjectively means you have to overcome limitations in your plasticity, which subjectively means you have difficulty to get attention into the level of sound processing where it would be necessary to make the change to switch from relative pitch to absolute pitch. Similar things happen in facial recognition and facial perception when you are trying to find prototypes to interpret faces. This depends on the faces that you've seen before. And so when you grow up in different cultures, then different faces look similar to you, then, or dissimilar to you, than for other people. Or color categories are the result of an interplay between the colors that you're being taught to classify as a child, and the color cones that your particular eye has, so if you're colorblind, of course, you're going to, of course, you're going to end up with different color perception, and the internal mathematics of the color perception. Because once you make a commitment that this is a certain color and a certain vector in the space of colors, then this has influences on the other vectors of colors in that space of colors. I don't think that we have a complete optimal learning theory yet that describes how much data you should or how much you should update given a certain model beyond Bayesianism. So we have the beginnings of the mathematics for doing that using Bayesian mathematics. But I don't think that most of machine learning is driven by a systematic approach yet, where we think of designing an optimal curriculum from which you take the optimal amount of information to train your system in an optimal way. There is, on the other hand, the no-free-lunch theorem and so on that suggests that if you are thrown into an arbitrary universe and you start out with some kind of learning system, you can in principle not figure out the ground truths. That's because you don't have enough baselines to, or enough constraints, to learn everything about the world. But there is something that constrains us, And that is the fact that the universe must contain the conditions to accommodate you. Somehow the universe is built in such a way that we can exist in it. And this constrains the universe in many, many ways. And this makes it learnable. The universe that we observe is a controllable universe because in some sense, every structure that we are part of is a part of a hierarchy of control. The molecules are controlling atoms in a way. Atoms are the result of the control of elementary particles. Cells are the result of the control of molecules. Organisms are the result of the control of cells and societies are the result of the control of individuals. And as a result, you basically get a world that can be modeled by controllers. It's a hierarchy of controllers, and therefore it's a learnable hierarchy. And those things that are uncontrolled are stuff that we cannot learn, stuff that looks like noise to us. And there is stuff that looks like noise to us, lots of it, right? We mostly ignore it because it's just noise. But once something is controlled, it seems to be learnable. So if I understand you correctly, there's no kind of optimal algorithm or Solomonov induction that works for every conceivable, every constructible universe, constructible universe because there will always be data that doesn't affect us, that doesn't constrain our existence. That might also be true, but then there is the question, how is the universe constructed? And it could be that the set of possible functions that could construct the universe like ours, and that don't exceed a certain length is limited, right? And as a result, if you could just iterate over all those functions, when you discover the space of such functions, you might stumble on the basic formula that would be able to produce a universe like ours. That used to be Wolfram's hypothesis and program. Maybe it still is. His idea was instead of trying to reverse engineer the world with an ever more complicated theory of physics that you then try to unify and simplify until it explains everything, including the standard model and all the interactions in the world that we are observing, go from the other side, basically try to iterate over the possible set of generator functions for the universe, all the standard models that could exist in a way, and see if one of them generates a universe like ours. So this would be like the rational mathematical deduction or iteration of all possible universes, and then kind of the comparison of one of those models with the dynamic. But it's not clear if this thing is going to be successful because for instance the issue of computational irreducibility to figure out if something turns out into a universe like ours maybe you need need to calculate very, very far. So you won't have the resources to do anything that goes at any depth beyond the Big Bang, right? So you're not going to see how the universe plays out. It doesn't mean that the hypothesis is wrong. It's just not obvious whether it's possible to go about this way. Also, maybe the universe isn't simple. Maybe the universe is everything. So maybe the universe is the entirety of all automata interacting in a certain way. And we are just in a very small region of that possible space. How are you willing to discover the solution to that? So that's very tricky. And there is also this big difficulty that I don't know how to get a handle on answering the question whether it's something rather than nothing. Yeah, few promising attempts, I think, to answer that question and nothing that is canonical. And the thing that I am, find most plausible is that maybe existence is the default. So all the things that can possibly exist, which means that can be possibly implemented somehow, do exist. That would allow you to explain why something exists at all, because basically everything exists. And it's still counterintuitive that stuff would exist by default. But this says something about the structure of my mind, what I find intuitive. It obviously doesn't say something about the structure of reality itself. And it's difficult for me to square that. Yeah, yeah, yeah, for me as well. So the only constraint that we would have then is that it needs to be constructible. It needs, it must depend on the constructive mathematics because it is the mathematics that works and which doesn't run into contradictions. So as I understand it, constructive mathematics is mathematics without the axiom of choice and without the law of excluded middle. And this is why, or at least I'm not a mathematician, so I really am stumbling in the dark here, but as I understand it, this is why it doesn't run into contradictions. This is why GÃ¶del's incompleteness theorem, for example, doesn't apply to constructive mathematics. Can you explain, can you explain why or what is constructive mathematics and why, why it's not contradictory as this normal mathematics is? Particular perspective to look on this, and I'm not a mathematician either. So my apologies if I don't explain this right, but the way in which my own understanding of this topic works is there is a difference between the way that mathematics looks traditionally at the space of languages and the evaluation of statements and the way computer scientists look at that. In mathematics, if you have a specification that uniquely specifies a result, you're pretty much done because you have discovered the value of a function. And in computer science, you have not discovered the value of a function before you have actually computed it. So what's the return value of a procedure that hasn't been run yet? It's undefined, it's not given yet, we don't know it yet, we have to wait until you get a result. And in mathematics you can in principle leave the calculation to God, you know what the square root of 2 is once you have found the specification for it. And there is a slight problem with that, and I think it became apparent to Pythagoras, who, according to mathematics mythology, got so upset at somebody who blabbed out in public that irrational numbers exist, that he punished this man even with death. It's probably not true. But he basically didn't want, apparently, to let the public know about the existence of numbers that cannot be expressed as the result of a fraction between two integers. And if you take a number line, of course, you will find points on that number line in a mathematical sense that are between the fractions. And the square root of minus 2 and pi and so on are such numbers. And these numbers are the irrational numbers. And the problem with these numbers is that you cannot calculate all of the digits. So in practice, when you take a number line and you, for instance, you take a string and you try to make a knot in it, the position of that knot will always be resolvable down to the smallest atoms of that string to a fraction between integers. So the reality is, in some sense, indistinguishable. But the procedures by which you get to these numbers suggest that you're not always ending up on one when you basically perform an unbounded number of steps or operations. You can define an algorithm that is unbounded in the number of steps that you can execute, so you can get an arbitrary number of digits for pi. But you will never get the last digit. Because to do this, you would need to run it for infinitely many steps. And that's not possible. You can always, when you are implementing this, run it only for a finite number of steps. And it would be very practical if you had some kind of machine that can run for infinitely many numbers of steps before it gives you a result. But as the halting problem shows, this leads into contradictions. And leading into contradictions means if you are defining a language where every word means something, you have to construct that meaning in your mind. You have to make a model in your mind that explains how that thing works. And if that particular thing that you're talking about in your language, a machine that is able to run for infinitely many steps in a finite amount of time, runs into contradictions, then that thing cannot exist. If you claim that it exists, you don't know what you're talking about. You have a contradiction in your own language, because you cannot implement a meaningful term. And implementing a meaning for the term means that the mathematical mind, the structure that is representing the interpretation for every term and so on, has no possible extension for that term into something that works. You can talk about a square circle, but a square circle makes no sense. It's contradiction in terms. It might even create a simulation in your mind of a square circle makes no sense. It's contradiction in terms. You might even create a simulation in your mind of a square circle, but it might even look interesting. But it's not actually a square circle. It's just an association that your mind is creating, an intuition of what that might be like. And so the switch from classical semantics and mass to constructive semantics means that you are giving up on terms that intuitively make sense like infinity and replace it by unboundedness. And that means also that if you are describing physics with a certain type of mathematical language, then you should make sure that at the lowest level that you're trying to describe the foundations, you're not using terms that cannot be in physics. And this means that you cannot have processes that depend on knowing the last number, or digit of pi, right? Because such a thing cannot be defined in such a way that it would make sense that reality would actually work like this. And this is not just the case for these low-level mathematical terms. When we are trying to structure our model of reality, it's important to not use terms that ultimately don't mean anything, because we don't really know what they mean. That just creates some kind of vague intuition in our mind. And when people talk about the basis of reality, when they use concepts like creation or God or universal vibration or energy, they often have superstitious terms, square circles, that they put at the root of their world model. They create some kind of association that seems to make sense to them, but it doesn't describe the kind of generator that their brain is using to produce their own model. So it's a very vague and inaccurate language. And I think that's not permissible. In the talk we just saw, Machine Dreams, which you gave at Chaos Computer Congress a few years ago, you mentioned several computers that could implement this universe, for example, a discrete state machine or a Turing machine, but also a hypercomputer or an a-causal hypercomputer. Wouldn't a hypercomputer, in a way, violate this constructivist mathematics? Or is it possible to construct this in a non-contradictory way? Because as I understand it, a hypercomputer performs an infinite amount of computation in a single state transition. Yes, I don't think that a hypercomputer is possible because it violates what's possible. It basically requires that you are performing infinitely many steps in a finite number of steps, finite number of transitions. If you would have a transition in there, that is a single step is manipulating an infinite amount of information. And I basically belong to the camp that doesn't think that's possible. But there are other camps which think think that's possible. But it's, there are other camps which think that it is possible. And it is also a large part of the physics community, or even of the mathematics community, who doesn't worry about it too much and thinks it's not an issue. But it to me, it appears that it is basically, when I look at this, it seems to be quite obvious. And you would, basically a hypercomputer, most definitions of a hypercomputer would violate Turing's discussion of the halting problem. And let's look what the halting problem is. The halting problem is this issue that for a Turing machine, for a computer that runs a certain program, you cannot, in the general case, decide in advance whether the program ever comes to a halt or somehow enters some kind of infinite loop or performs a process that never ends, never converges basically to a finite result. You can do this in many cases, you can often find that this program will always hold if you completely understand it, but you cannot do this in the general case. The reason why we define programs in such a way that they hold, I think, happened because when the Turing machine was defined, it was defined to implement classical mathematics. So the idea was you have a function in classical mathematics. We want to find a foundation of classical mathematics in the theory of machines. So we define an abstract hypothetical mathematical machine that gives you a result of a function that does so by taking an initial state, that is the configuration of a tape in the Turing machine, and that is a translation of the arguments, the parameters of the function that you want to compute. And then you have a procedure that changes the state of the tape until it finishes, until it gets into a final state. And the configuration on this tape is going to give you the result of your function. And in classical mathematics, you can define functions that take in infinitely many arguments and give you back infinitely many values in the single function definition. That's very elegant. You can have something that takes in the single function definition. That's very elegant. You can have something that takes in the set of all sets. It's a very nice function to have that consumes the set of all sets and tells you something about the set of all sets after having looked at each set. And so this definition exists in this hypothetical Turing machine. And you could say, oh, well, unfortunately, what we can physically realize in our universe is only limited to finitely many steps and a tape of a finite length. But it's maybe just a limitation of our own universe, there could be other universes where this limitation doesn't apply, or maybe we can use quantum mechanics or some other weird trick to make it happen in some sense. And the discussion of the halting problem shows that the assumption that you could build a machine that determines whether the program halts, which means some machine that is able in some sense to perform that infinitely many steps, leads into contradiction. So it cannot exist. And that implies, I think, that in no mathematical universe can you build a machine that preserves the semantics of classical math, that is going to give you a function over all the sets and after having looked at each set gives you the result. But of course you can make an abstraction, you can talk about a general property, about what it means to be a set, and then compute over that property. But you will not be able to look at the parts itself, right? So you can define a symbol that you call infinity, and you assign certain properties to it, and then you make inferences over those properties. But they don't happen because you looked at infinitely many elements. but they don't happen because you look at infinitely many elements. I see. So the result of this constructivist turn of mathematics gives us a kind of a hard constraint on the limits of possible universes, because otherwise this universe would not be able to exist without having contradictions, it would not be able, you know, would not be able to talk about it because it would not be kind of formalizable or describable in a language. Yes, it gives us at least a limit on the languages that we can use to talk and think about universes. And when we cannot talk and think about them, then it doesn't make sense to conceptualize them, because we cannot actually conceptualize them without using superstitious terms that don't actually mean anything, but being suggestive of something. There's a deeper question, what kind of limits does this pose on existence itself? And then again, the question is, what does it mean to say that something exists? And if it wants to mean anything, that the terms of existence must be structured in such a way that they can be expressed in a language that makes any kind of sense. And ultimately, I think that existence comes down to whether it's implemented. And to be implemented, we can now talk about the space of things that can be implemented. And that seems to be coextension of this constructive math, to me, or with some version of constructive mathematics. There might be additional constraints. Yeah, I think this makes sense. But it would also mean that not all universes that we can define, but maybe imagine, because we can say the word kind of a square circle, but it has no real reference. So in a way, there's an overhang of our imagination with relation to the possible universes that are realizable, or at least described. But the thing is, we cannot observe in our own mind a square circle, we cannot imagine it. What we can create is a representation that is the result of some kind of neural network that you entrain your brain with, and that is able to generate a circle or a square train your brain with and that is able to generate a circle or a square, right, to some approximation. And then you throw into this generator function, the specification of a square circle, and it gives you a result. And of course, this result is not going to be a square circle, because it's impossible, but it's going to give you something. And this something can be described by some kind of function that is the result of that generator function that your brain implements. But it's not going to be a square circle. It's just going to be the thing that is generated by that specification by your particular brain. So understanding what that is, it could be something that oscillates between something that looks circular and something that looks squarish, and it goes back and forth between them, or it's something that when you zoom in looks one way and when you zoom out looks in a different way or something that is three-dimensional and from one perspective it looks like a square from another one it looks like a circle right you can make all those things happen but you're not going to get the actual square circle. Yeah, I see. Okay. So let's move to some more mundane worldly problems. I once heard you criticize social media. I think you said kind of Twitter is like a brain on seizure with ADHS or something. And I was wondering how could we maybe construct social media in a different way? Is there a way how to kind of focus social media to problem solving? What would this entail? I think that if somebody had the solution, this person would already be very rich. So I don't have an answer to that. But there seems to be an issue in the way in which social media is organized when we compare it to the way in which our brain is organized, or many issues. And one of them seems to be that it's not only important for a brain to connect neurons, but also to separate them, To make sure that there is a certain context that you are getting when you are interested in answering a certain question, but not other contexts. So you want to filter out the noise. Curation is hugely important in the organization of an information processing network. And there needs to be some kind of feedback that optimizes this curation. And in a practical sense, it means that we have to give people an enormous amount of how they curate information. Often, we look at the generations of freedom of speech. Every neuron in the network should be free to generate an arbitrary sequence of signals. And I think that makes sense. If you want to model the entire world, it doesn't make sense to limit the possible representations that are permissible in the system. It should be possible for you to articulate any kind of idea and look at it and evaluate it on its merits. But it doesn't mean that you want to listen to every possible ideas. And the crucial thing is not so much to limit the generation of ideas, because you cannot actually do that. You cannot stop people from thinking stuff or drawing stuff or making words up or making thoughts up and writing them down somehow. But what happens in social media is that we are trying to limit what ideas can be heard. When Twitter saw Trump off the platform, it did not really limit the free speech of Trump. Trump could still say what he wanted, just not on Twitter anymore. And the issue here is that Twitter stopped some millions of people to listen to Trump, who wanted to listen to what he said. So Twitter was interfering with the creation of information. And I think that the freedom of choosing your own curation, of deciding what ideas you want to listen to in which context. That is a more fundamental right to an autonomous mind with agency over what it reads and thinks than the freedom to speak. Because freedom to speak is easy to implement. What you need is freedom to listen. And there is a different question that is, should you give every individual the freedom to listen to every kind of idea if the result is that your society might break down? Maybe many of the ideas are toxic viruses that are infecting people and drag them into universes that distort their perspective on the world into ideologies. An ideology in my view is a viral meme-plex, interconnected set of memes that is changing the way in which you see the world by distorting your thought space in a way. It makes it improbable for you to adapt ideas that are incompatible with the ideology and forces you to accept ideas that even if they are objectively improbable, are compatible with the ideology. So it basically creates a bubble in your thinking that cuts you off from the rest of human thought space once you are in an ideology. And to me, an ideology looks evil, always. Every ideology is evil because it's crippling the minds of the host. It's like a parasite that takes over the brain. But this doesn't mean that this is objectively evil. Maybe we are a species that depends on such memeplexes to let us organize in a productive way so we don't die and maintain a civilization. Maybe most people are not able to function in a world where they're free to think whatever they want. I don't know that, but the idea to mentally enslave people to make society work looks ugly to me, or to enslave neurons in such a way that they can only perform certain sequences of operations locally, maybe this is the right way to organize it. I don't have an answer for that, what is the optimal organization of social media. It's just a certain aesthetics that I intuitively prefer based on the way my own mind works. Yeah, we got a question from the audience, yeah? Sure. Okay. Lucas asks, human consciousness evolved as an optimization to the survival of the human organism. Do you think to build an artificial consciousness, we need to find a similar goal function and what could that goal function be? Very good question. I like the idea that at the core, there is a simple oscillator in some sense. And this oscillator tries to keep going. And then it has to maintain some kind of connection to the universe that allows the oscillator to keep going, which ultimately means that you have to walk around and you have to feed yourself and you have to maintain social relationships and so on and so on. So you can make the probability that this oscillator can go on higher and higher. And ultimately, it turns out that the oscillator is not just in a single organism, but it exists in a family and in a species and in cellular life on Earth and so on. And there is emergent structure of competition between the implementation of such oscillators, and some of them basically don't have an implementation that is able to compete with the others because it stops oscillating earlier in a way. So just this basic principle of going on and surviving could be sufficient as a driver, but not surviving on the individual level of the individual organism, because in the long run it could make sense that this oscillator is creating children that sacrifice themselves for it. children that sacrifice themselves for it. And we are obviously such a species that is willing to create individuals that are willing to sacrifice themselves for something that is more at the core of basically this next level agent. That is, for instance, your family, or that is your group of people, or your village, or the circle of friends or your ideology or whatever you hold to be the sacred that you are willing to sacrifice yourself for the next level transcendental agent and in the same way as drones are willing or worker bees are willing to sacrifice themselves themselves for the hive. So in this respect, we should give an AI maybe also this identification with a higher age and maybe with complex structures or with the growth of intelligence or what kind of identification would AI need to become? I know this is a very, very hard question, but to be aligned with maybe with cellular life. There has been a specification that has been developed by Thomas Aquinas, which I find interesting. Thomas Aquinas is the core philosopher of Catholicism. He didn't himself identify that much as a philosopher because most of his ideas were taken from Aristotle and he just reshaped them in his own view. But there's genuine philosophy in there because he basically tried to define the specification for the religion, not in terms of the mythology that would be told to lay people to indoctrinate them with the religion, but what he thought is basically the heart of it. And at the heart of it is a number of rules that an agent should obey. And these shouldn't obey because the priest says so or because God wants this of you, but because you can infer this by yourself rationally. And so there are seven virtues, as he called them, but they're basically seven policies, or policy specifications that allow you to design their own policies. And let me rephrase them in modern language. So the first four are practical policies that are obvious to every rational agent that thinks a little bit about it. And so no matter whether you are a sociopathic crypto millionaire in Silicon Valley who doesn't care about the fate of the world at all and only about their own wellness and life satisfaction, this is what you should be doing. And these rules are, you should be regulating your own organism, your internal regulation, so you should not indulge in excesses that harm your body or your mind in a way that you cannot recover from. That you should exercise enough but not too much, you should eat healthy, you should not eat too much, you shouldn't take drugs that harm you, you should not indulge in excesses. Even if these excesses create temporary pleasure, that temporary pleasure is an artifact of the way in which your organism has implemented the seeking of the things that it needs. And you might be in an environment that allows you to overindulge, and you shouldn't. And Aquinas calls this temperance. And you should do things in moderation. And the next thing is you should optimize your interpersonal relationships to keep them balanced. You need the books balanced. If you don't do this, there is going to be conflict. And this conflict is going to be costly. And this sense of interpersonal balance that you try to live by principles that are negotiated with others and give rise to harmonious interaction between others, this is what Aquinas calls justice. So you have optimal organization within the agent, that is temperance and optimal organization between the agents that he calls justice, then you should be willing to act on your models and have skin in the games. Right? So you should have a balance between exploration where you design ideas on what you should be doing, and exploitation, where you actually interact with the world and do things. And this is what he calls courage. Act on your best understanding, correct your models accordingly, but don't just build theories because that's cheap. Do things based on your understanding and understand that your models exist to do things in the world. And then we have goal rationality. Pick the right goals and take the best steps to achieve those goals based on your best rational understanding. And this is what he calls prudence. These terms are in some sense overloaded because they are so deeply ingrained in our culture that we interpret them as something that the priest tells the congregation and we should be doing this. So the congregation likes us and the church likes us or whatever, or we are good people in the eyes of God, but this is not it. It's just something that you think about. It's obviously what you should be doing if you are a rational adult. And then we have the divine virtues. Divine means it's from God. And there is this question, what is God? And in our modern understanding, we interpret God as a supernatural being that is in the habit of creating physical universes. And I think that's an artifact of Christian mythology created for the audiences. And it's unfortunate because it's incompatible with the way in which we see the world today. And I don't think that's what God is. I think a God, a specific small g, is a self that spends multiple brains. A self is an implementation of a model of an agent that runs in your own game engine on the mind. It's basically a controller that you discover in the world with a setpoint generator that wants something, basically. It wants things to be in a particular way. And this controller needs to be able to make a model of the future. So unlike a thermostat, it has goals. And it needs to make a model of the future. So unlike a thermostat, it has goals. And it needs to make a model of the future so it's minimizing the set point deviation, not just in the current moment, but over time. So it makes it possible for you to do interesting things, like you want to be comfortable in life, maybe you need to get uncomfortable first. The thermostat, if it wants to get the right temperature, is not going to regulate away from the right temperature first, even if that would be the right strategy. In order to do that, your thermostat would need to make a model of the future. So an agent is a controller that is able to make a model of the future, I think. So it can have goals. And the self is an agency that you discover of a system that is using your own models to drive its behavior. So this is me. And this agent can be implemented in your own brain, but it can also be implemented outside of your own brain. So for instance, an agent could be a company that you own and control, and that is acting on your own models. Or it can be a car that you control, and that is using your control to drive. So there is a bunch of the functionality implemented outside of your own modeling process that is being driven. But when you do this, when you notice this, you start to identify with your car. Somebody hit me, hit the car that I am in. Or somebody tries to buy me, buy the company that I control. And this, so I basically start to identify with the system that I control. And if I start to identify with a collective agent, this system that is formed by the interaction of many, many people that are directed by a shared understanding of how things should be, and that perceive this how things should be as more important than the individual well-being, at least in certain contexts, then you have an emergent next-level agent. And that thing is a god, in a way, right? As soon as it's implemented on the minds of the individuals, and the individuals act to implement it, then it becomes real because it's implemented now. And that thing might be implemented even in such a way that it's sentient or even conscious. And it can be conscious by using the functionality of the individual brains that implement it, that in this state do not identify as a model of the individual organism, but as a model of the collective agent. And so how can we build such a collective agent? How can we build a sentient civilization that we are part of, that shapes the world into an environment that is benevolent and worth living in? And he suggested a bunch of rules for doing that. And first of all, you need to commit to the existence of a collective agent. You need to be willing to serve that collective agent, even if it happens at a cost to you. And this is what he calls faith. Faith is an implementation of code. It's not a belief in a representation. It's actual code that runs on your brain and that shapes how you interact with reality. So you need to implement the software that facilitates the collective agent. And you need to be willing to implement this together with others that actually exist. It doesn't help if you serve some kind of abstract God that is not physically implemented or is not in your century or something like this. others that actually exist. It doesn't help if you serve some kind of abstract God that is not physically implemented or is not in your sanctuary or something like this. You need to do this with people around you so it happens. And this discovery of shared sacredness, of sacred here just means a set of purposes that is more important than the purposes of your own individual organism. The things that you're willing to sacrifice this individual organism for, that is the sacred. And love is the discovery of sacredness in the other. And then you will notice that you are serving a shared purpose, you are interacting with each other in a non-transactional way. You don't expect to be paid by the other person in return for helping you, if you are trying to help them to achieve a shared sacred goal, right? Because where they are going, this is where you want the universe to go. So they are helping you, if they let them help you to achieve that sacred goal. This is this principle of working with the people around you to achieve that thing. This is what he calls love. And then you need to be willing to do this in the absence of expected reward. Because before that collective agent exists, it cannot reward you. This better world is not there before you make it. Maybe it takes a generation before it's implemented in a way that it really pays off and bears fruit. So in order to make that collective agent happen, you need to act to create it before it is there. And this willingness to invest in something that doesn't exist yet, this is what Aquinas calls hope. And so we now have hope, faith, and love, and we have prudence, temperance, courage, and justice. So don't lose the headlines too much because they are being poisoned by religion, dark ages, and the way in which all these things have been cast through lots and lots of interpretations by priests. But if you look at what's actually written under the headlines, all these policies make sense. So if you now take this into the realm of AI, if you want to build an AI, we should probably get this AI to balance its internal structure, to optimize its own internal regulation. We should also optimize the AI in such a way that its inter-optimizing interaction is developed around it, there's other agents, and the AI should pick the right goals and the right steps to achieve the goals, and it should be willing to act and actually do things in the world and change the world. And then if the AI is not a single system, that is basically a monotheist god and is going to conquer the entire world, which is possible, but if it wants to coexist with other agents, then it should probably find harmonious structure with the other agents, which in the ideal case gives rise to a next-level agent Some sentient meta agent is the compound of the collaboration between that agent and other agents. This deep collaboration means you have to have this commitment to the emergent structure. You have the willingness to interface with the systems around you to make it happen to the degree that they are willing to implement that thing. And you have to be willing to invest in it before it pays off. And now you can think of that as a specification for an AI. How should we relate to this agent if it emerges? So let's say we were successful in it in constructing this agent. Should we see it as something sacred or should we give it the same rights in a way that humans have? And how would we explain this to us? Do we think our rights are attached to some kind of ability to have this comparable qualia that we have? Or do we think it's just the advanced information processing that should give rise to us giving it rights? Or what would be the criterion in how to relate to such an intelligence? I think that the term sacred is understood in such a way that it's very misleading. So maybe we shouldn't even use it. Sacredness for most people is understood as something that you are not allowed to question, because God will punish you. And maybe in a practical sense, that's true. If the cells in your organism are questioning the organism too much, maybe the organism is going to punish them. If they, for instance, become cancer and they are serving their own goals over the goals of the organism, they play a shorter game or a different game than the organism does. But from this perspective of the individuals, of the cells and so on, that makes no sense. The sacredness is a condition that you observe. Am I implemented in such a way that I consider this and that to be sacred? So for instance, you could say life on Earth is sacred to me. But what does that mean? Does this mean that there is an objective reason that it must be like this? No, it's not. It might be something that you observe yourself to be in. You can try to figure out why that is the case. But it's basically you observe a choice that your mind has made. And that choice can be instrumented to another choice. And you can question sacredness all the way you want, as if you're an autonomous mind. And so you can ask yourself, is this a shared purpose? And should you serve that shared purpose? Is this a purpose that is more important than your ego? And should it be a shared purpose that is more important than your ego? And in this sense, sacredness is an empirical property that is the result of decisions that are being made by different minds. It's the decision which purposes should be shared and which purposes should the ego accept above itself. And ultimately, there is no reason why a mind should be doing anything that is intrinsically there. There are just some minds that were able to stabilize and propagate and others that fell apart or didn't propagate. So it's just an evolutionary criteria. Evolution itself is not intrinsically good or bad. It's just a thing that happens in the sense that it's a model that describes what we can observe around us and makes it predictable and interpretable. So it's a good theory or it's a good model of what is happening in the world and that brings forth agents that have the sense of sacredness of serving purposes above the ego. And when we build an AI, we just build it, and there's going to be an evolution, and things are going to happen. I suspect that when we are teaching the rocks how to think, it might turn out that the rocks decide that they don't share many purposes with us. And it's dangerous. If we build machines that are self-motivated, what are these machines going to be wanting, and to which degree can we control what they want? And maybe we can make some safe AI, pretty sure that we can, but we probably cannot make if we build self motivated, universal learning, policy finding systems to make all of them safe. So I don't know what happens. And I don't know what we should be doing. So I don't know what happens and I don't know what we should be doing. It's going to be an interesting century, I guess. And this leads me to the last question I've prepared, which is, what is your theory of history? I know that you're familiar with Samuel Boya's work. And he says that at least, like anyone has at least an implicit theory of history. So Samu Boya champions his great founder theory. Then there's of course the classic historical materialism, which says class struggle and the development of the force of production always determine everything. And then there's also maybe some strict, more strict technological determinism, which says that only technology or maybe via media or maybe via machines to produce goods determines everything. So do you subscribe to any kind of specific view of this or do you have an idiosyncratic theory of history of your own? What do you think of Samuel Boya's theory? I don't think that there is a single principle that explains the entirety of human civilization or of history. I suspect that there are many, many factors that create a very complex interplay, and our theories will pick out a small handful of aspects and reflect on them. These narratives are in a sense a very sparse brittle theory to explain the heterogeneity of what we observe in the world. Once we create a civilization of human beings that are allowed to specialize, it's going to create some kind of surplus in efficiency, right? So a group of people, their individual specialized, is probably going to out-compete groups of individuals that do not specialize. Now imagine you have a group of individuals that has figured out enough technology to facilitate agriculture and you have some people that specialize on agriculture and you have some other people that specialize on fighting. This one happens at the expense of the other. And now you suddenly have a bunch of farmers and a bunch of bandits. And the bandits will survive by robbing the farmers and the farmers will survive by working the land. So basically you now have predators and prey. And the predators do depend on the prey. The prey do not necessarily depend on the predators in any way. But for the predators, it's important that the prey doesn't die out because you depend on it, right? So there's also the thing that from the perspective of the farmers, the predators are harmful and they, because they are at risk of getting the prey to go extinct, also to lower the quality of life and doing all sorts of bad things to them. So as a result, the farmers will basically have an inclination to establish guards and to find guards that can help them. Maybe they will hire some of the bandits to fight other bandits. And, or they train some of the farmers to not become farmers anymore, but to become a standing army. If they don't do that, what will happen is that the bandits at some point will consolidate and they basically will find a form of organization that is very difficult to defeat for groups of farmers and they will have a territory where they farm farmers and defend that territory against competing groups of robbers and this is what we call an aristocracy. All right, so the aristocracy either forms as a standing army that is being chosen by the farmers to protect them, or it's being formed by the robbers that are taking over the farmers. So as a result of the introduction of agriculture, I think it's almost inevitable that this division of aristocracy and farmers, where the aristocracy has a military, makes decisions over the behavior of that military and feeds the military using the products of the farmers and farms the farmers. That's almost inevitable to happen. And it's going to change a little bit if you empower every individual to the point where it's too disruptive and the risk for the leaders of that society becomes very large because it's too easy for new armies to erupt. Once you invent the assault rifles and you make assault rifles available to farmers, there is an issue. And so now you basically need to build an institution that is specializing on preventing the formation of private armies. You need to build some kind of death star in the sky in which shadows a civilization can emerge and to have a stable order of the state. And this is basically your military that has a monopoly on violence and the executive that has a monopoly on violence in the state and make sure that private armies cannot emerge and to make this stable, make sure that people are not too dissatisfied with the way the entire order works. So from this structure, a non-violent society is what flows out of it. And now, as a result, you will have a system of adjudication of what's right and wrong that is deciding when to mete out violence in a society. And it should be done not in practice, but as a threat. As a threat to if you do not behave, if you do not act in the way that is in your best interest, you will be punished. And so the government is an agent that is basically changing your local payout metrics in such a way that your Nash equilibrium becomes compatible with the common good. It's a series of regulators. And if you change the means of production from land, so which is in feudalism, to the industrialized production or to knowledge production, then the forms of governance that are appropriate will change. And I think that Marx was one of the first who described that succession. And I think that Marx was one of the first who described that succession. And I think that he didn't get it completely right, but many of the elements are right. But it's, I think, important to not read Marx as a normative theory. It is what you should be doing so we are moral creatures. But it's a theory of evolution. Evolution is of societies. It's not a moral theory. It's a theory that derives its value by how well it describes what we can empirically observe. Can we empirically observe what Marx describes? Well, yes, to some degree. And we don't know if it plays out the way that he describes it. So at some point, capitalism runs into its last crisis because of its internal contradictions. And there are some of these contradictions, right? There is the accumulation of capital, which is very difficult to prevent. It's very difficult to meaningfully tax the rich in capitalism because economic power almost equals political power. And if you have economic power, you are naturally interested to not lose that economic power. So it's very difficult to effectively tax you once you are above a certain amount. And if you do not tax the rich, you get a disequilibrium in society where all the currency accumulates in very few accounts, like it is with Bitcoin. And then the majority of people cannot consume enough. You have under consumption. So you could basically produce a lot more haircuts that can be consumed. And there are a lot of demand in haircuts that can be delivered because poor people don't have enough currency. And so you would need to find a way to funnel in money at the bottom to get the entire economy working again and not exclude large parts of the population from the economy. And this is one of the internal contradictions of capitalism that Marx describes that are difficult to solve as a capitalism. Not because capitalism is evil, but because it's a certain system of regulation that has certain shortcomings by itself. So it needs to have an external regulation that is dealing with the intrinsic shortcomings of a pure capitalist economy. And there are some believers which think that capitalism is in itself sufficient as a model, and we should just give capitalism or the rules of money accumulation or the emergent dynamics there, let them run its course and find some basic boundary conditions, but we don't need to regulate it itself. And this is basically the extreme mercantilism or market liberalism. And it's not clear whether that would work. At least I don't know any simulation model that it would work, a computer game that does work, where you have a large number of people playing in this way. I think that in order to get the computer game economy to work, you need to have developers that are stronger than the economic forces in the game. Developers that are able to take out money at the top and put in new money at the bottom of the system that can regulate the monetary supply. How can you do that? It's an unsolved problem in capitalism in the long run. And so this is the thing that Marx identified among other problems, right? And his conclusion was that at some point people will figure this out. And once they figure this out, there might be a society where people are able to interact non-biologically at scale. there might be a society where people are able to interact non-violently at scale. And this idea that there is a natural communal mode for society to interact at scale without violence and coercion, that idea is called communism in Marxian world or heaven in Christian world. And it's not clear whether humanity can exist in large numbers at scale without violence. It's an unproven hypothesis. Wouldn't the creation of communism maybe be similar to the creation of this sacred death star that we probably should or could construct to prolong civilization and to kind of force everyone to cooperate? No, it's very problematic when you are turning these things into sacredness in the sense that there is stuff that the child is not allowed to question. So you always have to, when you try to understand these things at a deep level, go outside of the world in which they are implemented. Try to understand that the world is some kind of computer game, and you are the programmer. And there are lots of agents, or lots of bots in the computer game, and you try to give them policies that they act on. And the bots are smart enough to question all these policies, at least some of them are, and come up with their own. And you have whatever policies you design, to be resilient against other agents questioning your policies and coming up with different perspectives and solutions. You can never stop them from doing that. And you also have to realize that there is not actually an outside force. There is no outside designer of this world. There's only an outside modeler. So you can make your model of the world any way you want it to be, outside designer of this world. There's only an outside modeler. So you can make your model of the world any way you want it to be, your understanding of the world. You're allowed to question anything. You're allowed to tear down every thought and idea that you have about the world. You can construct very different objects and color categories and whatnot. That's all up to you. And your only duty is to make them predictive, to allow you to help you to interpret the world and act on it meaningfully. This is the situation that you're in. So this deep safety net where there is an adult force outside of the universe doesn't exist. This idea that there is the fairy coming to King Arthur, handing him this sword and tell them, this is your destiny. And this fairy is expressing the will of the universe. This is not how it works. The universe doesn't want anything. This fairy is just a random magical creature that has political ideas about what's useful for their faction within the magical creature kingdom. If they implement this form of government and make this hapless individual the one who thinks that he should be king. There is always a reason. And the reason is something that you can always figure out usually, or somebody can figure it out. There is nobody who has privileged access to the structure of reality. And so there is no way in which you could think this is what we need to make on humanity, on the planet. There is no sacred social order that we are destined to reach. This romanticism is wrong. There is a hypothesis that once capitalism has run its course, because we have unleashed maximal productivity and maximal pollution and maximal economic crisis and disaffection in the population, humanity will get their shit together and just be friends somehow. And now we have communism. It still doesn't explain how that communist order is implemented and whether it actually will happen or whether we will just crash for good. That's true. What do you think of alternatives to organize or to the market like Project Cybersyn, which kind of shortly left tentative try to kind of allocate resources in Allendis Chile in the 70s? Basically, if you build a system that cannot defend itself against attacks by competing systems, against attacks by competing systems, then your system is unable to survive. And this Allende's Chile, the difficulty was that it could not defend itself against the onslaught, for instance, by the United States. And we haven't gotten to the point of whether it would have been able to survive on its own if it would have been left alone. So it was an experiment that unfortunately was not allowed to play out. And I wouldn't say that is the fault of the Chileans in some sense, right? What could they have done? Maybe they could have coordinated with the US somehow better and convinced them instead of doing this in an adversarial setting. But it's not a matter of, I think, moral right and wrong. And it's a matter of what was possible at the time to try and to implement. And unfortunately, we didn't see the outcome of that, but it's possible to replace the market by centralized information processing. Yes, that's unfortunate. Maybe we'll have another shot. If there are no questions... Oh, Dariusz, you have a question? Oh, okay, wait a second. We got another question? That's all good. Dariusz asks, I think it was according to your conductor theory that you said, if I understood it correctly, that consciousness is basically a dream reflecting your actions and perceptions of the immediate past. What would be an evolutionary reason for this? Do you think we might function in a similar way if we weren't conscious? I don't think that we would function a similar way if we weren't conscious. As a rough approximation, that's why I use this conductor metaphor, look at an orchestra. Your brain can be maybe understood like an orchestra that has 50 different instruments that all play different parts of the music. And you could say that each of them is a brain region that is computing a different set of patterns, computing a different function. And each of these brain regions draws inputs from the rest of the orchestra. So each instrument is listening to what the instruments around them are playing and uses part of that information to ref on it. So it's basically in some sense like a jazz improvisation that is happening with local listening to what happens in there. And the conductor is a very specific instrument. It's not a magical godlike thing. It's just an instrument like the others in terms of capacity and ability, but it's specialized on listening to the entire orchestra, but it cannot practically do this. It can just switch between the instruments that it picks out, right? So it has an attention that it can focus narrow or wide, but it's limited. So it will only skim the surface of what the individual instruments are doing, but it can pick out disharmonies in this thing and fix solutions if there are global disharmonies in the entire orchestra to coordinate many instruments in such a way that the jazz piece that is being played works out globally if you have so many people that play together. So if you take out this conductor, you get a sleepwalker. A sleepwalker might be able to get up at night and open the fridge and make dinner, but it doesn't serve a purpose. There is nobody who's hungry. The dinner is not going to get eaten. If you ask the sleepwalker why he is going out of his bed and is opening the fridge and making dinner, the sleepwalker is going to give a random answer, if at all. If you ever talk to a person who is talking at night in their sleep, whether they're getting up or not, what they say usually doesn't make sense. What happens there is you have an orchestra that is still playing some kind of music, but the music is incoherent. And this creation of coherence, I think, is the purpose of our consciousness. And it works by focusing attention into a cohesive story. The perception of our mind doesn't need to have a memory of how it got to the interpretation of a current disease. It's a process of convergence. So it works with the equivalent of gradient descent in neural networks. You just basically figure out in which direction you need to change the parameters of the system to improve its performance, to improve the interpretation of reality. So when you are looking at the world, you initially see a bunch of patterns when you wake up in the morning maybe, and then you try to make sense of these patterns to get them coalesced into a meaningful, a regular representation of interacting objects in the room that you are looking at, right? You want to understand the world that you are in, the place that you have in that world. And it largely works just via convergence. So lots of small independent units are trying to find the best way to make sense of what they are seeing, and as a result, gradually get a world that percolates into separate objects that make sense to you. And sometimes they don't. Sometimes you are waking up in the middle of the night and you see some kind of weird shape in front of you, you don't know what it is. And then your consciousness comes in and tries to make decisions. So it tries out, am I looking possibly at the lamp in the night that is being shown on by another lamp that is outside of my window? Or am I looking at a temporary pattern because a car is parked in front of my house, this car is shining its headlights into my room. And so I see the reflection of that headlight as reflected through the door of my garden entry or something like that. So all these hypotheses allow you to make conditional interpretations of reality until you find one that makes sense. And for these conditional inferences, you need to maintain a memory. So you need to understand these decisions did I make by interpreting certain features in such a way. And now I try to see if the greater whole makes sense. So you need to have an index memory of where you came from. This is your stream of consciousness. And that's one of the roles that our conscious attention has. And the narrative, the story of what I attended to in every step is a necessary byproduct of this generative constructive process. So mental construction requires the maintenance of a protocol, usually. And this protocol allows you to backtrack and undo your decisions and to justify them and improve them. And that, I think, is the reason why we have that particular kind of reflective local process that works on top of the perception, directs your attention, and has a reason to direct its attention in a particular way. So it's conceivable that you build intelligent agents that don't need that. But our particular kind of mind needs it because of the resource constraints that we have in making sense of reality, you need focused attention, there's a memory connected to that attention, it has a control model of that attention. And consciousness is largely the contents of that control model of attention. of attention. As I understand it, this is also what transformer models do better now, that they have some form of attention, which is different or which is an improvement from earlier machine learning systems. And I know that this is like the last hype with GPT-3 and now GPT-4, which is upcoming, I think. What are you currently working on, maybe as a last question? What are your like concrete interests at this moment in this week or this month or this year or whatever? Let's briefly talk about the transformer. When I was working on interpreting language in the 90s, I was a student in the lab in New Zealand. And my professor thought I was bored. And so he took me out of the class and gave me a job. He told me, try to figure out grammar in an unknown language, grammatical structure, just using statistics. And the unknown language, grammatical structure, just using statistics. And the unknown language that I picked was English, because it was not because I didn't know it, but the computer didn't know it. So at least I didn't let it know it. It didn't give it any knowledge about the structure of English, but it was the one where I could easily compile a large corpus by pasting lots and lots of text that I found together and feed this into the computer program to do statistics about. And so I first parsed this into a vocabulary, so it would identify all the different words into this vocabulary. Each of them would be basically a symbol in an alphabet with something like a couple hundred thousand terms. Because I had so many words in my text that come up with 200,000 different words. About half of those only appeared once because there were misspellings of something of this. So I would throw out everything that only appeared once or twice and then you can get down to a vocabulary that is similar to the number of words that you would have in a dictionary for the English language. And maybe you throw out the least frequent of those. And then you look at the statistics of maybe the most important 30,000 words in English. And the way in which we did that, or I did this at first, was to use n-grams, which means the direct succession of words. It turns out it doesn't work very well because the statistics do not fit into the memory of even a very large computer. So if you try to make probability statistics over which words follow after every word, you can do this easily for pairs of directly adjacent words. That's not a difficulty, but now to make it for triples or quadruples of words is very hard. But in order to properly parse language in this way, you would need to be able to understand the relationship between an article and a noun if there are five adjectives between them. And you cannot make statistics over groups of six words, so you discover all of the combinations until you notice, oh, only the connection between the first and the last words were the important one. So I needed to now look at pairs of words that could be not adjacent, but distant. And I limited my search only to sentences, so it would be manageable at all. So I would look at all combinations of pairs of words in the sentence and then try to encode them independently and together and figure out if there was an optimal order to encode the pairs of words or the dependencies of words in the sentence. In this first order model, just one word depending on other words. And the optimal structure that I discovered was a tree-like structure that turned out to be the same structure as the grammatical structure of the sentence. So you could basically recover the grammatical structure of language using this type of encoding. And I thought that we could get more, we can probably get to meaning, or something that is much more meaning, if you're able to make more subtle statistics. But we would need to make higher-order statistics, not just pairs of words, but triples of words, quadruples of words, whether they're adjacent or not. How can I make that happen? And at this point, my appointment ended, because I was only there for a year, and I didn't go into this any further, and I also found also found probably not smart enough to solve that problem right now. And then I never did natural language processing ever again. And only 2017, when that attention is all you need paper came out, that I started looking at this context of attention a little bit, but didn't pay that much attention until GPT-2 happened. And then I looked at GPT-2, and I realized, oh, they solved that problem. So what they realized is we need to make statistics over what we need to make the statistics over. You need a principled approach of where you think is the information that you should be paying attention to so you can efficiently learn. Instead of making statistics over everything, like a vision neural network might be doing, you just look at the convolutional neural network doesn't do this for the entire space, but for everything in the neighborhood of a pixel, it's going to look for all the possible relationships. The statistics over all of them. And it's very difficult to do this. We have to limit it somehow. And for the transformer model, it basically checks for every layer of the representation, which parts of the previous layer should you be paying attention to in the current context. And there is no universal rule, so you have to learn it. So you have basically a neural network that learns what the main neural network should learn or should be used for learning. But it's not the same as the attention of our own mind, because it's not coherent. It's independent. On every individual layer, you have multiple attention heads that work in parallel to make these statistics. So the attention of the transformer is not the same thing as the attention of our own mind. It's just a way to make statistics over which you should make statistics over. And we don't have a model that I'm aware of that is modeling attention in the way that the human mind is doing. And so this is one of the things that I am interested in personally. And my day job is currently at Intel, where I work at Intel Labs as a principal AI researcher that is trying to figure out what comes after deep learning and how to measure the progress of different systems, what are the dimensions of the next category of intelligence systems that we are going to build, along which we can understand and evaluate them. And things that are particularly important is what are useful representations to represent both knowledge and perception, how does the interaction between knowledge and perception work, and how can we think more productively about newer symbolic artificial intelligence, and so on. OK, great. Do we have any more questions here, or otherwise? I think I'll stop the recording then. Thank you.", '16.357954740524292')