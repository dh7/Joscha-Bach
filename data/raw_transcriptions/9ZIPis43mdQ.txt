('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/9ZIPis43mdQ"> </iframe> </center>', " to generate safe AI is just tremendous. There's so many contingencies in there. That's probably only a single mind on the planet that could attack this. And then I met Eliezer the last time. He suggested that we postpone AI for a couple hundred years until we solve the AI problem. So I'm going to talk about something that is, in some sense, much more narrow and humble and down to earth. That is the question, how to build a motivated AI system at all and to do it right here and right now. Let's see if we can get this HDMI to work. Let's see if we can get this HDMI to work. My computer thinks that it sees the projector, but it's not neutral. Thank you. I'm going to put this back. There are things happening, just wait a moment. Wonderful, thank you. First of all, work on my cognitive architecture is a collaborative effort. There are a number of people that are involved in this. And I'm very grateful to Ronny Wiener, Dominic Walland, and Chris Kajaga, and Jonas Kemper, who have contributed to the current implementation of MicroPsy. It's the second one. And I'm also very grateful to a number of other people and institutions that are making this possible. I'm grateful to Dietrich Doerner, Aaron Sloman, Marvin Minsky, Stan Franklin, and many, many others that have inspired the design of the architecture itself and have contributed ideas directly or indirectly to it. And I'm thankful to a number of institutions and places that are supporting this, like Humboldt University of Berlin, the Institute of Cognitive Science in Osnabrück, the Berlin School of Mind and Brain, and currently the Harvard Program of Evolution and Dynamics in the Media Lab that allow me working on these things. I believe for what we are doing, let me see, we need to build a system that combines universal representations, that is, somehow grounded, new or symbolic representations that integrate both symbolic and distributed aspects, and we need to have something like universal problem solving, that is, the number of operators over these representations that facilitate planning, reflection, and logical reasoning, and so on. And we will need to have a system that identifies goals autonomously, and this is what I would call universal motivation. And maybe we need something like emotion and affect if we want to understand how these things work in humans. And we need to build this into whole testable architectures, not just isolated components, because these things cannot be understood in isolation, I think. So, in my perspective, general intelligence needs something like general motivation. Rationality itself is but a tool. There is no rational reason to get up in the morning at all. The reason why we do things is not because we are intelligent, but because we are primates that use a specific kind of intelligence that enable us to do other things than most primates can do. But the goals themselves, they do come from us being the result of biological evolution, which tends to imbue us with certain action tendencies. The motivational system is what structures our cognition and what attaches relevance to content of our cognition and our interaction with the world. The motivational dynamics provide us this alternating behaviors that structure our strategies in very interesting ways. So the reason why we are not paperclip optimizers or something like this is because we have to serve many different goals. If we have an AI evolution that, for instance, has as a fitness function only the competition for bandwidth after the world has been turned into computronium, maybe the result is not very interesting at all, but we get something like a very smart but very boring virus that is just very good at replicating itself and winning over other solutions. And the thing that makes us so interesting in my perspective is probably the result of particular constraints that we built into us. For instance, our brains are relatively small but they cannot grow larger because they already consume 20% of the glucose of our body so the whole system doesn't scale very well. So at a certain point, if you want to become smarter, we have to collaborate with other individuals and specialize and this makes it necessary for us to have a bunch of social drives that structure the social interaction. And our cognition, arguably arguably and our rationality is informed by this need for collaboration, which is not necessarily a given in an AI because an AI, it's not clear why it wouldn't scale and why you would need to have several of them and why it would get better if you have several of them, not just one that integrates over all of them. It's just one of the many examples that is informed by the particular needs of human individuals. That's why I'm also not quite convinced whether it's a good idea to take human values and put them into an AGI, especially if you look at the fact that human values do not seem to guarantee human survival or survival at all on this planet. Nevertheless, I do want to understand how these human motivational processes work because one of my main goals when I pursue AGI is that I want to understand how the mind works and how humans work, not necessarily in the specific case of humans because I see them as just one of the very few examples of a working general intelligence that we can study so far. So what the motivational system needs to provide is intention selection and action control. And one thing that I'm also interested is in understanding the relationship between motivation and emotion, motivation and affect. Our architecture has been largely documented in this book, Principles of Synthetic Intelligence. Work on it started in the early 2000s. And this book is also partially based on theory by Dietrich Dörner, a German cybernetician and theoretical psychologist, whom I adapted to this basic framework of how to do the decomposition of a system that is able to come up with goals on its own. MicroSci is a neuro-symbolic architecture that is built on spreading activation networks that have both symbolic and distributed properties. It's basically, the idea is that you have a homogenous representation and different operations on them. So associative thinking, analytical thinking, largely use the same representational content, but it's a different class of operations that works on them. And the agents are all implemented within these representational paradigms, which we call node nets. We have representations that are all grounded, so everything that our agents learn and reason about is something that they've experienced in some kind of environment. Usually it's a virtual environment. At some point we also build robots, but because we are not very good at robotics, we didn't learn much from it, because I think that it's an affliction that currently all systems have. If you build a robot, it's going to experience the world through a very small and dirty window. It's basically just moving around on the lab floor. And its interaction with the world is mostly limited to pushing things and locomoting. Well, except when you're working for the military, then maybe also going to blow up things. But it's not like you're going to climb trees and build computers or discover fire and do very interesting things right now if you are a robot. So the embodiment that is provided by robots at this point is, in my view, not yet helpful for AI. And at this very early stage of our career as AI researchers, we are still very well off with using virtual worlds. MicroSight right now, this is one example of the node nets. They run in browser interface, can be distributed over different machines. At the moment we have an implementation that is based on Fiano and uses largely Python for providing the infrastructure to run these node nets. You can download it and play around with it. Lots of parts of it are public and can be played with. A goal in Microsoft Sci is a situation or an action that affords to satisfy a need. It's every action that an agent performs that is goal directed, is ultimately directed on satisfying a need or avoiding the frustration of another need. And the needs are predefined, the goals are learned. Because when our agents are born, they do not know in which environment they are born, pretty much like humans. And so they cannot know what goals are afforded by the environment. And if you are born into much like humans, and so they cannot know what goals are afforded by the environment. And if you are born into a very flexible open environment like humans are, it's a good idea that you do not pre-wire the goals, but what you do is you pre-wire the needs that the system has, and then it identifies goals through learning. And so we get an architecture that is based on needs, and these needs, there are some autonomous processes that try to regulate these needs. For instance, your body temperature might go over pretty high, and then you start to sweat, and this is autonomous regulation, and you don't need to think about this, it just happens automatically. But when this autonomous regulation fails, you need to start a cognitive process that tells you, oh, you're getting too hot, maybe you should be doing something about this, maybe you should find a place that is a little bit more shady or a little bit more air conditioned and then you start generating plans on how to enact this, you establish a goal, you commit to that goal and you act on this plan. And this way you establish goals based on needs. For instance, the need would be to maintain a certain target temperature for your body to work well. And if this need cannot be regulated autonomously, you get an urge. And this urge means cool down. And then you establish a goal. And that goal could be something like go to a place with air conditioning. And then you try to construct a plan to enact this. So you get these urge signals, and the urge signals, what they do is they act on your memory content and your perception content and on the space of possible actions by priming them. So when you are hungry and you think about things randomly, a lot of things that are related to food will pop into your mind, and if you look into the world and you're very hungry, then you will be able to recognize foodstuffs much better and they will look much nicer to you than they normally would look. So you have some training and attentional modulation going on there. Then these urge signals are important for learning. Whenever you satisfy an urge, you get a reinforcement signal, which we perceive as a pleasure signal. When something bad happens, you get a displeasure signal, it's also a reinforcement, and they establish connections between the urges and situations that we currently experience, and these associations define goal situations. And then we have decision making, of course, that is, we do have a number of competing urges at a time, so right now I have a strong urge to communicate my ideas to you, and this competes with strong urge to communicate my ideas to you, and this competes with the urge to sleep. And at the moment, the former is winning and the latter is losing, so the latter is being inhibited at the cost of the former. And the decision making basically works by balancing those urges and doing some lateral inhibition to make sure that conflicting urges do not win over. Okay, pleasure and distress are signals that are generated by the motivational system. Only five minutes left? Okay. So, here's an example for this. We have a bunch of physiological needs. Thirst, hunger, rest, warmth, libido and so on. And survival is not an urge by itself, but it's an emergent property of those things. We have a bunch of social needs. These social needs structure our social interaction. And the primary social need might be affiliation, that is, gain attention from others, the sense of belonging, and what we call external legitimacy. It's transmitted by social signals that other people give to you. You tell you, you're basically okay, you are conforming to the expectations that others have at you and you are part of a social group. Then we have something, basically affiliation works like a currency. It's a capital, a social capital that is distributed among people in small populations especially, so they can cooperate with each other and reward each other for cooperation without reducing the fitness of the group by giving actual resources to people who are breaking their legs if they do not cooperate. Both which would reduce the overall fitness of the group because they would cost actual resources. So we create as a nature, an evolutionary process, a virtual currency, this affiliation signals that enable us to reward each other with praise and smiles and so on, or punish each other by reputation loss, basically, if we do not cooperate. The whole thing also works when nobody is looking, and this is because we have a sense that gives these signals to ourselves, which we call honor in our culture. It's basically internalized social norms and whenever we conform to internalized social norms we give legitimacy signals to ourselves. Of course these internalized social norms are not identical to the group norms that we have in public. For instance, maybe you are allowed to pick your nose on your own, but not in public, but there's still things you might not be doing, for instance, stealing food from your flatmate, because you would feel bad about it, even if you wouldn't get caught. Then you have a bunch of others, like nurturing the desire to increase others' happiness, romantic affection, which is a desire to get close to others and identify with them, and dominance, the need to climb social hierarchies and so on. And as we can see, these are not in the same strengths in every person, and interpersonal variance exists by parameterizing those needs. So some people have low desire for dominance or little need for dominance, some people have an extremely high need for dominance. And this gives rise to varieties within populations and it makes it possible for our populations to adapt to different environments. And we have cognitive needs, especially a need for competence, which amounts to the acquisition of skills, the general need for being able to control our environment and to generate visible effects on it. And we have uncertainty reduction, which basically amounts to an exploratory drive. And we have aesthetics, which are either stimulus oriented, there are some stimuli that we find intrinsically pleasant because we are hardwired to do so. And structure oriented, that is abstract aesthetics, structure discovery, basically a reward that we get for finding better representations. All these needs are in a constant competition with each other, there's no Maslow pyramid, I think there's no hierarchy of needs, because we all know of these press reports of people in China playing multiplayer games until they fall dead off their chairs because they neglect food, drink, and rest, which basically means that if you are lucky, then the relative strength of your needs are wired in such a way that you eat before you fall dead off your chair, instead of leveling up. But it's not the case for all of us. Sometimes it's possible that we forego our physical well-being for social goals, for instance. So, we do not need a hierarchy here. I think that all needs are competing on the same level. A need is represented as something like a tank. And there is a target value and a current value in there. And the difference between the target and the current value is the urge strength. A deviation in the positive direction is what we see as the pleasure signal. A deviation in the negative direction is the displeasure signal. And the strength of the signal depends on the shortness of the amount of time in which this happens, basically on the delta and the time. We use this to create associations between the needs, signals between the urge indicators and appetitive and aversive goals, and we use this also to strengthen protocol chains and memory that lead up to the situations that are connected to those goals. So we learn automatisms that lead from the current situation to goal situations. And intention is basically a goal that we have committed to, one of those situations and actions, and a plan that enacts this. And with this mechanism it's possible to do motivational learning. Due to lack of time I'm not going to go into the details here, but you'll find the description of this in the paper. It's relatively straightforward to implement. Motive selection is based on checking whether we can do an autonomous regulation if the need becomes active. So if there is a body process that can do an autonomous regulation if the need becomes active. So if there is a body process that can do that. If it's not possible, then we trigger an urge signal. If the urge signal is there, then we check in the environment if it's possible to opportunistically satisfy the need. So if there's something directly in front of us that affords satisfying the need without changing our overall plans, we might just, for instance, eat a sandwich without stopping how to program because the sandwich was just put in front of us by some caring core worker. And we have an opportunistic way to satisfy that need. Or to drink some water because we have some glass on the table and we do not need to enact a big plan to do so and we do not disrupt our other goals. If that's not possible, we need to identify a plan and we do this by comparing the different urges, take their relative strengths, then we compare them against a suppression signal that basically is an adaptive stubbornness that avoids goal oscillation, and then we choose the strongest one to commit to it and I try to identify a strategy that enacts it. And then we calculate the motive strengths by estimating the expected reward, the urgency of satisfying the need, the competence that we have to basically the chance that we are going to be successful in enacting the plan and dividing this by the cost of the strategy so we have different motives and we are going to pick the strongest one. This is a relatively simple function that enables the motivational system to at any point commit to a motive. If we do not find any strategy that we can enact to satisfy our urge, we can increase our general need for exploration, which means that even if we do not know what to do, the probability that we start to randomly explore the environment or directly explore the environment increases, so we have a chance to do something about this in the future. Okay, we can parameterize these needs. There's the strength of the need, the relative importance of this, the decay, which tells you how often you need to replenish it, the gain, that is the signal that you get from satisfying the need, and the loss, which is the effect of frustrating the need. you get from satisfying the need, and the loss, which is the effect of frustrating the need. And the different configurations of need parameters give rise to different personality traits. In a similar way, we have modulators, and I'm not going to go into details right now, again, because of time constraints. So we have six modulator dimensions, arousal, valence, aggression, submission, flight responses to the environment. And we have attentional modulators, which direct the focus and direction of attention and the resolution that we have in our attentional system. And together they configure cognition at a given time. And parameters of the attentional modulators basically give rise to different temperaments. Whereas the motivational parameters basically define personality types, the affective parameters, these are personality types in psychology, the big five, define temperaments. And here's one thing that we are currently working on to evaluate this model. This is based on some early work by Richard Bartle who discovered that when people play computer games like multiplayer role-playing games, that there are different types of players. And they discovered that there are socializers, achievers and explorers, and killers. So highly competitive players, highly explorative players that try to see every corner of the game, players that are basically grinding up their scores and trying to maximize every skill that are the achievers, and then those that are mainly playing this to get social interaction. And the interesting thing is that we can model this very simply by saying that there are some affiliation maximizers, some competence maximizers, and exploration maximizers. And if we extend the model a bit and take our full motivational model in there, we also propose that there are people that are more focused on social interaction or on supporting others of being helpful in other groups. And additionally, we have also people that are interested in the hedonic aspects of the game that really want to see nice vistas in the game and want to have hedonic experiences in the game. And it's something that we are currently exploring by looking at behavior of people in role-playing games, getting the data from large game companies over player behavior. Okay, so this is what we're currently doing. I'm going to stop here. And you can get more information at our micro-psi homepage. And I thank you very much for your attention. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.", '14.635475158691406')