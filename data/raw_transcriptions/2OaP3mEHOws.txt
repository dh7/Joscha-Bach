('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/2OaP3mEHOws"> </iframe> </center>', " So I'll make one comment that will take us back to the very beginning. It was when your first talk, you posted a skeptical quote of Leibniz on, you know, what great a mind is in the machine. But Leibniz, I'm pretty confident, was actually reacting to the earlier work of Spinoza, who I deeply offended in by realizing in a way that I think is incredibly perceptive, given what was around at that time, that the brain is a complicated machine, and that consciousness and everything that goes along with it is just a description of what's going on in that machine. It's just a description of it. It's nothing more than that. And Leibniz, I think, so you get the quote of the guy who is much more progressive in some sense, compared to it's really amazing to me that Spinoza figured this out back when he did, given what was known at the time. Anyway, I would react also to the second talk, and in fact, all of the talks in one other sense, which is that in your core values, I think you, I would put economic modeling, a sense which would generate the organization, but more generally, I would say that what is needed in some sense, not necessarily the only thing that's needed, but one thing which maybe wasn't touched on that day is important, is some principle or principles that make the thing be grounded and generalized. I could come back to Atkins Razor, but somebody could talk about something else. But just adding lots of other stuff on top of it would worry me in the sense that there's a question of how do you add in more stuff isn't necessarily going to make it generalized. Something's got to constrain it. And you'd like to have some principle that makes things constrained, so that as you go on, economy might do that, or a constrainer or something else that constrains it so that you get to the point at which... so that it will generalize, so that it will do the right thing in new circumstances. And, right, so the other thing that we find missing in this field generally, in a large measure, is demonstrations. You get narrow AI demonstrations, but it's rare to see it talk in these papers also, where it actually talks about something that it's tested it on and has generalized. So without a principle and a demonstration, it really becomes, it's worrisome. And maybe the reason for this, you have to build the whole structure, and then maybe it will, it might be maybe need a certain size level of problem structure before it works, before it gets constrained. I mean, that's certainly a picture of psych. If they got enough logical rules there, eventually it would all be constrained and work. But in the meantime, it would be nice if we had some reason to believe that we're going to get there. And that's all I will comment. Maybe we have some questions or some answers, some response. Anybody want to respond to that? Or I stand? OK, you addressed this point that you want to have sound images grounded and general at the same time. And I think this is one of the very crucial things. I tried to mention this point with the benchmark. And I think, Vladik, the same time. And I think this is one of the very crucial things. I tried to mention this point with the benchmark, and I think Vlade did the same thing when he, for instance, came up with the 20 questions test. And for me, this is a very interesting point, because, for instance, I think that apes will fail the 20 questions test, and children will fail, most children, the 20 questions test. And in fact, a lot of people which are adults will maybe fail the 20 questions test too because they are not intelligent enough in order to succeed. And the question for us is should we build apes and extend them a little so they can manage to survive the 20 questions test or should we build advanced text retrieval and then add the A to split during copy break later. Of course, we are not necessarily looking at something that has to be human in order to be intelligent, but we are looking for something which has to encode somehow the same kind of constraints of representation and information processing. And this is the point where you need to start. And maybe to be like a human is the best way to embody the same kind of constraints of information processing and representation. But maybe not. The crucial point here is not that we need to be grounded in the physical reality, not necessarily, not in a philosophical, deep sense at least, in order to embody these constraints. Maybe it's possible to do it via sight to get constraints in the system. Maybe not. This is maybe an engineering question. It's a question of methodology that we need to choose. And I think this is what I want to learn more about when I come to this conference, now that I have this great opportunity to be here with all these nice, mad scientists which pursue this goal that nobody else wants to aim for. Sure. I have a number of questions in my presentation for you, so maybe you can answer them. But right now let me ask you, what is the actual embodiment? We hear that, well, there should be a robot or simulated environment, you just need to interact, etc. But is an embodiment not simple use in practice? Aron Solman has recently written about embodiment, saying that, well, he's a philosopher. He doesn't jump when he thinks about abstract ideas. Definitely, they're embodied in some way, but very much secondary. So embodiment for language, for me, is basically the use of certain concepts in practice in different contexts. And that's an embodiment which captures the meaning also. When I think about spatial concepts, that's different, or temporal concepts, that's different, because that relates directly to being in the world and our ability to act in the world. But in many cases, when I think, for example, about abstract mathematics, should there be an embodiment in this? I mean, that is probably doubtful. A brief comment on that. If you look at Lakoff and Nunez's book on where mathematics come from, they pose precisely the argument that mathematical intuition is grounded in physical interactions. I know. I read the last chapter, which is on the Euler's equation in there, which is over 50 pages of y e to the i pi plus 1 is equal to 0, and really I understand it much better than I used to, I must say, but then he also, what now appears was the mathematician at Howard who says, well, we will never understand the reasons why it's so, we just can manipulate it. And he was a very good mathematician. So even though his understanding was not grounded, he was very good at manipulating that and coming up with new theories. So I guess you may have a mathematician that is going to be very good without really grounding that quite deeply. Most engineers have a great lack of, and their intuition is not so grounded. They just see complex numbers and e to the power, and they don't think about self-referential, regulated processes and other things like that. So I mean, I'm not sure how much embodiment we really need in this case. I'd like to respond to that a little bit. When you talk about what kind of embodiment, a lot of the people who focus on embodiment like I do, we measure that in terms of how many sensors, how many actuators, how connected are you to an environment. And I think it's really similar to a lot of what your talk said. All those examples, there's always one bullet in there somewhere, no large scale experiment experiment, right? No large-scale result. And it's a very similar thought, it's sort of similar to the, my colleague IBM, we're in it at the moment, about the scaling, you don't apply convergence, you just throw enough of it at it. There is a feeling, very generally across all these areas, that there's some sort of a tipping point where before you have enough, you don't get interesting results, or you get a certain class result. And once you go past that number, you start getting different results. A lot of the algorithms that people have done in AI over the years work in the small, don't work in the large. A lot of theories may work very well in the large, but can't work in the small. And I think that just when you say, much you value it, I think it's a general part of it, a more general question about what's the scale of the system you're working in. And we're working on far too small scale systems right now. But that obviously also very much depends on the application, right? You talk about sensors and the real robots. And in this case, I quite agree that understanding the special relations requires some embodiment and relation of the perception to my actions, right? I mean, we explore the world by actions, and the circadian system, et cetera, is basically very important because we can focus on things we want to really see. There's a fellow at Stanford, Andrew Moon, who has done this fovealiation in vision, which is very helpful because he says that from a distance, the robot doesn't have resolution to see the object. So he can recognize the object with this type of vision. But when you focus just on a single object and you have all the power zooming on this, then you see the object much better. And the whole system, the biological system in this case, seems to be very efficient in this way. But in many other domains, where we think, for example, I mean about application of AI to, as a personal assistant or advisor in troubles, offices in other places like that, this type of embodiment can be really quite secondary. That's my feeling. Can we have embodiment in mathematics too, maybe? If you look at physical reality, this is maybe some kind of small dirty pond, and we wave our retinas around it and see if it, and from time to time we catch something, and then we encode it. And maybe when we do mathematics, something similar happens, because mathematics has its own constraints. It's not a small dirty pond, but it's a big crystal clear ocean, and then we do not wave our retinas, but we wave our small little sieve of reason around it, and then we find some regularities and then we encode them. And all that are these ideas of Nunes around which say that when we do mathematics we abstract over our experiences which we have in the physical reality and maybe this is true for humans and for the way, for the trajectory that most humans have when they approach mathematics at first. But when they are mathematicians, maybe they are different. And maybe an AI could be involved in mathematics. It's pretty likely that he could not talk with them anymore in the same way as many people cannot talk anymore to mathematicians twice a person. But I think it is conceivable that mathematics itself as an environment over which embodiment is possible. We're going to push a link so we get this, so we can use this idea. The plane brought me that thing. All right, this is more of a general comment that's directly related to embodiment. But it seems like the state of AI research currently is rather like the state of research on life 150 years ago. 150 years ago, we kind of knew what things counted as living and what things didn't, but our ideas of what life was were pretty naive, and rather than being dualists, people were vitalists. Then we came up with evolution and discovered the structure of DNA, and now we have biology. And despite this, despite a pretty good grasp of biochemistry, of embryology, and with a pretty good ability to see and actually physically manipulate the stuff of life at every step of its onogenetic development, after a lot of research, we might, just might, after 60 years, be able to soon synthesize a single, highly idealized and simplified cell. So we used to study life the same way we are now studying AI or cognition. 150 years ago, we took our folk ideas about what it meant for some to be living and hoped that if we jury-rigged them all together into some kind of life-enstancing Rube Goldberg machine, we would create something that moves on its own. And so it seems the key problem in AI is that we're not even sure what it is that we're talking about. Like life researchers of 150 years ago, we've got all these folk ideas of properties that we want from artificial intelligence, but we don't have any idea of what's going on at the cognitive equivalent of the cell level. Life was fundamentally intuitive to us and required that we look at it at its most lowest and general levels. And still a lot of people don't understand how life works today. So I guess my question is, why do we think that the structure and functions underlying cognition and artificial intelligence will be more intuitive and well-described by our folk ideas than the structures and mechanisms that ended up underlying life? In other words, why are we more interested in synthesizing the highest levels of something we don't understand, instead of trying to figure out what it means to represent and cognize at the lowest level? Thank you, Bob. If I can comment on that, I seem to be very much trying to understand. Some of my slides I have no time to show what's about where I am standing in the brain and what we know from neuroscience now about how these things are implemented. I think we know almost everything, but we don't have much experience with building very complex systems. And we haven't done even the basic things. When you think about language, you look at what's in there. There are some ontologies. But I can't find a common sense ontology. I mean, ontologists are very specialized. And if you're a biologist, you find beautiful ontology that tells you all the details. But that's not what normal people have in mind. Well, not biologists, not normal people. So we haven't yet really created resources that will, for example, describe every concept in relation to others. People do small semantic networks, and they work in their small domains. But we don't have anything like, well, WordNet is this huge dictionary that people have been downloading for more than 10 years. If we had something like that, but more on some symbolic level that will show us, OK, this is a concept. The concept has relations with many others. And could you use it in different domains and have this type of descriptions? We could really do some progress. That hasn't been done. So lots of things have not yet been tried. And I don't agree at all that we don't know what we are doing. In fact, certainly, there are all these programs, that we don't know what we are doing. And certainly, there are all these programs, sorry, there are all these programs for game playing that can learn. And to make them flexible so they can learn more games and then learn to interact is not so difficult. Sir? I'd like to draw an analogy between embodiment and dissipative systems in physics. These systems take energy from the environment and they undergo spontaneous decreases in entropy and self-organization and phase changes. An intelligent system needs an information environment in order to learn and adapt. And that's the kind of embodiment that matters. It doesn't necessarily have to be the physical world, but it has to have an information-rich environment and condense the incoming information into models. But it needs, what it needs to be is grounded in the sense that something comes out, right? That it, yeah, that it, you know, makes predictions or something, that gets right, you know. you", '9.755600214004517')