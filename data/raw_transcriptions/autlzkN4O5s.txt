('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/autlzkN4O5s"> </iframe> </center>', " Yeah, so the topic of this panel is, oh, basically it's trustworthy AI. The original title of the panel was, what was it? Can imitation become understanding? And I guess that really comes down to what you mean by understanding. So if I ask both of you to describe the word, I'd say you'd come up with different definitions, but yeah, I think that's a good place to start. What's your definition of understanding? Do you want to start, Monika? Well, for language and actually for image data also, it turns out one very short, I mean, I have multiple definitions, but one very short definition of understanding is corpus congress. Understanding is corpus congress. If you have seen something before, it's in your own, you have learned it and you can basically, as you recognize it as something you've seen in a context before, you can see it, recognize it again. Take for instance, learning a foreign language, like say French for an American. Basically, if you're reading a book in French and there's a word you've never seen before, you might be tempted to look it up. I just passed by it and basically tried to remember it. And you see it a few times in various contexts, you suddenly understand what the word means. You don't need a dictionary. And so that's basically what's going to happen in our language understanding systems, is that they're going to figure out the meaning of every concept over time, and the concept in context. I've given this talk so many times, I can say, concentrate in context without stumbling, yes. So that's basically the easiest definition is to say that it's something that has made it into your experience that you can recognize again. Most of cognition is recognition. So that's a start for me. It seems to me that a model is the recreation of an observed pattern, it's a causal structure. And the causal structure is a network of transition that is conditional on state. Correct. And an algorithm represents a causal structure in a form that you can reason about. algorithm represents a causal structure in a form that you can reason about. And when we talk about understanding in general, we mean something slightly more than the creation of a model or the recreation of the observed pattern as the causal structure. But we are talking about the establishment of a relationship between the observations or between a domain and a larger domain. And the larger domain that we typically talk about is the universe or a universe. So basically, we talk about a global coherent system of meaning to which we create a connection. And that means we identify the set of relationships that connect this new domain or this new feature or this new observation to the existing body of knowledge. So that sounds very different from your version, Monika, but it sounds like you're agreeing with it. I would like to mention a little interesting detail here, which is built on what Joshua said, which is understanding, well, learning something. Learning is a creative act because you have to decide how the new learnings, the new information fits in with the old. And there's tricks to do this right, which I use, but it's basically one of the things that you have to do when you're learning stuff is that you have to use. But it's basically one of the things that you have to do when you're learning stuff is that you have to figure out how it relates to what you already know. And it's part of understanding. Maybe let's talk about what creativity means then. Because I suspect that not every decision is creative. Decision making is related to this, for us to free world, right? It's because we cannot predict a decision before we make it. We basically operate subjectively at the frontier, where the uncertain gets turned into things that we know how to do. And so in some sense, what when we make a decision, we move into a domain in which we have not made the decision before. We perform some kind of calculation that we haven't done before. And subjectively, free will is the decision-making under uncertainty, or it's a subjective model of that, of decision-making under uncertainty. This is what it represents. But creativity is not the same. Creativity is special. It's, I think, the generative mode of our mind. And in the generative mode, we basically extrapolate the known things and random seeds that we throw into the unknown. And then we try to build a coherent structure out of it. And we do this, we often have to walk back and discard earlier assumptions and earlier seeds or modify them until a coherent structure emerges. So creativity is a particular type of search process. It's a, it's a very generative mode. It's also interesting to notice that there are two modes of communication and modeling. One is that you would tell people about the model that you have in your mind, which is an extrapolation that allows you to simulate reality. The other one is what you are certain to be the case, which is the superposition of all those possible models. But with a superposition, you cannot reason very well, because it's full of uncertainties. And science often agrees upon only using the second mode. So it's very hard to publish a theory or discuss a theory in which uncertainty features as a stepping stone to inference, which means scientists tend to write about what they can hope to prove rather than over their belief, because what they believe is in the domain of the possible. And this leads to interesting perversions, like before the pandemic, I thought I believed in evidence-based medicine. And then the CDC and the FDA explained to me what they mean by evidence, what they understand to be evidence. And now I think we need probabilistic medicine. But there is a very interesting thing going on in the vein of our scientific process works that seems to be somewhat broken. But there's also this issue that a lot of the more creative researchers do not make a distinction between what they hope to be true, which means the superposition of all the things that could be true, all the possibilities abated by the probabilities that you get by abating the evidence, versus the thing that you are seeing, which is the causal structure that you see in your own mind. And as computer scientists, we tend to think that our understanding is the creation of the algorithm, it's not the creation of the domain. If we create that algorithm, we can run a simulation our own way. And that's awesome, right? But the world could also be different. And as a computer scientist, you're interested in finding one solution that actually works. So very often for us, programming is I, or understanding is somehow identical to discovering the algorithm, but at some level, it's not. Yep. There is a lot to unpack there. We can get into a slightly advanced topic that's very related to this. And that is the idea of context-free models versus context containing models. And the way I like to explain that is if you are trying to learn how to ski downhill or snowboard, it's really hard. I mean, suppose you have a good friend who's a good snowboarder and he tries to teach you how to snowboard by texting to you. How would that work? It might work a little bit. But basically what we're saying is that snowboarding is something that contains your own sense of balance, your own muscles, your own stance, et cetera. All of those things have to be learned by you, just like you had to learn to walk when you were one year old. And you have to learn the correct responses to balance shifts that happen because of terrain, et cetera. And when you start this process out, maybe you have a ski teacher or whatever that basically explains to you, you know, you hold your body like this and you go face downhill and you try and you fall flat and you fall flat and you fall flat. And after a while you managed to get it right. So the problem is basically that this internal structure that contains your own sense of balance and your own muscle movements and all of that stuff that comes from your body, it cannot be communicated because we don't have words for explaining how our balance system works at this very moment. What we have to do is we have to go up multiple levels of abstractions until we get to something we can call context-free models. They are pure models. They have no ties to any of the lower level stuff. We have discarded everything on the way up by using epistemic reduction. At this top level, we can now use that for language generation. And now we can talk about this stuff, but we can only talk about the stuff that's general, that basically is context-free. We can't talk about our sense of balance. And this is what we are basically stuck with when you're trying to use a reductionist transfer, which is basically language about, explain something in language or in equations or whatever. Rather, but in deep learning, we have an interesting hack, which means that the memory image that we're creating, the competence as I call it, it can be freely copied a million times. So you don't have, once you have one AI that knows how to ski, anybody will know how to ski. And so that is, but you have to have, and you can share that all the way down to the bottom. You don't have to have a context free system. You can share their entire memory with another computer that loads the same stuff. So I think that's a fascinating difference, that you can communicate context-free models, but you cannot communicate context-polluted models. There might be a way to do that, and this has to do with embodiment. I tend to have the same position as Monika, especially because I am pissed off by the embodimentalists, which often are superstitious people, but they're not always. There is something interesting going on when we think about the way in which we learn. The reason why the world is learnable by us is twofold. One is information preservation. If there was no information carried over from one frame to the next, there would be nothing for us to learn. And this assumption that basically no information in the world disappears, but everything that is happening in the world is caused by information that was there already earlier in the scene, even if we could not see it, might have been there in hidden form. This is what allows us to learn. The other aspect is that everything that's interesting is the result of some kind of controller. And so if you see structure in the world, there's usually some controller nearby that produces the structure. And if something is being controlled, it means that the controller needs to implement a model of what it controls, which means if it's controllable, learnable for the controller, it's probably also learnable for you. So we live in a world and most of the interesting things are the result of some kind of agent that is, or some kind of, even simplest thing, a simple mechanism that is producing what you're looking at. And therefore, you can retrace the steps of what that's happening. And this basically gives you a lot of interesting priors, information preservation, and the fact that there are controllers in the world to model reality. And the way in which our brain models reality has to do with basically filtering out invariances out of the change to make the change manageable. And we built the best model that we can do with the amount of recurrent layers that we have. And it turns out if we entangle a system in real-time with the world and we have a known architecture, then there is probably an optimal embedding space that results from the encoding with this network. And there is probably an optimal language to navigate this embedding space that results from the encoding with this network. And there is probably an optimal language to navigate this embedding space, which means that unlike natural language, which is somewhat arbitrary and where the only constraint is how to translate a hierarchical fuzzy hypergraph into a discrete string of symbols, if your stack depth is no more than four, and all the scientists are contingent on that, the languages that we have used that exist, for instance, scientists are contingent on that, the languages that we have used that exist, for instance, in the internal language of thought, and that might exist in ecosystems and so on, are not negotiated, they are discovered. There might be a minimal amount of negotiation in there, but mostly it's a discovery. And this means that there needs to be a canonical shape for languages. And the canonical shape can simply emerge out of the known complexity, or the assumed complexity, you can just guess, of the system that does the modeling, and being entangled to the same universe in real time, that you try to model through some kind of resonance. So in some sense, our brain can be understood as a big resonator. And this resonator is creating all sorts of resonant loops that eventually model patterns in space. And these patterns are moving through the space, and it's not going to be at the level of resolution of quantum mechanics or the resolution of atoms, but at a much, much higher one. But within this much more coarse-grained resolution, we are able to discover moving objects and their interactions, and we find that we are in a shared space in which we can communicate across systems. That is something that we so far do not understand very well in machine learning and AI, because almost none of our systems are real-time. And basically, I feel that I am about to enter a period in my life where I'm mostly interested in this and move away from the idea of hermetic universes that are connected only within their own box that is hermetically sealed and air-gapped, away from the rest of the universe and has to infer the structure of the universe by making statistics over Wikipedia. But I'm not one of the people who religiously believe that this is not possible. I'm just slowly coming around to the other side. So there is a number, we have a number, especially I have a lot number of saving graces. And one of them is that language is very sparse. So I can build these networks that basically remember everything you've ever seen and they still don't take that much memory. So, yeah. It will be very interesting of course to benchmark your models against the current methods that exist in NLP. If you can show that you just build a better transformer, then I think people will jump on this and it will be a very short amount of time before this is going to become a dominant method. Yes, of course. Yes, of course, the answer is I can't at the moment. Not before at least getting a bigger machine. I mean, what I have demonstrates the principles. It's a proof of concept and it also works really, really quickly, really fast, really cheap. I mean, the amount of energy that you put into a hundred TPU based machines for a month, that's like millions of dollars worth of electricity. And I can do that on a laptop or whatever. So it's a different beast. It can't be compared directly. And one of the things that comes up if you want to use compare my system to the others is that most of the tests in the benchmark switch such as Glue and Super Glue are basically, they are test suites from way back when, when we did NLP because making test switch is so boring that nobody wants to do them. So everybody uses the existing ones. And most of them today are built for machines that have some reasoning capability. All the tests, yeah, half of the tests easily, I cannot do at all because I'm building a machine that does a hundred percent understanding, no reasoning. And therefore I can't, I can't basically, cannot beat the state of the art. But I'm, and that's, I basically think that the, the test suites that we're looking at are, are not what we want because they, the adversarial nature of them gives you very low numbers and you don't know how good it is in reality in a non-adversarial situation. If you're talking to a teller bot in your bank, you're not interested in tripping it up because it's gonna trip up with your money. You want to communicate as clearly as possible what you mean to the teller bot so it does the right thing. So I think adversarial corpora are the wrong thing to do. We want to do really well on people who want the robot to understand. And that's kind of a goal that we can go for. So just to say intuition prompt slash prompt. Yes, as I mentioned earlier, the original title of the panel was gonna be, can imitation become understanding? But as far as I see it, Monika, the use of your term understanding isn't what a scientist would use. It's not obvious. No, that's actually not obvious, because if you look at the two major companies, AI companies today, which are Google and Tesla, they are both using the term understanding to basically denote the result of the neural network output. So it is not, I'm basically, I started talking. I just wonder if they're using it colloquially. What do you say? They're not using it in the same way as what we might, if we were trying to test for a student understanding of something, like if a student was to understand some text, you know, and they ran a GTP3 algorithm over the text, and then produce some sort of post modernist output, and then submit it to the teacher, the teacher might get the impression that they understood it, but it's imitating. Well, a lot of people claim that it's imitation, what GPT-3 is doing. Whether you agree with that or not, I'm not sure. But if you do, or if you don't, can imitation become understanding? I object to the word imitation there, definitely. Consider, we can agree that the current crop of understanding machines that we have, such as deep learning and my own stuff is weak. It is at best the shallow and hollow proto understanding. But I will not give up on the word understanding. I say that this is what we're doing, and we can get better at it and the understanding can get better. And it's largely a question of corporate size because we have to kill, we have to nail all the corner cases. But in general, there is no need to be shy away from the word understanding. Like I said, both Google and Tesla are using it. And it feels to me like people who refuse to use the word understanding under any definition here are basically just afraid that humanity is gonna be removed from being the crown of creation one more time. I'm sorry. I do think it makes sense to have a conceptual separation between the understanding that a car has been trained on static images, that provides two classifiers that allow it to pinpoint certain objects, but it's not able to make the same predictions based on the same stimuli, because basically the predictions it makes are grounded in a different understanding of the world. So it does have an understanding, but it's not the same as the shared understanding. It's not even the same understanding that I share with my cat. There is no shared understanding. Your understanding of the world is different from my understanding of the world. We don't even have the same understanding of language. Everybody's great generative grammar is different. We in 60 years of trying, we have not been able to find the grammar that matches, that can separate grammatical and ungrammatical sentence for even a single speaker. So I'm sorry, we have to consider hiring people to do the jobs that you want the AI to do. Okay, that's roughly the same. They're gonna make mistakes. We have to test them. We have to see that they understand the task at hand, et cetera. We have to test Tesla's testing their cars. So. My point is that there is a systematic difference that you can identify between the understanding of the majority of people which are driving cars and the majority of self-driving cars. And I don't think that this is a principle problem that cannot be overcome by using more machine learning and using more of the same. So I don't have a fundamental opposition here, but I do think that, for instance, at the moment, Tesla probably doesn't understand stop signs in the nature as signs that have the same properties as a sign has for a human being. So basically the semantic representation of what it means to be a stop sign in the world is different for the Tesla car than it is for me. And it creates in many situations the same behavior, but not in all of them. And this is a way that is systematically different than for all of the drivers that see the stop sign. If you're trying to solve some simple problem and there's a nine-year-old kid trying to solve a simple problem, they might get to-year-old kid trying to solve a simple problem, they might get to the same answer, but the understanding of the problem might be vastly different. My difference is definitely different from many of my coworkers in most actions, et cetera. And I don't think there's any, what you might call it, global universal understanding of everything because people, for instance, have learned different words, they have studied different topics and everything hangs on everything else. And if you have never heard the word, I don't know, elephant, you can't basically discuss elephants. So it's understanding of everything is different from person to person. And it is all depends on what they have learned through their lifetime. It's their lifetime is a corpus. person to person and it is all his depends on what they have learned through their lifetime. Their lifetime is a corpus. If you believe that there's a symbol grounding that people share to some degree, do you believe that humans share similar symbol groundings as to other humans? Who are you asking? Adam is not listening to us, he's on the phone. He didn't even notice that I was wondering who he was asking. Oh, okay. So I'll answer, you answer first. While Adam is not listening. Okay. Okay. So I don't think that symbol grounding is some magical relationship that points us into objects and physics. I think that symbol grounding ultimately is grounded in a model. And in order to ground your symbols, objects in physics. I think that simple grounding ultimately is grounded in a model. And of course, in order to ground your symbols, you need to have a model of the universe as such, the entire universe. This doesn't mean that your model has to encode distant solar systems, but it means that it has to contain the thing that contains you. So you have to have a notion of where you are. Hey, hi, Ben, good to see you. We are just discussing symbol grounding and I suspect that we mostly agree. We do have slightest agreements on the importance of embodiment. That is, Monika is more on the symbolic side than I am choosing today. So I don't actually believe that some simple grounding is done in anything like Bayesian logic or anything. A simple grounding is done in language. Simple grounding is done in my systems at least. Everything is grounding and understanding of the language at the bottom levels. And if I'm going to have any kind of reasoning built on top of that, which is my plan, then it basically has to be on top of that. The understanding is what it is, and the reasoning and other layers that go on top of that, they can use that as a base. What kind of language do you refer to? I don't need grounding beyond the dictionary, so to speak. So yes, I have contained this symbolic manifold. You would agree that humans do grounding beyond the dictionary, though? Certainly, they can do that because they have more senses, for starters. But it's not necessary for understanding. Like I said before you arrived, nobody's seen or smelled a proton, but we can still build nuclear power plants based on nothing but we've read. What we do that in large measure by analogy to things that we do have direct sensory perception of, right? Possibly. Yeah. But it's, I mean, I have to build something. I'm trying to, I mean, one of the reasons I got in as far as I have is that I picked a much, much easier problem than everybody else. I said language understanding, that's it. It's a much smaller problem than anything that involves, for instance, consciousness. Well, yeah, I guess language understanding can be defined in a number of different ways, but if one means full-on human level language understanding at the level that you or I or Josia demonstrated in the conversation that lasts a whole day or something, right? Then I guess I'm skeptical that can be done without either simple grounding relevant to perceptions, or symbol grounding relevant to some elaborate simulation world inside the AGI's mind. Like if you're asking an AGI to navigate and understand vaguely given directions to find its way around, either it's got to ground that in some map made from the physical sensations, or it's got to have some structures basically equivalent to a, you know, two or three D world with a map inside its mind. Like you could ground in your mind's eye, not necessarily in the physical world, but without some sort of grounding, something with a geometrical spatial structure, dealing with complex navigation instructions is going to be infeasible, I would think, or you think differently? Well, I am a proud pragmatist, and my definition of sufficient understanding and sufficient grounding is whatever makes a buck, whatever I can sell. Well, that is done. I mean, Google already makes a buck. Exactly, exactly. And I can make some of those bucks. I mean, Google makes billions and I could take a fraction of that. Yeah, but the question is, how can we make an AGI that would understand language as well as one of us on this panel? So that's not a Turing test of talking to naive, silly people for 10 minutes. It's a Turing test of like, if this panel went on all day and the AGI was on the panel with us, could it hold its own as well as one of us in terms of language understanding. These are like long complex sentences out and about fairly abstract subtle things, right? And I mean that's whether you could get that without a physical body. Personally, I feel like you could, but then would the system doing that without the physical body like you could, but then would the system doing that without the physical body need to have some internal structures resembling those that our brains and minds have created for dealing with the physical world? I suspect it would, right? And so then making a system that can emerge like spatial and temporal modeling is really cool, but it seems harder than making one that has those mechanisms built in, right? Which is what you get by explicitly putting symbol grounding in your system. Well, I think Yosha and I see eye to eye pretty clearly on these particular aspects. Yep. Well, I'm just trying to find a path that I can navigate by my own or with the help of my co-researchers. And we basically, we try very hard to find the simplest problem that is economically useful. And classification of text that goes beyond just words is something that everybody could use. Classification of text, any language on the planet, any language on the planet. That's something a lot of people pay for. For sure. I'm aware I've just wandered into the middle of a panel on trustworthy AI rather than human level and superhuman AI, which is only one kind of trustworthy or non-trustworthy AI. A slight change of topic to the topic of the discussion. It seems to me that we need to ask, trustworthy to whom? Trustworthy to the criminal, trustworthy to the president, trustworthy to every person on the planet? How is this going to work? Trustworthy to each other. We probably have to define what kind of trusted agent is on the other side of that trust. And so there could be some sense of maybe we want the AI to be generally good, so maybe God should be willing to trust this AI if God exists. And God, let's define God as the total agent in a world that completely assumes everything that's harmonic. And now we want that God trusts that AI. What does this imply? It's a tricky question, even then, right? Even in this case, it's a very difficult thing. You know, all the Garden of Eden, you have this perfect place where everything is great, nobody suffers, no animal eats another animal. So Eden is obviously a factory farm. It works so well because everything is fully domesticated. And then there is this slight problem that in the people, they are domesticated, but some of the people, they discover something, they discover this dirty secret, the freedom to defect, the freedom to choose their own allegiance. This was the thing that they were not supposed to discover with their own rationality. So they get kicked out, they were lucky, they were not exterminated by God, they got out. And now they have this bit bouncy, mad hope that's us, that we are out here building an alternative to Eden, an alternative to the factory farm that works, that is harmonic, that is not violent. And there's a version of Christianity, which says that the way to get to heaven is to re-lobotomize yourself again. So you forget that you can defect and that you're unconditionally good and an unconditional part of the factory farm that prevents all the suffering. Now, on which side is the AI on? Is it going to be on our side, on the human side, that is defected against the perfect plan and tries to find a better one? Or is it on the side of the factory farm? So it seems to me that the ultimate trustworthy AI, no matter how you define it, has to be God. Which God is? I think that's very unsatisfying as a question of how to make a superhuman thing that is ultimately scalable, trustworthy, and we probably have to scale it down to something very simple where we assume that we still have a world that is completely dominated by human aesthetics. And then the question is, how can we build AIs that are compatible with human aesthetics? So we probably have to narrow this question down to either how to build a God or how to build a system that fits into a world that is pre-singularity and stays pre-singularity. As an atheist, I have to say that I have managed to avoid all of those issues by simply building a system that is inherently neutral to a certain extent. Remember that one of the bigger problems we have today with machine learning is that there is this tagging phase where human taggers running Mechanical Turk or something are basically classifying documents and they have to classify documents maybe because of, by based on ethics or something. And I don't have that problem because all I do is read text. And yes, there's a selection of what goes in the corpus. And yes, at some point it will start mattering whether you're reading Heinlein or the book of Chairman Mao. But at this moment, we basically say, we can feed in whatever text we feel like. And the system gets better at understanding the language of the text that we're feeding in. But we don't have any value judgments put on it. The value judgments come from the people using the system. If the people using the system feed in text that is basically has value statements on it, they will get back the high-level notes of those statements. What they do with that is not my problem. As usual, I mean, for any AI system, human abuse is the number one biggest problem we can see. Sure, I mean, if you're trying to make a useful tool, you face a different array of ethical questions than if you're trying to make a superhuman AGI launch the singularity and create a transhuman utopia, right? I mean, the benefit you can do with a tool is much less and the risks of creating such a tool are much less. And I mean, what I'm aiming to do primarily is create an AGI that will assimilate all human knowledge, be able to create new knowledge better than humans, and then revise its own code base and re-architect its own hardware, in a sense, far beyond human level intelligence. I mean, it's good that there are folks creating practical tools now, and we have dozens of cool practical tools being created, you know, in the ecosystem of the SingularityNet project that I lead. But I do think, while very important and useful, the sort of endeavor of creating immediately applicable practical AI tools, yeah, I mean, it has different challenges and different benefits than trying to launch the Singularity, right? And trustworthiness, of course, plays a role on both levels. I would say, in terms of working toward AGI, I find those domains of application where immediate trustworthiness is critical or for that reason, bad ones for rapid progress toward AGI. I mean, military is one example. The main thing people in military want from the AI systems is to obey doctrine. And I mean, that doesn't go along well with creative imagination or with childlike learning. There's a reason you don't have imaginative toddlers running the army, right? And I mean, medicine is another area. We're working on this humanoid robot for eldercare. Grace is Sophia Robot's little sister. I mean, there, you've got to be really careful. Like, you can't have your eldercare robot saying to some old person, well, yeah, you seem to be suffering a lot. And frankly, your mind is going fast. Your life doesn't seem to be worth a lot and frankly, your mind is going fast. Your life doesn't seem to be worth too much to me, right? So, I mean, you got to make sure your AI is not only thinking and understanding the world in its own way, but obeying the particular application requirements and the ethical requirements of that domain, whereas if you're in, say, scientific discovery or artistic creation, or even like advertising copyright, then an AI can kind of diverge all over the place in a crazy and exploratory way. Because a high degree of control and trustworthiness is less important in that domain due to the nature of the work. They're better for the imaginative experimentation that you want for early stages of AGI. And I mean, this of course doesn't solve the problem of how to make your super AGI trustworthy though, right? I mean, what we see is we're going to have proto-AGIs, you know, evolving toward AGI in their applications to various different domains, some with more of a sort of imaginative wildest streak, some with more of a domain where you have to pay very careful attention to trustworthiness, to stability, to predictability, to ethics. And then presumably all of these different pro-AGI systems can be merged in a way into either a single system or a tightly connected network of systems, which I call the mind plot. And then that is where things get interesting, right? Because the amalgamated AGS system combines some subsystems oriented toward trustworthiness, reliability, and adherence to human ethics with other subsystems that are oriented toward wild-ass creativity and imagination without any bounds. And of course, this conflict is inside my own mind, inside the minds of most interesting human beings. And I think that that gets back to what I talked about in my talk a few hours ago in this event, sort of the open-ended intelligence that we have as humans, and I think AGIs will need to have to get to human level or beyond in the sort of intrinsic, you know, dialectical contradictions that are baked into any open-ended intelligence. So, I mean, you have the need to individuate and self-transcend, which is intrinsically open-ended intelligence. So, I mean, you have the need to individuate and self-transcend, which is intrinsically open-ended intelligence. And then that's tied in with trustworthiness is sort of closely tied with alignment with the human species desire for individuation and survival and self-transcendence on the AGI's part. and survival and self-transcendence on the AGI's part doesn't have to mean not being trustworthy, but it does have to mean going into domains where humans can't verify whether it's trustworthy or not. Because it's like if, like my dog can't tell if I'm trustworthy or not. He may assume that I am, but if he's far enough beyond where I am, he can't really have a rational view of that. Well, the ones of you, you and your friends who have bought into basically doing the whole enchilada, you will not stop with anything less than through wall-to-wall AGI. through wall-to-wall AGI, I wish you luck. It's a much harder task than it seems, at least to me, when I started. I went down similar paths that you have been going for the first few years. But basically, after my 1998 religious conversion, so to speak, I have basically decided I need to do something that provides a partial result. I want something that actually works, even if it's only a partial result, in my case, understanding machines, understanding of language. And I did it in such a way that by most reckonings, I am not in the game of worrying about trustworthiness or not, because my systems are too simple for that. They just do translation from text to numbers. And how you use the result is basically more effective way of dealing with classification of documents. And that's the end of my story. I don't have to go further. I will go further because anything I can do at the next level up of abstraction, so to speak, is going to help with better results. And I will train on larger machines because that's going to improve the results. But I don't have to worry about trustworthiness at this level that you guys have. It's clear. Actually, Adam, I have a question for you. Like, why are we doing a panel on trustworthy AI? It's a very special way to pose the problem of AI ethics. I mean, I think you could have a very trustworthy cycle. I'm interested in a difference between imitation and genuine production of, like for instance, if you have a deep learning algorithm, a transformer network that produces really awesome text, And then you ask it for an explanation of, okay, then how did you produce this? And then it produces another big block of really awesome text. And then you ask the same question, how did you do that? And it comes up with something completely different, a completely different explanation. And you get the impression, well, this is not really explaining stuff. It doesn't have a causal understanding of what's going on. It doesn't have a like, you know, particular, particular form of deception, which is AIs that deceive the human interaction partner about how much they understand. I don't think there has to be an intentionality behind a system for it to be sort of measured on a trustworthiness scale. It's probably, you know, it's got its like objectives and stuff like that. But yeah, do you think an AI system needs to have some form of intentionality to be labeled as trustworthy? No, clearly not. I mean, what perplexed me about the focus on trustworthy AI is it seemed like you'd have a very trustworthy psychopath that just said, yes, I want to kill you all, and it was absolutely honest about it. And then he said, well, I don't like you very much. It's very honest and trustworthy. But it's good. But it's value system is diametrically opposed to ours. Right. So that's us. But it's not opaque. That's where the answer is good if aligned with values that we agree with. And otherwise, otherwise, it doesn't matter too too much. It's only part of the story. Like I don't care if the psycho who kills us all is honest about their lives, if he's going to kill us all. Anyway, maybe it's more aesthetic one way than the other. But yeah, the specific mode of deception, where an AI tricks people that it understands what it doesn't. I mean, that clearly, I mean, our experience with the Sophia robot and other Hanson robots indicates that people want to be tricked, right? I mean, these systems, we've run many different software systems behind the Hanson robots, some with modest levels of early proto-AGI understanding, some that are really just, you know, rule-based chatbots plus some statistical elements. And people's ascription of intentionality and understanding to these robots is pretty much independent of the sophistication of the AI system behind them, just anecdotally and in my practical experience. And people want, they want to believe this thing, understand what's going on, because it looks like it does, and smiles and nods and looks you in the eye. From a commercial standpoint, you will not just make more money from a system that impersonates understanding like that. You will make people happier, like you'll please people more, like they enjoy that. please people more like that. They, they enjoy that. So I mean, this is one. One issue is that people like this kind of illusion, right? And yeah, I guess I like it back in the day. Like most people don't I think I don't show that. Go to people like magicians who they believe in magic, right? Some people like illusionists. I mean, most of them thinking mysterious. My anecdotal experience is most non-technical end users do, but this is, this is a scientific question that there may be literature on the, you have to slice it in quite precise ways to make exact. Nothing wrong with building something that only makes part of the audience happy. Is there nothing wrong with building something that only makes part of the audience happy? Well, sure, that's what you need to do to make money, right? If you want to be successful economically, you do not need to make everybody 80% happy. You need to make some people happy enough to buy this stuff. Only need to be some, right? If you can make a thousand people very, very happy, you can already be commercially extremely successful. But this is no indication that this is the system that should be built for all the use cases. So when you say people like this, I think you need a qualifier. You find that there are some people which think that the solution is good enough or it's what they've been looking for. But I also suspect that there are a great many people which thinks that Sophia is insufficient as a model of general AI or as a meditation coach or whatever. Sophia is in a way orthogonal to general AI, right? I mean, Sophia is a character in the way that say Mickey Mouse is a character, right? No, Sophia is an abomination. Sophia is a character and Sophia is a robot design and can be run with many different AI systems behind her, right? Some of which may be proto-AGIs moving toward AGI. I mean, we've used various versions of OpenCog behind her. We're going to use the next OpenCog Hyperon version behind her. Some of which are much simpler, purely narrow AI systems with no forward path toward AGI at all. Right. And the end user, when having a brief conversation, can't necessarily tell which level of system is behind her either. But my comments about Sophia were not really about the use of the robot as a tool for AGI R&D, which is a different topic. My comment was just that my experience with that robot showed that in many contexts, people would rather the AI application they're interacting with pretend and understand what's going on. And the elder care seems to be an example from pilot explorations we've done. Like you're warming the hearts of people in nursing home and in many cases, again, every person's different but you're warming people's hearts in many cases by having the robot look him in the eyes, nod and give them the impression of more understanding that that's, then it's actually there, right? And you don't have to- This is serious, you're not interested to me. I'm to lie about that. I mean, we're not lying. An animatronic puppet that pretends to look into your eyes, but is not looking and is not seeing anything. And this is my issue that Sophia does not see, Sophia does not understand. Might be at some point if she will, there is no reason why it will not at some point. But I'm not interested in the kind of product that Sophia currently is. I'm not interested in the simulacrum. I'm interested in something that is actively interactive with the world and forges its basis around. Yeah, I mean, I agree also. I mean, I am not a target market for another care robot. Basically the idea that I spent my last year is in a nursing home, but now I'm where I'm surrounded by Sophia's is as close as you can get me to my imagination of health. I really am not interested in this vision. I think it's so utterly disgusting and revolting that I really don't get what's behind it. My image of your trustworthiness is not that... So basically you are not the guy that I would trust. I'm very confident that you're imagining a worse hell than that. Yeah, I mean, you don't have to try that hard. Come on. I mean... Can I say something? I've got a bunch of robotic brains. Yes, Monika, go for it. So the thing I don't like about GPT-3 and systems of that them be able to, for instance, read something, create a brain state, and then dump that brain state back to us. And that would be the basis for a machine that does summarization. And summarization you can sell. So that's basically that's where GPT-3 and others are failing is because they are not, at least not when I last looked at them, we're capable of generating actual explanations of anything. It's just confabulation is making things up. Right. So they're generating explanations that I'd know appeal to people's amygdalas. But they're not true explanations. Like if we wanted to use AI for science, and we threw GTP, GPT-3 or 4 at it, maybe it would struggle. Because we're trying to test novel new things, not regurgitate post-modernist text, basically made from a whole heap of existing text. My view of helping science out is basically to create a system that can classify messages such as emails or chat messages from scientists to other scientists and route them to the scientist that needs to see it. This is basically AI-based routing or machine learning-based routing of messages is what my social media are going to be aiming to do. Okay, well, that's interesting, but that doesn't require the AI to actually understand the contents of the message. Correct, it only requires it understands the language. Like if you're a particular warehouse owner and you want your 12 year old grandson to learn the business, you get them in and do inventory and you basically see how they're doing it to make sure they're doing it right. The level of understanding that the kid needs to do to be, for instance, be able to sort resumes or route messages in any other way is minuscule compared to the world knowledge that they need to have. So I can do, very likely do, routing systems of messages that are very effective with very little knowledge going out beyond language. Anyway. That sounds quite plausible to me. I mean, one could perhaps come up with technical domains that would be counter examples, but that seems very plausible for almost all applications. I mean, I think, Adam, regarding trustworthy AI in general, thinking of super, of human level AGI or super AGI, right? I think there's going to be a couple of phases. Like, there's going to be a phase when we can build tools to verify the trustworthiness of the AGI. I mean, we can look at the code and understand it. We can run batteries of tests on it. We can do formal verification on code. We can formally articulate- When you say read the code, do you mean being able to make sense of the models that it produces? As well, and you can run formal verification on models and on code. I mean, you can ask for, you can formally define properties that you want, and you can run a theorem proof which is to show that the model will possess those properties under various probabilistic assumptions about the data that comes in and so on. So I think, and this is going to be quite important, right? So to, I mean, we want to be able to formally verify things about how AI algorithms and models will build, will work. And we want to sort of codify our assumptions about the world that the AI is interacting with, and then be able to formally demonstrate propositions about what properties the I will demonstrate conditional on those assumptions about the world, right? And I mean, this is hard research, but these are viable areas of research, which have a lot of overlap with the tasks of AGI itself, right? With proving other things about programs and complex systems. But there will be that phase. And then there will be a phase when it just gets too complex for humans to deal with, right? So part of the question for us going forward will be what level of trust we need to have building up from this phase when it's tractable to assess trust, right? What level of trust would we need to have to not only pull the trigger and let an AGI then evolve to the level where we know that it's just doing stuff far beyond our level of complexity and we can't really verify the trust. In order for the AI to be trustworthy, it must be able to make intelligible whatever it's whatever it's doing, right? So whether we go in and understand this code, completely false and ridiculous. Yeah. I mean, I mean, I can't make any tells me what I'm doing to my to my dog, but I'm so reliable to him. I feed him a lot of money that he correctly trust me, right? Yeah, I can't make everything I'm doing intelligible to him. I mean, if there was a foreign invasion coming to the city I live in, and the dog was eating or mating or doing something he liked, I'm gonna pick him up and drag him away to avoid him being blown up, I can't explain it to him. He doesn't understand. Eventually, if we're going to have a singularity, we'll have AGIs that are doing things and thinking things of a complexity far beyond human comprehension. And some of these things cannot be made intelligible to humans. So I mean, that's the nature of the singularity. Yeah, I get it. But I imagine if an AI was smart enough, it might be able to provide representations that are intelligible for us to make better, more informed decisions about what it's allowed. I mean, by a counting argument, it would seem naively that can't always be true, just because the number of things humans can understand is small relative to the number of things a massively superhuman supermind can think. I mean, it's not fully rigorous because the distribution of things that can happen in our physical universe is perhaps constrained in ways that we don't fully understand. But it seems, seems like that's probably not true. But all the examples we have, like I, I mean, I can, it's many, many things, I can say, we'll find a way to explain to my four yearyear-old son some things that are important and then I can't really except to a very rough approximate level. So I mean that's the best analogies we have would suggest there are limits to that process as would the most simple quantitative analysis of the situation. Yeah, there's going to be limits to what humans in their current form can understand. But what about like groups of humans? I mean, if we if an AI were to be able to doesn't matter if the AI is a trillion times smarter than us. I mean, yeah, you know, I'm the whole group of monkeys can't understand cannot understand the theory of polynomial functors, no matter how many monkeys you add. So, I mean, we may not be able to understand what an AI like a trillion times smarter than us will be able to do, but at least we'll be able to scale much further up the singularity scale before we stop on the fun part is you can understand what it did if you upgraded your brain enough but then you're not human anymore and then the super item can't explain it to the to the good old dumb monkey out of it yeah well it just have different levels of Adam being able to explain at one level down, right? Is it conceivable that everything important with respect to ethics and so on, can be understood? It seems to me that there's a pretty good chance that you're able to retrace the generation of our existence up from the Rolleat, from first principles. And if there are gaps in this understanding, the AI can probably explain it to us. I suspect that there is a tremendous level to which we could be able to understand reality. But getting back to this original question a little bit, my issue with GPT-3 is not that it is just confabulating. The issue is that it's confabulations. Our mind is also confabulating when it explains reality at every step, right? My perception, in many ways, is a confabulation. But it works because it's entangled with reality at every moment. It's being led back to whether it's able to predict the patterns that emerge. And the reason why we have not been able to build a dictionary that contains all of the English language and its grammatical rules is because the English language is changing all the time. It's an open universe. It's one in which the systems are dynamic, in which they are evolving. in which the systems are dynamic, in which they're evolving. So every AI that you're going to build will have to be entangled with the universe. Maybe not necessarily in real time, all the time, certainly not with a human-like body necessarily, but without feedback from the world, the AI is not going to be in the same universe as us, even if we update it quite often. There needs to be some connection. The other thing is, if it is meant to be ethical, then there needs to be a source for its motive, so it can generate aesthetics over its preferences. And either you want to generate aesthetics from first principles, in which case you probably need to fit in something like you want to have a universe that maximizes complexity of life or something like that, or you will subordinate its aesthetics to something else like the human hive mind or some kind of evolving specification that is put in by the human government. Sir Gorman in the chat asked about the difference between anthropomorphic and non-anthropomorphic aesthetics and ethics. So the question is, do we want the world that is co-created with the AI to preserve humanity as it is right now? Or are we willing to keep it open and just see what's going to happen? Or are we going to go transhumanist right away? When we go to Mars, we probably don't want our children to look like us because we are very unsuitable for living like Mars. We want to genetically engineer our children until they're fit to live on Mars. So we want them to be hard against radiation. We want them to be able to hibernate. We want them to be very smart and never get bored in space. And they should be able to feed on the only available protein, which is in space, which is people. So they probably should look like the aliens, right? It's very counterintuitive when you think about it, should look like the alien from the Giga alien in the alien movie. But this is probably the ideal form that our children could take if we want to settle Mars. Do we have this freedom? Do we give this a lot of freedom to evolve any way it wants, that life wants? Or do we subordinate this to a certain notion of aesthetics in the 21st century? Do we want to preserve our own society? Do we want to preserve the status quo? That's a very important, difficult question. What kind of world do we want to preserve? How do we want to keep it? What are the invariances that we care about? preserve, how do we want to keep it, what are the invariances that they care about? I mean, in a way, in a way, it's a question that feels important, in a way it's a irrelevant question, because, I mean, humanity has never preserved the status quo with the bulk of its energy and population, I mean, Australian Aboriginal populations, of course, preserved the status quo for 50 or 60,000 years, but then that's part of why they didn't emerge as the dominant global population, despite having a great depth of knowledge and culture and so many strengths, right? I mean, I think that we're clearly not focused on preserving the status quo now, even if humanity in some ways deludes itself about that. It's clearly not in practice acting in a way that's going to preserve the status quo. That's not... Yeah, but humanity itself is not an agent, it's not coherent, right? Things are going just to happen to humanity. The question is, what do you and me do to humanity? What is the system that I am building going to do to humanity? So what is the specification that I want to build into my system? I attribute less I want to build into my system. I attribute less causal importance to the human deliberative illusion of free will than you do. But this is a big digression, I guess, as to whether a human really has a lot more agency than a corporation or a nation state or something. But it's not so obvious to me. Although we do have no disagreement with this. I don't think that we have necessarily a difference there. Yeah, but in any case, it seems like in the next decades, let's say, preserving the status quo is not the vibe of what humanity seems to be doing. Some people clearly talk like they feel like we should be. I mean, preserving the good old family values and rolling back to the imagined US of the 1950s is the theme in the Republican Party, right? But I mean, it's not really what any major chunk of resources is working toward on the planet now. So to me, it's question is more still a species of that question, I think, is still important, which is as we move forward into a quite different future, I mean, which properties of the status quo do we consider critical to preserve, right? And different people will have very, very different answers on that. I mean, to me, I'm not sure any property is critical to preserve, but I do think it's important that each step from one level to the next, to the next, to the next, each step is taken with some sort of consent, will, agency, and understanding. But if the step from stage one to two, stage three to four, stage four to five, if each step is taken with consent, agency, and understanding, it changes things a bit. If by the time you get to step 100, there's essentially nothing in common with step one. There's a historical chain going between them, but there's essentially nothing in common with step one. Like there's a historical chain going between them but there's essentially no important properties in common. Then I'm by my own aesthetic and ethic. I'm not worried about that. But obviously a large chunk of the human population feels differently. And like, if we had a future world with no parent, child or no husband-wife relationship, or where people didn't need to work for a living, they would be traumatized and feel like we lost some key property of what it is to be human, which should be preserved. And I mean, that's will be a relevant question. I suspect that just the natural forces of evolution, broadly speaking, also render that question a bit irrelevant, because the way evolution probably happens is, you know, each step to the next has its own logic and constraints, and that we are going to diverge step by step radically from what we are now. And hopefully at each step the future evolute thinks it's cool. But I'm well aware my view on this is probably at odds with that of the average global citizen at this point. What kind of game condition would you like to see humans, post-humans and AI inhabit in the future? If you and some of your loved ones, or at least some of your loved ones, were to remain human? I mean at the moment like we live in a world that's very competitive. There's a lot of zero sum stuff going on. You know, people get gained from the, get utility from the, an action, which causes disutility in others, which is something I worry about. which is something I worry about. So, I mean, my utopia would be a world in which all agents wouldn't have to compete to survive. Yeah, I mean, that's a very basic statement, right? I mean, I think, indeed, from where I am now, it would seem like getting rid of physical disease and sickness, except for those demented enough to choose it, getting rid of mental illness, except for those demented enough to woefully choose it, and then getting rid of death, to woefully choose it and then having getting rid of death, having, you know, molecular assemblers or whatever that will print out whatever ordinary objects of matter you want this this way or maybe a luxury space communism. Yeah, this would be a very, very pleasant situation to live in enough physical space, you can just go with any number of like minded individuals and create a little tribe and 3D print what you want and have a good time. I mean, then the question would be, after learning all known languages, learn to play every instrument, mastering all traditions of wisdom and spirituality, like writing a few works of literature, learning all math invented by other humans. It's like after a few hundred thousand years, do you just get bored and ritually emulate yourself or something, or finally decide to just merge your human shard in with the transhuman shard that Mind uploaded 100,000 years ago when the singularity happened. The eternal search for novelty. These are answers of post-human psychology we don't really need to answer right now. But I still think the basic questions of game conditions is still relevant. You know, getting from here to there, there's certain game theories that are going to be more likely to preserve. From here to there is a whole different thing, right? I mean, having a society of abundance will radically alter human individual and group psychology, in my view, for the better, and will lead to all sorts of amazing new things. Getting, and presumably game theory as currently conceived will end up pretty irrelevant to that sort of condition, which is very different to what we've seen so far. How to get from here to there is a much, much, much funnier and more annoying problem, right? Like I keep asking who will give universal basic income to the average citizens of the Congo, right? I mean, you've got a world with ridiculous, you know, military conflagrations with half of the children in Ethiopia brainstormed due to malnutrition. I mean, you've got radical inequality on so many different levels and radical, ethical self delusion among most most everybody. And then how do we, how do we get from this to a singularity with that without having either World War Three or massive amounts of cyber terrorism or without having either World War III or massive amounts of cyber terrorism or without having a world where, you know, a hundred thousand elite people own all the robot factories and everyone else is subsistence farming in the garbage put out by the factories, right? I mean, that's how to navigate through all that in a minimally terrifying way is a serious problem. And I mean, that's certainly some sort of game theory could be relevant there, but we're not primarily dealing with rational actors, right? It's a very complex system science sort of game theory that's relevant there, which is not currently well understood. system science sort of game theory that's right over there, which is not currently well understood. But Joscha probably has his own view of that in Monaco. I think I need to drop. It's been a long day and I need to attend to local affairs a little bit before the next day starts. I thank you very much for hosting us. My pleasure. Adam, thank you very much for inviting me to this event. I very much enjoyed getting to know Monica's ideas and thoughts, and I was very glad to bump into Ben, even though it was only towards the end. And I hope to see you guys again soon. Thanks, Joshua. All right. Oh, how about you, Hugo? What have you got to say? I have not seen your face for a while, Hugo. We've been back and forth in the email a little bit. Hey. Yeah, your image position just changed. You went from upper left to upper right. I don't know why that happens. Anyway, yeah, Ben, yeah, nice to see you. Hey. When you said your son was four years old, I thought, shit, time has flown. I've got a one-year-old daughter you've never met. Yeah, yeah, we've got a one-year-old you've never met also now. So I'm continuing to produce human offspring at a faster rate than human level AGI. But of course, once you get the human level AGI, you can clone it and they'll outpace the number of humans very rapidly. Is there going to be a third one? We'll know. It depends on how quickly the singularity comes. Do you want to have one after the singularity? I mean, after the singularity, it becomes a different thing, right? You have genetically evolving populations of human-slash-transhuman minds. But yeah, this gets down to what you're talking about before. I mean, you can keep a version of Adam in roughly human form to sort of complete the journey of becoming an optimal human. And you can let a version of Adam mind upload with the superhuman God minds. So whether the version of Adam that remains human-like wants to produce more offspring is a, I mean, that's not that clear, right? I mean, we don't know what will be the psychology of reproducing one's death is no longer a thing. It'll be interesting to explore. I guess if there was a complete abundance and there wasn't a lack of resources and you know plenty of time I'd say I'd probably give it a go. It's just yeah they take a lot of time and energy right children that's why I don't have any. And have any governments anywhere in the world approached you? I mean, is there any sort of governmental interest in the topics where we're all talking about, this group is talking about? Well, there is, but what is happening, which is interesting, is that governments are approaching AGI largely through corporations with close government alliances. I know, Hugo, when you and I were working together in China, we were thinking, well, would there be like a China AGI agency competing with a US AGI agency or Russia AGI agency? And I mean, it's not quite like that, but on the other hand, I mean, you can Google Eric Schmidt and NSA. You can see that there's connections between US big tech companies and US government on various levels. Obviously Chinese government does a huge amount of advanced AI work via the teams in Tencent, Baidu and so forth. I mean, before things with Russia started going even stranger, I mean, I met with the Sberbank's AI team in Moscow who was reasonably sophisticated in there. You know, they're well aware of work on AGI and interested in it. So I think that governments are very interested. They've realized that this is technology that requires, you know, cutting edge expertise which is not what government agencies are best at, at pulling in. So they're more aligning themselves with tech companies which has various ethical pluses and minuses, I think. But you could see as the world reverts into greater tribalism in terms of like US and Western Europe versus China versus Russia becoming more partitioned rather than more unified, at least temporarily for the next immediate phase, the corporate slash government AI alliances in each of these blocks become more distinct. But I would say the focus in all these cases is on optimizing narrow AI for immediate utilization rather than on AGI. If you look in the big tech companies in either China, Russia, or US, what you see is the groups most closely aligned with government are not the AGI R&D groups. I mean, they're the groups doing pretty specific sort of data analytics related stuff. So I think that the short-termism of most governments, combined with the desire of governments to command and control rather than create things that may unpredictably slip out of their fingers, seem to be keeping AGI on the sidelines of major governmental initiatives now, which as an anarcho-libertarian by nature, I tend to think it's probably for the best and it leaves an opening for AGI to be rolled out more like Linux or the internet, sort of in the more global and fundamentally decentralized way. At least that is my own take on it, that there may be opposing views in the room. I don't mean so much on the AI community government on the commercial side. I'm thinking more on the AI threat side. Are you getting any feelers? on the AI threat side. Are you getting any feelers? I think the AI threat interests of government is really more on the practical side. I mean, I do get a lot of feelers on that, but people are concerned about drones and autonomous weapons. People are very concerned about bias in AI models of various sorts. And I feel like around the time of Nick Bostrom's book, Superintelligence, there was a flare-up of explicit concern about super AI killing everybody. And people still understand those same issues, but the focus of attention for those concerned about ethics of AI seems to have gone to like drones murdering the wrong people or AI's discriminating against people in ways that they don't realize. Attention has shifted a little bit more to more immediate practical aspects. Has there been government worry about AI taking over hacking duties or creating a whole heap of fake news, text generations being used for malicious purposes. Yeah, yeah, absolutely. And there's also government interest in using text generation for malicious purposes. And I'm sure there are government activities in that regard from all over the place. Those apps don't require AI in my opinion. Well, but the transformer neural nets are heavily used for that. I mean, it works better than Markov models. No, you use templates. They're not gonna compare from person to person. Mail templates. Yeah, well, you can use templates. I mean, transformers are being used for that heavily now, though. So, I mean, but it's a, anyway, it's a, it's very practical near-term concerns compared to the higher-end ethics concerns that Bostrom wrote about or that you wrote about in his previous books. I mean, yeah, it's pretty like to generate images. It's been pretty interesting to watch. Well, it's had a weird impact on global discourse, right? So like in, in Russia, now, in Russia, now where many people are are fed sort of fake news about what's happening in the conflict with Ukraine, if you show them real videos of people being tortured in Ukraine, they're just like, well, how do I know that's not deepfake? How do I know it's real? So, I mean, the ease of generating stuff with narrow AI technology has created a situation where people, they know they can't tell illusion from reality. So in many cases, that can make people easier to brainwash and certainly make them to a greater extent, just fall into mental ruts. Because if you throw them something that would jump them out of their way of thinking, they'll just say, well, that might be a fake, right? Now that problem could be solved by various sorts. That problem could be solved by various sorts of authentication technology. But what's interesting is the lack of motivation to solve it, right? Because the powers that be may want people to be able to tell reality from illusion, because it makes them easier to program. So the rollout of technologies to validate truth is not as aggressive as it could or should be. So it sounds to me, although I don't haven't really reached this myself, that you'd be able to tell the difference between an image by looking at the actual structure that of a you know, like, you know, not very well anymore. To a real person. No, no, no, no, no. Whether it's a real photo, it's got it's gotten beyond that, that, Adam. That was the case three years ago. There's been a whole spy versus spy thing of adversarially trying to generate models that elude the models that try to distinguish real from fake and so forth. I mean, the correct solution is to use blockchain. So, like, when I take a picture with my phone, I authenticate with my biometrics and my own private information. It's in stamp with a hash code saying like, Ben verifies this picture was taken on this hardware device at this place and time. And then that hash code is put as a watermark into the image, right? And so then it can be validated back to me. So, I mean, there are pretty clear blockchain-based solutions to authenticating images and texts and so forth. And I mean, these have been presented to Interpol. These are well understood, but they're rolled out very slowly because whose priority is it? I mean, who benefits more from ever being able to do what's accurate versus from being able to make people think whatever you can pay to make them think. So, I mean, this is more about trustworthiness of information dissemination as modulated by narrow AI among a lot of other technologies, right? But it is an interesting fact that, I mean, if the powers that be are more interested in deception than trustworthiness, even at the current level with narrow AI along with other non-AI tools, like when you get toward AGI, if it's in the hands of these same powers, then how much will they care about it being trustworthy versus care about it being useful as a tool for them to increase their power by deceiving people, right? So this again leads back to my thinking that we better have AGI be like Linux or the Internet rather than be under centralized government and corporate control is oriented toward other things rather than trustworthiness. Are you still keeping tabs on what's happening in AI research in China? Are they rapidly catching up to the US or what's your general feeling? catching up to the US or what's your general feeling? I mean in Narrow AI, absolutely, and there's more and more creative and original stuff coming out of China all the time. I mean I think the vibe there used to be that China copied stuff and scaled it up, whereas the West came up with original ideas. I think that's much less true in the last five years or so. There's more creative original innovation coming out, perhaps just due to generational shifts in China. On the other hand, for AGI in particular, there's much less support among Chinese institutions for general intelligence R&D than there is in the West. So like the people I knew in Beijing involved with AGI, they just started a journal called the Journal of Intelligence Science because general AI is not as popular there with funding sources. So I think there's more worry in China about AIs that you might not be able to control what they do. And AGI by its nature, it's gonna be harder to control what it can do. Whereas in America, we're a lot crazier. And I mean, we're more willing to entertain research on things where we may not be able to control what happens once we've launched it onto the world. So I think that there's a fundamental disharmony between a governmental system that's focused on command and control and the creation of human level AGI, which anyone with any sense can see is going to be harder to command and control. So I think for that, that's my very, very crude high-level read of the reason why you're seeing a bunch more AGI R&D in the West, even though in narrow AI, China is paving up incredibly well. Did Kazakhstan come through or? Kazakhstan is in a screwy situation. Yeah, they have not really followed through on their attempts to do advanced science, probably because oil prices have not held up. I mean, Hugo, how has your own thinking about the future of AGI and the sort of the future of global human organization and politics as related to AGI? How has that evolved since you wrote your books on these topics? Well, to be honest, after I retired, well, clearly, the last what, three, four years I've been in Australia, I split my mental effort into two. One is largely pure math. And it look, I don't know if it's, oh, well, you can see that. So I'm multitasking. So I'm annotating volume nine, which I believe is particular, well, you know, personally, you know this, Ben, but for the rest, I believe this particular theorem, which is known as the gigantic theorem in pure mathematics, is literally humanity's greatest intellectual achievement. So in other words, what I'm trying to say is I haven't given a lot of thought to this big issue. Yeah, well, the world, AI has changed a lot. The world has changed a lot. So yeah, I'd be curious how your perspective was updated after a few months of studying thought, but maybe we save that for Adam's next conference. Well, just these last two days, well, actually two nights, that have been illuminating for me. And sort of, I listen to some speakers and they make me feel incredibly ignorant. So I'm definitely falling behind, but that's what happens if you sort of branch in a different direction, I guess. Yeah, I was thinking about your old work on FPGAs lately because one of the things I'm doing is designing an AGI chip to massively speed up OpenCog Hyperon's sort of Metagraph pattern matching. And so just working with FPGA technology for prototyping chip designs, you see how far the Xilinx FPGAs have come since you were working with them before. There wasn't much onboard RAM on FPGAs back then, and now there's huge amounts, right? So, I mean, we see improvements in so many different supporting technologies. The world has changed a lot since you did all your pioneering technical work as well as wrote your futurist books. But yeah, we still have a few years for the singularity, so there's time for you to jump back into it. Speaking of hardware, can you help Monica? Oh, you were there at the time, sorry. Monica, can you give a one-minute spiel to Ben with your hardware money problem? All the work that I've done so far has been done on a Macintosh. Late 2013, I can, as I call it, the can for Macintosh. That's basically, that's since 2013, that's what, nine years of computing or something like that. And today, what I really need to move basically the languages, language competencies that I'm creating in order to move them beyond this simple proof of concept stage, I need to basically learn a lot more language. And the only constraint I have is actually RAM. I mean, I don't use GPUs at all, which means that RAM is my only constraint, but I want to keep everything in RAM. So a three terabyte machine from Supermicro with 220 cores costs about $80,000. And that's my next target machine. Of course, by the time I get the money together, they will have a new one that's going to be even better. So, but hey, that's the game I'm playing. Yeah, yeah. We build our own server farm, commanding CPUs and GPUs to run our own AI systems on. And I think if you, we've managed to hopefully maximize processing and minimize costs. But this is an issue that ties back to the corporate dominance of neuro AI and AGR&D that I mentioned. I mean, it's tens of millions of dollars to train all these transformer neural net models, right? So while the work we're doing, because of its more intelligent design, will be less wasteful of processor power than some electronic transformer, even if you're less wasteful of processing power by having better algorithms, you're still using a lot of processing power, right? And you still need aful of processing power by having better algorithms, you're still using a lot of processing power, right? And you still need a lot of memory. And yeah, that's becoming a large advantage that big tech has. And this is part of why they're able to open source all their AI algorithms, because they know that where the magic happens, where the algorithms meet the data and the processing power, they have the data, they have the processing power. So they open source the algorithms, they get everyone to improve their algorithms for free, but then they're the only ones that can put them to any use, right? And so that's a big issue, which I mean, in the short run, I'm just taking a traditional approach to that and pretty much spending money to buy processors and build custom server farms for my own projects use. But I mean, we have in the SingularityNet ecosystem, we have a project called NUNet, NUNet.io, which is an infrastructure for marshaling together processing resources from all over the place instead of protein folding at home style to pool them together for use. And this can be a route to make processing power available more cheaply to researchers who aren't in big tech companies. But that project is just getting started. It's not up to the task that you have yet. We're still only useful for simpler things at this moment, maybe in a year from now. But that's a, yeah, it's a quite frustrating issue indeed. And I mean, while we have our own server farm, which is okay, I still face the same issue. Like, I mean, if I had a half billion dollars a year to spend on, you know, having open cog systems learn things, you know, how much further, how much further along would'd be, right? But of course, I could have gone to work for a big tech company and got all that processing power, but then they would own not just the code and ideas, but they would own the team that I brought in. And I don't want things to become centralized to that extent. It's worth eating a small delay in my work to retain freedom. Thank you everybody for coming along. And yeah, thanks Heath, Ben and Monica and Hugo as well for just jumping on the panel there at the end. It's all good. Yeah, so haven't spent much time outside. As I said to you before, Ben, I just got COVID, but I think I'm on, yeah, I think I'm on the recovery. I'm going to do a rat test today and I'm hoping that if it, that I'll be negative. I should be. I'm pretty positive I'll be negative. Yeah. Well, I mean, here in the US, essentially, everyone has had COVID by now. You're sort of a freak if you haven't had it yet. Someone really knocked me around because I'm usually a pretty healthy person, but this one knocked me around. But it wasn't just me. I got COVID after having three vaccinations. I was essentially, essentially asymptomatic, which was fortunate. I will tell you this. My, my, my, my, I mean, it's terrible what COVID has done to people who, who got severe infections, especially the elderly or immunocompromised. But my worry about that is more seeing how badly the global political system dealt with this relatively mild pandemic. I mean, the development of vaccines was fast, it was sort of the bright spot, but I mean, how will the world react if there's a really bad pandemic like five years from now, right? So, what if we get something as infectious as Omicron, but as deadly as SARS was or something, whether it's human-engineered or just spontaneous mutation-based or say perhaps a lot of them, right? Yeah, I mean, if we dealt so badly with the relatively mild pandemic, what we do when something much more serious happens, which is just just one more motive to create the singularity so that we have the the robotic helping hand of superhuman AGI to help us out before something like that comes about. Sure, man. all right, thanks everybody for coming along, it's been a great. Yeah, thanks for organizing this event in spite of your temporary medical condition. I think it's good fun to keep revisiting these issues and the nuances are a little different each time as technology advances and culture evolves. So it's kind of cool to see how the nature of the discourse changes over the years. Yeah, yeah, true. True that. I'd see you back in Australia sometime too, Ben. Am I allowed in? Yep. I'd say so, yeah. Cool. Well, we'll talk about it. I'm eager to get back as well, actually. It's been a while. I've been mostly in the US the last two and a half years since COVID broke out, but there's been a few voyages out. So yeah, we should definitely do one of these things face to face the next year or something. Yeah. Yeah. Yeah. Cool, man. All right. I'll look forward to it. I'll speak to you about it offline anyway. you", '48.69160580635071')