('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/RGoaDdvsp6k"> </iframe> </center>', " out of time. Yeah. Ah. OK. I guess I've got the mic. The question is to the panel, but maybe I can start with Josh. The question is about utility function. In your view, what is the relationship between your needs and utility function. And then more generally, I'd like to ask the panel if they believe utility function is fixed, and if so, why that should be, because a lot of utility function is sort of a crucial part of AI safety in terms of whether it should be retained or not. So is it a static thing or not and how does it relate to me? Personally I'm very skeptical of this idea to define a utility function based on some kind of ultimate or first principle. For instance, if you take, like Alex Vinsner once suggested, option maximization for the future, you ultimately get a tautological principle that you can use to explain both why survival is your ultimate probability imperative, but you also need to account for somebody who would do a suicide attack to do this. So ultimately, I don't think that you get a criterion that is able to, in a meaningful way, give you a prediction on the behavior of an agent or a design guideline for a behavior of the agent based on a single principle that accounts for all the things that the mind wants to be doing. And so, in my view, utility only exists with respect to a goal. And there is no way to give an ultimate rule to derive all possible meaningful votes from. Maybe this one works. And if we think about what human minds do, probably utility is not a very good approximation. Because sometimes we do things because we woke up in a room that was slightly more yellow than the room that we had yesterday, and this might influence our behavior in a particular way, or because we had something very specific for dinner today, uh, yesterday, and this influences our behavior today. So if we want to design a utility function that is able to account for all these differences in our behavior, and our motivation and so on, we will need to put the whole cognitive architecture into the utility function, which obviates the concept of utility function or makes it pretty useless. And that's why I think it's very easy to define particular needs of a system. For instance, a system that says I need a certain amount of energy to function over time, or I need a certain amount of social interaction because I just happen to be that kind of system that needs this amount of social interaction over time. So we can easily define needs, and we can build functions that optimize for the satisfaction of several needs in a given amount of time for a given environment based on expected probabilities. But I do not believe in global utility functions. So I think I kind of agree with you. You definitely should not have a static utility function. It would not work well. Problems begin when the agent itself is capable of adjusting to a utility function. I have a paper on utility function security. It talks about wirehing and things like where agent gains the system, tries to either modify preferences of the programmer to obtain maximum utility, or just directly goes for switching payout values for the utility function to make it easier to score. So, in my opinion, still an open problem is how to actually accomplish this in a safe way. to the mobile problems, how to actually accomplish this in a safe way. I very much like Josje's approach, but I think that perhaps there are multiple ways to make AGI. And I think Josje's approach especially works well if you want to model, for instance, how humans behave. But if you want your AGI to do something that you want, then perhaps the utility function framework is a little bit more natural. Although it can obviously lead to safety issues. I also think that some of the tool, it seems like, yes, you would want to be able to have the utility function change maybe a little bit over time, but like Raman said, you don't want the AI to be able to adjust it itself. And I also think this may be problematic because it seems like there's never really a good reason to change your utility function because your utility function defines what you think is good. So if you change it, then presumably in the future you're going to pursue different goals and that doesn't really work well with what you currently want. that doesn't really work well with what you currently want. So, yeah. So, a few things. I think one of the things you asked was, do you think there is a state of the function that is stationary? And if you ask it for humans, for AI, so, For forever. For forever. So, in principle, it might be possible to have a stationary function that captures what humans want, but I think it is quite hard. And there is also a bit of confusion that it is that important to point out utility, so in reinforcement learning, there are the reward function and the value function. And the value function actually captures the expected utility, not necessarily the utility. So I'm not very familiar. I don't know the details of Trojan's work, but in his work, he talks about needs and also goals. And needs are basically things that are hard to align. And the goals are things that you use to achieve your needs. So in a certain sense you might be able to capture your needs with a stationary utility function, where the goals would be learned by expected utilities, but necessarily very more gansy. Okay, so, well, okay, here, Chris, come on. I just have a very, I'm sort of confused by the talk of the utility function. In 1972, Kenneth Arrow got the Nobel Prize for what are called the Arrow Impossibility Theorems, which say in social choice there is no such thing as utility function. It can't exist. It's impossible. That's why they're called the impossibility theorems. So why, 50 years later, are we talking about utility functions when we know that they're fundamentally impossible to use or implement or drive any sort of social or AI behavior? Or am I misunderstanding what utility function is? Nobody knows. Mostly because it works in certain contexts. So if you want to make an approximation of an agent in a given environment, if you want to have a simple model of agent interaction, very often utility function is a good proxy for detailed view of that agent. I'm not 100% sure, but I think those limitations apply to kind of voting the mathematic process with multiple agents with interfering choices. If it's a single agent, I do have top preference in those cases. So now the question is, how do we combine values of individual humans? And you're right, I don't see an easy way for it. I have a couple of comments, mainly on Yosha's presentation and ideas, but they may be things of relevance to others also. I mean, as many of you have seen in previous presentations, we've taken some variations of Yosh's motivation in actual selection models and implemented them in open cogs. We had a crude early prototype that we did in 2010, 11, and we're doing it in a more sophisticated way now. So that's caused us to work through some of these ideas in more depth than you get from reading a paper. So the first brief comment I had had to do with your comments on the mass hierarchy of needs and so forth, and you sort of said there is no hierarchy that we need to worry about all the urges are on the same level. And I think that can be true in the implementation, like you don't need to go in the hierarchy of needs, they're just all there, social affiliation, intellectual learning, and the need for electricity or food or whatever your basic survival is. But I would say that within OpenCog and probably within your system as well, this hierarchy of needs is going to emerge anyway as a consequence of the dynamics. It's not going to be infallible. You're going to have some crazy guys like the Koreans who dehydrate themselves and die due to playing Starcraft for 48 hours consecutively. But those are uncommon, they're pathologies, right? That's not the most usual emergent behavior. The most usual emergent behavior from the whole system is, you know, if I'm really starving, no matter how interesting the talks are, I'm going to run outside and miss a talk and get some food. My hypothesis would be in a well-functioning cognitive system, that hierarchy of needs in some form is going to emerge from the behavior, but you don't need to put it in. And this sort of ties in with my second point, which is that the need for or value of a sort of ties in with my second point, which is the need for or value of a sort of higher level of control dynamics above the level that you're looking at here, which probably in humans is to some extent hardwired by evolution and to some extent learn during life. And the way that we're seeing that in our use of our open side system, which is a mutation of your micro side version of Jordan's side model in the first place, is adding a layer of what we're calling micro narrative based control on top of these side based control mechanisms. So, a simple example of that in human psychology would be a behavior pattern, and this is not a tremendously productive example. You could have a behavior pattern of saying annoying things to someone until they get really pissed off, and they're about to punch you in the face or run away, then you say nice or amusing things for a little while until they laugh a bit, then you start annoying them again, over and over again. And people follow this kind of higher level behavior pattern. A more productive example is a little kid who goes, why, why, why, why, they keep asking questions over and over, and they want to get the answers, they're learning. They finally stop and they go away and stop asking any of their questions, and then they start again once you've calmed down a bit. Now these sort of micro-narrative patterns, they're like cognitive control schema that manipulate your motivations and emotions, which then drive actions indirectly. And they seem to be sort of cognitive programs that are twiddling the levers that your model provides in a systematic way over time. It would seem that we have some of these built in, which kind of structure our behavior and explain things like why we all like the same kinds of stories and structures for episodes and narratives. And to some extent, this is learned also. But I wonder how much you've thought about these both emergent hierarchies and emergent sequences and other relations among the activation of these different energies and then these higher level structures for coordinating the leverage that your system provides. Because I do think this is a good model, but it's very sort of low level, sort of animal level almost. And it seems there are much higher order structures above that. But it's not clear to what extent those are wired in versus emerging humanity, and to what extent it should be wired in versus allowed to emerge in an AGI system. So there's about 500 questions that I've got. Good? Good. Good. Yeah. Now, this thing is out of power again. Is it? Do you want me to put that? What do I have to push to get this on? Go on and push. You can go on and push. It's not a matter of preference. Is there any more questions? No, I don't. Do you have any more questions? No, but I can hear you. Okay, this is hopefully working. Obviously, there are very many ways to build AI systems that could be working and that solve this problem. For instance, it's entirely possible to build a system that does have a fixed hierarchy, so that basically does satisfy one need only if a particular other need has been completely satisfied first. So, I think that's a good point. I think that's a good point. I think that's a good point. I think that's a good point. I think that's a good point. I think that's a good point. I think that's a good point. I think that's a good point. I think that's a good point. I think that's a good point. I think that's a good point. I think that's a good point. I think that's a good point. I think that's a good point. I think that's a good point. I think that's a good point. I think that's a good point., so that basically does satisfy one need only if a particular other need has been completely satisfied first. For instance, it's possible to build a system that, before it engages on your social interaction, always checks whether its battery level is sufficient. And if this battery level is not sufficient, it completely stops the social interaction and does this first. Whether this is the case in humans, it's more or less an empirical question. The question would be if there is a single need that humans always satisfy first, and if it's unsatisfied, they don't do anything else. And I don't think that there is a single need in humans that cannot, under certain circumstances, be overridden by another need. And that's why I don't think it's a hierarchy. There are different strengths attached to the needs, and the strength of an urge signal is proportional to both the need that you have, the deviation between current value and target value, and the relative strengths of the need compared to the others. So obviously you can get something that almost looks like a hierarchy, because most people will think that hunger and thirst are more important than social interaction, but for some people it will be different, and in some circumstances it will be different. So pain avoidance is something that is extremely important for humans. Most humans will stop their social interaction if they undergo pain, and the pain is intense. They will rather prefer to go away from the pain first. But there are some people that are willing to die under torture for social goals under certain circumstances. It's very hard to model this by assuming a hierarchy of needs. Whenever you want to engage into a social goal, you first check if you're physically very comfortable. That's why I don't think it's a good model. With respect to the- I would say that's sort of a strong adversion of Maslow. Because he never claimed that there was such a rigid hierarchy of needs like that either. Yeah, but I just don't see that this pyramid is working. This is only a minor criticism on Maslow. The more fundamental one is that the things that he suggests there are often not functionalist, in the sense, I don't know what self-actualization is in terms of an algorithm. I can express what internal legitimacy is. I can easily model this. That's a very nice workshop. And it's something that I can see how my system would produce what Maslow called self-actualization. But the self-actualization as envisioned by Maslow is something that, in the way he formulated it, is something that cannot easily be implemented, because it's not entirely clear what he means by it. So this would be a slightly more fundamental criticism. I believe that everything that we do is something that we do to satisfy one or more of these needs. So if you take this example of the child that is caught in this Y loop, of course, there is some interest of the child to explore things. There's also some need of the child to get some validation from the parent by the interaction with the parent, and by getting some validation for itself by being able to engage in that interaction. And so it's kind of self-reinforcing. I found that there's a very easy way to stop a while loop though. It basically works every time. I ask my son, what do you think? And then he comes up with an explanation. It might take him some time, but it's much more satisfying for him to do this. And every time, it stops the while loop. So it seems to satisfy the need that the answer, but I'm not going to go into what you don't know because it's a long time. I'm not going to go into what you don't know because it's a long time. I'm not going to go into what you don't know because it's a long time. I'm not going to go into what you don't know because it's a long time. I'm not going to go into what you don't know because it's a long time. I'm not going to go into what you don't know because it's a long time. I'm not going to go into what you don't know because it's a long time. I'm not going to go into what you two questions, because answers are very long. A question for Jordi. You said that humans can see if the AGI is essentially planning to do something very detrimental to us, and then just kill it before it does that. is essentially planning to do something very detrimental to us, and then just kill it before it does that. But that would require an extreme level of introspection and interpretability to do that, which by default, most AGI systems don't actually even have. So how do you propose to deal with that? So first of all, I said that you couldn't really do this for like adult level AGI right so and I think that For when the system still in a kind of baby like state then it's going to be easier to See what it's doing what it's doing, what it's thinking. But of course you do need to develop the tools for that, maybe you need to develop your AI for that. And I'm also not suggesting to just create this baby, AGI, and then sort of just leave it out in the wild, but rather in a fairly restricted setting at first, where we can sort of experiment with it, and we do have the ability to do these kinds of introspection without it getting out of hand, without it getting access to all kinds of knowledge that is beyond our control. But yeah, I mean, I don't have any specific architecture in mind right now, so I don't have any specific solutions for this. If you have a purely sub-symbolic AGI architecture, like, I don't know, maybe it's a brain emulation or something and we don't really understand what's going on. But yes, of course it's going to be a problem. I agree with that. I have a question for you with respect to this. It's something I don't understand. By baby AI, we probably mean not one that is very stupid and shitty, but we mean one that is in the process of bootstrapping itself. It's at an early stage, it's currently learning and in a development process, and is about to become a fully functional adult AGI. How long do you think is AI going to stay a baby AGI? How many hours are you going to have until it's going to be a superhuman AI because it has now learned everything it can scale. I think that depends very much on what, well it depends actually on a lot of things. It depends on the actual architecture but it also depends on what knowledge and resources it has access to. So I think if you just leave your baby AGI in a box somewhere and you don't give it any stimuli, then it's going to take many, many hours. And if you give it very limited stimuli, then it might take very long. If the baby AGI gets to a point where, for instance, it can read, at which point maybe it's not baby, but it's like child AGI or something, and you release it onto the web and it has all its huge amount of working memory and insane reading speeds, then yes, maybe it can go very quickly. So I'm not suggesting to do that. OK. For combining the sets of, about combining the sets, I mean, not the sets, the values of a set of agents, I have a question. Because I think learning the values seems easier than combining the values. Do you need, don't you need to kind of implement a categorical imperative? Because if we average all the values, don't you need to kind of implement a categorical imperative? Because if we average all our values, and I act like my values are Bob gets all the money in the world, and then we average that, and then I get 1 20th of all the money in the world, wouldn't the value function have to have a parameter for the agent, so that you don't just combine them, but generalize over the value functions that you arrive at a general categorical imperative value function that you can then give an AGI. I have a question that's a good question and it's an extremely hard problem and I actually have no idea how to do it in a smart way. Also, a laser is supposed to be kind of discussed in CEU, in Hebrew. It should be the things like coherence, etc. So, it's a hard problem. Why do you want to give a human values? Why not instead decency, compassion, honesty and so on? I mean, can you formalize compassion? Yeah, I think it's much easier to formalize, this set of values that you give, like 5, 10, would capture what you actually want? Yes, I think it's much better than taking a bunch of humans and then throwing them out and then defining them as compassion. And then, Yoshua, being a discipline of moral philosophy, has spent a long time to try to formalize human values and we've very little in success. And I've always thought, this is one of the points that the Singularity Institute slash Mary has made, which makes a lot of sense. Like these human values, we wrap them up in single words, but they're actually very complex and particular sets of human ideas and response patterns. What absolute value the human notion of compassion or goodness really has is a different question. But I think wrapping up what humans mean by compassion in a formula would be very hard. And that would have to wrap up the reason why we don't send all our money to help starving and diseased people in Africa, but spend it on nice meals here in Berlin instead, for example. I mean, what humans actually do and mean by compassion seems very complex. I agree that there is a large body of work with respect to understanding and defining compassion. And unlike you, I'm not that optimistic that the people that currently look at these things in some of these futurist communities are very good at this, because they mostly ignore this existing body of work. But the existing body of work doesn't really solve the problem, either. Well, I don't think it's entirely useless with respect to this. There are some arguments for, well, I would say there are several classes of families of answers. And each of them have very long traditions of arguments, and it's worth looking at them, if this is really your goal. It's not particularly my goal, but I think having several classes of families of arguments may be a long way from having an agree on formalism to program as the goal system for an API. I'm very sorry for terminating the panel discussion, but we are out of time. There are very many questions because the topic is hot and rather controversial, but I hope you will be able to discuss it during coffee break. Thank you. Thank you.", '14.622365951538086')