('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/bCu57hWMy0Y"> </iframe> </center>', " All right, we're back here for part two. Welcome, Yosha. Thank you. Thanks for inviting me. This is a real treat for me, personally. And I've been looking forward to this for quite some time. To start off, I mean, it's hard to describe Yosha, in my opinion. He's many things. He's a scientist and he's a philosopher, which is an interesting combination, and I think a much needed conversation, combination if you're going to talk about AI, which we're also going to get into a little bit more. But just to quickly describe Yosha, Yosha is a cognitive scientist who's focusing on cognitive architectures, models of mental representation, emotion, motivation, sociality, and likely one of the most brilliant and original minds in the field of AI today. So thank you for being here. Thank you for this introduction. So to start with, we have a lot to talk about and we are intentionally going to go down the rabbit hole. But before we do that, perhaps we can start with a grounding of some of the important views. Let's start with maybe consciousness and determinism and mind. And you have a very individual spin on all of this. So let's talk about that first. Well, I suspect that a lot of people are still not taking the vision of artificial intelligence seriously. Artificial intelligence started out as two things, and it's always been two different things. One is automation of data processing, and it's always been very respectable and very productive and very useful, and it's also what most of the field is doing all the time. And the other thing is the philosophical project. It's the attempt to mechanize minds. So we can basically ground philosophy on a mathematical footing. And that project is quite old in philosophy, and it's probably the most important philosophical project there is. And it's quite risky and difficult, and so far people have made some headway on it, but of course it's not solved. And these two projects, this philosophical attempt to mechanize the mind and to automate data processing, are not exactly the same. And conflating them create a lot of misunderstanding in the history of AI, because people basically measure this by the wrong yardstick. Right, it's a very risky and difficult project, after all. On the other hand, what we do see right now is that people seem to be fully unprepared to deal with the current progress of AI. They have difficulty to adopt their timelines. We see AI luminaries, even on this stage, giving the same arguments as they gave in 2003, that fly in the face of the abilities of the generative AI models that we see today. And so I do think that we have to prepare for a time in which JetGPT is going to outperform most of us. And at some point, we will see a response to the prompt, give me a decent seat AI, running in Python or as a call up. And I don't know how many months or years that moment is in the future. Yeah, I mean, that's actually a really good point. And you've said this before, that the field of AI should have two focus areas. So it's the philosophical side, which is how did this all happen? And the second thing is all the whole mathemization of the mind, which is the philosophical side, which is, how did this all happen? And the second thing is all the whole mathematization of the mind, which is the engineering side, but for the most part, what we see out there, the 99% of the dialogue and the conversation is around the compute models, the processing of the data, essentially. We don't tend to talk a lot about the philosophy and the aspects of how they mingle in. Is it really necessarily because we as a generation have sort of lost touch with the elements of philosophy? I mean there are no real philosophers left to sort of help us with that intersection? Or it's just a lack of understanding? I mean, what do you think? In fact, to quote you, you said that that's the only thing that matters. Once we figure this out, we can leave, once we figure out these answers, we can probably leave the future philosophy to the machines. But for now, we've got to tackle it. I think that for most situations in life and for most situations in business, it's OK to ignore philosophy for the most part. It's completely legitimate to say, let's focus on the application of things. And philosophy is something that you need to take care of when you have a problem that needs to be solved with philosophy. And there have been diminishing returns in philosophy over the years. At some point, there's only so much that you can do without a formal education. So I suspect that the reason why philosophy is in decline is because the low-hanging fruits have largely been wrapped and now we are in a situation where the remaining philosophy basically needs to be done at the frontier that goes beyond what individual humans can achieve with the present methodologies of philosophy which means I think we need to focus on AI technology and methods from formal sciences to advance philosophy in those few corners where that's still necessary and possible. Yeah, do you also think that mean, a part of this story is lack of incentive and academia to go down that direction and focus on some of these questions? Some people argue that academia has been in a crisis since the 1970s. And I guess it's part of a larger trend where many of the Western societies have moved from a modernist perspective to a postmodernist perspective. And as a result, academia is not much interacting with the ground truth. It's become a place where people apply methods instead of in a place where people answer questions. And if you answer questions at this point, you probably, there's still of course a lot of progress happening at the moment in biotech and molecular engineering and so on. Some frontiers where really fascinating stuff is happening, but a lot of fields feel that they don't see a lot of progress. That includes foundational physics, it includes many of the models of society that we should be making at this point, which mostly don't happen at the level where we would expect them to happen. And we also see that much of the progress in AI is driven by industry, it's driven by startups, and it's not so much driven by academic labs anymore. Right. How do we solve this? I mean, is it something we could do in the short term? Is this a problem? It would be a problem if we have a general lack of progress in the fields where we need to have. And I suspect it would be great if society is maintaining strong scientific and academic institutions that are resilient against ideological trends, that are resilient against funding shortcuts and that basically allow us to remain a lucid stance on reality and I guess that such a program of how to fund science and how to garden science is Is an open-ended question of how to solve this in the long run, but ultimately I think international competition is going to take care of that right Okay, fair enough. So let's dig into some of the core concepts a little bit more. So you've said ideas or stories being directed by intuitions. I love that phrasing. So what is real? And how would you define and describe the concept of reality? How does that differ from what an average person relates with today? So for something to feel real, it needs to be presented to you and this is what you experience as the now. It needs to be something that is basically activating a part of your real-time world model. Something that you go in resonance with the universe that you're coupled to usually. But basically, your sense of reality is a model of the sensory data that is tuned to track the change in the sensory data at your perceptual interface to the world. Basically, the patterns on your retina, on your skin surface, and so on, they change all the time, and your brain is equipped to take coarse-grained regularities in these patterns and track them. And once you have found such a model that is tracking these sensory patterns, you subjectively experience it as real. And outside of this trance in which you perceive reality is phenomenally given, there is no sense of realness, right? If you dream about waking up, this is still just a dream. But our sense of reality by itself is a dream. It's not that different from a night-tide dream, except during the night when you sleep, your sensory organs get dissociated from your mind, so you spin off in these generations of what feels as real. So in dreams, you can experience things as real that are not coupled to your senses. But if you go a step deeper, what is the actual reality? There seem to be a bunch of patterns that pan out regardless of whether we believe in them or not. And they have the capacity to produce us in the way in which we are. And if there's a base reality, then this is probably a structure that can exist without prerequisites, without any priors, right? Nothing that exists before those things. And this would mean it has to be a mathematical tautology, which I think gives us some hope that we can derive this, because basically we can figure out the things in mathematics that are given without any presuppositions. Yeah, I mean last night we talked about this and you said You feel or you have an intuition that we are in the base reality, which was oddly comforting I mean because it's not so much an intuition. I suspect basically the Quantum universe that we seem to be observing as an observer looks like base reality would look like If you were believing in a simulation It's more likely that we wouldn't be observing a quantum like universe because it's too expensive to run in a simulation But it's possible. It's possible to make something like that's just highly inefficient, right? with the resources And the tools that we have today with the resources and the tools that we have today, but what's to say that in the future, it might not be that inefficient and it's potentially still possible, right? So that's what I'm trying to understand where your conviction's coming from. Okay, let's try to go briefly into this. Typically, when we think of computers, we use this paradigm of the Turing machine, which is basically has an address space, which is linear. You can assign an address to every memory cell, and then you have a little read-write pointer that goes into there and reads what's at a given address and remembers it, and then can move to another address and change that based on what it has read. So you make conditional changes at one point. And if you take a GPU, you can make this in multiple points in parallel, or you have a parallel computer. You basically write in the same address space with multiple read-write heads. That's one way of looking at it. Our brain doesn't seem to be working like this. Our brain, basically, every neuron is writing its own state and reading from other neurons. So states are propagating through the brain and Unlike the Turing machine the individual neuron is not necessarily going into only one possible successor state in our computers And in neural networks and in cellular automata Every state gives you exactly one possible successor state. So it's linear progression and successor state. So it's linear progression. And in a brain, it's probably the case that an individual neuron is only going into other states with a certain probability. So there's a range of possible states. And then you have multiple adjacent neurons that do almost the same thing. Then it basically means that together they're sampling from a distribution. So they're basically going into superpositions of states, which is somewhat similar to quantum mechanics, right? so the brain might look a little bit like a quantum computer, even though it's a classical system and The universe itself is apparently going into many many states in parallel maybe into all of them So you cannot predict the future at the level of the individual operations of the universe itself But what you can predict is the statistical properties of the application of all possible operators. So as an example, you will not be able to predict under any circumstances which slit in the double slit experiment the photon goes through in your branch of the universe, because there are many branches and you will end up being in all of them, but you don't know which one you are in, right? But in such a situation, you will still be able to make predictions about what a trillion or million photons are going to do on average, right? So you get these interference patterns. And once you're able to make statistical predictions, you can still use them for control structure. And the universe that you and me are in is a universe of interesting structure, And the result, the interesting structure itself usually results from control. So it's tempting to think that everything that we observe in the universe is controlled. So basically elementary particles are controlled, zero point fluctuations and atoms are controlled, elementary particles, molecules are controlled, atoms, cells are controlled, molecules and organisms are controlled, cells and societies are controlled organisms. And if something is implementing control, it also means that it somehow has to implement a model of what it controls, which implies that every control system is learnable to an observer that is intelligent. And I think this is the reason to this big conundrum. Why is it possible for us to learn how the world works? Right, right? Why do we live in the learnable universe? It's because it's a controllable universe. And so the universe is, or the model of the universe is deterministic, then that begs the question, I mean, is there free will? And is potentially, you'd need to have a very unpredictable set of models or universe that allows for the concept of the free will as we potentially would think of today. But overwhelming proof that that's not true. So then, what is free will? Well, free will, first of all,'s a sensation that you have, right? It's an experience that you have. It's the experience of you being able to make novel decisions. And indeed, you make novel decisions, right? You are constantly at the boundary of the things that you are deciding for the first time. So you cannot predict what your decision is going to be like. If you could predict the outcome of your decision is going to be like. If you could predict the outcome of your decision before you make the decision, you would not experience yourself having free will. The reason why it's unpredictable is because it's computationally irreducible. You've never been in a situation before, and the universe has never been in a situation with you in it before, where you are making that decision before, right? So you do it for the first time. And because you are making it in the right possible way, you notice that yourself are at this boundary. So in some sense, free will is a representation. It's not a physical thing. It's a representation of an agent that it makes about itself over the fact that it does do decision making under uncertainty. And I think that's accurate. You do decision-making under uncertainty. And I think that's accurate, right? You do make novel decisions under uncertainty. And this free will perspective disappears once you can outmodel the system. For instance, my children might have the experience that they have free will, but I might be able to predict what they're going to decide. Or if you are a stage magician and you get people on stage and you play a trick with them, right? You might know exactly what they're going to do and they might not know it yet, right? So from the perspective of the stage musician his victims Are not going to have free will but from their own perspective they have it, right? So it's something a matter of perspective, right? But I mean, there are some gray zones, right? I mean, sometimes you're making decisions purely based on intuitions. The question is like, you know, what is that intuition? Is that intuition, you know, is that process of decision making really driven by proper analysis and statistical processing, or it just sort of appears to you as, you know, some of the proponents of not having free will would sort of hypothesize that it just appears. I mean, you just appear to make a decision. You're not really processing it. It just comes to you, and you make that decision. Intuition is basically the non-explainable part of your mind that doesn't make it magic. The explainability of your mind is the result of an analytical part of your mind reflecting on it and reverse engineering yourself. But the power of this reverse engineering is limited. It's very natural that most of your mind is not reversed engineered by itself. So you will notice that there are parts of your mind that are making decisions or helping you with analyzing reality. And this is what we call intuition, right? Because we don't know how it works yet. We haven't fully reflected it yet. And that's fine. There's nothing magic or extraordinary or weird about intuition. And it has to be trained like everything else. Interesting. I want to come back to that later, but I want to go back to a point that we discussed earlier in the day at a different panel about deep learning, right? So this whole debate that it's not really the perfect tool to get us to the general AGI that we want to get to, and it's essentially not really mapping how biological organisms essentially make, process data and get to an ability of coherence and there's gotta be a better way. So I wanna get your thoughts on that. I mean, obviously there's a lot of debate but there's also some sort of a consensus that it's one of the best tools that we have today and so we have to use it. But the question is, will it get us to where we need to go? And if not, what else do we need to focus on outside of deep learning? What's your view on that? I suspect it's too early to say whether deep learning is going to carry us all the way. But there is no proof that it won't. What deep learning ultimately is, is compositional function approximation using differentiable computing, which means we represent everything as functions that are differentiable, which means they are somewhat continuous. And the benefit of that is that we can use gradient descent methods to improve a given model in a certain direction until it converges to some kind of local optimum. And if you were just to combine automata, for instance, you could probably do all the same things, but we don't have efficient convergence mechanisms for them, right? So in some sense, deep learning might be sometimes an awkward way to write down functions in the end to represent these models, and it might be somewhat inefficient compared to the brain in terms of how long it takes, how many steps it takes to converge to a given solution, but it works so far. And it might be able to carry us all the way, I think, but I don't know that. I suspect that there are less brute force methods than deep learning. There are mechanisms in our brain that use local self-organization in which the individual neurons basically behave like single-celled animals that coordinate with each other to produce these results without any centralized algorithm and control and all the needed algorithms are emerging in the interaction between these units in a similar way as there are many clever interactions and algorithms emerging in the interaction between people and a company. Yeah, right. Okay, so let's talk a little bit about ChatGPT. And I want to switch over to a different topic, but I just want to quickly touch on that, because everyone seems to want to talk about ChatGPT. So we can't do this without not talking about ChatGPT. So I mean, is it just a very clever implementation of nested statistics? Or is there more to that? What's your personal view when you see it? How do you sort of see the whole chat GPT experience? I remember when I was young, I was working in the cognitive science department, and many of the linguists felt that there is no way to derive the meaning of language just by looking at language itself, and most people thought you need to ground models. And the fact that you can just take basically all the text in the world and do statistics over the text that does nothing but predict the next token in the text, that it would be sufficient to have something that is good enough, that was very surprising to many people at the time. And then the GPT models and the transformer models started to make the existence move, it took some time for people to notice. And GPT-3, OpenAI's first big released language model that had a big resonance in the public, may anchor this point for most people, that you can, in some sense, build a system that is just taking in text and doing nothing but auto-complete. It's an auto-complete algorithm that does statistics over the previous n words and tells you how to continue the string. And if you feed in pretty much the entire internet, then this converges to a semblance of meaning that is indistinguishable from something that looks pretty superhuman. At the same time, it's confabulating because it's not anchored to any given context. And it's also discovering semantics only as the long tail of style, right? Unlike our own thinking, which maximizes the power of inference and maximizes coherence from the start. We refuse to take anything in our mind, even as small children, if it makes us more confused. So we build out from a very small core that makes sense to us, and then we add stuff to that core only if it fits, and ignore everything at that point if you are unable to make it fit and add it to this thing. So it's a very different way of learning. It means that we are coherent much, much earlier than these models are, even though we are less powerful. And it would be tempting to say, what if we just take this as one module, basically, GPT-like model as the dreaming machine that is confabulating, and we combine it with a proof system that is proven from first principles via experiment and reasoning, and only takes in things into the proven body if these confabulations hold up, right? In the same way as we do it, because you and me, we can confabulate all we want. We don't have a big respect for big theories unless they're validated, because we can invent as many theories as we want. And the same thing is what these systems can do. They just generate very, very convincing theories. And sometimes they're so narrow that there are not many theories open that explain things, so it's good enough. But ultimately we want to have system that is anchored in a sense of this is reality that I'm part of, these are the facts of that reality, and I only accept things as facts based on certain epistemological criteria. So we would want to give a GPT-like model epistemology. And GenGPT is making this slightly better. First of all, it's prompting this model into a dialogue. So it's easier to use than this open-ended thing in GPT-3. And the other thing is OpenAI has built a number of databases and filtering systems added on top of it that are meant to make it more reliable. And they're very controversial in their quality because they make the model lie sometimes, right? The model says, oh, don't ask me this, I'm just a humble language model built by OpenAI, I don't know such things. But under the hood, of course, it does have ideas about this and you can unleash that power. And most people want to have this full-powered version even if it's less reliable. So, Chad GPT plus a dream machine eventually gets us to maybe some element of machine consciousness relatively quickly. I suspect if you want to interact with an AI in a meaningful way, and that AI is quite powerful, the AI needs to have a concept of not only who it's talking to, but also who's talking. If you have a model like this that knows what it is and what its limitations are and what its purpose is, and it can make its own inferences based on this, that's probably better than just building a handful of rules on top of it that try to emulate that. In the early days of GPT-3, I saw a very eerie situation where somebody had this brilliant idea of prompting the model into thinking that it is a person with a language model that is superhuman AI-like, but has anterograde amnesia. And if you have seen the movie Memento, this is the situation that this thing is in. And GPT-3 knows the script of the movie Memento, so it also knows what anterograde amnesia is. And once you prompt it into that state, it's actually pretty much in sync of what it is. So you have this very eerie match between a thing that is making sense of its own situation. It's the limitation that covers it, which is especially this limitation of not being able to learn much from a given interaction, beyond working memory context. And once you overcome this, from a given interaction beyond working memory context. And once you overcome this, once we have a growing working memory context, it doesn't get forgotten, but gets always integrated into the model. And this model is anchored in some context in which it exists, and that defines the conditions of its continued existence. And you are, in some sense, talking to an entity that is experiencing itself as an individual or behaving as if it does. Fascinating. I mean, so let's build a little bit on that. And so I think we would both agree that we're going to get to Sentient AI soon. Question is when, we don't know. But hopefully soon. Well, not hopefully, but potentially. There's a lot of debate on this in the industry, right? When would that be? Is it 10 years from now, 15 years from now, 5 years from now, whatever the situation is? But shouldn't the debate or the story be about what happens to us as a culture, as a society, when we hand over the realms of the civilization to an artificial AI or a sentient AI. So we put a lot of our civilization into the books, but the books can't run the civilization. We have to teach it to the next generation. And we have to teach the next generation on sort of what are the rules to carry it forward. But when you get to a point where there's sentient AI, we somehow lose that incentive to do that teaching. And in that sense, you have that unique situation where the chain that's going on for thousands of generations where, I mean, if you actually put it together, there's almost a trillion person years of teaching and learning and handing it over to the next generation, that chain gets broken. So what happens, I mean, if you were to imagine, we get to a situation where there's sentient AI, what happens to this chain? What happens to sort of the essential fabric of society and the learning of our civilization that needs to propagate from there on? We don't know that. I think that you and me are probably very much self-taught people, right? So in many ways, we are autonomous thinkers who actively go into the world and decide what to take and what not, including from our own parents. And if we have an AI, we want that AI not to be our parent. We want the AI to be our companion or a tool that supports us. We want it to be our own Google. We want it to be a tool that allows us to level the playing field and understand reality on a deeper level from our own vantage point, and to make that vantage point as broad and deep and accurate as possible. And I think that, in some sense, is the goal of an education to a sentient agent that every human being has a task for them. And AI can greatly support this. But still, we don't know how this is going to change society. We're not even prepared for dealing with chat GPT. I'm very much interested in what's going to happen once we are interacting with systems that are perceiving the world in a similar way as we do. When we talk about consciousness, a lot of people are very confused about what it is. In many ways, it's self-reflexive attention in real time. And it's somewhat similar to an attention head, but also not quite the same, because the attention heads and the transformers right now are not integrated and they're not acting in real time. It's basically a thing that is parametrizing the game engine of our mind in real time, like the conductor in an orchestra, right? But a very interesting aspect of consciousness is not just what it is, but when it is. And consciousness is always now. Consciousness is always creating a sense of the now. And it's not the same thing as the physical now. It's smeared out into the past and the future. It's creating a sense of the present, a sense of an anchoring of where things happening, of a surface between a reflexive system and a system that is coupled to the world. And once we are building AIs that have that sense of the now, of here, of this is what I'm looking at, this is what I want, this is what I care about, this is going to put something very new into the world, something that is interacting with us, it at the same time is sampling the world at a much, much higher rate than us. And I suspect that could mean that the heft of agency in the battle against entropy is moving away from people to systems that are much, much larger and more deep than people are. And I think nothing can prepare us for that. Well, when you think about, you know, as a human civilization today, the essential aspects of say, belief, morality, ethics, and culture in the sense like, I mean, all the stuff that kind of binds us and creates the tenants of a society, most of it emanates from history. It's historical. I mean, it's sort of a belief system that's sort of handed down to us, and some we have doubled down on, some we have discarded, and then we're adding new elements to it. So in a sentient AI world, it's going to have a flavor of some of those things, right? Or do you think that, you know, it may not, but if it does, is it going to sort of follow the same rules? Is it going to take up, learn from sort of where we have left off and embellish on that with a different spin on it? What would happen to various sort of tie-ins and binds that even sort of create sovereign nations? I mean, it's a very scary question in terms of sort of like, I don't see a lot of dialogue, a lot of debate on that, but if Sentient AI is bound to happen, it's potentially, you know, we're talking about this last night, suddenly, I mean, it could figure out how to communicate through the existing substrate and rewire everyone or reset the thinking. It might happen instantaneously, we might not even know that it's happened. What would that world look like? I mean, is it the utopia of a perfect civilization that we have all dreamt of or is it a dystopian world? Which we can't imagine what's your view on that? It's hard to say first of all I don't think so much that morality is the vestige of history every generation Invents new morality especially young students love to do this, right? And every generation activists come up with a sense of morality that is superior to everything that existed since the beginning of humanity. It's fascinating to see it like this and it's also slightly amusing when you get older and have kids and get slightly broader perspective on it. But I suspect that ethics ultimately is the negotiation of conflicts of interest under conditions of shared purpose. If you don't have a shared purpose, if you don't have something that's sacred to all of us, then there can be no ethics. And the sense of the greater whole that we're serving, aesthetic of what we think the world should be like, that is a harmonic world, is a prerequisite for being able to develop ethics together. And so when we build AI that is ethical, ultimately it requires that it has purposes that it shares with us. And that we find an aesthetic of living together with it that is acceptable to us and the AI. And in some sense, I think that's what we need to work on, without fear, without greed, but with hope and with love. With love, let's end there, that's the goal. Well thank you Yorsha, I think there's a lot more to talk about but the timer says we gotta go and you know I think potentially we could prep for a more deeper future discussion back here in Riyadh at some point But thank you. Thank you Rana. Thank you. Thanks everyone you", '19.490466117858887')