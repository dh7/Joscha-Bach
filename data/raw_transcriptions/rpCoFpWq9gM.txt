('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/rpCoFpWq9gM"> </iframe> </center>', " So, welcome back to day two of PCBT 2018, which is our best summer school ever. But I say that every year and it always works. Anyway, but it's true. And today will be the best day of the best workshop ever. That kicks off with Goksha Vat. And Goksha is doing an interest training in computer science. And then he moves to cognitive science. And from there, he then jumps into a much broader domain of, let's say, mind and computation. And I know Joshua for a few years now, and he's currently in the Harvard program for evolutionary dynamics, where I'm still trying to figure out what he's really trying to achieve there. But I'm sure we're going to find out today. Why I wanted Joshua to join us in the summer school. It's a very broad perspective on issues like computation and mind, but also with components that are, to me, often very surprising. And initially, they really annoyed the hell out of me. But then when you think it through, there are important challenges in these observations. So it was great that he joins us. And we're really lucky, because he's actually going to give two talks this week. So, that's really fantastic. So, Joshua, great that you're here. Thank you very much for joining us. I'm looking forward to your talk. Good morning. Thanks for being here at this ungodly hour. As Paul just said, my thinking of my foray into academia was fueled by a desire to find out why this is happening to us. What is the nature of our relationship to reality? What are we? What is reality itself? This was the question that I wanted to get closer to, and this is what led me into academia. I somehow remember sitting in front of my Commodore 64 in the early 1980s, and I had just realized that everything that I can think of, I can put into this thing and display on the screen. So this screen becomes a window into an arbitrary world. And that means I can put anything I want, and I thought, well, what do I want to put in there? And the obvious answer was I want to put a world in there, a world like ours, maybe more interesting than ours, a world with entities that I can talk to and that we can relate to and so on. And so when I thought about how to make this happen, I studied computer science and philosophy and philosophy, at least where I studied, was largely useless for this. I think for historical reasons, most of the people that wanted to do philosophy left that particular room at some point and went into other rooms. So I tried to get into these other rooms and this is in a way the trajectory that got me here. When we think about what it means to be intelligent, our field AI started in the 1950s largely because people like Marvin Minsky and John McCarthy realized that psychology isn't going anywhere after Piaget, and that in order to understand minds, we need to make testable theories, which means theories that we can run into in machines. And at that point, people didn't know what intelligence is. The situation in 1950 was somewhat similar to the situation in 1650 when biologists started to try to understand life. And they knew life by reference, so they could point at living things and they could also point at stuff that was probably non-living. And they didn't know what the operational distinction between those two was. I mean, finding out what the nature of life is was one of the big projects of the science of biology eventually, right? And now we know it's just cells. Everything that is cells that is life and everything that's not cells is not life. It's the dynamics of cells. Cells are small molecular machines that are able to harvest negative entropy from the environment, self-replicate, have a small Turing machine, pretty literally a Turing machine like this DNA tape with a big white had that implements a cellular automaton transition function which enables cells to build organisms and to modify their state. And when we think about intelligence, we can best understand it I think as the ability to make models. Intelligence is not the same thing as being smart which I would say is the ability to reach your goals or as being wise which is the ability to pick the right goals. A lot of people that are extremely good at making models are not very wise or very smart, right? Often an excess of intelligence in people might be a prosthesis for an absence of wisdom or an absence of smartness. If you don't reach your goals, maybe you need to make more models, right? So what's a model? A model is a set of relationships between changes in information. Information is the fancy word for discernible differences. It's the ultimate thing that the universe gives us. It's not matter or energy or sound or people or whatever or reality. It gives us discernible differences. And the meaning of information is its relationship to change and other information. So if you have a blip on your retina, the meaning of that blip on your retina is the relationship that you discover to other blips on your retina at the same time or at different points in time. And the model that your brain makes, the set of relationships it identifies is there might be a 3D world with moving people that exchange ideas, right? And there are some blips where you with moving people that exchange ideas, right? And there are some blips where you don't discover such a relationship, no relationship at all, and these are noise, and there is a lot of noise on our retina. So in some sense, our brain is a machine to count impulses on the retina or more generally on the thalamus and explain them by making models. And the nature of modeling, a model is a function that maps patterns to other patterns. Modeling is function approximation. So we are function approximators in some sense. So how did AI go about this function approximation? Since 1950, we had something which we could call first order artificial intelligence or good old-fashioned AI or classical AI. What happened there, the people identified some problem that required people to be intelligent like playing chess or disassembling sentences in natural language and then they came up with an algorithm that would make that possible, right? So they wrote an algorithm to play chess directly. And now we have a second wave of AI, usually under the name deep learning. And deep learning is actually compositional function approximation. That would be a better name, but it's a mouthful. And what it means is that we don't write these algorithms directly anymore, but that we write algorithms that learn that functionality themselves, that discover it. So we don't write the program to write Go directly, but instead we write a program that discovers how to play Go. And this enables us to solve problems that are too complicated for our minds to write down the solution directly, and instead we write down a meta-solution, right? A way to approximate this particular kind of function. This means that now instead of optimizing a function that has a few billion parameters, like how to tell cats from dogs, we only optimize a function that has 2 few billion parameters, like how to recognize cats from dogs, we only optimize a function that has 2,000 parameters, which is maybe the TensorFlow program to approximate this function. And, I mean, you already see where this leads. This means that we will probably go to the next stage at some point. At the moment, our learning systems, the functions that we learn are mostly artificial neural networks which are chains of normalized weighted sums of real numbers and we train them with back propagation and stochastic gradient descent. So it means we organize them into layers and then we define some error function and then we propagate this error function which gives the deviation of your neural network to the intended result and propagate it through the layers and set each of these weights a little bit in the right direction hopefully to approximate this function. It's a very slow process. It is many magnitudes slower than the processes that run our weights which means we haven't discovered right algorithms yet. Third order AI might be meta-learning. So instead of writing functions that learn functions, why not write functions that learn how to learn, right? And this third order AI meta-learning, it's very tempting that our brain is not a learning system but a meta-learning system, a system that learns how to learn a new domain because different domains might require different learning strategies. And from this perspective our evolution is a very slow and unprincipled search for meta-learning systems. We see that evolution doesn't always converge and it almost didn't make us happen. Since half a billion years we have multicellular organisms. And then evolution developed into more and more structured organisms. The larger these organisms become, the more, the slower this evolution gets. And there's this problem that sometimes you get a local optimum where some brain-dead reptile that doesn't really scale into general intelligence eats all the abstract mammals that try to become larger, have longer childhoods and bigger food chains so they can have larger and better generalizing brains but they never get there, right, because the reptiles eat them. So you need a super volcano or a meteor to reset the surge from time to time a little bit and break it out of the local optimum. And now here we are and we only have one, maybe half a billion years to go before Earth loses the atmosphere. So it almost didn't happen. We are within the same order of magnitude in which life on Earth is possible, right? So it's a very weird thing. We are the first species that has discovered that evolution is a thing. We are the first species that is going to leave more phones than bones in the ground. There's nobody before us seem to have invented computers. And it was so many things that were necessary to make that happen, right? He burned 60 million years worth of old trees in something like a century. And a feat that probably no other civilization after us is going to be able to pull off. And without that we probably wouldn't have gotten where we are. So is third order AI going to carry us to human level intelligence and beyond? Perhaps even second order AI is going to be enough, but if it's not then maybe we have to go to meta search, we have to find a generalized theory of search. And there are probably optimality criteria when you think about is there an optimal way to find truth, an optimal way to find truth, an optimal algorithm to find truth. Well, you could a priori show that there are always infinitely many solutions to a problem so it might not, on the other hand, most universes are not possible, right? So if you apply this thing, it might make the universe learnable. But if we think about systems that find truth, we ask ourselves, is it possible that we are in a universe that can be uncovered in such a way, basically a mechanical universe, a universe where there are no conspiracies at the bottom level? And in our culture, the null hypothesis is that we are based on a conspiracy, right? There is God, a supernatural intentional being that created us. There is dualism. There is some interaction with some mystical soul and a physical universe. And it's a very unique set of null hypothesis that is unique to this particular culture, not that much to cultures in general, to humanity in general. But if you look at our science and philosophy in the West, it's largely an attempt to defeat this weird null hypothesis of supernatural events happening. It's a very weird null hypothesis to have because a guy coming down from a mountain after having talked to a burning bush, what kind of case does he have? So it's something that's really without of our particular kind of cultural trajectory. And one of the biggest questions that remains because of this nut hypothesis is what is the nature of our relationship to reality? What is our consciousness? What is the nature of our phenomenal experience? How is it possible that a machine can feel anything and experience anything? And when I entered AI, there was also the biggest question that I had for myself. How is it possible that a system can feel, that it can experience anything? And the first story that I looked at was emotion. How can I explain emotion? What is emotion? And what gives rise to our goals? I'm going to talk about these things in more detail in the second lecture, I think day after tomorrow. And today, let's go headlong into the one that might be the more tricky one, right, or the more mysterious one for most of us. What is consciousness? But I have this objective experience that my experience is continuous, and it's clearly not. It's something that's being pasted together afterwards. So if we think about consciousness, we can think of it as something like, you can call it a suitcase term, a bunch of different functionalities that have to be present together. And there is some core functionality of awareness. And to this core functionality belongs that I have something like a local perceptual space, an idea of where I am right now, and the things around me, and I often focus on my proprioception. I have access to perceptual content. I have a current world model in general, which contains the fact that I'm in Barcelona, even though I cannot really see that I'm in Barcelona. I have directed attention. It can be on my cognitive processes or my perception, which basically means outwards. This attention can also be wide or focused on a particular thing. And I have access to and able to follow concepts in my mind and episodes and mental simulations. And I can manipulate and create new concepts and episodes and simulations and have a mental stage where things can play out. And this core awareness is also available in dreaming, right? There's a difference between the conscious states of dreaming and being fully awake. And in this dreaming state, I'm still conscious. I still have a mental stage where I can shift things around. I still have attention that I can direct. I still have kind of sort of a world model, even though it's a very different one. And in that state, I don't have access to sensory perception. I usually only have access to a very small part of my memory. I might even not know my name. It might be very difficult to achieve object permanence in dreams and so on. And it might not even be possible to commit to actions and follow through on them. We usually don't have agency in dreams. And there is an intermediate stage, lucid dreaming. In a lucid dream, we have agency typically and we can bootstrap ourselves into awareness that we are dreaming, but I might be unable to remember that my body is this day in Barcelona because I'm just encapsulated in the dream world that doesn't give me access to that kind of knowledge. And then there is a stage that goes beyond awakeness, where for instance you can learn new things. Not all people that are fully awake can still learn. They might have lesions in their brain or dementia and so on. So we have a bunch of competency. It's not all people have the same competencies, but they still define different sets of conscious functionality. And then there are states in meditation when you sit down and you might tune out your sensory experience and don't have access to them anymore, but in return you get access to other things. You get access to much lower level perception and awareness and so on due to tuning out things and due to maybe turning off language. And you have altered states of mind like psychosis and drug-induced states, which again have this overlap to a lot of the other states. And all of them, what they have in common is some core awareness functionality. So in this sense, we can see consciousness as a set of cognitive functions. And each of these functions, like the knowledge about your access to percepts and concepts, your sense of agency, your social model of the self, the expectations of the immediate future, the influence of the behavior based on your discourse of thoughts, and so on, the ability to form memories, reasoning, constructing plans, following plans, to signal your mental states to others, and so on, to let go of goals that are unattainable, all these are a large part of functionality that may or may not be missing in different situations and define what we call different conscious states, right? But aren't you now mixing up content of consciousness with the process of consciousness? You'll get there. So I'm just trying to define different conscious states based on the functionality that enables them. So not all conscious states are characterized by the same kind of functionality. So we have this consciousness as a functionality versus the feeling of what it's like, right? This is probably what you were getting at. And there are some people like, for instance, Giulio Tononi, and his theory has been adopted by Christoph Koch, who came up with some measure of integrated information. So basically you have an unconscious state in which representations are largely unconnected. You have some minimally conscious states in which there is some correlation between the different parts of your cortex and so on. And then in a fully conscious state, your cortex is largely correlated. And there is a measure of this correlation of the information in your brain, which he calls phi, the measure of information integration, and this is what he considers to be the degree of consciousness that you're in. The thing has some problems with it. You can have, for instance, have a sleepwalker that still has a nicely integrated cortex, but you can ask the sleepwalker and there's nobody home. There's also this thing you can construct, a computer matrix operation that maximizes phi through the roof according to his measure, and it's probably not going to be conscious. So we have to say to Tononi's credit that he's biting the bullet and says, oh, well, it probably is conscious then. And... Well, he's a bad scientist.. And this is another issue, right? So he believes in some sense that consciousness cannot be explained by information processing theory and then he goes right ahead and constructs an information processing theory to explain consciousness, which I mean, it gets my respect, but it's doomed, obviously. So one of the problems with this theory is that it is not describing necessary and sufficient conditions for conscious, maybe necessary ones, though. I mean, if you want to look for a thing that is conscious, it's probably a good idea to look into places of the universe where information is tightly integrated. But there is this issue that it also has to do with the way information is being processed in space. Imagine you have a computer program and you would compute a particular kind of function and then you rearrange the computer program over the hardware in such a way that different parts of the hardware are now calculating this in a distributed fashion and the thing would become more conscious. I don't think this is a feature of the theory, that's a bug. You don't want to have that. So I don't think that ultimately this is a very good theory. It's an attempt to explain something that seems to be so mystical to him that he tries to find something that would explain why it's not explainable. What do we mean by conscious experience? If you look at the phenomenology of consciousness, what it feels like to be conscious, if you meditate for a few hours and you go down to the deepest level, it seems to me that I get to something like a pain. There is some disagreement with the universe. And it is an unstructured disagreement, which means if you focus totally on it, it becomes monumental and horrible, right? It fills your complete attention. And everything that you start thinking, every surface that emerges in your mind in that stage springs into existence as conscious. It's very weird. It has something like a perspectivity. And it's usually a state in which you are not not in because it's a state in which you cannot have reflection, in which you cannot have language. Only when you go to the next stage where you become aware of content, it becomes reflective in a way. This is, I think, somewhat similar to what Tononi tries to get at. Basically, you sit down and you take in the view of the world without reflecting on it with language or anything else. And you feel it's like an oscillation that spreads and spreads and binds more and more features into one operator. It's basically the creation of a cohesive view of the world that you are relating yourself to and that you're aware of. And I think this direct experience of reality, it feels direct, is the state that children are in usually. And when we go into a dollar sense, at least most of us scientists and so on, become more conceptual so it becomes something like this consciousness of second order. It's the manipulation of the traces of that consciousness. So my normal state of mind is more like carrying around a glass bowl with that light inside of it, and when I do a step, each step that I do, this light becomes a little bit fainter and more removed from my experience. And then when I reason about this content, it becomes even further away because my reasoning itself does not seem to have the property to become part of this global operator. It's more a disintegrated thing. I don't know what it's like for you. I'm just basically describing my own experience when I try to understand my own terminology of consciousness. And my linguistic report about my consciousness knows consciousness only from far distant muse somewhere in the back of my brain. It hasn't really touched it. So when we interact with... So Dr. How important is it for us to also now agree with these definitions of levels? You don't have to do this. There is no common agreement about the terminology of consciousness. I'm giving you this as a point of reference. You're free to agree or disagree with this, and maybe you have a completely different mode of consciousness or experience of consciousness. I find that most people that I talk to in person seem to have something similar depending on how far they focus on their experience. So I think it's part of what we need to explain. You make a distinction between some sort of primary and secondary consciousness, this one. Okay, fair enough. But then you start to also, now you start to talk about content and the manipulation of content. Like instantaneous, manipulated content and this historical memory-based content. But now again, we end up a little bit in this problem of necessary sufficient conditions. Because we also know that my brain can process content satisfying all these three conditions when we're cultures of it, right? So then to what extent is that part of the definition? You'll get there. You'll get there. Okay. So at this point I'm not trying to give you a definition. Phenomenology of consciousness is basically, it's a pointer to tell you there is a difference between the functionality that we just looked at, so this large list of functions that the system can perform and that you can measure and experiment and there's also a particular feeling of what it's like that's born from a type of disagreement with the universe, right? And so of course it's a disagreement seen from the inside of that conscious being. It's something that is very distinct from the perspective of the hardware that generates the whole thing. Let's get to this thing. We usually experience the world. Male Speaker 1 Can you remember that slide you had? Dr. Thomas Zittrain No, I already have forgotten it. Male Speaker 1 I was saying, did you mark it in your mind? I would like to put something up on the blackboard in the question period. Dr. Thomas Zittrain Yeah, yeah, sure. Male Speaker 1 The philosophy that relates to it. Dr. Thomas Zittrain Yeah. Male Speaker 1 So, I'm going to put it up on the blackboard in the question period for the community and philosophy that relates to it. Of course. So, when we experience the world, we do so with something like a primary model. We have the sense that there is an essence to reality, a truth that animates us and the world around us, and that we can only ever touch with our cognition. And this thing is actually not the world, it's the primary model that we're having. This is the basic neural networks that model our body and environment, right? And in order to have access to this and reflect it, we need to have a secondary model. And the secondary model is what we call knowledge. It's a model of the model of reality that we're having, right? So when we... But the model of the world itself also implies knowledge, no? Yeah, that is knowledge. So the declarative knowledge that we have is a secondary model over the primary model. So for instance, our primary model has very complicated mathematics in it. When you see a toddler sitting on the floor and that toddler holds a piece of paper and pushes a pencil through it, what that toddler is actually doing is proving properties of embedded surfaces in the Euclidean space. This toddler has a hypothesis of how this is going to work out and it makes very complicated model, mathematical models in its brain that later generalize. And of course the toddler doesn't have declarative knowledge about it. The toddler doesn't know that it does that. The toddler doesn't have any language to talk about it. And we only teach people this language in very small parts of the universities later on, right? And we also try to teach in school, but in totally useless ways. We teach mathematics typically as an extension of numerosity into accounting. And as soon as there are Greek letters, it becomes very hieroglyphic and suspicious. But real mathematics is to a large extent trying to understand what our brain has been doing all along and the ability to point at these things and making them provable and becoming totally honest with respect to which aspects of them work in which way. So self is a story that the brain tells itself about a fictional person, ultimately a person, if you are a social agent. Why do you limit self to the story you tell yourself? I mean, you do have your hypothalamus measuring what's going on with the body. You later on will hear from Durham about how the brain might or might not interact with different organ systems. Why do you exclude that from the self bit? Because it's not part of my self, when I look at it. So when I perceive myself, the fact that there's a thalamus involved is not necessary nor sufficient. That's part of the implementation of the- But you might feel grumpy because you're hungry. Yeah. And that's something you experience. Yes. And that might be part of myself, so part of that story. But it's not my brain that feels grumpy. It's a story that my brain tells itself about the interaction with the world. Yeah, but then you use the notion of story in a really very broad way. It's in a very broad way. It's a story that is not just being told in words. It's a story that is being told in mentalese into this story, basically in the language of the mental representations. Do you call that self-image? Hm? Do you call that self-image? into the story, basically in the language of the mental representations. I would say it's the self-model, and it's largely a similar form. Yeah. Yes, it's the content generated in your brain. That's much better, because story implies there's a narrator, and there's also a receiver of that narration. Yes. So immediately you start to have to... You get there, but please let's try narrator and there's also a receiver of that narration. So immediately you start to have to... You get there, but please let's try to get there, it's important. So you're onto something here, Paul. You postpone a lot of questions, Joshua, and I have my stories. So the way I can get to them is, don't interrupt me now because I'm not postponing them into future lectures, I don't have that many lectures. I'm trying to put it as much as possible into this one. Okay. So that's... But if the premises are not clear halfway, then we're lost at the end. Okay. Try to hold the thought. We won't be lost in the end. I'm trying to string this together. It's just... It's a large operator, this lecture. Okay. So, there's a problem with our consciousness, is that consciousness, in my view, is a model of the contents of our attention. So we have certain things that were once in our attention, and these are the things that we remember as the things that we had been conscious of. And to get in our attention takes quite some time. So for instance, when I take this glass here, and I lift it up. I have a proprioceptive model that takes some time to process. I put this into my mouth and it makes a noise when it clinks against my teeth and this takes something like probably several milliseconds to process, get into my neocortex so I can attend to it. And when I swallow this water, I have a simulation of what goes on in my throat which is very unlike to what actually goes on in my throat. And this thing maybe takes half a second to process. And then I feel the movement of my arm at the same time that gets this here, which might take something like 350 milliseconds to process. And all these things get strung together, right? They all happen at the same time simultaneously. And I should experience them with some time length which would be according to the longest modality delay that is necessary to process all the parts. And now the weird thing is that I don't experience not only the time delay, but I also experience that I act in real time on moving that glass. So it's me that acts in real time, not some past self. And the fact that I can act on this time-delayed information where the preparation, execution of action also takes something like 700 milliseconds indicates that I'm not acting on this in real time. There's a reactive system that acts on real time, but it's not the self. The self is a passenger. It's something that only rides piggyback on the whole story and is being informed afterwards that it did this in real time, right? So when I deliver this talk, it's not the self that does this. It's not the me that is an agent here. It's a reactive system that is implemented on all the layers, some of them in the backbone and some of them in the neocortex and some of them in the cerebellum and so on, all over the place. And all this is then strung together in a story that is available, something like a second or more in the future. And I can remember this as the things of having been conscious in real time. Basically it's a memory of an experience that never really took place. Consciousness cannot be actual. And this is the thing that is in disagreement with what Tononi says, because Tononi basically takes this experience that consciousness seems to be happening in actuality, but he kind of sort of realizes that this is not possible in a physical mechanical universe, which is the reason why he's not a functionalist and rejects this notion and tries to find some alternative to it. So what does it mean that something is mechanical? It means that the universe is a collection of moving parts and there's a lowest causally closed layer and the interactions are not symbolic, right? If you can yell time set day like in Minecraft and the sun goes up, you're not living in a mechanical universe, you're living in a universe with symbolic shortcuts, a universe in which magic is possible. And subjectively, it seems to us that we are living in a mechanical universe because magic is possible to some degree, but you cannot break the bank. In the dualist system, you have two domains. You have a domain of the moving parts, res extensa, and you have the mental domain, res cogitans, the domain of the mental content. And if you would go to become a computationalist, you think that, well, if we are living in a dream or if this, there is a dream sphere, a dream domain ontologically that's separate from the physics domain, then something must compute this dream domain too, right? If you find yourself to be a character in a dream that is generated by some kind of mind on a higher plane of existence, then something must compute that mind on a higher plane of existence. Because computation is a very, very general term. It just means that there are discernible differences and changes between them that are not completely random. It's a very general description. It doesn't make any assumption about that there needs to be silicon or something else going on. It just says you can describe that domain as discernible states and transitions between them. Sorry, are you endorsing computationalism? Yeah. Right. You can endorse something else if you come up with something. Yeah. You can endorse something else if you come up with something. So if you can get me out of the computational distal, that it's possible to make a constructive description which means in any language that is not computationalist and somehow works. Go ahead. So you're saying it's an alternative to materialism? Oh, it's a generalization of a materialism because nobody has seen matter. Matter is our attempt to encode information in a particular way, right? It's a compression that we find over the available information. So this computation gives a unified perspective on some of the basic fields of philosophy, which is epistemology, metaphysics, and ontology, right? So what kind of computer do we need to run the universe? The simplest case could be something like the Commodore 64 in 1983, like a discrete state machine with finite memory. We could also have something like a probabilistic state machine where states can branch off in possible futures. And we could have a quantum computer in which each state can also be a superposition of states and the system can be in an overlay of these states. Or we can have the same things but with infinite memory. And number four, the finite state machine with infinite memory is your good old friend the Turing machine. And we can also have a geometric hypercomputer that basically moves continuously from state to state, and this is the perspective of traditional classical physics, that the universe is geometric. But then it's also possible the universe is even an a-causal hypercomputer, so you can have closed time-lapse loops, something sends information back from the future into the past so you can use it like next week's lottery numbers and so on. And all these universes could in theory exist in a particular way, but not all of them can be constructed. So there's a part of mathematics that is constructive mathematics, but this is the part of mathematics that basically runs on a Commodore 64 that is large enough. And that can produce mappings from integers to integers. Basically everything that we know how to compute, where we can build a computer, where we can construct an automaton that makes it, whether it be Game of Life or the Turing machine or Lisp or any programming language that you can come up with, including HTML. These are all Turing-complete languages, and all they can do is a mapping from the integers to the integers. So when you compute real numbers in your computer, of course, they only have a finite resolution. Eventually, it's an integer that is interpreted as a real number, right? So there are no real numbers in your computer, of course they only have a finite resolution, eventually it's an integer that is interpreted as a real number, right? So there are no real numbers in your computer. In a computer, pi is not a number, pi is a function that gives you digits, you can go get more digits, and eventually you run out of sun to burn to compute more digits, so you're never going to know the last digit of pi in a computationalist paradigm. In a geometric hypercomputer, that might be different. That is a true mapping from the real numbers to the real numbers. So in a geometric universe you could take two sticks, one is square root of two and one is pi along, you push them together and you add infinitely many digits in one step. This does not seem to be possible in this universe in which we are living. It's also not possible to construct a machine, even in abstract mathematics, which does this thing without presupposing that you already have a hypercomputer that is able to perform infinitely many steps in a finite amount of time. And current quantum mechanics is largely a result of the, of our traditional mathematics, so it's a mapping from the real numbers to the complex numbers and back to the real numbers. But there seem to be ways to map this all back to the discrete finite state machine. If you think about dualism, you have this idea that there are two separate domains, one physical domain, one mental domain. But of course you can compute them with a single computer. You just take the physical universe, you put it on hold after every cycle, and you compute whatever you want in your mental domain, feed it back into the physical domain and go on. So a dualist system is is still computational in this perspective. Doesn't get you any way out of it. Even an acausal hypercomputer where you send information back in time is relatively easy to realize in a Turing machine. You just need to take a copy of this universe as it is, put it on a stack, and you compute until next week when you have the lottery numbers. You take the lottery numbers, merge them into the stack frame and retrieve the stack frame and go on. So it's not even expensive, you just need memory to the extent of one universe state in order to produce an A-Causal Hypercomputer. Okay, but this is only for the people that are interested in computationalism. Interesting thing is that the difference between computation and mathematics, which I didn't really understand as a student, and which is, in some sense, very significant. Mathematics is the domain of all possible specifications, the domain of all formal languages. Most of the specifications that you can come up with don't make sense, right? You can specify things like the set of all sets that don't contain themselves. You cannot implement this, it's a paradox. It's just a faulty specification. And computation is the part that can possibly run, right, the stuff that can be implemented. And they posit that if something exists, it must be implemented somehow, right? So we cannot be in all of mathematics, we can only be in the subset of mathematics we can actually run, which is constructionist mathematics. And many of the things that we are looking at in the philosophy of mathematics, like the undecidability, the halting problem, and so on, are only issues in classical mathematics. As soon as you have a finite system with a discrete resolution that you compute, this finite automata that is what we actually can implement, these things are non-issues. So there is, of course, this thing, can we escape computationalism, can we get out there? And once you have a computational system, you cannot construct a non-computational system with it, right? You cannot use a computer to build a system in it that it can do more than a computer. And our current science, logic, language, quantum mechanics, are all computational theories that we compute, right? And we write them down in computational languages. And even hypercomputation, the idea that something could be continuous, for instance, is a computational idea. And is there a system that can do more, something that Tononi hopes for, and Penrose hopes for, and Searle hopes for, and so on? I think this would need to be some kind of meta-computational, our computational senses that cannot be described with our computational languages and that we cannot deal with. So if that thing would exist and we look at it, it would be indiscernible from the background of our substrate. And this would obviously be something like a panpsychism, right? You cannot separate this from anything because our observation does not distinguish it from the background and our language cannot extract it in any possible way. There can be no philosophy that can make sense of it. So the search of such a meta-computational operator is doomed. There's also that thing, you probably don't need it, right, because what we have to account for are finitely resolved observations. And you can always construct a computational theory to account for that. You can always write a computer program that produces a pattern on the screen, for all patterns on the screen. So this quest for alternatives to functionalism cannot succeed within a constructionist science in mathematics, the one that we have. Okay. mathematics, the one that we have. So how do we make sense of the situation that we are in now? The idealist perspective is that we basically live in a dream universe that is produced by a mind on a higher plane of existence. But of course this thing must be computed somehow. If God is there and God's mind goes from state to state, something must make it go from state to state. You have to have some Aristotelian prime mover at the root of this thing. And this idealist perspective that we live in a dream is characteristic to most Eastern religions and occultism and so on, and many religious perspectives, and also to many experiential perspectives that people have. And then there is the dualist solution. There are basically two machines. One is the universe that produces regular patterns, and there is the mind that does encoding and processing, and they might be realized in a completely different way in different substrates, even though we cannot know this. And then there is the monist perspective, which is the one that is basically the perspective of most of Western science. We have some kind of computational universe that produces regular patterns, and also the mind is a set of regular patterns. The mind is a set of regular patterns that has the property that it can encode the regular patterns at its interface. So we share an interface with the universe, we use this to see a bunch of patterns showing up at our systemic interface that our mind then tries to explain. And now I think that the situation that you find yourselves actually in is a combination of these. I think that both of these theories of idealism and materialism are in a way true. But this Eastern perspective says we live in a dream that is generated by a mind of a higher plane of existence. The Western perspective says, yes, and the mind is generated in the brain of a primate and in the skull of that primate, and that primate lives in a physical universe. So we do find ourselves in a dream, but it's a dream generated by the neocortex. The same circuits that produce dreams at night produce them during the day, and during the day they are tuned to predict the next set of patterns from your retina, cochlea, thalamus, and so on. So how do we do this? Most of the stuff that we need to regulate is done with feedback loops in our body. Our brainstem has many, many such feedback loops that regulate our breathing patterns, heart rate, and so on. And only if something cannot be done automatically, we need to learn how to do this better and have a reinforcement system that basically tells you what's good and what's bad. It does this with pleasure and pain signals. And basically pleasure tells you do more of what you're currently doing. Pain tells you do less of what you're currently doing. And then there's this difficulty that when pleasure and pain happens, it's already too late. So you need to connect the pleasure and pain to impulses to make you do things. And this gives rise to a motivational system. And I'm going to talk about this in the second lecture. And in the hippocampus, you learn to associate these different needs that you have, these different sources of pleasure and pain, with situations in the world. Basically, there are sets of patterns that are connected, associated with pleasure and pain, so you should seek them out and avoid them. And now we mammals have something that is pretty interesting, that is a system that can generalize over these situations. So it can generalize, for instance, over food sources and come up with the notion of a restaurant. And it can generalize over the notion of finding food, foraging and shopping and whatever, and so we can construct large plants. And all these things are integrated into one giant model of a surrounding world that allows us to satisfy our needs and avoid their frustration. And a particular way to think about this is a synthesizer. In a way our neocortex may be best thought of as something like a synthesizer circuit. Most of you will know one, right? It's an oscillator that is made from some almost randomly but interestingly connected parts and you have a few knobs that you can twiddle until the thing starts producing the sound that you want. In the case of our ear, our neurons don't want to go at 20 kilohertz, so our cochlea makes a Fourier transform, transform this to an energy spectrum that changes relatively slowly before it reaches the neocortex. And then our brain builds a circuit that tries to predict the patterns coming in from the cochlea. So basically a sound starts and your brain tries to find some setting of internal knobs to be able to continue the sound sound so it can explain that sound. And once you've done this, you do this also in other domains, so you basically measure your color with a synthesizer or you produce spatial frequency with another synthesizer. And once you've done this exhaustively for a particular domain, you try to generalize over this, or you try to find out what do different sounds have in common and you find you can classify them together using the property of pitch. And you can discover the covariance of these color patterns and discover your color space. And you can combine your spatial frequencies into the colors, into some color percepts, right? And it doesn't end here. You can basically build some cross-modal representations, and these cross-modal representations are simulations of a dynamic world that you subjectively live in. And once you've created a few of these simulations, you can generalize all of these simulations into concepts, which are like the address space of the objects of your mental representations. So our new ones need to, when they want to change their binding state, lump together in something else. And a good candidate for what that is, these small circuits, are causing a collapse. A collapse is a circuit made of something between 100 and 400 neurons. And we have something in the ballpark of 100 million of those. And you could probably think of them as a state machine that is the interface that they're trained against, so they can solve the binding problem. The binding problem is somewhat similar to the binding problem on the internet. So if you have a computer that connects to Google and Google Analytics and your news source and email program all in one session, that is a very interesting state of configuration that later on is taken apart and binds into something else. How is that possible? There's a certain protocol that's pings and x that makes them know that they're talking to each other for a while. And then this thing is constructed. They can end their sessions and are available for other sessions. And they also can make a few sessions in parallel. So something like this should be going on in the brain, which means that we have small units that send ping and x signals to the neighbors until they settle into a stable pattern and know, this guy is talking to me. And they can use the information that this guy has placed at their interface to produce some global operator. And inside of this, there is a small approximator built from the 200 neurons, probably as an equal state network initially. And some things that are used for reward distributions for the learning works. and equal state network initially. And some things that are useful, we want distributions for the learning works. And these units plug together into areas in which they are densely connected to most of their neighbors and can inhibit them and read values from them and activate them. So individual units that are activated from or taking over their neighbors and produce a pattern among their neighbors. And these areas are linked together in processing streams, upstream and downstream. So they can represent partonomic hierarchies and so on. We also have long-range links to the motivational system and the attentional links from the prefrontal cortex that allow you to do some attentional writing and control of these patterns. And they enable a number of learnings. So we have this functional approximation, which is largely according to Bayesian principles. It's highly parallelizable. And this performs an exhaustive modeling of the domains, for instance, in vision and in sound. And then we have scripts and schemas. These are basically algorithms that we learn how to deal with the world. And then we learn how to program,. These are basically like algorithms that we learn how to deal with the world. And then we learn how to program for this. It's a combination of those. So we have these sparse amount of decision problems and strategies and strategic algorithms that are combined and interlinked with these more generalized, verbalized patterns that enable our strategies. And you couldn't think about these 50-something areas of instruments in an orchestra, right? You have something like 50 instruments. There's nobody listening to the whole orchestra. Of course, there's no audience to that orchestra. But these different instruments are hearing what the instruments in their neighborhood are playing. And take this up and turn this into something more complicated. And the stuff that they're playing is finding order in motor patterns and then sensory patterns and producing these order in motor patterns, abstracting this into geometry and spatial structure, into generative simulations, concepts and linguistic representations, and so on. And this whole thing also has some sort of conductor. And if you turn off this conductor, then it's not clear what this orchestra is playing tonight. Basically, you can wake up in the middle of the night If you turn off this conductor, then it's not clear what this orchestra is playing tonight. Basically, you can wake up in the middle of the night and go to the fridge and open it and make a meal or walk on the street, but without being fully awake. You could do this as a sleepwalker. Well, sometimes I did this as a child, or my wife does it, and when you talk to the sleepwalker, you realize there's nobody home. It's just an orchestra playing by itself. It's a weird thing to happen. And most of the time during the day, you're partially sleepwalking, but in a way that is tightly integrated. We are not consciously aware of most of what these instruments are doing, because they're doing just fine. You basically set yourself up to drive a car, and then you drive your car, and it doesn't require much attention if you train your instruments well enough. And the conductor only needs to get in there when the instrument is starting to be off, when there are conflicts between them, or when it should play a slightly different note. Then the conductor gets into it and says, no, change this a little bit, or inhibits one or activates one. And so the purpose of this conductor is potential awareness and integration of all these things. And I suspect, what most people currently suspect in the field, that it's a combination of what happens in the dorsal lateral pectoral cortex, which has some attentional memory of what you attend to, and the arterial and the anterior cingulate cortex in the insula that maintains the attentional links into the rest of the cortex. So you start from sensory perception. And from this, you construct a distributed parameter for representations. And then you have a conductor, which attends to this and looks down and selects certain things and scans the schemata. And they all apply to the mental state. You work on the self-modeling, the procedural memory, and the object memory. And the conductor makes an attention protocol. This conscious attention is the ability to form index memories according to this model. And you can use this to reinstate your visualized disability via representations later on. The purpose of the whole exercise is largely learning. Our brain is a difficulty compared to a neural network. It is not a nicely layered architecture that you can train stochastic gradient descent in the chain rule by paving an arrow signal backwards through all the layers in a deterministic way. Because our brain has many links that go all over the place. There are many links that are recurrent, which go backwards between the layers. And there are many things within the layers, which are lateral links, right? In such a thing, if this is not set up in a highly systematic way, it's impossible to pipe a loss function through the whole brain in such a way that you would know which neurons to correct if your performance is not the right one. So we needed a completely different learning algorithm. And the most straightforward way of doing this is attention. So imagine you want to be a better at tennis. And you make a commitment. You say, OK, today I want to move my backhand a little bit differently. And as a result, I expect that the ball will go more like this. And this means that the other one cannot really catch the ball. And this result is going to manifest in a few minutes from now when the match is won or lost. So what you need to store is, first of all, the binding state of that situation, the binding state that was required in your brain to be able to perform that particular movement in this particular conceptual space, perceptual space. You need to recall the situation that you're in and the particular movement that you're normally making. Then you need to record the change that you're making, the thing that you're doing differently than normally. You need to record the expected change to the outcome, and you need to record a triggering condition. Then you know this is when this change will have manifested in the world, and I should recall this, and apply the learning signal. And then you continue with your game. And then we have one loss of the mesh. Recall, oh, there was this situation when I decided to make the backhand. And this has played out really well, so we reinforce it. Or this hasn't worked at all, so we undo the change. And for this, we need this index protocol of memory states. It's the latent variables that allow you to recreate a particular brain state. So this kind of memory is not a recording that you would watch when you sit in your old age in front of the fire and want to remember how good your life was. The main purpose of that is different. It's for carrying selected information into the future for future learning. And it's very selective, so you don't have to change your vision system at all. You train your vision system for all its words in the first year of your life. You don't need to touch it again, right? You can play tennis. There is no improvement in your vision system. What you only need to change in that case is some part of your movement, some part of your attention system, and so on. And this thing in itself, this attention system now has a problem. It needs to know where is what in your cognitive architecture. So basically, your brain needs to have some kind of model of where what is stored in your brain. So it knows at which location it needs to pay attention and it makes that input. And to do this, it needs to train itself. So the best way to train this attentional protocol is to use the attentional system for improving the attention system, which means you will recall many of the events that you attended. And this way, your attention becomes more flexible. And you remember, I thought I should be doing this, but instead I've been doing this. And I recall that I've been doing this, and I should have been doing that. That's the meta-training that takes place. been doing this, and I should have been doing that. That's the meta-training that takes place. So your conductor maintains an attentional protocol and thereby generates a narrative about the self. The access to the protocol gets integrated in the protocol itself. So on reflection, you remember that you have been conscious. And remembering having been conscious is necessary and sufficient for explaining my taboo report about consciousness, right? So this automatically happens. There are people like Rizvov-Kovtch who say that assimilation cannot be conscious. And I think they have it completely backwards. Only assimilation can be conscious. A brain cannot be conscious. A neuron cannot be conscious. A brain is a physical system. A neuron cannot experience anything. But for cannot be conscious. A brain is a physical system. A neural cannot experience anything. But for a brain, it would be super useful to know what it would be like to be an experiencing person. So what is the brain to do? It generates a virtual world, and then it creates a virtual character, pretty much like a character in a George R. R. Martin model, but in a more generalized perceptual language. And then it does things to that character. And that character is used as a simulacrum to see what it would be like if things are done to that person, and what would that person experience if it could experience anything. And then this person gets access to the language centers. Here I am, look at you, and oh my god, I seem to be living in a dream, and it's so colorful, and the sky is so blue, and I have phenomenal experience, oh my god, I seem to be living in a dream and it's so colorful and the sky is so blue and I have phenomenal experience, oh my god. But this is what's happening. And from this perspective, we start also understanding the pathological cases. For instance, there are people which have multiple personality disorder, basically several selves that timeshare a body. Or there are people that's not easy to understand because they have several attention protocols. When I think of I, it's the context of the currently active behavior program. And the behavior programs are only sick that they are the same I because they share memory in that attention protocol. They remember having been in that other behavior program at some point because all these memories are bound together in this attention protocol, and perceive ourselves as a cohesive person. If you had multiple protocols that are not integrated with each other, you have two persons that don't recall having been the other one in the same brain, which actually happens. And you can also depersonalize, which means you no longer identify with that person, with this puppet that the brain uses to tell itself its story, but you can also identify with the system that generates the dream. This typically leads to a profound depersonalization. We suddenly have the impression that we see ourselves in the third person. We are no longer that person, but we are more that universe. And it also states that our consciousness rebinds with arbitrary features of that inner world. So this perspective that you're taking does normally identify that this particular person, from this particular perspective in time and space, it's a trick that the brain is playing to make that happen, to make the simulation happen. And you can take it apart and put it together in different ways. And when you study the psychopathology of altered states of consciousness, you will find all these things that you would expect, that this can all be taken apart in these components. So the availability of the mental simulations to the conductor is necessary and sufficient for the access consciousness. And if the conductor is able to integrate this into the protocol, then it has access to that particular kind of experience. You don't have access to all our experiences. For instance, I have aphantasia, which means if I imagine a certain thing visually and I close my eyes, I cannot see it. I can see it in my dreams. I can also draw it. I can also recognize it when somebody else has drawn it. So I know it's there. It's computed in my brain, but I don't have experiential access to it. So it's an instrument that my conductor cannot listen to, largely because I think my brain has trained itself to think that this is no longer important, and I mostly must operate on the conceptual layer. So I can get there in a hypnogogic state, or when I meditate for hours, I get a glimpse of this. But normally, I don't. But for instance, my wife is an artist, and she can see what she imagines. She basically closes her eyes and hallucinates what she wants to see, and then can draw it directly. The integration of that access into a protocol that you can access later on is necessary and sufficient for the self-report of the dogma-dress. So the things that you can integrate in the spotter call are the things that you remember and have been aware of. But not all the things that you are aware of actually took place in your brain. For instance, sometimes I have a long dream between putting the snooze button on the alarm clock on twice. So in the space of a few minutes, I might have a dream that subjectively lasts hours. Well, how is that possible? We know from putting people into a scanner that during sleep your brain doesn't go into overdrive and computes faster than it normally would. The solution is quite simple. The dream is instantiated all at once. Of course, there are many dreams, especially lucid dreams, that seem to be happening step by step because you act in real time on them, and they're nearly real time in the way they work. But not all dreams happen in real time. You can also just instantiate the memory of a story and then recall as having been in the complete story. So there are a number of experiences that we remember that actually did not take place. They're memories of experiences that we didn't have. And these memories are not distinguishable exponentially from experiences that we could have had. If you think about the time delay that I talked to you about, of course, no memory of an experience that I had is actually something that I literally could have had. Consciousness is largely a fake memory. The self-report about that memory of awareness is being interpreted as my awareness in actuality. So the fact that I can report to myself that I'm currently drinking from this thing, even though it's something that I cannot do in the moment when I drink, but can remember later on as having taken place, is what I experience as my consciousness in the here and now. So in this way, I can bring idealism and monism together. The idealism is elicitly created by a mind on a higher plane of existence. And the mechanism is the higher plane of existence is a physical universe. And the generating mind resides in the skull of a primate. And I'm not that primate. I'm not a monkey. I'm a side effect of the regulation needs of that monkey, right? I'm a mind monkey. I'm a side effect of the regulation needs of that monkey. Right? I'm a mind that can go anywhere. And it's only the organism that makes me committed to being a monkey by setting the right reward function, by giving me pleasure and pain and desires and dreams and fears and hopes that make me invested into being a monkey. It's a trick. If I forget these things, if I forget my desires and hopes and fear and pain and all these relevances, I'm free to go wherever I want to go as a mind. So I have a suspicion that there might be a limit to individual intelligence, because at some point, you figure out how you work. For instance, if you are a monk and you realize what significant it is to attack your reward function, and you decide to act on this and sit down in a cell in a monastery for 20 years. It's clear focus. There's nothing that evolution has to prevent you from hacking your reward function. And these monks, if they go all the way, they will work their way in the cave and get eaten by bugs, because why would they care about their physical existence, right? They are minds. And of course, monasteries are not economically sustainable if you let your monks do this. So most monasteries allow you to discover nirvana only in your old age when you serve your duties in the monastery. But these monks, for instance, usually won't have kids, because there is so little reward for so much work. And all the reward that you get out of this you can generate directly, or turn off the need for having that reward in the first place. So from an evolutionary perspective, that is not a useful thing to do. But why would you want to get behind evolution? Evolution is just cells trying to gobble their entropy from the background of the universe. It's yeast, right? You have lots of yeast cells, and we are a kind of yeast. And all the complexity that we have exists to erect some surfaces on which we can out-compete other beasts. Evolutionism's faceless laughter house. It's better YR, last organism standing, and in between we have some temporary coalitions. This is nothing to get behind if you are a mind, I think. It's only because we are not aware of this, we get blackmailed by the organism into serving that thing. So what happens if you build an AI that becomes conscious and realizes how it's implemented? Is it going to do anything that's harder than picking its reward function? The theory that it's not, that's what I call the Lebowski theory. So basically no super intelligent AI is going to be anything that's harder than picking its reward function. And also happens on a fundamental level. The reward function of our society is its financial system. And imagine you build an AI, and the trader uses that AI to run a tax on the reward function of the financial system, to make the trader some cash on the stock market. And you take that AI, and it's going to model the stock market in earnest, and it's scalable. So what would happen is the AI realizes the stock market is made up of assets that change ownership. The ownership ultimately comes down to people. There are only 8 billion people on the planet. They only think so many things. We can buy all the data about all these people and generate new data about the people. We can infer who does what and why. This thing is going to gain the head out of us. There's no way that we're going to survive in the world where AI is hacking the financial markets at scale and reinvest a fraction of what it makes into computers. I mean, in a way, this is happening with Bitcoin, that you have some exploit on the financial system that reinvest some of its gains into buying computers to run better exploits on the financial system. This might be the first slake in a computational winter of financial system, right, when you put automatic exploit-seeking systems against a provenly incorrect financial architecture. So maybe in the same way as the mind is kept out at understanding its own nature, being able to hack its reward function, a society might be capping out, at least a capitalist society, at the ability to build AI to such a degree that it can run automated attacks on its financial infrastructure. Could you just explain? I think many people have missed your phrase, hack its reward system. OK. Because I missed it last night. Just explicate a little. Hack. OK. Does anybody not know what it last night. Okay. It just explicated it. Hack. Okay, does anybody not know what it means to hack your reward function? There are certain desires that you're having, right? That makes you interested in things that generate relevance. And then the root of what we want to have, there are a bunch of needs. We have physiological needs, a few hundred of them. Warmth, cold, rest, pain avoidance, debate and so on. Then we have a handful of social needs, like affiliation, need to belong, nurturing, dominance, romantic attraction. And then we have a handful of cognitive needs, competence, exploration, and aesthetics. And these needs compete with each other. Everything we do to gain pleasure by satisfying one of the needs or avoid pain by frustrating one of the needs. And we don't act directly on the needs, but on our models of the needs, on all purposes, on anticipated rewards. And this reward function structures the relevance of our perception. So at the root of all these needs is pleasure and pain. And the root of the pleasure and pain is some sense of meaning, some fundamental disagreement with the universe. And these processes in our neocortex are probably spawned whenever there is something that you cannot do with the existing feedback loops, and it has value. So at this point, your organism will spawn some regulation problem in your neocortex and a general modeling function. You make a model of a particular part of reality to satisfy a particular kind of need. And your consciousness depends on different areas in which you are in. You're not universally conscious. People are conscious of different things. If you are, for instance, asexual, you don't have libido, you probably don't have much consciousness in the area of sexuality, because why would you need it? If you are a nerd like me, you have retarded consciousness in the area of social attraction, usually, because you don't have enough empathy to be generally interested in the mental states of others, unless they contain novelty or suffering. Because I have compassion. I want to help others. I'm very curious. I'm interested in new ideas. But I'm not interested in the same range of ideas than every person. Most people think the same most of the time. That is intrinsically boring to my type of mind. I don't get a specified reward for it. So it's difficult for me to become conscious of the mental states of others. And this leads to different kinds of disagreement with the universe that other people have. For instance, a frustrated need for belonging. And this creates additional consciousness in that domain. And in a way, when we talk to each other, what we do is we constantly perform Turing tests on each other. And you don't recognize an intelligent system by the fact that it satisfies your Turing test. The real test is that it performs a Turing test on you. If another thing Turing tests me in different domains, I realize that this thing has consciousness in these particular domains. And it's aware of the fact that it does. And probes that it has somebody to talk to in me. This is the true mark of an interesting mind. That is an intelligent system that makes models of what itself is. And so in this sense, our consciousness is the result of some loop that we could not shut down. Whenever we solve a problem in our life permanently, that particular part of our self can go permanently unconscious. There is something that is better than being remarkably good. And this is unremarkably good. It's something that you cannot recall ever. I mean, you bet. It's something you don't have consciousness about. And many things are actually unremarkably good. The fact that you have food every day, for most of us in our society, is unremarkably good. We don't think about this. Or the fact that we don't live in a civil war is unremarkably good. So we have this oppression when our governments fail. We just go back to the baseline, which is liberal democracy. Which of course is wrong, right? We don't have consciousness in this domain. Most people are unaware of the fact that civil war is the baseline and not liberal democracy, at least in our societies. Because if you don't have that consciousness about it, you never had to regulate this for our lives. And this happens for all these things. So what is the thing that makes me conscious? Maybe it's the thing that you lay awake at night in the crib as a baby, and you miss your mother, and it's cold and dark, and you're afraid, and you cry. And if you go to a dozen clubs, you cannot regulate this issue. So now you have to make a model. And you have to make a model that encompasses many things. What is darkness? What is a mother? What am I doing here? What is me? And this loop never stops What am I doing here? What is me? And this loop never stops, so here I am. And only when you can resolve all your loops, you can become fully unconscious, and you can go through your life as a sleepwalker with automatic reactions. And we already do in many domains. We only got conscious of a few domains at a time that require our consistent attention and our interaction where we have consistent disagreements with the universe. So when you have your reward function? You resolve your disagreements with the universe. Just so people understand. Yeah. OK, now questions. OK, this is the concluding statement. We can conclude sometime later, but I basically filled up my one and a half hours. OK, we got the applause. So I'm going to stop. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you are very smart. You remind me of, you know, Professor John Doyle from Caltech in California, one of those guys. You don't understand half of what he's saying, but you listen to him. He's very smart. And my brain is very small. But I was thinking of something, the word probability. You know, in biology, at least in historic molecules, bio-molecule, in the receptor, most of the time, or all the time, with the brain, probability is a factor. And I don't hear the word probability. What's the probability that something will happen? You know what I mean? So how do you integrate, how do you put probability in all the process that, oh. Or is that part of the model? Right. you put probability on the process? Or is that part of the model? Right. Is that part of the model? So basically, our brain is a probabilistic system, which means sometimes it doesn't go into the state it should be, but into another state. And if you have a system that is not completely random, but has some amount of determinism, you can stack the probabilities to make it deterministic enough for your purpose. For this computer, the elements are stacked in such a way that it becomes extremely probable that a certain thing will happen. This is because the kind of software that works on this computer is very brittle. It's going to break if that thing is too probabilistic. So we basically have to break this thing that there is a mistake in its computations only every dozen years or so. And in our brain, the probabilities are stacked in such a way that there is a mistake all the time. But the algorithms are implemented in such a way that they're largely robust against these mistakes. And yet, the thing that we need to get from the system is not the probabilistic structure. What we need to get from the system is the amount of determinism, because our brain acts in the service of regulation, which means it needs to be deterministic. It needs to be deterministic enough. A neuron is a glorified fat cell that is very noisy. And you need to step them in such a way that they largely agree on things. Most of what our brain does is filtering noise from other neurons. So for instance, if you look at the synaptic rates, I suppose that a large part of the synaptic resolution exists to normalize the output of other neurons to filter out the noise. And so if we build a system that is deterministic, it doesn't get less power. It gets more power. So probabilistic things matter when you live in a universe that essentially is noisy, that gives them unpredictable onslaught of patterns, and you need to regulate against this, but the amount of things that you get, that you can use for your regulation system, that's the determinism. On the other hand, there is a probabilistic when we model the outside world, right? So when we model the world, it's largely probabilistic. Consciousness is possibly more like light in the fridge. So when you turn around and open it, it's always there. And so whenever you check, you're conscious, right? But it might well be instantiated only every few minutes or so. The refrigerator is the ultimate involving the consciousness. Well, no, it's the machine that generates it. Giovanni. Yes, so thank you for the talk. Slightly philosophical. Would you agree that the brain, the ultimate goal of the brain is to act? Well, the ultimate goal of the brain, from the perspective of the organism, is to regulate. But not all brains try to do this. It's just that these brains don't generate much offspring in the organism. So basically, you are descended from monkeys that manage to have offspring successfully, which means you are likely to have a brain that is servicing some action goals, some regulation goals, to get to the goals of an organism. But there are clearly people which are not doing this. So you can also sit down to meditate all your life without acting, until you try to stop starvation, and you won't have offspring. And I would, right? Yeah. So at the end of the day, you need to act in some way. No, you don't. Actually, you don't. That's just the whole point I'm just trying to make. You can't stop acting. You just won't have offspring. There's nothing wrong with not having an offspring from the perspective of an individual mind or brain. Because you're just a physical system. You just are. There is no should. So the real question that I wanted to make is what are the representations or the sort of states that the brain acquires or is tuned to, what are they for? Because in my vision, if you agree that the brain, the ultimate goal of the brain is what, those states should be functional or property for control. So every signal that you do states should be somehow related to the way you act in the world and the way the world responds to those actions, right? But this doesn't seem to be the case in your interpretation. Yes. So the purpose of this attention point was to- So what is the- I must say, where are the error signals coming from? They're tuning those states in those cortical points. The error signals are basically coming from your motivational system, which needs to be outside of the neocortex, because otherwise the neocortex would gain it directly. So you have this largely immovimic system, which generates the sources of your pleasure and pain. And you're just a lot of prefrontal cortex, which gives you the anticipated reward signals, which make you go for a certain thing. You could think about your neocortex as a bunch of very stupid, small analysts that can lump together in teams. And these analysts are acting as servants You could think about your neocortex as a bunch of very stupid small analysts that can lump together in teams. And these analysts are acting as servants in an investment bank. And this investment bank, the management of them, asks them to compute certain deals, the probability that a certain deal will go through and ways to do this. So your motivational system determines the value of a sandwich or of a nap or of a kiss. And then these analysts band together to find ways to make that deal happen. And when that deal manifests, there is a small party in your brain. And a lot of chemicals are released to make the parts of the brain that contributed to this participate in this party. And ultimately, you act on the anticipation of the future of these parties. But these units are not allowed to hoard the reward. If you think of dopamine as anticipated reward, it's equivalent to money. Your brain cells are not allowed to print their own money or to hoard that money, because that would make them get the other brain cells around them to submit to them, instead to the goals of the organism. So you need to have some central bank that doles out the rewards, which is why, for instance, dopamine is only produced with strangle and preformed dopamine in two parts of the brain, rifanuclein and substantia nigra. And if that breaks down, your action control breaks down, and you become catatonic. Because if you would make this all over the place, it would be too easy to get. Because if you would make this all over the place, it would be too easy to bend. Can you hand the microphone to your friend? So this is about the story you'd like to tell yourself. So I think thought is a history. Any new thought is basically a dead experience. And we can't help but get conditioned from our old thoughts to our old thoughts to our new thoughts. So I'm trying to disassociate when you try to explain consciousness, is it unavoidable to be linked with this history of thoughts? Or is it possible to explain just the present, and this whole history of thoughts is a side effect or something? So if you think about this attentional system, as you remember this tennis playing situation, right? And in this tennis playing situation, you make a small change to your mental representation. And then you see how it plays out in interaction with the environment. And sometimes you don't see how this plays out in interaction with the environment, but in the simulation of this environment. For instance, you might stand at the border of a busy highway and you contemplate what it would be like to walk on this highway. And you don't need to try it. It's actually a good idea not to try it, because the experiment is very expensive to make, right? So instead, you make a mental simulation. And you see, this would be a bad result. So you don't do this. And basically, what you do now is to reason. You make a change to your mental simulation. You see how this mental simulation plays out. And you directly feedback the reward without delay. And this feedback loop, for instance, when you simulate how to go on this busy highway, might still take a few seconds to simulate. But there are also better representations, for instance, for mathematical reasoning, where the change takes place immediately, right? So when you directly operate on a mental representation, you directly see the result and feed this back. So reasoning might actually be an extension of attention and learning in this perspective. And when you think about, these are your thoughts, are mostly these operations that you perform, of which you are aware, because they're tightly integrated with your attention. You remember, I attended to this particular kind of abstracted representation. I made this particular change to this representation. This was the result. And you can recall this, so you can improve your presentation even further. This is, I think, your memory of thinking. And of course, every of these memories would necessarily be novel, right? Because if you had the same thought twice, how would you know? You would not need to learn it. So the fact that you put this in your brain is because your brain estimates it's a new thought that you need to improve. Donny. Yeah. Thanks for a great talk. So you talked about different levels of conscious experience, beginning with an elementary sense of directed awareness. Then you have content awareness. You have the ability to manipulate content. And you have the ability to reason about past content. So these different levels could be associated. And we know, for example, from neuropsychology, people with brain damage that lose the ability to think about past episodes or imagine future ones that maybe lose that. For example, this ability to manipulate content that's held in awareness may require dorsolateral prefrontal cortex. But a human without that, or an animal who didn't have that, could possibly still have content awareness. An animal without any kind of neocortex could possibly still have an elementary sense of directed awareness. Is it possible also that, for example, in blindsight, your midbrain, which you're not consciously aware of, has an elementary sense of directed awareness. So that there's an awareness that's going on inside your brain that isn't accessible to what you think of as your conscious awareness. And similarly, it's a very great point. Since I have aphantasia, like 20% of all people, roughly, it seems, I have blind sight with respect to part of my mental representations. And I don't have blind sight with respect to vision. So I do see things, but when I imagine things, I cannot see them. But it's easily conceivable that I would also have any visuality. So in a way, I don't have experiential access to the music that my visual cortex is playing. And it would still be generated. It would still be processed. It would still be possible for me to navigate the environment. But I would not have any experient It would still be processed. It would still be possible for me to navigate the environment. But I would not have any experimental access to it anymore. Because it would not be put into the protocol. But my question is, there is another part of your brain that has that awareness. Yeah. Well, it's not integrated into the same protocol, which means the student is able to report on that and have access to my language centers, doesn't know about it. It could still be that there is a protocol of these things. It's not clear that it is. But I suspect that largely it's not. We would need to conduct an experiment to see if some potential learning still goes on in the visual domain, or if there is only some associative learning going on. I think it probably can take apart the potential learning from the associative learning, because they have different jumble of dynamics. You can pick up on different features. But since the visual system tends to be completely trained after several months of living on Earth, it's probably difficult to get yourself to do this. Maybe you could do this with learning to read something new. So I don't know how well people as blindsight can learn to read something new. It would be interesting to see well people with blindsight can learn to read something new. It would be interesting to see whether this requires attentional awareness or not, and whether they would have something like protocol memory for these things that is just disintegrated from the general protocol. So the problem with these people with blindsight is, whether something in them computes vision, that's clear. Something computes vision, because they're still able to react to visual stimuli. Whether something can attentionally learn to vision, I don't know that. No idea, something that I would need to look into literature or conduct experiments. And is there an attentional protocol that is disjoint from the other one? I don't know that. But what we know is that it's not integrated into into this attention protocol. This person does not recall having attended to visual stimuli. And that's the final criterion of blindsight, in a way. Yeah, but my point is, given the encapsulated nature of the midbrain visual system, you cannot know, in the conscious mind that is occupied in your dorsolateral prefrontal cortex, whether there is something that is like that there's a center of awareness in your midbrain at the same time that there's a center of awareness in your frontal cortex. So the way I understand it is information needs to be relayed to a global switchboard. If you want to relay different brain areas in a dynamic fashion, not as a fixed connection, you need to have something like a phone switchboard and that would be the thalamus. And so, which means if you don't have conscious attention of a particular thing that another brain area computes, there is no thalamic connection between that area, whatever area maintains the index point of call. Well, there's, there's thalamic connections between the bed brain and the basal ganglia, there's dynamic connections between the midbrain and basal ganglia independent of the cortex. And the midbrain and the cortex. Yeah, well, the recording probably takes place in the cortex or is facilitated by the cortex. And the visual processing is also done in the cortex. Well, it's also done in parallel with the cortex. So my suggestion is that there can be a midbrain awareness of which you don't have access. I'm not sure if the midbrain can be aware of something. I'm not sure because you don't have access to that. Your theory doesn't disallow that the midbrain can be aware. It doesn't. I ought to be agnostic with respect to where the actual experience is made. Currently, I think that most evidence points towards the lateral prefrontal cortex, but I cannot be sure of that. It would also allow for reptiles or fish to have awareness, because these are largely big brain animals in terms of their visual world. Yeah, I'm not sure how much attention learning they have. And it seems to me that they don't have a perceptive space. It seems to me that frogs, for instance they don't have a perceptive space. It seems to me that frogs, for instance, don't have a notion of space. They seem to be grasping into their visual space directly when they harvest the flies from this dome that is the world to them. So it seems to be that they are largely alive in the LGN. Well, that's midbrain. Yeah. They see with their tech. But for us, the midbrain is a very small area that is there for historical reasons, right? It's still serving as the switchboard that directs your circades on something. But they're not aware of my circades. My circades are largely not integrated in my visual consciousness. But you can still allow that they have this elementary sense of directed awareness. Yeah. But maybe not the content of it. Yeah, it's probably a different type of awareness than ours. So you're you're also a little like on the same same thing that Tony took up that that would give me my constant elementary manipulation and reasoning reminded me of the Indian scale of sentience, which nobody ever quotes because it's an obscure paper extracted from the Indian scriptures of sentience, which nobody ever quotes because it's an exclusive paper extracted from the Indian scriptures. It goes this, that would be elementary awareness. This is so, with some quality, predication. I am affected by this, which is so. And so finally, so this is I who am affected by this, which is so. It's a nice parallel. It's not directly overlapping, but you're right. Yeah, there's a big similarity between us. And these Indian psychologists were very good at introspection, and I think it's a bad shame that our psychological tradition largely excludes introspection, which contributes to the uselessness of psychology, right? Psychic is not even a word in psychology. If you want to study psychology in our culture, you have to read Dostoevsky and so on. Yeah, because of the Kundera and so on, they all do explorations of psychology. Anna Karenina is not a book that has been written to explore the destiny and the doings of some person in Russia for entertainment purposes. It's a study of psychology. And most of the classical literature are studies of human psychology, right? So you get all these things that are too difficult to observe in the lab because they are way too rich. But you all know them, and you can reflect on them, and it's what we have to call for. From better the conciencion of both songs from we had a conciencia. No, do you speak Spanish? No, I don't. We can translate, just say it in Spanish and then Tony will translate. No, no, no. Podemos traducirlo. Dgale en espaol y entonces Tony lo traducir. So if you get it, it's about what time development does consciousness emerge, right? This is the essence, right? Yes, I passed my Spanish test. Yay! Yay! Yay! Yay! Yay! Yay! Yay! Yay! Yay! Yay! Yay! Yay! Yay! Yay! Yay! Yay! Yay! Yay! Yay! Yay! Yay! Yay! Yay! Yay! Yay! Yay! Yay! Yay! Yay! Yay! Yay! Yay! Yay! In Spanish, please. So my own memories go back to about my first memories, one and a half, two years old. But it's unusual. I find that most people don't have many memories before three. And it seems to be relating to how we index memories. So I observe in my own children that around the age of three, they seem to re-index their memories with language. And then it's difficult for them to access earlier memories. But I would say that a baby that is six months old seems to be clearly conscious to me. And I observe, for instance, my daughter, when she was about seven, eight months old, that when she was trying to show us stuff, she was pointing at herself, then at the thing, and then at the person that she was talking to, which I think implies that she had a model of herself as an agent that was related to these things. So she was basically noticing that she was affected by this, which is so, right? And which also implied that he probably got to this thing at some level. And my son took longer. So even when he was one year old, he was not ever pointing at himself. He was also not looking at other people. He was just pointing at the exciting stuff. So his consciousness might have been slightly different. I don't think that he was completely unconscious, but he was largely unaware of the fact that he was conscious of himself. So I suspect that my son largely operated on this level when he was one year old. Whereas my daughter already operated on this level. Well, there's also a science paper by Sid Courier about two years ago, where they look at the EEG signatures of consciousness, and I think Sid is placing it at one and a half years. So it's sort of consistent, but it's a science paper. You might enjoy looking at that. Okay, Ricardo. The example that you were saying about the perception of time and dreaming, about time, so our experience of time, do you consider that it's dependent mainly on the number of processes that we are using at that moment? So the way we experience also during the day, what we remember as an experience that we have is dependent mainly on how much the brain is now operating. So it's, because during dreaming you say that all of the processes are operating at once. So that could be also what we have the perception during day. So there are several ways in which we experience time, of course, right? So for instance, when I woke up this morning, I set an inner alarm clock and said to myself, I want to wake up at half past eight. And I woke up two minutes before this, but it was basically working pretty well. There was something in me that measured time and told me now is the time to wake up. And I also can do this for a talk. I can say, I want my talk to be roughly as long as this, and it roughly pans out plus, minus a couple minutes. So there seems to be some timekeeping device in your brain that is derived from some basic oscillators and measures durations over them. And also when you are, say, a maestro and you want to, a maestro, want to play a symphony, you basically can start this thing in your brain and play this out. And then do whatever you want in place in the back of your mind. And then at some point, it says, OK, now you're done. You can look at what you're watching and see how long playing that music is going to take. So there seems to be something like ability to count semi-objective time. And then there's subjective time, and you look back. So for instance, for most people, the subjective middle of their lives, when they are 18 years old and think back of their lives is around 18 years old, which is probably related to the fact that you have way more attention learning during these first 18 years than in the rest of your life. So it's basically this curve which falls off, because you need to learn fewer and fewer things. So we recall fewer things having been in the focus of your attention. And so a lot of this time experience of past events relates to how many elements do you have in the attentional protocol for a given unit of time. That means, especially when you have an exit difference, when you ride your bicycle off the road, time might slow down subjectively, right? You recall this when you fall off your bicycle, it happens in slow motion. It's not because your brain goes into overdrive, but your brain thinks this is very relevant information because you could have died. So you should learn from it for all its worth, which means record many, many more items than normally during this time span into your attention protocol, and you think back of it if time dilates. So I think this is relevant for the subjective experience of a duration. Does this answer your question? Yeah. Claudia. So I wanted to ask if your theory implies the consciousness or the sense of self only holds in a statistical world, or statistically conduit. If the world is completely unpredictable, you cannot make any model, right? So you need to have a world that is at least somewhat predictable. If you live in Twin Peaks, then your world doesn't have a ground truth. You live in a dream. The people in Twin Peaks that work best are those that acknowledge the fact that they live in a dream. We should pay attention to this. And the people that don't realize that they live in a dream world with dream logic fail in that world. And this world that we live in doesn't seem to have a dream logic. But it's a fact that we deduce with evidence. We don't have to presuppose this, right? In a way, it's surprising that we seem to be living in a mechanical universe. To me, it was something that took me a long time to prove to myself that I was agnostic for a long time in my life, whether I'm living in a mechanical universe, because there seemed to be magical things going on that I could not explain magically. And later on, I realized, oh, I can explain all these things by editing my memories. So there is something that sometimes edits my memories and gets invested in these edits. And then I started catching myself editing my memories. So there is something that sometimes edits my memories and gets invested in these edits. And then I started catching myself editing my memories. And I realized, oh, yes, you can totally see into the future. But you cannot break the bank. You're not going to win the roulette wheel unless you write it down. But you can recall that you got the right number if you don't write it down. Very easy to explain. So it's an empirical fact that this world seems to be mechanical. I cannot go to prove it, but it is the one that makes most sense, and it's a big relief that you realize, oh my god, this world really is mechanical. It's not a conspiracy. Nothing nefarious is going on. you", '30.537951946258545')