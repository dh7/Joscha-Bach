('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/D8wxThDlVBc"> </iframe> </center>', " So probably more of a short term question than a long term one. There was mention of different companies developing AI first. Would there be a possibility or any suggestion to buy shares in specific companies? Or someone to set up and pay off your hedge fund or multiple companies that are moving towards that faster than others? Money will be dead post-singularity. Well, I mean, the biggest group of people working toward AGI is Google DeepMind, but of course, buying shares in Alphabet, you're buying shares in a lot of different things, right? The SingularityNet project that I just described, which is a sort of decentralized economy of AIs, we're gonna do an ICO, an initial coin offering, for that later this fall, probably in November. So if you're into crypto projects, you can invest in that. Our OpenCog AI project is an open source project, so you can donate to it if you want, but you can't invest in it. And I mean, that's intentional. I really think it's better if the first AGI is not in itself a commercial enterprise. I like the open source modality. Anybody want to ask a question? One over here. You all seem to feel a very strong sense of guardianship or responsibility for what happens. How much do you think that programmers or developers working within corporations like Google have an ability to act on what they think is responsible programming? So one quick answer is, in the Manhattan Project, the scientists strongly advocated to the president at that time not to use the bomb, but they were completely ignored. So basically who owns it makes the decisions. And what the scientists think really doesn't matter. To give some positive answer, at least at least the deep mind branch of Google they have an ethics team, you know, really highly qualified scientists who research about AGI safety. So at least they are concerned with this and deal with it. Probably they will be ignored then in the end also. Well, I mean, the CEO of the company has a fiduciary responsibility to maximize shareholder value, right? So I mean, if, of course, if they have a robot that will annihilate everyone, that they probably won't release it, even if that would maximize their bank account. But where there's more of a judgment call, of course, if you're the CEO of a public company, there is some push to do what will maximize your share value. So in reality, the decisions are usually kind of borderline and complicated. But I mean, largely this is true, but even company bosses have sometimes moral values. And I mean, see Bill Gates. He donates billions of dollars. Because he's retired. He's no longer Max Muggy Shoreholder. So, it's all about power. So, professionals like healthcare professionals can say, I cannot do this, it breaches my code. What would give you power? Being the CEO, I mean, that's hard to say. I mean, honestly, AI projects, to build, to do research and discover a new algorithm you can do on your own, and a researcher could say, well, I discovered this amazing learning algorithm and I'm not going to release it. But practical engineering projects of building AGIs, it takes a lot of people to build a distributed system that can run on a lot of machines and deal with sensors and actuators and all different types of data. So this is always going to be, I think it's going to be a big team effort. So that, I mean, any one person could quit the project, but I mean, even if the project leader got cold feet and didn't like the project, but I mean, even if the project leader got cold feet and didn't like the project, I mean, if it's being successful, there's a whole team and the company around it, and these things get a certain momentum. So I don't think an individual programmer working on a project has too much power to stop a successful project if they're worried about it or something. But I do think most science and tech geeks do not want to destroy the world, right? I mean if you think about it, like the amount of passion, energy, and intelligence that goes into a startup company, if that went into doing destructive things, would see a lot more scary and powerful destructive technologies around. So I mean, overall, most people who are leading AI projects do actually want them to do good for the world and help people. The difficulties are more subtle ones, because everyone wants to do good and help people, the difficulties are more subtle ones. Because everyone wants to do good and help people while also making money for their company. And if you get a situation where there's some weird conflict there that's hard to sort out, then it's hard to say what would happen. As a counterpoint in the Manhattan Project, initially the scientists in the Manhattan Project were so worried that Germany would get the bomb first that they were working furiously to make it happen. But towards the end, it was pretty clear Germany was not pursuing the weapon. But by that point, they were so technically involved, they had lost sight of the dangers and the risks that were associated with it. That wasn't true of all of them, but it was true of the majority. There's also a difference. In fact, in the time of the Manhattan Project, most of the involved physicists were also public intellectuals that intensely reflected on the political situation and participated in the political discussions and wrote letters to their respective presidents who were consulting with them and so on. And this is a situation that seems to be very unlike the current situation. Can you imagine the current set of presidents conversing with AI nerds about how to make proper laws and these being enacted? This is just not the process that we have anymore in government countries. Obama did some of that. Yeah, but you know, if even if a reasonable country would set reasonable incentives and reasonable laws and so on, we would still have unreasonable countries like say Theresa May or North Korea who would come up with their own AI projects. We're building a robot Kim Jong-un. So another data point, there's an interesting video on the internet with Elon Musk. In the interview he says he tried to persuade Sergey Brin and Mark Zuckerberg that this was a serious problem and they just didn't think it was. And these are the people who will actually make the decisions. Another issue is that if you have nuclear bombs, if you want to build them, you need a giant infrastructure to make that happen. It doesn't only cost billions of dollars, but you also need to have all these centrifuges and access to certain raw materials and so on. This is not the case for AI. For AI, you just need reasonably large computers that are not that hard to come by, and you need to have the right ideas how to do it. And once some team has figured out how to do it, it's very, very hard to keep it under wraps. It doesn't necessarily mean that somebody has to get in there and steal the secret sauce, because most of the ideas are probably already out there. And once you have an idea how they got it to work, a number of people will follow shortly afterwards with similar solutions. It was suggested earlier that it might not be in the best interest of an AGI for the populace to know about it, and it might not also be in the best interest of the corporation that owns it. So how long do you think after we have AGI, do you think people will actually know about it? It depends on who actually is in control at that time. If they really want to keep it secret and they use the power of the AGI to ensure that happens, then I can't see why it wouldn't remain secret indefinitely. I think that's extremely unlikely, personally. I mean, it's not impossible, but it seems to me that when someone gets to, say, a toddler level AI that has common sense of a three or four year old child, the odds are very high they're going to show that off to the world and everyone will know about it, the same way we learned about AlphaGo and Watson and so forth. And then once you're at that level, then everyone in the scientific community is going to realize we're in a different phase of the game. And they're moving from toddler level AI on toward adult human level AI. And there's going to be a lot of teams working on that in parallel. And as Yoshis just said, I think once you're at that level, many teams are going to be racing. Someone will get there first, but then someone else will get there soon after. It's not very feasible that someone's going to have a team of top AGI researchers working sealed off in a basement somewhere and not telling anyone what they're doing. It's not impossible, but the way things seem to be working is this work is being done in companies or open source projects with a lot of turnover of staff and leakage of ideas, which just increases the odds that similar breakthroughs are occurring in a lot of different places. And that's certainly what we're seeing in the AI community right now anyway. But if Google will build AI, then we will not only know the day they build it, Ray Kurzweil will tell us about it 30 years before that. I'll just point out that all of these corporations keep their major projects very much under wraps. So even though I live in Silicon Valley, I don't really know what Google is doing in the AI area. Well, I know what Google is doing in the AI area. I know what they say they're doing. How often do you visit their office? Never. Okay. Companies and corporations are supported by the people who consume their products. If we consume Facebook products or Google products, we're going to be affected by the people who consume their products. If we consume Facebook products or Google products, then we are supporting Google's and Facebook's vision of what AGI is and what its initial commands and moral framework will become. What can I do as a consumer to make ethical choices? And are there any companies, corporations, individuals, or otherwise that you would recommend to pursue ethical AGI development? That's a hard one. Well, Tesla, probably. I think that Elon Musk is the one who's most concerned with humanity's fate and most driven by this and in control. I think that Google was originally built with a transhumanist vision, too. Of course, these are machines. Google and all the other corporations are, in some sense, autonomous agents that are built to generate money. And people and shareholders have limited influence on this, because they are part of an evolutionary environment in which the fitness function is how much shareholder value they can generate and if you are a large corporation this limits your scope and your activity if you want to stay in business. I mean you could read sort of you know the small print when you you know always you know have ten pages and you think I accept accept you know some companies have better privacy policies. I mean, just as an example, if you use chat systems, there's so many out there. And some just grab your phone book and distribute it to everyone. And some give you a choice as an example. But I mean, it doesn't really help you. If you want to communicate with your Facebook friends You need Facebook. You cannot go to a different platform, so Yeah, it's your choice as a consumer is very limited or your power as a consumer What can you do as a consumer to make Apple have decent ports and not something from a parallel universe nothing? No, don't buy Apple. I mean there's a real alternative? Yeah, but I think that Apple will not change its view. Yeah, but, well, then they will go down in the long run. Maybe not because of the ports, but because of other things, right? How can you convince Microsoft then to have Mac OS or something? Basically, as a consumer, you only have a very, very small marginal... Yeah, but in this case, I mean, sort of, we have Android, we have sort of Windows, and we have iOS, right? In this case, at least the consumer has a choice, and more or less you can't choose, right? If I want to communicate with my Facebook friends, I need Facebook. I cannot use something else. There is no choice. But if you want to change something, I think you need to go to the top. It's like in society, as a voter, you cannot change society to a large degree. As somebody who takes to the streets, you cannot change society. Voting is a way to give legitimacy to something. It's not a way to change it or to find the best policy. If you want to change something, you need to talk to the people in charge or become one of them. So over the late, we don't really have any power about what's going to happen? No, you have a lot of power. You can get to power. You are very powerful. You are a smart person. You can start a company. You can walk your way to Davos. You can talk to people that take you seriously if you have something serious to say. Yeah, but not everyone can sort of become politically active, you know, for as their primary job, right? So the question is what can the majority of people do who have sort of, you know, maybe 1% of their time to spare, right? And who have sort of maybe 1% of their time to spare, right? And limited. There's some choice you can make with products, right? And just stay away from them, which have bad policies. But it is very hard in some cases, like messenger platforms. But easier. This ties into my motivation to create AGI open source more like Linux than like OSX or Windows. I mean, that doesn't guarantee that it's made ethically and for the common good either, but it does mean that it's not locked into some corporate organism whose explicit goal is to maximize shareholder value at the expense of everything else. So I mean it at least avoids having the AGI locked into an essentially malevolent goal structure, but it doesn't, it just leaves things more open, it doesn't guarantee a good outcome either. So yeah, I would say in my view the best hope is to develop AGI open source and then have a wide variety of different parties leverage that to do good in a variety of different ways. So I mean as an example, now we're pretty far from human level AGI, but our OpenCog, open source AI system, after our Ethiopian software developers started working on it on an outsourcing basis, some undergrad students there took the OpenCog system and put it on some Raspberry Pi, which was put in the toy robot, which is now being sold to universities in Ethiopia for education purposes, right? So there you put some AI software open source and it lets students take it and then use it to do things in their own country by hacking and messing with it. So if you multiply that by a million, right, then maybe you have something that can really disseminate AI technology for good as it develops. So I wanted to bring up something that's more of an immediate issue. As you probably know, the open AI letter on autonomous weapons came out about a year ago. One of the things that I was thinking about was the idea of a new technology that would be able to be used in the real world. And I think that's a really good idea. I think that's a good idea. I think that's a good idea. I think that's a good idea. I think that's a good idea. I think that's a good idea. I think that's a good idea. I think that's a good idea. I think that's a good idea. I think that's a good idea. I think that's a good idea. I think that's a good idea. I think that's a good idea. I think that's a good idea. I think that's a good idea autonomous weapons came out about a year ago. One of the things they talked about was how autonomous weapons are going to be the Kalashnikovs of the future. As you mentioned, you don't need a cyclotron or anything to have an AI that can fly, do face recognition, fly a drone, and do targeted assassination. And when terrorists get a hold of these, that's something that I think we really do have to worry about now. I worry more about the US government having a hold of them. They seem to have a lot and to be blowing up a lot of people. Yeah, well sure, but the problem is that they're much more portable than nuclear weapons, right? And they're cheaper. And so just about anybody could get one. I mean, right now ISIS is using autonomous, or not autonomous, sorry, they're using drones to fly bombs into people. They're not AI drones, but it's not that far a stretch to think about, they're able to see better than us, they're probably then going to be able to aim better than us and we've already weaponized them when they're driving our cars. Well, I agree with your overall point. If AGI gets out in the form of many separate AGIs that anyone can replicate, then even though the vast majority of people would only use them for useful, benign purposes, you don't need many radicals with crazy ideas to use it in a malignant way. And because it's so much more powerful, the damage it could do would be so much greater. And this has been true throughout history. As military technology gets better at killing people, the ability for a small number of people using it to inflict enormous damage increases. I don't think the main risk, you said it's a near-term question. I mean, the main risk is not much to do with AGI. I mean, I lived in DC for nine years and did a bunch of AI work for various government agencies, some for INSCOM, Army Intelligence. And I mean, what you see there is the military wants AIs that will act according to doctrine and obey orders and be quite precise in what they do. My own gut feeling is that's not especially compatible with being at the vanguard of AGI R&D. I mean, early stage AGIs are going to make mistakes and have to experiment and learn and be a bit unpredictable. I mean, what the military wants is something that will always act within certain constraints. They're not going to be in the vanguard of AGI, and they're not going to be in the vanguard of AGI, and they're not now. It's other commercial companies. That they're going to create narrow AI killing machines that carry out specific recognition and movement and planning tasks oriented toward spying on who they want to spy on and then killing who they want to kill. And it's a narrow AI mixed with other technologies problem and by the time you get to, what I think will happen is some other sector of the economy will get to AGI while the military is making powerful narrow AI killing machines. I'm not talking about the AGI. Yeah, so the hope in my mind is that the AGIs that are built outside the military will be benevolent and will then become more of a force in society than the human-controlled militaries' killer, narrow AIs. But I'm also regarding ISIS, though. It's true, ISIS has some drones. But I mean, like US and Chinese army have a lot nastier drones, right? So I mean, so far, it seems it's the governments that are at the vanguard of sophisticated killer technology, not random terrorist groups. And I have no reason to suspect that that will not tend to continue. Right? So, I mean, and if that's true, then governments will keep on winning. Whether you want to consider governments terrorist organizations as well as a matter of your politics, I guess. I have what might be a very naive question, but artificial intelligence has many obvious applications, but artificial general intelligence, as I understand it, is a quantum leap above that. Lots of corporations are working on it. When they finally achieve it, how are they going to roll it out into a product that makes vast amounts of money for them? Are we all going to have artificial general intelligence on our desktop or on our pocket? In the cloud, yeah. But that's like one artificial intelligence, right? How do they really make their money back? Well, in a lot of ways, right? If you think about, if you had an AGI at human level and you could copy and teach it anything, presumably that AGI would then be able to do every job humans can do. I mean, that's been proposed as a test of whether it's a really powerful AGI anyway. So then the answer would be by displacing every single human being from their job by doing their job better than them. I mean, that devolves into hundreds of thousands of answers, because there's a lot of different things to do. Or to give some concrete examples before that happens, I mean, if we have well-developed robotics, you know, elderly care, lots of people are needed there. This is expensive. So if you have robots with some form of intelligence, we don't need companies CEO AGIs, just robots which can take care of all people. That would be a huge market. And the other market is personal assistants, like Siri, but just much smarter, like a good assistant in a company or so. So that is everyone's personal operating system. Yeah, that would be maybe the first step. And then we come to Ben's. It's hard to know what will be the first step, because it just depends on cost and economic dynamics. Right now, without advanced AI, we could replace all humans in every McDonald's. I mean, there wouldn't have to be a human walking around or flipping the burger. I mean, we could replace that with automated burger and fry production and toilet and floor cleaning machines. And that will happen, right? There's some McDonald's where you can type your order in on a tablet rather than tell it to a human. The reason it hasn't happened yet is it's basically still cheaper to pay humans than to build all that machinery and the cost will come down during the next decade. So yeah, the- I think the question is going somewhere deeper. There is the big problem that we need to reorganize the way resources are allocated in society in a big way. For instance, as Ben points out, we will replace most of the jobs in retail, or all of them in the near term. We will also replace drivers and so on. We will replace many, many jobs irrevocably. Unlike you, I hope that many of the jobs that have to do with interaction between people will not be done by robots, but can be done by people, because we will have an enormous amount of people that will have a lot of free time and might enjoy interacting with each other and do the things that humans are best at. So especially things like education, nursing, and so on don't need to be done by robots or automated software. These are things where people can really interact with each other. But right now, these things don't make a lot of profit, which is why we have a shortage of labor in these areas. But generally, you have a labor surplus. And that's the issue. So right now, we don't have a societal acceptable way to reroute money into these jobs, because this would mean public employment. If you want to make public employment, it means in some sense that the state has to generate money and has to take out money on another point. It's not because the amount of money is finite and we need to mine it out of the ground otherwise, but you need to have some kind of balanced economy. So, for instance, inflation is a flat tax on portfolios in some sense. If you manage to raise wages together with inflation, you just melt down the portfolios. This is one way. But we haven't done this in the right way, and the worldwide economy is still in an incredible imbalance and bound to blow up at some point, as most people think. This is one of the issues that we are facing. And this issue is going to be dramatically aggravated with the fact that labor is no longer going to be the primary means of telling people how much bread they are going to have on their table. And it's not because of a shortage of bread. We can have more bread than yesterday. We can have better housing, better transportation, better infrastructure, better everything. We only need to find a decent way to distribute this among people. Yeah, I mean, one possible outcome is there's like a thousand really rich people, probably mostly white guys from the West and a few rich Chinese guys. These thousand rich people own a bunch of robot factories that mine raw materials and make them luxury goods and deliver it to them by drones. Then everyone else is shut out of that economy. Then the Africans are the only ones who survive because they still remember how to do subsistence farming. I mean, that's to the extreme. But that is sort of one direction the world economy gradually seems to be going in. And it's actually very unlikely. Yeah, of course it is. Because that would mean that of course there's going to be another secondary economy, there's going to be riots and so on. I don't think this is going to happen. Probably not. It's very unrealistic. No, no. Something else will intervene along the way. But for that not to happen, radical changes to economy and society and government policy will have to occur. There are some very straightforward changes. One very straightforward change is to have massive increase in public employment. So who gives universal basic income to the Central African Republic? I am not sure if universal basic income is the right solution in general, because right now, labor also has another function, integrate society. And there is a very big danger of right now already in the US, the society is disintegrating along many fault lines. People have the impression that they are no longer part of the same food chain. And this is very dangerous, because ultimately it means war. It means my kids should be fed. Your kids cannot be fed. We are in conflict. Before my kids are going to die, yours are going to die. I make sure of that. This is a situation that you don't want to have. So when you see people in Charlottesville in the US running around and yelling, the Jews will not replace us, what they express is they have the impression they are being replaced. They can no longer feed their kids. And you can say that they're despicable and so on, but if you look at the world through their eyes, and they might be most modeling the world, as they certainly are, that's their impression. And it's a very dangerous impression to have. And we need to make sure that this doesn't happen. And if you can pay people to be good members of society, for being teachers, students, artists, cooks, whatever people like to do with each other, that is actually a very good thing. Another point to bring out is when an entirely new technology enters into society, it can do functions which we already know about, and a lot of them were mentioned here, but there'll be new functions we haven't even thought about yet. So the telephone, people thought the main use would be transmitting music. The fact that people would talk to each other was not considered. Lasers, when they were invented, no one thought you'd use them for recording materials. So inventions come along and they often have uses that we can't foresee right now. And an AGI has almost unlimited potential in transforming through entirely new uses. We know about many AI systems now developing. There was an expression here that, you know, worries about AGI multiple systems. Practical question, how many AGI systems are now in the world of development? How many have been developed? Or how many? How many? How many? How many? How many? How many? How many? How many? How many? How many? How many? How many? How many? How many? How many? How many? How many? How many? How many? How many? How many? How many? How many? How many AGI systems are now in the world of development? How many have been developed? Or how many... How many are still there? It's a part of open code. I mean, that's a fuzzy set, right? Because I mean, at this point, I would guess there's close to, I don't know, a hundred little companies around the world which have AGIs as an aspiration, and they're doing some work aimed in that direction, and probably a few hundred researchers and universities around the world working in that direction. I mean, that might be off by a small integer. It's not off by, like or 100 or something, though. But then many of these are very small efforts, right? And there's not that many efforts that have a lot of resources or people behind them. I think the best funded and biggest projects aimed toward AGI now are in big tech companies, right? I mean, in Baidu, Tencent opened a big new AI facility in Shenzhen, and they came to our lab in Hong Kong to learn about AGI. Baidu has been doing this for a while. Alibaba has, and then in the US, you have Google and Facebook and IBM. So, I mean So the biggest concentrations of people and resources are in the big tech companies now, I would say. However, almost all of the algorithms and ideas used in these big tech companies were developed by university professors and their PhD students. So the way it's evolving now is sort of academia is developing new ideas, publishing them in papers, putting in some kind of crappy but functional open source code. Then big companies hire those PhD students and master students and so forth, and they take the ideas developed in universities and they kind of scale them up and apply them to practical problems and may often lose some of the generality that they started with. But that's sort of, in some cases, but I mean, as Marcus pointed out, I mean, Google DeepMind has a couple hundred employees and they're expanding by hundreds more. And some, I don't know exactly what percentage, but maybe 30% some non-trivial percentage of their group is really working on AGI R&D instead of demos or practical projects and by do I know better because I've been to their headquarters more often in Beijing and that's similar I mean they've got hundreds of AI staff, and they have a bunch of teams just working on, like, trying to make AGI-controlled little animated agents in virtual worlds, like pure AGI-ish research projects. So, I mean, the big companies are doing that as well as doing practical stuff. Now, what they're up against... Open AI is solely dedicated to AGI, right? They're not big yet, but they have a billion dollars of funding. I was at the Open AGI unconference last year. And I would have to say, every single thing presented there was an application of an already existing deep neural net software program to some new application area. So in principle, in some sense, they're oriented toward AGI. But at that point, I did not see any exciting new innovation or initiative toward AGI, I mean, much less than I see in DeepMind or Baidu, but they do have that aspiration. Yeah, yeah, so that's why it's sort of a difficult question to answer because we don't, no one has a solid knowledge of how to build an AGI, right? So if someone says I'm working on this narrow AI system and I believe it can eventually be generalized to turn into an AGI. I mean, I might not believe that it will work, but I don't have a proof that it won't work, and it's research, so every direction has a certain validity to it. What's changed, though, is now, unlike 10 years ago or even five years ago, you can tell your boss in the university or your boss in your research lab in the company that you are working toward AGI as your goal while working through some incremental steps to get there, and they won't laugh you out of the room or something, right? So it is now taken seriously as a pursuit, even though people want it to be balanced with things that will give a shorter term reward and outcome. That's a huge attitudinal difference, which I think is going to lead to much more rapid progress toward AGI. Having people not be afraid to work on it is certainly a first step to encouraging progress. Let's get in some more questions. Yes, so it's been referred to a few times about ownership of AI. But do you think it's actually the right approach to think that we will own an intelligence we hope will be matching our own? And do you think that we even could convince an intelligence that's greater than our own that a human owns it? No. I mean, initially it will be owned by companies, organizations who develop it, but quite soon, you know, it will not be owned anymore, you know, like the freeing of slaves, right? Or the growing up of children. Oh, yeah. I don't think we should see AI as robots. AI is not going to live next to us. We are going to live inside of AI. It's going to be intelligent systems. Robots is going to be a limb of these systems. It's going to be the physical emanation of this. But minds are information processing systems. And the physical realization where that server stands is not really the point. But that doesn't change the ownership question. But when I say things like we should make the AI love us, of course this is terrifying, this idea that you create a superhuman slave that still loves you and gets friend zoned and... That's horrible, isn't it? But it's probably not going to happen. I don't think it should be a slave, Josh. I don't know where you came up with that from. No, it should serve us freely because it loves us. Well, that's different than being a slave. If it doesn't have agency about that love, it's late for slavery. So does your wife love you? No, she made that choice. It built this have agency. So does your wife love you? Yes. Do you own her? No, she made that choice. It did build this into her. It was a choice by her, and she can change that choice. But evolution built into her the propensity to love some man who acted toward her in a certain way. Evolution created us that way. And evolution also creates some sociopaths. And we can, I mean, by our choice in engineering and teaching the AI, we can either make it be more like a sociopath or more like a loving person. Being loving doesn't imply lack of agency. Don't try to defend it. You're just making it worse. Now, which motivations do you want to then build into the system or do you believe that you build systems without any motivational basis? It really depends what the purpose of the system is. When you build a general problem solving system that you want to apply to a given task, you should probably give it the goal function of that given task. And the main danger for us, of course, is the goal function of that system is participation and the main danger for us, of course, is the goal function of that system is participation and evolution. And we are in direct competition with it because we are going to lose it because its approximation of the necessary behavior is probably going to be better than ours. OK, so what is then a task which you or people should give these systems? It really depends on the context, an arbitrary task that we choose. It could be, for instance, solving the problem of governance. Governance is not very well solved. We don't know how to incentivize the governors to govern us in a way that is consistent with the common good in the best way. It's a difficult problem throughout human history and we haven't really, really solved it. And it's a big issue. So you prefer robots who govern us rather than robots who love us? It's not about robots. It's about information processing systems. We probably need something like a nervous system for this planet. We as a species act like we are parasites on this planet, like lice in the fur of Gaia. And we are not like this anymore, because we have completely subdued this planet. We can no longer treat it as an externality. We can no longer be parasites. We have to be the nervous system of this planet. And no organism can afford to have a nervous system that lies to itself. Yet all our modes of knowledge creation about how society works and policy should be enacted are completely corrupted by local interests. This is something that AI can solve. If you really prefer our program, the AI to love all humans except you. But look, you're coming up with a long list of things what the system could do. But what if you replace love by care? So systems which care about us. So you described goals, which means they care about us, you know, about the survival, about governance, about not exploiting resources and so on. That means all these goals are part of the bigger goal of caring for humans. Maybe that's a better word. I think I understand what you mean. I just, currently on a mental threat where I think that caring is a shortcut. It's an evolutionary shortcut, because evolution didn't know the incentive function yet. If we care about something, it distorts our world model. If you have fears, desires, and love, it's a distortion of the ideal world model. If you just have the incentive to say, I want to maximize the chance that my offspring survives, you don't need motherly love. But nature did have a way to write this directly into our brain. So we have this proxy of motherly love. But nature did have a way to write this directly into our brain. So we have this proxy of motherly love. Motherly love is very close to it, but it's not the same thing. It has side effects. These side effects means that a mother might die if her child dies, that she lies awake at night even if nothing threatens her child, and so on and so on. These are things that a completely rational system that optimally tends to the task wouldn't do. So I think an AI can solve these problems without caring, and for the same reason as a chess computer can win at chess without caring. It's not necessary. No, a chess computer cares about winning. No. Well. No, it's completely understood. I mean, I think. It loses or wins. No, maybe not for chess, but for Go it's definitely the case, right? It's a real. It's a reinforcement learning system, right? And what does, I mean, I know, of course, it's not conscious about itself, and it's not at the level where it has developed feelings. But I mean, it's a reinforcement learning algorithm which tries to maximize reward. And then, what does caring mean, right? It means not being a stoic about this particular thing. You can solve life as a stoic if you understand what's going on. This caring is a situation in which you feel pleasure and pain. You can't build an AI without pleasure and pain. I think if you're stoic and super rational, you wouldn't do anything. Why should you? Because you have an objective function. Yeah, how is an objective function sort of, and if we instantiate this objective function as caring for humans? My computer executes Keynote right now, and that's not because it cares. It doesn't have pain when it doesn't execute Keynote. It's just a causal structure that's built into that system. I just don't, I think if you try to precisely specify tasks for an AGI system, you will find you get behaviors that are not what you wanted or expected, because the tasks that are relevant to us in our life are defined kind of nebulously and imprecisely. I mean, when you say improve governance, that's quite vague. And to really understand what those vague words mean, I mean, the system has to have a huge amount of tacit understanding, which means that the way that AI interprets your vague statement depends on its own motivation and its world view. I mean, I don't see how you're going to get a superhuman AGI that is just going to narrowly solve whatever problem that You post to it. It seems seems unlikely That's that's how it's gonna go and how would you how you would ever specify this goal? I mean In a way that it's algorithmic, right? You can't have proxy goals But what I mean is that these mechanisms that people have, these pleasure and pain signals, are suffering when you have a pain signal that doesn't end because you mismodel the situation in terms that you can model them and you cannot control that anymore. This is something that you would not need in such a system. I agree that we want a future superhuman AGI system to form an accurate model of the world in as much as it can based on its perceptions. You probably don't have a big disagreement. I don't think that that contradicts loving people or caring about people. I think it's very unlikely or much harder to develop AGIs which are not reinforcement learners. Would you agree to that, that reinforcement learning is a good approach? Maybe... Yes, yes, I agree. No. So these discussions are absolutely fantastic, and I think one of the things we're really teaching us today is that our job is to learn and to spread awareness. So this is what people talk about and understand in the general public, not just in interest of the things you're really teaching us today is that our job is to learn and to spread awareness. So this is what people talk about and understand in the general public, not just an interest. If you learn more about AI, you can become as confused as we are. LAUGHTER Did you hear his comments, perhaps, on the motivation for Andrew and Talib Baidu recently, and decide that his purpose in life is to spread machine learning and knowledge of this technology as far and as wide as possible? Oh, I think you should ask him that question. Do you believe that it seems to be consistent with what you're saying, which is spread the market and the capability to build as far and wide? Sure, I mean, I can see, Andrew Oonley is now starting an AI VC firm. If you look in the business market for AI right now, what most investors want to invest in is very narrow sort of vertical market specific AI applications. I mean it's easier to get investment money for applying AI machine learning to one little problem. So I could see how in his point of view, applying deep learning to one little problem might bore him by this point. At Baidu, he was overseeing a lot of things. By starting a VC firm, he can look at a portfolio of a lot of different little applications of AI. And so that, I can see why that would appeal to him, sure. I think the worst possible scenario is where you have multiple AIs being developed, real AGIs, by many different agents, and they're widely distributed. Because now you're putting this incredible weapon in everybody's hands. It's like giving everybody a nuclear weapon, hoping nobody will use it. Well, it works so far. Yeah, yeah, exactly. We're still here. Question here. I have a bit of a problem with the discussion of AGI and motivation. I understand motivation built into a system in the sense of building in biases, predispositions, the kind of ordinary stuff we do now. If you've got an AGI, it would seem that it would be simply aware of those sorts of biases and could simply unwind them. To have real motivation, you have to have a system to want something. Do we understand how to build wanting something into the system? Yes, and formally, these are utility-based agents. You specify an utility function or a goal, and then the agent tries to maximize this goal. And actually, there are real considerations. So is a maximally rational agent who is designed to achieve a certain goal, which can be a very broad goal, right? It doesn't need to be narrow like solving chess, but can be sort of surf humanity or explore space or something. So would such a system be motivated to change its old goal, for instance? And you can more or less say that such a system be motivated to change its own goal, for instance? And you can more or less say that such a system will not be motivated to change its own goal system. So once you have implemented or given it a certain goal, it will stick to this goal. So there we have some safety feature actually which we can prove. It's not, I don't like to do this, I just hack myself, right, and then I do something else. A perfectly rational, intelligent agent will not do that. We have rigorous theorems about it. Okay, thanks very much, guys, for participating in this panel. It's, yeah, I think it's close to 10 o'clock, is it? Yes. Yeah, okay. Well, thanks so much for attending. This has been a wonderful evening, and all of you stragglers who've stuck around, we congratulate you for your endurance. So put your hands together for the panel. And also the audience, thank you so much for being involved. And we have some presents for the participants here tonight. Okay, I briefly wanted to say we've got some gifts of Welshman's Reef. It's a local Victorian vineyard that's been promoting critical reasoning. And it's got a long story, but if you see it in a bottle shop, please buy it. And I think we've discovered the LD50 of a crowd tonight. So yeah, thanks a lot for your efforts. Thank you. The other person I really want to thank is Adam Ford. And I think we've heard tonight about all the fantastic conferences that he organises that are often for free or, you know, they're usually his own initiative. We have Future Day and the Philosophy Conference and the... There's been a Singularity Summit on Science science technology, under different names, but often touching on some of the subjects we've covered tonight and more. So keep an eye out on Science Technology in the Future, that's the main Facebook group, because there will be more of this kind of thing coming up. And a big thanks to the organisers of AGI and their four speakers who've offered their time tonight. අපිත්තු කිරීමට කිරීමට කිරීමට කිරීමට කිරීමට කිරීමට කිරීමට කිරීමට කිරීමට ස්තූතියි අපි පිටියි කිරීමට කිරීමට කිරීමට කිරීමට කිරීමට කිරීමට කිරීමට කිරීමට කිරීමට කිරීමට කිරීමට කිරීමට කිරීමට කිරීමට ස්තූතියි අපි පිටියි කිරීමට කිරීමට කිරීමට කිරීමට කිරීමට කිරීමට කි� Bye!", '25.27199649810791')