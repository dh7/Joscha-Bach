('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/Rgjw8O3vLBs"> </iframe> </center>', " Hello everybody. My name is Javier Snyder and I am a member of the CCRC group of the University of Memphis. And today I'm going to present the LIDAR framework. Several of you have been in the tutorial earlier in this conference. So probably you know what this is about. OK, software frameworks are reusable pieces of software that implements part of the main structure of a system. The idea, the main idea of a framework is complete or fill in the blanks with the specific parts of the problem. And probably this is one of the main advantages of frameworks, the idea that the developer only needs to concentrate in the domain specific, the important part of the problem that it wants to solve. It also, frameworks also, promotes good code in the form of best practices, design patterns, design principles that leads to good designs of software. All of this speed ups the development of new applications and this kind of frameworks have an enormous success in the enterprise application world, yes. And cognitive systems are really good candidates for this kind of frameworks. It's an idea that we can extract from the software engineering world and apply to this kind of applications. The LIDAR model is the architecture that we developed in Memphis. I am not going to go in detail of this architecture, but it's the base of our software framework. You can go online and read about this architecture in detail. But this is a good example of a cognitive architecture or maybe an HCI possible architecture where you have several modules that are interconnected. And most of these modules have different implementation, different algorithms as part of its implementation. So what is the idea behind the LIDAR framework? The LIDAR framework is a software framework implemented in Java. Java, so if you use it, you can run it in almost any platform. This framework implements the core of the light architecture. And most of the boxes that you have seen in the previous slide are already implemented. You don't need to create from scratch all these boxes, all these modules. And you probably only need to do the domain-specific classes, the domain-specific modules of this system. Another important idea behind this framework is that the customization and the change of the implementation of modules of or other part of the system should be easy. And in that way, modules, processes, even data structures are very easy to interchange in the currentIDAR model, you can change that implementations and integrate in your application very easily. For example, the current implementation use sparse distribute memory for episodic memory, but if you want to use a database, for example, it is very easy to implement that module, integrate it as part of your architecture. And of course we use a lot of design patterns and good practices in these frameworks that again leads to easy to maintain, easy to modify, and easy to understand software design. Some of the most important features of this framework is use multi-threading to execute several tasks at the same time and we create a kind of transparent multi-threading using an element that we call tasks and a task manager that controls the execution of this task in parallel. Also the architecture, the agent that you want to create is defined using an XML file instead of doing that directly in Java. So first, our basic customization of your agent can be done simply changing XML definition. We also provide in this framework a very customizable GUI and a logging mechanism that facilitates the interface with the application. And maybe most important for this conference, the framework itself doesn't have very strong commitments to the LIDAR model. It was built in such a way that it's very easy to create different modules that are not part of the LIDAR model. And eventually you can implement very different architectures using the same framework. Actually, I envision that if this happens, we can combine modules for different architectures and try different combinations of the architecture. So again, it's possible to implement other architectures using this same framework. The main elements of the framework are modules that represent the boxes that we saw in the diagram before, listeners to implement the arrows and the connections, nodes, links, and node structures that are the main data structure in the framework, and as I told you before, tasks and task manager that implements the behavior and the execution of the modules in the framework. Well, this is a screenshot of the GUI of the framework in one application. Again, this GUI is completely optional. So if, for example, you want to use the framework to control our robot, you don't need to use the GUI. And also it's dividing panels, different regions here in the screen, and you can create your own panels. For example here the hamburger world that you see in the left, it's a module for one particular, sorry, it's a panel for one particular application and you can create your own for your own applications. So summing up all of this, this framework facilitates the development of software agents. There is less time spent in generic parts of the applications. You don't need to start from scratch, one and again, each time that you create an application. Developers can concentrate in the specific problem, the specific application that you want to implement. It's highly customizable and extensible. Supporting tools are provided like the GUI and login. And hopefully it could be a general framework for HCI. I want to thank all members of our team, the CCRC group at the University of Memphis. Thank you very much. So we'll take questions in the panel session at the end of all the speakers. Next up, we've got Paul Rosenblum. Hello. What I'm going to do today is tell you about an experiment in mechanism reuse in a graphical cognitive architecture in going from a memory architecture to a problem-solving architecture. I'm going to start with just a touch of background on what a cognitive architecture is. So it's essentially a hypothesis about the fixed structure underlying intelligent behavior. It defines the core memories, the reasoning processes, the learning mechanisms, and so on. The idea, well, see, here's a quick concrete example. This is the SOAR architecture in midlife, essentially versions three through eight. It consists of a symbolic working memory, a long-term memory of rules, a preference-based decision procedure that decides what operators or actions to perform, the ability to reflect on its own performance when it can't make decisions, the ability to learn from that reflection to generate new rules, and the ability to interact with the world. So that's the SOAR architecture kind of in midlife. When you combine the architecture with knowledge and skills, the idea is that you get intelligent behavior. And the goal here is breadth or generality. So by adding different kinds of knowledge and skills, you should is that you get intelligent behavior. And the goal here is breadth or generality. So by adding different kinds of knowledge and skills, you should get different kinds of intelligent behavior out of the architecture. An architecture can then serve as the basis for a unified theory of human cognition, as the core of virtual humans, intelligent robots, intelligent agents, or as the basis for artificial general intelligence. Now, the issue I've been working on for the last couple of years is something called a diversity dilemma. or as the basis for artificial general intelligence. Now the issue I've been working on for the last couple of years is something called a diversity dilemma, or how to build architectures that combine theoretical elegance and simplicity with broad functional capability and applicability, where under capability, we like essentially a superset of the kinds of things you see in existing architectures. Not just cognitive, not just perceptual motor, but also motive, social, adaptive, and all of those kinds of things. Now the problem is architectures often start out simple and as they live over time, there's push for more and more functionality. They get more and more complex and you start losing all the benefits of theoretical elegance and simplicity. So again, if we look at the SOAR example, again, up to SOAR 8, it was a fairly simple elegant architecture with one kind of each thing. When you go to SOAR 9, the push for increased functionality led to a considerably more complex architecture. There are now three additional long-term memories, not just rule memory but episodic, semantic, and visual, an additional working memory for visual, three new learning mechanisms plus some additional stuff. So that was all driven by functionality. What you can, one way to view what I'm trying to do is to say, all right, I want to be able to take that same level of functionality. In fact, I want to go way beyond that level of functionality but with the kind of simplicity and elegance of the original system by greatly generalizing each of the individual components that you originally saw. So that's the big picture of what I'm trying to accomplish. What I'm going to talk about today is one particular piece of this, which is to extend a graphical memory architecture, which I'll tell you about, to SOR-like problem solving. That is, how does one generate, evaluate, select, and apply operators internally within a memory structure in order to do problem solving over sequences of possible actions? And the goal here is to reuse as much as possible of the existing memory mechanisms, which are based on graphical models. I'll tell you about that as well. In order to be able to evaluate whether as we add new functionality to this architecture, whether it can stay simple or whether we keep having to add new boxes onto that architectural diagram each time we want a new capability. Okay, so here's a brief intro to problem solving in SOAR. There, it's based on three memories and four processes. You've seen two of the memories before, long-term memory and working memory. There's also a preference memory that usually doesn't show up in architectural diagrams. It's considered to be an implementation detail, but it's critical for problem solving. Then there are four processes. Three of them are driven by rule firing, generation, evaluation, and application. Two of those occur in a retractable fashion, like a TMS. So the idea is that the generation and evaluation operators is always sensitive to the current situation, and those adapt automatically as the situation changes. Application occurs by latched rule firing. The results of that stay around until they're changed explicitly, kind of an implicit frame axiom. And then the actual decisions occur by taking the results from preference memory, which are generated by evaluation, and applying a separately implemented decision procedure in order to decide what goes into working memory for the operator that's to be executed. Now, it happens by some processes I'm not going to have time for. And there's also a meta-level aspect for reflection, but that's not the focus here. OK. So, the new approach is based on graphical models. I'm going through this rapidly to get through in 11 minutes. What graphical models are about is enabling efficient computation over multivariate functions by decomposing them into products of sub-functions. There are a number of different variations on graphical models. There are, for example, Bayesian networks, which decompose joint probability distributions into products of prior and conditional probabilities. They then map that onto a directed graph with variable nodes. Factor graphs are a bit more general. They're about dealing with decomposition of arbitrary multivariate functions. And they map onto bipartite undirected graphs. What's particularly interesting about graphical models in general from an architectural perspective is that they yield broad capability from a uniform base. In fact, they get state of the art performance across symbol processing, probabilistic reasoning, and signal processing, all from a single representation and a single kind of processing algorithm. And you can see on the slide, I won't list them, different kinds of algorithms which are all essentially the same algorithm within graphical models. They support mixed and hybrid processing. So mixed is a mixture of symbolic and probabilistic reasoning. Hybrid is discrete plus continuous. And in fact, some of the neural network models map onto them. So they have a broad uniform capability. I'll try not to hit that again. The architecture, the memory architecture that I'm talking about, is based on factor graphs, the most general form of graphical models known at this point, plus the summary product algorithm, which processes memories on the links. And I'm not going to go into detail on how this works, but messages are distributions over the domains of variables. Those get combined by piecewise products. Then you sum out variables that you're not concerned with. And all in all, a single settling of a graph then can generate marginals on all the variables in the graph or it can also generate map probabilities if you're familiar with those are. So that's a very brief introduction to graphical models. In the memory architecture, one of the key issues is what is the representation used for the functions and the decomposition and therefore the messages that are sent around in the graph. And what's used here is a hybrid mixed representation that it's based is a multi-dimensional continuous function approximated in a piecewise linear fashion. So you break that up into regions, and then you get a linear function on each. That lets you then approximate arbitrary continuous functions as closely as you want given the cost of representing it. You can then discretize the domains to get discrete probability distributions and symbols. And you can Booleanize the range and assign the symbol table which is for the programmers use and that lets you do symbol processing. It turns out as far as the factor graph is concerned, it's all continuous functions. It doesn't know the distinctions. This is useful for us in putting knowledge into the system. But the architecture itself doesn't know the distinction. Okay. The graphical memory architecture was developed by creating a general knowledge representation layer on top of factor graphs and the summary product algorithm. It differentiates long-term and working memory. Essentially long-term memory defines a graph. Working memory, then, are peripheral factor nodes within that. Working memory is defined in terms of a set of predicates. Again, I won't go into the details with typed arguments. And what the working memory does is provide the evidence for settling of this graph, familiar with how, for example, Bayesian networks work. Long-term memory then consists of a set of generalized rules called conditionals. They're based on predicate patterns, conditions, actions, and conducts, which I'll tell you what they are on the next slide. And functions are mixed hybrid over pattern variables. And each predicate induces its own working memory node. OK, conditionals, again, very rapid. So conditions test elements in working memory, and actions generate them. You put those together, and you get something that looks like a classical rule. And at the bottom is the factor graph for it. In fact, if you know the read algorithm, it looks a lot like that. Conducts do both. So you get bidirectional message passing as needed for probabilistic reasoning, partial match, other kinds of things like that. Functions then modulate the variables. So here's an example of a conditional which is based on two conducts and a function. It essentially defines the conditional probability of an object's weight given its concept. And then the factor graph below looks somewhat like the other, though there are differences I won't go into. All four kinds of things can be freely mixed in conditionals. So then the memory architecture uses conditionals to implement many different kinds of memory capabilities. In fact, all the kinds that you see in SOAR plus more kinds. So we have a rule-based procedural memory, which has things like what you saw on the previous slide. Semantic and episodic declarative memories. This is a view of the semantic memory. In fact, the conditional we looked at is one fragment of this. And essentially, that plus a bunch of others define a naive Bayes categorization memory for facts and categories. There's also, you can do constraints. And I've got the beginnings of an imagery memory. This is a 3D hybrid representation for the 8-puzzle that uses two continuous spatial dimensions and a discrete tile dimension. In fact, that's used in the problem solving here. OK, before we get on to problem solving in the architecture, there are three additional aspects of the memory architecture you need to know about to understand how the problem solving works. The first one is the distinction between open world versus closed world predicates. So in general, that's a distinction between when something in working memory is unspecified, do you assume it's unknown, or do you assume it's false? It turns out to be a key distinction between declarative and procedural memories. And I can answer questions about that, but I need to go on. Open world allows changes within a graph cycle. So a graph cycle is passing messages into a quiescence and then changing working memory. OK. Open world lets you generate results and retract them all within a single graph cycle. Closed world requires it across cycles. Second one is universal versus unique variables. Universal are like those in rules. They match everything and generate everything. Unique are like standard random variables where you get a distribution of the best value. Pick one value. And I won't go into that. And there's one other thing, which is that there's a memory on every link that stores the last message. Turns out that implements some of the stuff that's in the read match algorithm, but it's also important for problem solving. OK, problem solving of the graphical architecture. So again, there are three memories. But now the link memory not only subsumes read-like things, but also the preference memory. They're the same four processes, but now the distinctions that occur on them are not driven by problem solving, but driven by the memory. So we have open world versus closed world actions to get retractable versus latched. We have the link memory instead of the preference memory. And we have functions for doing things like preferences. And again, we'll skip over all of that. A version of the 8-puzzle's been implemented where functions and things like negated conditions, the arrows moved on the PC, are used to express the preferences. And you can, in fact, encode a full version of the 8-puzzle in this using 19 conditionals, a number of 747 Boeing-like number of nodes. Takes about 6,000 messages over nine decisions to solve 8-puzzle problems. Conclusion, since I'm running out of time. So, SOR-like base-level problem-solving grounds directly in mechanisms in the graphical memory architecture. So, the factor graphs and the conditionals provide the knowledge used in problem solving. The summary product algorithm provides the processing of that knowledge. The mixed functions provide both symbolic and numeric preferences. Link memories provide preference memory. The open world versus closed world distinction distinguishes operator generation versus application and universal versus unique variables distinguish operator generation versus selection. So it turned out it was possible to implement problem solving in this memory architecture where the only bit of actual new architectural mechanism was an architecturally defined predicate for the operator. And that's actually not used for this. It's used for the reflection process. So the memory processes were general enough to handle all of this. This work is also progressing in a number of directions. Some of them that concern problem solving do have an implementation of SOAR-like reflective problem solving as well. That does require some new additions to the architecture. also a very Unsur-like version of POMDP based operator evaluation which doesn't require any extension to the architecture whatsoever. Thank you. Okay, this session in some, is all about convergence, as we'll see. I'm going to argue that general intelligence needs something like general motivation. And the motivational system is what structures our cognition in some important sense. We're going to look a little bit at motivational dynamics, physiological, social, and cognitive drives. And we are looking at intention selection and the control of actions, and maybe a little bit on how this can be evaluated. AGI has the problem that it specifies a goal, but not a method to reach that goal. And the existing methodologies in AI have evolved to be pretty much incompatible with the goal. That's why we are sitting here and have our own conference and are not all going just to San Francisco next week. And AGI must break free from this methodologism of AI, which pretty much has overcome the original field. And yet, we need methods. And we need to integrate over the boundaries into the other disciplines and domains. And I think a paradigm which should direct these methods is something like cognitive AI, as opposed to logic-based systems, to search, to machine learning, but also to the strong and activist approach in some kinds of robotics. What we need for cognitive AI, what I mean by this is we need something like universal mental representations, which should be compositional and distributed, and in this sense, neurosymbolic. And we just saw an example which is very similar to what our own architecture does in Paul Rosenblum's talk. We also need some kind of general problem solving or semi-general problem solving, because it seems that humans are not able to solve all kinds of problems, but a lot of problem classes very well. And in order to do that, we need to have operations over these representations. We also need to be able to do perceptual grounding on these representations. That is, we need to be able to connect them to sensory input, and we also need them to inform our actions. And we need to be able to learn and feedback between actions and perceptions so we can model the world and get some kind of situation image, a world model. And we need to be able to transmit this or transform this world model into a long-term memory, a protocol of past world models. If we abstract this, we also get, among other things, a model of the agent itself. We'd get abstractions of objects, episodes, and types. We also want to have something like a procedural memory, so we learn what worked in the past and what didn't, and what happened and what didn't. And we also want to anticipate the future, so we can have plans and we have maintained an expectation horizon of events that might or might not crop up in the future. And if we generalize this, we have a long-term memory, a world model, and something like a mental stage. On top of that, we need some way to select actions, of course. And in order to select actions, we want to have something like a motivational system, because we cannot expect our system to know all the goals beforehand that it needs to have in the world, because the world is very complex. But we need to have a mechanism that identifies goals in the first place. And this motivational system is what selects intentions based on demands of the agent. Well, additionally, you also want to have emotional modulation and affect. And I think Ben is going to talk a little bit about this in the next talk. We want to integrate this into whole testable architectures. And as we have seen in this session, we have a number of components identified. We want to have frameworks to test them. We want to put all those pieces together. We have an architecture which is pretty similar to LIDAR in framework terms. We have similar representations, as we have seen in Paul Rosenblum's talk. And if you want to learn more about this, you might want to have a look at the book on psi theory, which is called Principles of Synthetic Intelligence. OK, today we are just looking at motivation a little bit. And I apologize beforehand, because in 10 minutes there's only very little I can do, so it's going to be very cursory. And there are a lot of entities that need to be carefully argued for, and I cannot do this. So I apologize, and please approach me afterwards if you have questions and complaints. OK, so all goals of our system are there to satisfy a hardwired demand. We do this so we can have completely flexible goals, which only depend on the kind of environment which we are in, but which always lead the agent toward a suitable behavior. Suitable, for instance, in an evolutionary sense, if you are a biological agent or if you are an AI suitable with respect to the kind of environment that you are in. So we need to define a set of drives which correspond to a set of demands of the agent. And these drives in our model are either physiological, social, or cognitive. What does that mean? Physiological drive basically kicks in if the autonomous regulation of body processes fails. For instance, you really need more food, you need more water, you need to heal, you need to get shelter, you need to rest, you need warmth, or you need to cool down or whatever. Whenever this doesn't happen autonomously, you need to trigger a kind of behavior based on these demands. So you can escape perilous situations. This leads you implicitly to seek physical survival. Physical survival itself is not a demand of the agent, because it's very hard to define what physical survival means and how you weigh this against other things. But basically, this is an emergent property of these basic physiological drives. Then social drives. Social drives structure our social interaction beyond rational utility. Affiliation is an internal variable, basically, which is increased by so-called legitimacy signals. These are OK-ness signals that you can get from other people or from social groups or from the environment in general, usually from other agents. And it's decreased by anti-negativity signals. And it is also decreased adaptively over time. So over time, you need to replenish this by seeking out social interaction. This allows groups to have non-material reward and punishment. You want in a social species, as we are, people to cooperate without an immediate benefit like a sociopath or calculated utility, because otherwise you would always have to mete out punishments, which decrease the fitness of individuals. Or you have to give them rewards for their cooperation, which might not be very good for the resources of your group. So what you have is a virtual currency, which delivers reward and punishment to incite individuals to behave altruistically with respect to the group. And this affiliation can be structured into something like an external legitimacy, which is amongst a group acceptance, and an internal one, which reflects the conformance to internalized social norms. So you are behaving well if nobody's looking. OK, now the most interesting ones. These are the cognitive drives. These are what makes us curious, which makes us explore, which makes us represent the environment in interesting fashions. The first one is competence. The drive for competence means basically that you are able to affect things in the way you want. And the first one here is epistemic competence. It's related to different tasks that you have learned. For instance, doing the dishes, driving home, or whatever. And to be able to do these things, to be able to program in your computer language, for instance, creates a pleasure signal and thereby satisfies this drive for competence. Then there is something like a general competence, which means that you are generally able to behave well, which amounts to something like a self-confidence. And this measure is important heuristics, whether you try something new, whether you are fit to go into new environments and take on new challenges, or whether you are in a world that is so challenging that you should stick to routines. Then there is effect-oriented competence. It's basically what makes the baby pull down the laptop, because it makes such a nice noise, and it's very satisfying. So it's directly proportional to the things that you can blow up, that you can make happen in the world. Then there's uncertainty reduction. Uncertainty reduction is a different drive from competence. It amounts to finding out how things work out, what details are in things, what's going to happen next, what's going to happen if I do this or do that. And it amounts to some kind of novelty seeking. Then we have aesthetics. Aesthetics is the desire to seek out certain kinds of stimuli. If it's stimulus-oriented, that is, for evolutionary reasons, humans prefer certain body shapes which have no logical foundation whatsoever, but just evolutionary reasons or certain kinds of landscapes or certain kinds of melodies and so on. And these preferences are basically somehow hardwired into our brains, and then are then adapted and built on. And we also have abstract aesthetics. These are representation-oriented. They make us prefer more elegant, more concise representations over more convoluted ones. So we derive a pleasure from consolidating mental representations. OK, so all possible goals of our system correspond to at least one demand, to a physiological one, a social one, or a cognitive one. A demand is indicated by an urge signal, which basically just measures the deviation of the demand from a target value. And this gets associated with goal situations. And it also gets associated with aversive situations that are detrimental to the demand. For in this case, something to drink if you need more water. You might want to avoid going out into the desert alone in California if you are bound to get thirsty and don't have anything to drink with you. OK, so you associate this by learning. Initially, you don't know this, all these things. You only have these hardwired demand in the agent. And by learning, you can find out what things are good to satisfy the demands and what things are detrimental to that. And basically, you do this by measuring the change of the demand over time. If it's a change in a good direction, it triggers a positive balance signal. If it's a change in a bad direction, it triggers a negative balance signal. We call these pleasure and displeasure. And they act as reinforcement signals on creating links between goal situations, the situations that lead to the satisfaction of a demand, and aversive situations. We do not only create a connection to the goal itself, but we also strengthen the connection to the goal and the situations which led up to the goal situation. So we are able to memorize plans which lead to that situation. And we also use these planned fragments later to construct new plans from scratch, or not entirely from scratch, but basically out of situations that led us to find something to drink in the past, for instance. OK, this is what we call a motivator, or a motive is basically a connection of an urge indicator with a goal and a plan which leads to that goal. And an intention is a particular action-leading, action-controlling motive, which we might select out of many possible ones. A motive is raised to an intention based on the relative strength of a demand, the opportunity to satisfy it, the expected probability of success of the plan. So we have heuristics which tell us how likely the plan is to succeed. And a selection threshold. The selection threshold is important to avoid oscillating behaviors. So if something is very important to us, we might increase the selection threshold, some kind of adaptive stubbornness, so we don't get swayed so easily from our goal. And we might, for instance, delay our meal if we just want to finish programming this part of the routine, and so on. OK, and this gives rise to a certain set of dynamics. We have internal modulator parameters, the arousal of the agent, for instance, which corresponds to the activity of the drives. If something very urgent is wrong in the agent, for instance, which corresponds to the activity of the drives. If something very urgent is wrong in the agent, for instance, it really needs to drink now, then the arousal goes up. Or if it has an accident, so the desire for or demand for physical integrity is frustrated, the arousal goes up, because this requires immediate action. If the arousal goes up, the resolution level goes down, which means we get faster responses. We think less. We don't look at details. But our cognition is basically sped up at the price of resolution. It also increases the goal directedness and so on. And it also increases the selection threshold. I'm not going into all the details of this because we are not going to do this in this short time. But this model allows us to ground personality properties from psychology in a functional model, which I think is a very good thing. So let's just look at the famous big five of psychology. Openness, which basically amounts to the appreciation of arts and new ideas. Conscientiousness, extroversion, agreeableness, and neuroticism, which is something like emotional stability. It's pretty straightforward how to explain with this functional model by showing how the individual relationships are parameterized, how you can express this in our computational model. So we can do high-level cognitive modeling of an abstract theory which is grounded in psychology. And this might open up an avenue for actually testing this kind of theory. What we are currently doing is we are working together with psychologists and have designed a set of problem-solving situations. And we are in the process of getting them to the point where people have choosing strategies which correlate with their personality properties that we measure externally with traditional methods from psychology. Once we can show that people with certain personality traits prefer certain strategies in a problem-solving game over others, we can have our cognitive agent be part of this game. We can let the agent play this game. And then we can change the parameters in the agent and show whether our model chooses the predicted strategies. So this is what we are currently working at. Thank you very much for your attention. So our next two talks are on OpenCog, and I think they're both by Ben. No, one's by Matthew, but the first is by me. So there are two talks in this series on OpenCog that will be presented now. I'm going to talk about work that we call OpenPsy, which actually follows on to Josh's talk that you just heard about some work we've done embodying some of the same ideas from his AI framework inside the OpenCog framework to help OpenCog do motivation, emotion, and action selection. Then Matthew McLean's talk following on will be about attention allocation in OpenCog and some new algorithms for that. And this work was largely done by a guy named Jemwa Shai, or Shai Jemua, as he would say in the Chinese order, who worked with me in Xiamen University and is now working on the OpenCog project in Hong Kong. So just to give a very rough impressionistic snapshot of the OpenCog system, It's an integrative system aimed broadly at human level AGI. Currently we're not at the human level yet, obviously. We have knowledge stores corresponding to different types of memory, declarative, procedural, episodic, attentional, sensory, motor. We have proxies that connect OpenCog to virtual world agents, and I'll show a video of that at the end of the talk, and also to control a now humanoid robot. And in order to get the robot sensory motor stuff to work well, we're actually integrating OpenCog with Itamar Aral's Destin sensory processing subsystem, which would be the topic of a different talk. And OpenSci, it's kind of hard to see, but this box here is based on Josh's ideas and sort of intervenes between all these memory and learning processes and the concrete stuff that the system does. So fortunately, since Joshua just explained all this, I don't have to go over it again. But you see a blowout of this box in our architecture sketch. Fits with the stuff that Joshua just talked about, the basic architecture of Dietrich Doerner's Psi architecture. And the big difference in what we're doing is that the long-term memory box in Psi up at the top there, that in OpenCog consisted of this whole conglomeration of OpenCog memory subsystems and learning systems, which is pretty different than how DÃ¶rner did it originally or than how Djoser does it. And for action selection, planning, motive selection, and the various aspects of PSI, we use OpenCog's knowledge representation and dynamics. But the basic structure and flow of information is the same as what Djosha has described. And many of the strengths of Djosha's work and Doerner's ideas carry over. For example, just as they use various complex networks of elements to ground concrete reference, percepts, and actions, and so forth. We do a similar kind of grounding tied in with the Psi architecture, but our nodes and links are a bit different than the ones in MicroPsi or in Dorner's work. So MicroPsi, as Josje has been developed, which is a very interesting architecture, it gives a specific software architecture building on top of the conceptual Psi model. And it uses a knowledge representation, which in some senses is a little lower level than what we use in OpenCog. And if you look at Josh's book, Principles of Synthetic Intelligence, which I'd recommend to everyone, I mean, he describes some basic net entities and units and how you wire them together to give a sort of neural symbolic knowledge representation, which has a lot to recommend it. In OpenCog, we have our own different sort of knowledge representation, which could also be broadly conceived as neural symbolic. It's a fairly fuzzy term. It's something we call the atom space. We have a number of different types of nodes and links. They, many of them are weighted with probabilistic truth values, often using imprecise probabilities. Instead of spreading neural net type activations around, they spread around short-term and long-term importance objects, which Matt Ecclea will talk about in his following talk. And then we think about both explicit and implicit knowledge representation, as Joshua does in his system as well, with the explicit representation being things that are sort of transparently represented by nodes and links, more in the manner of probabilistic logic. Implicit knowledge representation being representation of knowledge as attractors or other patterns of activation in the whole network. And I think you need representation on both those levels, the global and the local. So we had a paper calling this global knowledge representation, although I've concluded that that term will not catch on as well as the term AGI has. On an implementation level, all these different types of memory and the learning processes that act on them, they're implemented in sort of a manner similar to an operating system metaphor. So we have the Atom Space is a knowledge repository and there are different repositories tied into that to do specific kinds of knowledge like sensory motor or procedural knowledge. Then there's a number of learning processes called mind agents which embody different cognitive processes. It could be reasoning, could be procedure learning, could be sensory motor pattern recognition or frequent subgraph mining. And these are all scheduled by a scheduler acting on this repository of nodes and links, which sort of plugs into the long-term memory in the Psi architecture diagram, except in my model, that's where most of the learning and thinking goes on, is in the kind of active dynamics of that long-term memory. We then have a notion called cognitive synergy, wherein the learning processes corresponding to these different types of memories, say probabilistic reasoning for declarative knowledge, probabilistic evolutionary program learning, which is based on Moshe Lokhspey's thesis work, for procedural knowledge, economic attention networks, which Matt and Clay will be talking about in the next talk for dealing with attentional knowledge and the importance values. And there's been a lot of work going into how these different algorithms all cooperate together. So if one of them gets stuck, the others can come in and help out using a different kind of knowledge, all represented in a way that links into the common atom space representation. So this is easier to see if you read the paper and the proceedings. But we've made a careful mapping between the various aspects of the psi architecture from Doerner and Joshua Bach's work, the memory demands, urges, urgency, pleasure, goals, motive, and action selection, and the various modulators that Joshua discussed. And we've gone through and mapped each of those into things you can represent in terms of open cogs, atoms, and open cogs control architecture. The result is a system that has some similarities and some differences from my group's side. I mean, the basic logic of action selection and motive selection and the role of the modulators is arguably the same and is modeled on human psychology in the same way as what Joshua has done. On the other hand, the knowledge representation is quite different, and the learning algorithms used are quite different. So it's been not at all a matter of just plugging Psy as a separate module, together with OpenCog as a separate module. It's really been implementing the Psy conceptual framework partially within OpenCog and then keeping the knowledge representation and learning algorithms more OpenCog-ish. And this is just like a one and a half minute video showing the Psy model implemented within OpenCog to control a little robot in the Unity 3D engine. And pretty much it's probably hard to see the indicators in the corner there, but there's two demands here, integrity and energy. And the robot has more demands, but this demo shows integrity versus energy. It feels more integrity when it's in its house. It feels more energy when it gets a battery, because that gives it electrical power. So pretty much, this shows a very simple oscillatory dynamic where it sits at home because it likes the integrity there until it starts to run out of power. Then it says, oh, I better go get another battery. Then once it gets the battery, its energy goes up and then its energy demand no longer needs fulfilling. So it goes back home where it feels more integrity. I mean that's, it's kind of a trivial dynamic, but that's what we can show in a one minute video. There's a lot more complex, interesting dynamics you can, you can get out of OpenPsi. And this, this shows some bits of the open psi control panel, which was closely modeled on some parts of the micro psi control display, just showing the fluctuations of some of the modulators' feelings and demands during the course of that simple simulation. So let me try to stop it in the middle. No, I can't stop it. So it's a complex, nonlinear, dynamical system with the interactions of all the modulators, urges, demands, feelings, and so forth. And it's linked in with the complex dynamical systems of OpenCog's atom space with all these different learning mechanisms, working on it, doing reasoning, and trying to figure out what to do. What makes it tractable to work with is the fact that we're using it to control some pretty simple concrete processes, controlling a little robot in this Minecraft-like virtual world so you can see what it's doing. You can see, do the emotions that it's having and the way it's fulfilling its demands using planning, does it make sense in terms of what you'd want this little guy to do in the world? So our plan over the next year and a half is to get it as smart as we can in the virtual world. And in parallel, we're working on integrating Itamar's destined vision and audition system with OpenCog, working with the Now Robot, and then to piece those two things together to try and get the little robot to do what this little guy in the virtual world can do, which is all kind of step one of a grander plan to build like a toddler level intelligence building from this little virtual robot and the now robot. And then hopefully, assuming all goes well, have that go, ascend the ladder of Piaget in stages of cognitive development and get smarter and smarter. And I'll turn it over to Dr. Eccle to talk about attention allocation. If we can get the other laptop to work. This is basically I'm going to be talking about economic attention networks within the as basically the attention allocation system within the OpenCog system. So Ben actually kind of started everything off making everything easy because he went through most of this but. So I'm going to start off, well, the first part was a brief review of OpenCog. You just heard of quite a little more in-depth version of that. But one of the things I really want to start with are what are the goals of attention allocation? That's one of the critical ideas that we were trying to address with the series of experiments that I'll briefly run through at the end. The one of the, I'll talk a little bit about what our attention or economic attention networks and then talk about one of the problems that we need to work on there which I call the goal traversal problem. I'm going to follow that up with one possible solution that we've come across and that's using ideas from an area in statistics called information geometry. And then, how that ties in to what we call mind geometry. And then I'll talk in brief detail about some of the actual experiments that we've done to implement these information geometric ideas within the ECANNs or Economic Attention Network System. So, as Ben already covered here, OpenCog is an integrative AGI system. It's got quite a few different memory types and different modules covering things like uncertain logic, the PLN system, computational linguistics, Moses says, Mososhi's, Moses program for evolutionary program learning. All within a basic connectionist system for attention algorithm. In essence, a lot of what's going on here is how do these processes integrate to encourage higher level emergence or emergence of higher level structures. So this slide basically covers a lot of what Ben just talked about. It's a different types of memories, what are the specific cognitive processes associated with those memory types and the general cognitive function. As Ben also just mentioned, the a lot of, so if we look at the attentional system here, that's going to be covered by ECANN which stands for Economic Attention Networks. And that's going to cover things like association, obviously attention allocation. But again, this ties in with Josha's work as well and that's pretty much guided by the goal systems going to be guided by the size system. So what are some of the goals of attention allocation. Obviously it's basically your central process, basically controlling your going towards your goals. You're going to be, what are some of the interests here that we're looking at? It has lots of knowledge. So how do you weight this knowledge, the pieces of knowledge to each other? You're going to be interested in both the current importance and prior importance. The longer, long-term importance, short-term importance. It's to guide your reasoning system, content creation, and to identify these high level emergent structures. Key ideas obviously allocating resources among the different goals and guiding the system toward those goals. One of the things that's really crucial for any tension allocation system is the ability to scale and scale in an accurate way. And that's one of the things that I wanted that our series of experiments was really related to. So a little bit about the ECANN system. As the name implies, we're using an economic metaphor for guiding attention allocation. So there's a system of wages and rents. It's a monetary system. So within the system, we've got two basic what we call currencies that relate to the short term importance and long term importance. For the purposes of the experiments what that we ran, we can think of this as a conservative nonlinear dynamical system that matches the space of input patterns to the connection weights of or the weights of some connection matrix that connects the different nodes together. It contains an essential notion of what's called the attentional focus. As the, again, as the name implies, it's where is the attention focused? What nodes are most relevant at any given time in the system. So that brings us to the goal traversal problem, which is, so if you're in some specific state right now, and you're seeking to some goal, the question becomes, how do we get there? So one of the answers we came up with is the idea of information geometry. To try to address this in an efficient manner. We could obviously try to scroll through all possible states. That doesn't work. So what is information geometry? So basic idea is that it's application of differential geometry except instead of to physical spaces, it's going to be to spaces of probability distributions. So, what it does is it treats a probability distribution as a point on a mathematical manifold or a surface, some curved surface. And how this is curved depends on the metric that you choose. In information geometry, there's a key concept called Fisher information which basically is a measure of the information about the parameters which are our connections, our weights between nodes carried by some observable variable. And it forms the basis for the metric that we'll use. So Ben already talked a little bit about this although this is a will be covered more in a poster session this evening, three hypotheses of mind geometry. The key ones for the purposes of this talk right now are the second two which are that intelligent minds tend to follow geodesics or the path of shortest path or path of least resistance. And so, if we think about the goal traversal problem, we've got some sort of current state, some sort of current probability distribution and we're trying to get to some sort of goal state. And how are we going to get there? What's the path that we want to take to go from the current state to the goal state, to guide us in an efficient manner. And that's where information geometry, this idea comes into play and basically it's the shortest path which is called the geodesic and differential geometry. And the second concept is this cognitive synergy principle that Ben also talked about which basically says that this shortest path may require passing through multiple memory types. So, talk a little bit about the actual algorithm. The concepts are really pretty straightforward. The only real thing new here is that we're in a space of probability distributions. So Amari introduced an idea called natural gradient learning on this space of, on this manifold. And basically, all that is, is it's a generalization of the idea of steepest descent numerical analysis, water flows downhill at the fastest rate. And so, basically, now all we're going to do is follow this natural gradient from our current path to try to seek our, from our current state towards our goal state. This idea was extended to cover stochastic neural networks which is basically what an an ECANN can be viewed as, our Economic Attention Network. Although Park and Amari and the others originally only considered this for feed forward neural networks, the idea actually holds through in a much large for a much larger class. I'm going to pretty much skip through this. This is details that are covered in the paper. But the key concept is basically how it, because of the way the system is designed is that the probability distributions decouple and decompose. And that's what allows everything to work in the space on the neural networks. So what we ended up doing is then performing experiments. We had our original economic attention network system and then we would do another experiment. I ran an experiment like that. And then another experiment where I would interweave. So we do an Ecan step followed by a natural gradient step to adjust the parameter space. And then go back to the economic attention network followed by an adjustment of the parameter state using the natural gradient. And basically, the series of experiments showed a pretty dramatic difference between the original E-can system and the coupled system with including the natural gradient in every experiment that we ran. So there are some issues, however, that I'll talk about. Future plans are how to implement this in an efficient way. We want to try experimenting with this for other components of OpenCog. And one of the big issues is when I performed this experiment, it was basically a toy problem, small numbers of nodes, standalone system. So the first thing we really need to do is to study this within the entire open cog system. I just have not had the opportunity to do that. It appears that it solves the goal traversal problem quite well. Obviously much more experimentation needs to be done. So I think that covers it. So we've got a panel session now where you can ask questions to any of the speakers or all the speakers and co-authors are also welcome to come up. How will cloud computing impact your architectures in the future? Well, our architecture right now works in a multi-threading fashion. And it is not implemented, but it could be the same way that we divide the tasks in different threads. It could be possible to process this in distributed systems and of course it could be done in the cloud. I think that most of us haven't run into troubles due to lack of computational resources. So we have only scaled up to the point where we wanted to have several computers be connected in a local network. We haven't gotten to the point where we thought this is not enough. This might happen in the future, but so far, our troubles are conceptual. And I think in the future, cloud computing might affect us, but probably more if we want to have joint development with many people in the world and find a way to have some kind of collaborative development. I guess one of the big issues is still with cloud computing would be the network speeds. I mean, on local networks, you're going to get quite a bit of speed up. But as soon as you start going to the network, things are going to slow down quite substantially. In general, a lot depends on the granularity at which the parallelism exists within the architecture. So, many of them have very small grain parallel things. I had 6,000 messages going that can be done in parallel cycles. But you need parallelism that can handle the small granularity of that if it's going to be effective. Then you have usually the big boxes, which are fairly easy to parallelize. And then you have multiple agents within these, which are even easier. So it's not clear if it's cloud computing or parallel computing, what's going to matter. But there usually is a potential for parallelism. But it's usually hard to extract much more than fairly limited parallelism out of these things when you get down to it seriously. So it's common to have architectures where we can represent and work with knowledge. And we've heard some of those today. But I would say that it's much less common to actually be able to learn that knowledge. We can work with the knowledge once we have it, but can we actually learn the comparable knowledge? And I really mean learning from sensory motor data, the kind of data you would have all the time, rather than from prepared data sets. I think this is a big weakness in what's been done so far, that we don't really have a facility to learn massive amounts of knowledge, whereas the whole idea is to take advantage of having massive amounts of knowledge. Whereas the whole idea is to take advantage of having massive amounts of knowledge. Well, in the case of our work on OpenCog, I mean, it is learning based on the experience. We don't feed in prepared knowledge. I guess the, there's a question, which I don't know the answer to, of how important is really high dimensional, high bandwidth knowledge. So we're doing this work controlling a virtual robot in a virtual world. And that's experience. It comes into the system as perception, and it generates action signals. But of course, it's much lower dimensionality than controlling an actual robot. But it's still learning from experience. It's very different than SOAR or something where you feed in databases of rules. So what we're trying to do is first refine the whole architecture and get everything working doing learning from experience in this simpler experiential domain and then use the same system later to control a robot which has a higher dimensionality of experience. If you have a certain point of view, you might say that'll never work. You just need to be dealing with high dimensional data from the very beginning, but I don't think that's demonstrated. Philosophically speaking, I think it's important that you have the right constraints realized in your nervous system. It's not that important how they get there. This is a technical question. And I'm completely agnostic yet whether it's possible, for instance, to abstract the knowledge that you need about cycling, for instance, not by interaction with a bicycle, but by taking all the Google videos on somebody riding bicycles and doing a very thorough and very clever statistical analysis on this. I don't know about that. In our model, we do the bottom-up approach. So our agents don't know anything. All our representations are grounded. We did find that if you have a complex world, that we need either to teach the agent, because some things are incredibly difficult to get to, or we need to equip it with some biases. So for instance, if it is hungry, the sequence of actions that leads for this hunger to be satisfied in a reasonably complex world is so complex that you're not going to stumble on it by accident if you don't have some kind of bias for putting things in your mouth or some kind of reflex chain. But this is not knowledge. It's just reflexes. And all the knowledge that we have in the agent is learned from scratch at the moment. I don't think that's invalid to go another route and to try to work from that down, because it's going to be a long time until we get to proper language, for instance, from there. Because we need to do much more abstractions than we actually can do with that approach. So it's a methodological question where you start. But I think you need to bear this in mind. It's a very good point. So I guess the SOAR 9 architecture, which I showed, does have four learning mechanisms, including reinforcement and episodic learning and so on. In my own work, we're just starting to get into perception and learning stuff. So one of our most recent results is we showed some relatively high level perception using conditional random fields that are essentially implemented in the same factor graphs. And we're starting to look at learning. Very interested in how you do pervasive learning in combination with pervasive prediction in architectures like these. But haven't gotten to that yet. Yes, in the same way that several of these architectures, LIDAR has the premise that all learning should be online, so no separate stage for learning and one for execution. And of course, as Joshua said, there are background of hardwired knowledge, but most of the new knowledge should be by experience and online. Yes, hi. I have a question to the last presentation about attention. My question is like this. How would you drive it? How would you connect it to the goal of your system? Would you be able to do this? Are you sure? Yeah, so let me get the question straight. I mean, where does the goal come from? How you are driving the attention mechanism. For example, something comes into your view. What is the mechanism of the driving the attention? I think Ben actually covered that. It's basically the incorporation of Joshua's. What are the motives of the attention mechanism? I'm not quite sure what the question is. But I mean there is the attention of the system is driven by the self-organizing propagation of the importance values. But the system has some top level goals and those goals, each of them propagates short term importance values in OpenCog. And the goals then spread importance to the things that the system believes based on the knowledge that it's acquired will help it to achieve its goals. But then the importance values also just spread around non-linearly. So it's not extremely rigidly goal-directed, but it's heavily goal-gu guided because the goals spurt out importance values. So, what is driving the goal? Is driving the attention or some kind of miracle? Between those two, I'd have to choose the former. So, my question is mostly about the sign motivational system. I can see a really strong case for it as a scientific model of how people are motivated. What are your thoughts on the sign motivational system as an engineering tool for sort of giving a top-down goal to a system and then having it achieve some complex task in the world? It depends what your world is. I think if you have an eye which lives in a completely virtual world, you probably don't need the equivalent of physiological drives, except if there are constraints like bandwidth and so on, which you want to map on this. If you don't have these, I do think that you need some equivalent of the cognitive drives if you have something that needs to solve problems on its own. So in my imagination, general intelligence amounts to the capability to solve problems plus having the motivation of doing so. problems plus having the motivation of doing so. So I'm curious about what level, at what level you're talking about when you talk about your architectures. So for example, you're building a computer, you talk about chipsets or you go even farther down and talk about logic gates and how information is transferred at those levels. But when you're talking about making a computer do something, you talk at a different level altogether of maybe programming languages or maybe even higher nowadays. So, there's a level that you talk about the architectures at. But does that easily map on to the level where we talk about problem solving? Is there a direct mapping there or is it, I mean, I don't know if you have thought about that in detail. Should I start? I'd say 40 years ago, I would have answered I'm looking at this at a cybernetic level. And today, probably would say, we're looking at this at an information processing level level. And today, I probably would say, we're looking at this at an information processing level, which is not to say very much. So we do have implementation levels. And above that, we have certain abstractions on information processing. And goals are information processing entities within a certain computational framework. So we're not looking at individual neurons, but we are looking at, for instance, spreading activation networks, which embody some kind of Bayesian formalism, among other things. Maybe this helps a little bit. So one of the classic ways of talking about levels in cognition is Alan Newell's diagram of scale counts in cognition, where you look at the different phenomena that occur at different orders of magnitude of time. He divided it up into several bands, the biological band, the cognitive, social, or rational, and social. I think most of the architectures deal with often the upper aspects of the biological band and some aspects of the cognitive band. Problem and some aspects of the cognitive band. Problem solving usually starts in the cognitive band. And so some of these, some of the lower level architectures might not directly hit there, but most of them I think actually would. If you're talking in terms of, of encoding problem solving in terms of rationality, that's often above what, what most of these guys are talking about. So my question is, there seems to be a lot of similarity between the cognitive architectures and this kind of, I mean there's differences but this sort of convergence is sort of seen as a good thing. Isn't there a danger that everyone's doing the same thing and that everyone will just fail together as opposed to, you know, doing wildly different architectures? Uh, my, my view, oh, oh, there's two comments. First of all, I think presenting things in ten minutes with box and line diagrams may tend to give a greater impression of commonality than you get if you actually look at what's going on inside the systems. But the other comment is I think the AGI field is, it's pretty damn diverse with people going in so many different directions. I mean if you just, if you look at the three keynotes we had, the first was engineering, the second is psychology, the third is biology, right? And I think there's a lot of diversity and I'd say most, if you picked a random pair of AGI theorists, the two of them probably don't understand what the heck each other are doing. Because it's actually very hard to understand someone else's architecture. So I think any movement toward convergence and mutual understanding, given the current state of things, is probably moving in the right direction. I think we're a long way from having too much convergence in a small set of ideas now. I think that convergence would be a good thing. And maybe it should be task and paradigm driven, not necessarily architecture driven. So for instance, a robotic soccer has led to convergence of architectures, not because people just copied their code, but because they had a problem set which they found could be approached in a pretty good manner by this kind of architecture. So it all coalesced in some kind of four or five major architectural paradigms and stuck with that. And I imagine if we find good paradigms for doing AGI experiments, for instance, child developmental paradigms and so on, we might see a similar thing if we had competitions and so on where people really need to do results without hand-waving, without just drawing boxes and lines. And I do think that we have similar influences in many respects so that's why we do see some convergence here. But the individual solutions that we find are pretty different still. Yes. I'd reiterate basically what Ben and Joshua just said. I mean, I was, yes, I, on the first day with the tutorial with the light up, I was in some ways struck by a lot of similarities except then you look at where the arrows point and it's like, and then you start opening those boxes up and there's almost no similarity whatsoever. On the big picture, the boxes may start looking the same but as soon as you start opening them up, it's like a Pandora's box. I just had a variant on that. It seemed to me from what I've seen, it's again hard to tell in these short talks. But there seems to be commonality in terms of the kind of capabilities people are hypothesizing are needed in these systems. And therefore, in terms of a top level description, in terms of a set of boxes, there also seems to be just a syntactic description in terms of a set of boxes. There also seems to be a just a syntactic similarity in terms of you're seeing graph structured memories in all of these systems, but what is meant by those graphs are completely different across them. And then between those two we're not seeing a lot, but that may be where this Pandora's box is. Yes, well I have a different vision. I think the architectures are similar not only at the level of the boxes. Of course, probably we choose different implementations for the boxes, but those are just algorithms and maybe slight differences in the representations. But what I want to say is that if starting from different points or almost no contact and several architectures converge to similar models, well I think this is a good signal that we are going in a direction that converge from different points. One last question. For many years, I've been recommending that what's needed to address that issue is a study of the space of possible sets of requirements, the space of possible architectures, and the mappings between them and systematic investigations of that sort would I think be much better than trying to focus on one subset or one set of problems. It would turn this whole thing into something more like a science. That's a good segue actually. So I have a segue from that to my current question which is an engineering question. So what I'd like to do is apply a cognitive architecture say either to a robot or family robots. Can you talk to an implementation question that I have which is the difference between a real time, non-real time part of the architecture? So wouldn't you talk to scheduling or planning? In my mind that's more of a non-real time. You get seconds, on order of seconds to do that. There's a real time component, actually, that needs to be implemented all the time, which I think you've referred to as this urge-based time step, this time stepping, this emotion. Can you tell me how you're doing that? Say the real-time component that you're currently, this maybe goes back to your attention. I can tell you how we're doing that. Okay. It's not so much like what you were saying, I guess. I mean, in our work with the robot, I mean, there's code that runs on the now robot itself, which is fairly simple. And then the now talks by Wi-Fi with a process coded in choreograph, which is the scripting language that comes with now. But then that program sends XML messages to and from our OpenCog proxy. So OpenCog right now communicates with the now proxy kind of the same way it communicates with the Unity proxy we use for the virtual world. And it gives more high-level commands. So like the OpenCog may give the robot a command, like take a small step forward. And then the robot controller knows how to do that. And the low-level robot controller may, it looks at the world, and then there's low-level vision code that does edge detection stuff. and it sends a kind of sketch to OpenCog and then puts it in its space representation. So what we're actually doing is using the same system for the virtual world and the robot world and we put a kind of proxy in between that deals with all that nasty low-level robot stuff. And I don't think this is the ultimate be-all solution. So what we're trying to do with NMR's destined architecture is build something to go on that lower level between Open Cog and the robot that's smarter and can accept feedback from Open Cog. But just from an engineering point of view, we wanted to start by building something simple that actually functions, so we just abstracted out all the low-level, real-time stuff. And then even so, it's a lot of work to make things like learning, reasoning, planning work quasi-real-time, so it can make sub-second judgments and decisions with an open cog. And we can do that, but not microsecond or something. I would like to address what Aaron just said. This idea of mapping the space of possible demands and possible tasks and creating a relationship between those two, I think this would be a task for philosophy of mind. And I think that we can say with some justification that philosophy of mind has completely failed at that. So it's left to us. And the problem is that the engineers in the field and the scientists in the field do tend to invent hammers. And then they're only looking for nails. And this happens consistently over and over. I mean, the most dramatic failure of this was behaviorism in psychology. They invented a hammer which was extremely productive. And they denied the existence of everything that the particular nail they could hammer with. And the same thing happens over and over. So we have people which honestly believe that the solution to AI is case-based reasoning, or description logics, or whatever. And they refuse to look at other problems. And I think that, of course, we are all prone to that fallacy. That's why I'm asking for benchmark problems. And the good thing in RoboCup, I don't think that RoboCup is a very good paradigm for AGI, because it leads to soccer playing toasters very much as the chess problem leads to chess playing toasters. We need problems which we cannot solve by hand waving. The good thing is that you can focus on RoboCup on reinforcement learning only, and just borrow an architecture from last year's winner. But you always have to keep in the back of your head that this architecture is important, because your agent is not going to play without this architecture, without all these components. That's why I think that we need to find good benchmark problems, which refocus us constantly to the real problem. It is to have an integrated system that does all these things. And the good convergence among this group is that I think that there's some joint awareness on the breadth of components that we need to realize. It's not a single method. There's not a single silver bullet. It's not a single paradigm. But rather, it's a bunch of things that the system needs to do and integrate nicely. And the difficulty that one runs into, as I said in my remarks on the AGI Roadmap Workshop, is even though we all, many of us have a broadly similar view of the breakdown into high-level functionalities that a human-like cognitive system has to have. Each of us has put more emphasis on certain boxes than others. Each of us has a system that's more developed in certain aspects than others. So when you get a bunch of people, AGI researchers, and try to decide what scenarios, benchmarks, and tasks are the best ones to focus on. Each of us naturally gravitates towards those things that we've thought about most. And our architectures are best at. Which is not to say that it's infeasible to do what Josje has been suggesting. But it's a challenge. It's an interesting challenge that I hope we can meet. Maybe you shouldn't generalize too early, Ben. I guess this is because you and me, we both got big egos. But not everybody is like this. I completely disagree. I know. It's actually, there are some people where it's possible to convince them. So RoboCup did take place even though there are some people which don't play soccer. And people do converge sometimes on paradigms if they perceive them as interesting and productive. It's not all just you and me. I think so. The Beaker group is trying to specify types of architecture, but I don't think they've also been trying to specify types of sensible problems. I think in the original proposal, this was one of the things which they wanted to have. The original BICA proposal had one third of the effort dedicated to finding benchmark problems. That's not the same as sensible requirements. OK. The problem will be a particular collection of requirements which need to be satisfied. The space of sensible requirements may not be capable of generating particular benchmarks ar gyfer y cyfathrebu ar gyfer y cyfraniadau sy'n rhaid eu hyfforddi. Y rhan o'r cyfraniadau sy'n debyg efallai nad ydynt yn gallu cyfraniadau ar gyfer unigol sy'n cael eu cyfraniadu ar gyfer yr holl cyfraniadau, oherwydd yna byddwn yn rhaid cael microbe, sgwyrwyr, pheirwyr, porpoise, cymdeithas. Rwy'n credu bod hyn yn ymwneud ag adroddiad Erin. Rwy'n cael broblem o ran broblemau cyfraniadau I think this relates to Aaron's point actually. I have a bit of a problem with benchmark problems which is that there seems to be the potential for going against the very idea of general intelligence. And it seems to me that the point of general intelligence is that you don't know what the problem is in advance. And as the moment you start to set out a collection of benchmark problems, you're already putting a box around things and making it not general intelligence that you're tackling. Well, the idea is not to have only one benchmark, but a collection of benchmarks that try to cover a broad spectrum of possible problems. If you look at the original father of all the benchmarks, the Turing paper, he tried to define discourse, recent discourse, the ability to understand what somebody else is saying, to make itself understood, and so on, as a benchmark. We might spot some limitations. And if you take it too literally, we get to something like the Loebner Prize, where a system is rewarded for faking this most convincingly, maybe. And this, indeed, is a big problem. And I think that the search for benchmarks is a very, very difficult problem. But I don't see a shortcut or a way around that, because everything else, without really solving problems, leads us to the point where we cannot put our ideas to the test. And as computer scientists, we know if we don't put an idea to the test, our solutions are not going to work, because they are going to be full of bugs. So we do need some things like this and we need to find something which is where we cannot have a shortcut. So I do believe that child performance fighting your own way towards intelligence goes in an interesting direction. But... the rate of widespread of it. So I agree a little bit more with the attitude of the questioner, but I think that having common environments and scenarios within which to qualitatively compare different AGI systems would be really valuable. It would be really interesting to have all the architectures proposed and built by all the people in this room, like acting in the same world, controlling the same bodies, and doing the same sorts of things. I mean, even if you don't boil it down to who can get a better score on this particular measure, because that has some strengths, as Josh was saying. It also leads to problems, like we've seen in the machine learning community, where everyone just bangs on, can I get 0.2% better accuracy on this thing? So it may be that the measurement and competition isn't the key thing. But having a common environment to qualitatively see the strengths and weaknesses of whatever a system is doing, I think that would be a very valuable thing. Because it's frustrating now. You see my system doing something on one problem, someone else's system doing something on another problem. How those problems are set up in detail is technical and will require a lot of study to even understand. So it's hard to understand what someone else's system is doing, because we're all evaluating and testing them so differently. So it's clear to me that shared environments and tasks are valuable. And to my mind, there's pluses and minuses to quantitative benchmarks and competitions. So I think that's a way we're well over time. So we can continue this discussion over lunch. We get meet back here at 2.30.", '32.036322593688965')