('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/da-9zPgxWBY"> </iframe> </center>', " Get this to work, amazing. Okay, I think the last, or the best contribution of the last century to the sciences was the discovery of the concept of computation. Computation treats arbitrary systems as sets of discernible differences. The fancy name for discernible differences in science is information. And we can group these discernible differences. The fancy name for discernible differences in science is information. And we can group these discernible differences in states and then can describe in arbitrary systems by how these states change. And the way they change is usually described with some kind of transition function. And the description of a system using a transition function and states is the most general way to capture the notion of computation. Digital computation is a very special case. In digital computation, the states are discrete and the rules for state change are deterministic or probabilistic. We can use this language of description to describe any system that presents itself to an observer as a set of discernible differences. For instance, the universe or the observer itself. So, if we look at these different computers, what kind of computer would we need to run the universe? At the moment, we don't know that, but imagine I describe the universe as a set of discernible differences, observations that I can make, and how these change into the next state. And I would want to build a computer that can run our universe and I want to buy the cheapest computer that I can get away with. What would I need to look at in order? The cheapest one I could buy is probably something like a very big computer like this one, just with much, much more memory. It's a finite state machine, discrete and so on. Way more memory. Speed doesn't matter if you're inside of it, of course. Because you won't notice if it's a finite state machine, discrete and so on, way more memory. Speed doesn't matter if you're inside of it, of course, because you won't notice if it's slow or fast when you are inside. It could be probabilistic, which means you can branch between the states with some probability function, or it could be a quantum computer in which each state can be a superposition of states. Or maybe this is not enough and maybe we need something like one of the three versions that I just mentioned with infinite amounts of memory. This is going to be infinitely more expensive, but it could get even worse. If it has infinite memory, number one, that's your Turing machine, as we know it from these usual definitions of computation. It could also be a geometric hypercomputer, which is what physicists believe for the longest time, that is that the universe is something that is continuous and is able to perform continuous operations. It's something that you cannot do with a digital computer, can only approximate it with a digital one. Or it could be an acausal hypercomputer, one where you can take information and calculate it and send it back in time and use these observations which break causality in a closed time-like loop. That's also a computer, it's just one that is not, cannot be simulated on a discrete state machine. So, computation is the description of an arbitrary system as states and a transition function and computationalism is the description of an arbitrary system as states and a transition function and computationalism is the notion that computation is necessary and sufficient for describing all conceivable observables, including of course consciousness. By the way, number one and two are the only ones that we know how to build in this universe and one of the speakers after me, Hartmut, is going to talk about that he's working on building number three. And one of the speakers after me, James, is going to talk about that the universe is possibly in a causal hypercomputer. Roger's thesis is more radical. He says that there's something going on in the universe possibly that is not computational, that cannot be described as states and transitions between states. Something that is not afforded by current physics, so we would need to go beyond physics, and something that boggles the mind because it's very hard to imagine what that could be. What is not computational? Well, there is stuff that is not computational. For instance, most of mathematics. How can that be? Well, mathematics is not computational? Well, there is stuff that is not computational. For instance, most of mathematics. How can that be? Well, mathematics is not computation. Mathematics is the domain of all formal languages. It's the domain of specification languages. You can specify arbitrary things, most of which won't work, right? In language, you can say stuff that doesn't pan out. And most of mathematics is not computable, whereas computation is the domain of possible implementations. So, computation itself can never run into a paradox or be impossible or anything like this. Computation is just what it is, a state, a transition function, then comes the next state, transition function, comes the next state, that's it. So, if we try to describe the universe, we do this with physics. If we try to describe minds as information processing systems, we can do this, for instance, using artificial intelligence. This was largely why this field was started in the 1950s, after people realized that psychology isn't going anywhere. Can AI reveal the nature of our minds? Current AI, probably not. Current AI is in strong movement, which is called deep learning. It's super successful. And arguably, we started pretty much five years ago. Five years ago, a group of engineers, led by Andrew Ng from Stanford and Google, built a neural network with nine layers on 16,000 computer cores, and they showed it randomly selected frames from YouTube. 10 million of these frames. So, order of magnitude more than a baby is going to see in the first months of its life. And after that, it was able to recognize this stuff from an image database with 15.8% recognition accuracy, which means you could show it an arbitrary picture and it could tell you in 16% of the cases what's on that picture. And it hadn't been trained to know what it was seeing on these YouTube frames. It was just looking for structure, for anything that makes a discernible structure in there. And it got internet famous because one of these things it could recognize very well was cats, which is not surprising when you remember that it was trained on YouTube frames. So deep learning is basically using stacked hierarchies of layers, which are feature detectors, and then uses a method, largely gradient descent, to approximate functions that allows us to classify objects and learn policies. And this means that in 2015, these things outperformed humans in recognizing images in their database that didn't stay at 16%, they really outperformed people at doing this. They also got better than people at playing Pac-Man, being trained from scratch and not being told what to do. Last year, they outperformed people at the game of Go. And maybe 2020, we will have the first self-driving cars. At the moment, they're already outperforming the average humans in driving a car, which we, for some reason, don't think is good enough because they sometimes make accidents, but on average, they make fewer accidents than people. And there is an argument to be made, if it's really about saving lives, we should switch right now. Of course, this is very far from the mind. And when you think about building artificial minds, we often think about the Turing test, a system that convinces people that it's intelligent. I don't think that is the actual challenge. The actual challenge is a system that performs a Turing test on us, right? It's what we do with each other. We talk to each other to find out what the other person is conscious of, right? We don't try to convince others just that I am intelligent. I already know that. I want to find out what you know. I want to find out what you think about, what you are aware of. And once a system does this with us, we can recognize that the system has an idea of what it is, that it starts to become a mind, that it recognizes us as minds, that it makes models of the world in which minds figure prominently. How long does it take to get there? We don't really know. Marvin Minsky said, if I believed in realism, if we work really hard, we will get there in between four to 400 years. And we are pretty much on track, I think. But we could say that these systems that we need to build cognitive AIs are different from the narrow AIs that we have right now. The narrow AIs that we have are largely pattern recognizers and object classifiers. So for instance, if you want to get such a system to recognize Alyssa, one of the dancers in that image, it means that the system has to filter out everything in the image that is not Anna, that is a variant, to find the invariant. The invariant would be Alyssa. So it treats to filter out the pose, the lighting, the dress, the facial expression, and so on, until only Alyssa is left in all the images that contain her. But this is not quite what people do. What people do is they take all these different concepts and don't filter them out. There is no background. There is only partially occluded other objects and images, right? What we do is we create a whole world from what we look at. And we relate the features of that whole world with each other. And we do this starting from patterns, then we go to percepts, and then we go to mental simulations. And then we go to concepts, which are abstractions over these mental simulations, and then to linguistic symbols that allow us to synchronize our conceptual domains and to some degree our mental simulations. So, minds are not classifiers, they are simulators and experiencers. And they start out with feedback loops because they are in the service of control of the needs of a social primate. Most of the feedback loops that we have are not available to our conscious interaction with them because they don't need to. For instance, they regulate our body temperature, our heart rate, our breathing patterns, and so on, autonomously with a lot of feedback loops, many of which are in the brainstem. But for some of the things, we need to be able to control what we are doing and adapt our actions. And for that, we have pleasure and pain. They are residing in the limbic system, largely, and pain tells us, please do less of what you're currently doing. Pleasure tells us, do more of what you're currently doing. Pleasure tells us, do more of what you're currently doing. And then we have impulses, and they are directed on future pleasure and pain. And we have a whole motivational system that gives us impulses, many of which are physiological, some are social, and a handful are cognitive. My own work is related with discovering these motivations and building computation models of them. To associate these motivations with situations in the world, many animals have a structure that is called a hippocampus that matches patterns in the environment with the needs of that organism. So it knows what impulses should lead to which world situations and which world situations it should try to get to. But of course it's very difficult to make sense of the world if it's just a bunch of patterns. So we have something more than a hippocampus, we have the neocortex. And the neocortex allows us to generalize over situations. To say for instance, what do all the restaurants have in common, how can I recognize all these situations where I can get food and so on. Or how can I recognize Alyssa in many, many situations. And to do this, our brain needs to do something very tricky, it needs to generalize. And a good metaphor to understand what it's doing is a synthesizer. If you ever played around with a synthesizer, it has a bunch of knobs and buttons, and you can twiddle these until the synthesizer makes exactly the sound you want. Now imagine you take that synthesizer and you connect it to the output of your cochlea. This is the organ in your ear that basically performs a hardware Fourier transform on sound waves. And this is then piped to the cortex and the cortex takes these signals and it fiddles knobs and buttons until it's able to predict what signals are going to come next from the cochlea, which means it understands the current sound. And once it has done this for a number of sounds, it generalizes over those sounds and looks, what do all these sounds have in common? So for instance, all these different sounds have one property that makes them different, and that's the pitch. Once you filter out the pitch, you recognize you can now take a whole class of different sounds and lump them into one, into one function that makes it predictable. And then once you discovered the nature of pitch and the different sounds that you can generate it, you'd find another feature, for instance, loudness. And once you've done this, you get to the next level and you can build something like a sequencer, something that is able to assemble sounds into rhythms. And then you go a level above that and you can build harmonies and melodies and so on. You go a level above that, you can build songs and you go a level above that, you can build harmonies and melodies and so on, you go a level above that. You can build songs and you go a level above that. You can build musical styles and so on. And then you can go across the modalities and do this for different modalities. You do the same things for color, for spatial frequencies and so on. You take your synthesizers, plug them into high-level synthesizers, and you do this until you can create a whole mental simulation, a dream. Because this is what the world is to us. It's a dream. We don't live in the world out there. We live in the dream, in the simulation generated by our neocortex. The same circuitry that produces dreams at night, produces dreams during the day, which gets loosely synchronized with our sensory input. Right? I think most of us understand that, that we don't live in the world out there. The interesting thing to realize is, there is no colors in the world out there. The interesting thing to realize is there is no colors in the world out there, no sounds, no people. There is a weird quantum graph out there probably. We don't know what it is, but colors, sounds, people, and so on exist in our neocortex as mental representations that allow us to predict the next set of bits that is going to hit our retina. And above these mental simulations, we create concepts as abstractions of what elements in the simulations have in common. It's like the address space of our sensory motor scripts. And then on top of those, we can create linguistic symbols to talk about them and self-report and so on, control our self-reports. For this, we need different types of learning. One is function approximation, which is Bayesian, which is parallelizable and it's exhaustive, so we use this, for instance, to need different types of learning. One is function approximation, which is Bayesian, which is parallelizable, and it's exhaustive, so we use this, for instance, to learn the laws of perspective in the first months of our life. And then we have scripts and schemas, which are narratives, stories that we use to describe, for instance, what happens in a restaurant, or what happens in the financial markets, or in society, or in politics. And this is largely what we are talking to each other. And we can use these conjunctions of those to direct our mental simulations. So this is what the neocortex is doing. It's simulating a dynamic world. And at the moment we don't have all the elements for that in place in our machine learning models. I think the basic unit is probably not a neuron because it needs to be compositional, it needs to be able to rearrange the different things. The basic unit is probably a cortical column. We have something in the order of 100 million of those. Each of them is probably a state machine made from something like 100 to 400 neurons which has a function approximator and combines it with different learning modes in different ways. It can link up to its environment. And these cortical columns are linked up into cort its environment. And these cortical columns are linked up into cortical areas. And these cortical areas are linked with each other into processing streams and form these hierarchies of synthesizers, so to speak. And together, they are something like an orchestra. We have something like roughly 50 brain areas that are made each from an order of 100 million of these units, and they talk to the neighborhood and they form processing streams, right? And it starts out from sensory patterns and motor patterns and gets integrated into more and more abstract things. And there's also a conductor. And the conductor is not a homunculus. The conductor is not something that has superpowers or something, it's a brain area like others. In the same way as a conductor in orchestra is not a superman, but he's similar to the musicians, he just has a different task. And the task of the conductor is very similar to the conductor in orchestra. Its task is to resolve conflicts and tell individual instruments to be a little bit louder or a little bit louder, or to make something slightly different, or to listen more to this guy to his other side, and so on. And a conductor is in this way controlling what's being played tonight. If you take the conductor away, the person becomes a sleepwalker. The sleepwalker can still do interesting things, like get up from bed, make dinner, open doors, walk on the street, answer questions in random ways, but there's nobody home. Nobody is able to integrate this with the motivation of a consistent agent and create a consistent protocol of what's happening and learn based on that protocol. So this is what this conductor is doing. It gives you attentional awareness and integrates experiences. And the brain, it's all done largely by the dorsolateral prefrontal cortex and some neighboring areas that provide infrastructure for that. But an important thing is to realize, of course, you are not your brain. You are a story that your brain tells itself, largely through the protocol of the conductor. So you have the sensory perception, you will distribute the trivagical representations from them, which are your mental simulations, and then you have your conductor that is able, in a secondary process, to scan part of these mental representations, basically see what your brain has decoded from the world. And most of the stuff is done completely subconsciously, autonomously, in parallel in the background by your brain. Your conductor is really paying attention to only very few things there, and more largely not in real time. And on your model, you have your mental stage, world model, self model, procedural memory, object memory, and so on. So all this is part have your mental stage, world model, self model, procedural memory, object memory, and so on. So all this is part of your world representation, including your model of yourself, of how you interact with the world, and the model of that model, which we usually call the self. Your conductor also keeps a protocol of what it attended to. And this protocol is the only place where experience is integrated. It's something like an equivalent of experience is in many parts of your brain in distributed fashions, but these different parts don't know about each other unless they talk to each other. And when you self-report or report to other people, you need to have an integrated place where this information comes from. It's exactly that protocol of what you attended to. You can only remember of what you attended to and put in that protocol. You can also then use this protocol to get activation into it to recreate a mental simulation or partial mental simulation that is very similar to the simulation that happened when you looked at a particular thing or were in a particular situation. So you can use this to recreate situations, you can recreate memories, you can recreate particular dreams and learn from them. And they're sufficiently similar to the situation that you had in that recreate particular dreams and learn from them. And they're sufficiently similar to the situation that you had and the situation that you can learn from them. And then you can also put the fact that you accessed that protocol into your protocol, which means it now becomes somewhat recursive. Doesn't mean that you are truly recursive, you just become aware of the fact that you did access your protocol and this created a particular mental state inside of you. So basically you take the model of the fact that you did access your protocol, and this created a particular mental state inside of you. So basically, you take the model of your own recollection that you model at some point in your mind, because it's part of your mental landscape, and put that into a protocol itself. So this conductor maintains an attention protocol, which is a narrative about the self, and it has access to the protocol, which gets integrated into the protocol itself. I think that remembering having been conscious of something is both necessary and sufficient for consciousness. That is, I don't need to be actually conscious of something, it's necessary that I remember having been conscious of something, so I can report about it to myself and to others. The availability of my mental simulations to my conductor is necessary and sufficient, I think, for access consciousness. And the integration of the access into the accessible protocol is necessary and sufficient for my self-report about the awareness. And the self-report, in turn, about my memory of awareness is interpreted as my awareness in actuality. What does that mean? It means, for instance, if you are drinking a cup of coffee and you feel how the coffee goes down your throat, this is of course not what's actually happening. What's actually happening is that you have some sensory neurons in your throat, in your larynx and so on that get stimulated by the coffee, and some of that information gets integrated and transmitted to your throat, in your larynx and so on that gets stimulated by the coffee and some of that information gets integrated and transmitted to your brain and there it gets raised to a simulation in your somatosensory cortex which happens something between I think 700 milliseconds or one and a half seconds after the fact and it's totally different from what physically happened with the coffee in your throat, right? But you perceive it at the same time as which you subjectively take the cup and take it to your mouth and you hear the clinking of the cup to your teeth and so on, all these things happen simultaneously even though the processing of the sound took a very different amount of time and the processing of your movement took a very different amount of time because they're very different sensory modalities with different mental representations. How is it possible that they subjectively happen at the same time? Because they are patched together in the protocol and you only get access to them long after the fact. They're patched together in the protocol in a way in which they didn't happen together. Which means you're not conscious in the right here, right now, when you drink that coffee. You're not aware of that coffee while you drink it. You're only aware of it a long time later. And when you actually navigate the cup, you do this with reactive subsystems that have been primed by previous conscious interactions. Right? So I think that you're probably not conscious in actuality of things like that. You're probably having a memory of having conscious of something, of experiencing something that didn't take place in exactly the same way as you remember it. That's the easiest explanation of how to make a machine conscious, is to give it a memory of being conscious of something. That's it from me. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.", '4.633877754211426')