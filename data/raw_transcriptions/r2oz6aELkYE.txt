('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/r2oz6aELkYE"> </iframe> </center>', " Okay, I would like to emphasize the importance of AGI as a foundational discipline in cognitive science, and I know that might be seen as a little bit provocative in many respects. I agree with Christian. For instance, they need to break free from the methodology of cognitive science, which in some sense is either engineering or applied mathematics. And our goals are different, but we do need to borrow the tools of computer science for some particularly good reasons. I would like to talk how we got here in the situation that we have to do AGI as a new discipline outside of the disciplines that were originally designed to study the mind, namely psychology and cognitive science. Why we should be doing computational models, why we don't do this within psychology, why we don't do this in neuroscience, and why we don't do this with the traditional AI, but why you add new letters to the old acronyms. Psychology struggled out, in my perspective, pretty reasonably with psychophysics, the idea of having a scientific mind that is not a cultural science or a descriptive science, but that tries to find laws. In the language of the day, the Occasionalism and so on, the scientists divorced from the humanities, that meant that the science of the mind should be homosthetic, it should find rules that describe dependencies, today we would say, in systems and so on, and make them accessible to measurements and empirical study and to conceptual analysis on a level that goes beyond description, especially subjective descriptions and hermeneutical descriptions. The guys that you see down there, the first one is Hermann von Helmholtz. He did, for instance, studies in the stretch of activation in nerves, how much distance could be covered in peripheral nerves and so on. He also studied aspects of what we would call today cognition. The guy in the middle, this is not Freud, this is Wilhelm Wundt. He is basically the father of experimental psychology. He set up a lab in Leipzig in the 1800s and did basically proper experiments and also theories on what he was actually looking at, theories on emotion and so on. Gustav Fechner is also considered to be one of the fathers of then abstainable physics. Coming from physics, he studied, for instance, the resolution that we have in terms of stimuli, and he discovered what is known as Fechner's Law in 1816, which basically says that the subjective strength of the sensation is related to the logarithm of the strength of the stimulus. So this was pretty early work. Now, this guy is going, and psychoanalysis existed more or less in parallel almost at the same time. And Freud came up with very ingenious descriptive models of the mind. Models that you, that are in a sense not predictive, that you can disprove them or falsify them, that give you handles on descriptions, but that are not accessible to measurement. This has some benefits but it also has the drawback that for instance if a psychoanalyst has a diagnosis of you that you have no way to disprove this diagnosis ever. It's always right. This is the way it is defined. And in the early 20th century there was this movement of positivism in science, which said the value of a theory is basically the prediction that it makes that can potentially be falsified. Well, this is clearly not scientific, and we should try to get psychology into a scientific domain. And in order to get there, they said, okay, what things can we measure or we can make predictions? Well, this this observable behavior and the way it relates to stimuli. And looking at things that are hiding behind the stimuli is not something that we should be really doing if we are to be taken seriously as scientists. And the resulting movement is called behaviorism. And at first, it was a methodological point of view, then we had ontological behaviorism, which was, for instance, promoted by Skinner, who said there is basically nothing but operant conditioning, there is nothing but like blocks that does reactive stimuli and associates them with behavior. From our modern point of view, I don't think this is a very dangerous perspective. It was very productive, we could do a lot of experiments and get lots of results. But it's limited because we can show that without hidden layers, we can learn certain problem classes. And we do need hidden layers. We do need something that is akin to mental representations. So in a very strong ontological behavior, it can easily be disproved. But the next generation of psychologists was no longer ontological behaviorsists, but it was pragmatic behaviorists. It said, okay, maybe there is more to psychology or to the mind than observable behavior. Maybe things like thoughts and emotions and so on do exist, but it's not scientifically proven at that. And this is a very dangerous perspective because it means that you do exclude certain things from scientific scrutiny and it's difficult to have theories around that. So, from this perspective, psychology moved into a direction where it became very difficult to look at errors in problem solving and so on. And psychology, for the most part, even when it's readily recovered from behaviorism, still suffers from its legacy. It's still in a kind of defensive position against behaviorism, it has little focus on the non-behavioral aspects of condition, and has more or less narrow experimental paradigms, that is, you are encouraged to formulate a hypothesis and then come up with an experiment that either disproves or proves the hypothesis for a number of subjects in a certain setting. And if you look at very impregnable architectures like John Anderson's papers, it's typically that he achieves this way you can publish by having an introduction that tries to give some references to ideas, theoretical ideas that he has. Then he comes up with an experiment with proper study of subjects, data, and so on. And then in the conclusion, he tries to hide a few other elements of his larger theory. But the paper that just publishes a theory with many free verbs and tries to integrate what you all know about psychology also from settings outside of the lab because they are too complex to fit into a lab like the settings that you would find in the Dostoyevsky model. It is very hard to publish in psychology. You don't get any score for that basically. So psychology in some sense is very antagonistic to formalized theories and a theories of how the mind works as a whole. There are very, there are no systematic and systemic theories on how the mind works. And then again, Christine also said that there's this exclusive focus on human performance or maybe animal performance. There's not really a study of the mind per se, of the class of systems that would be considering sentient or conscious or emotional or reasoning and so on. So in a way, we had to reinvent this and one way of reinventing this was artificial intelligence, which treated the mind as an information processing system. That is a system that is, in a sense, computational. Of course, the mind is less than a Turing machine. It's not a Turing machine. We'll be talking about this in a moment. Thinking, perception, volition, and normativity, and so on, can all be explained in terms of information processing. That's the idea that we have behind AI. And you can test our ideas on how the mind works, not just with a single experiment. You don't have to confine this to one or two free variables. And you can test our ideas on how the mind works, not just with a single experiment, you don't have to confine this to one or two free variables, but you can ask the big power questions by asking theories in such a way that you can answer them as constructionist approaches. You can build theories that you can implement on a computer that you can then run and test whether they produce the desired performance. that we can then run and test whether they did produce the desired performance. If you look at the name artificial intelligence, it has a pretty long legacy, which probably started with logic and automatic theory and the idea of computability. Then we had computers, we had information theory, challenge work and so on, and programming languages. We went to cybernetics and simple systems, and later on we came to implement neural networks and autonomous situated agents. So this is basically a long trajectory of stuff that contributed to our current understanding that also is being found in AGI today. If you look at the start of AI, it was probably the Dartmouth conference where we had people like especially Mark Kaminski, Nathaniel Rochester who invented the first assembler and also did the first neural network implementations ever. John McCarthy who is interactively responsible for this among many other things. Paul Chen, the founder of modern information theory. And at this Dartmouth conference, or near the Dartmouth conference, he coined the name of this distance and artificial intelligence for better or worse. McCarthy did it. Hmm? McCarthy. Yeah. Not Minsky, McCarthy. Yeah, McCarthy did this. Yes, you are right, sorry. Yeah. Not Minsky, McCarthy. Yeah, McCarthy did this. Yes, you are right, sorry. Okay, so in a way, I think that our AI approach might be marked as a tumor instant in the 1950s paper, where he, among many other things, sketches the idea of the tumor test, which is not really a test for artificial intelligence, arguably, and also answers many of the objections to AI that have been brought forth in the next 50 or 60 years, and gives very often reasonably good answers to those objections. So this is the paper that we make all our students read in the beginning. Then in the 1950s and 1960s, a lot of low-hanging fruits were read in a very, very short amount of time. We had the first planning systems and scheduling systems, the first simple natural language parsers, and first learning systems, and all this made us super optimistic to believe that this progress would continue at that pace and very soon we would be at human-level intelligence systems and would solve that ribble. 1969, Marvin Minsky accidentally killed connectionism. It had then to be reinvented. 1970s and 80s saw the rise of expert systems that were slightly overhyped and then crashed, and are still alive, for instance, in the form of Watson. Then, connectionism was reloaded. We've got novel AI, which among other things, focused very much on robotics and dynamical systems and throughout many games with the bathwater. And then we had the agent paradise, which emerged in the 90s. And currently, we have this battle of statistical and probabilistic modeling versus computer systems. But most of AI currently does applications and applied mathematics, and there is a particular reason for that. In 1973, there was a big department in Edinburgh. And this department had Don, was part of Don Ritchie, Rich Gregory, and Logan Atkins. And they wanted to have new funding, a big funding initiative, and applied to the Royal Academy of Sciences to get them this funding. And the Royal Academy of Sciences decided to have someone who's completely independent look at it. And this completely independent guy was Sir James Lighthill, basically an atmosphere scientist. He was also a mathematician and interested in many areas of research and he was trusted to be a complete outsider so he could give a reasonable estimate whether this was a good field to invest in for the Royal Academy. And then he went to Edinburgh and totally hated on Ritchie. And as a result, he wrote a very damning report. It was first published in, it was not really published, it was not intended for publication, but was leaked, and then it was published in Ixgerts, in the paper, it's Artificial Intelligence, a general survey. And Leibniz basically said that there are three domains in AI. One, A, is basically automation, building robots, having applications, it's a very simple thing to do. Then you have C, which is basically model-competitive systems based on what we find in biology, very closely aligned research to nervous systems and so on, or individual neurons and so on. Obviously, this is what we wanted to do too, it's very reasonable, but then there has to be some kind of a bridge discipline in between. This is about building robots to understand cognition, and this doesn't work at all. You shouldn't be doing this. And then Midgey and McCarthy and others spoke up and said, you know, it's not really about building robots, it's not really about building robots. It's about building computational theories of thinking and the mind. But nobody would be interested. And the Lighthizer report basically killed funding for AI large scale in the UK. And in its wake, it had a very detrimental effect on funding for AI in general in the world, and initiated what is generally known as the winter of AI. had a very detrimental effect on funding for AI in general in the world. And initiated what is generally known as the winter of AI. So at that point AI basically became hesitant of following this big goal of understanding the mind and building human level intelligence and beyond in terms of its applications. Okay, so Christopher Leclerciggins, one of those three guys, by that time already had coined the phrase, cognitive science, the name cognitive science, and went to a different department, originally was a physicist too, and there was the idea to have a science that is drawing from all the other sciences of cognition and combine this into a common paradigm, this computational models of course. that is drawing from all the other sciences of cognition and combining this into a common paradigm as computational models, of course. And in a way, it's now popped up again in the context of psychology. And we have Newell and Feynman, who just had done the general problem solver, now turned towards SOAR and came up with the first generation of cognitive architectures. To that first generation, I would be able to also R and Epic and a number of others. In this form, a small be, but also R and Epic and a number of others. In this form, a small movement, mostly in the psychology, but these cognitive architectures have never really become mainstream in psychology, I think. But the goal of this, these unified theories of our cognition is of course to integrate all the sciences of the mind. From the perspective of the computer scientist, the people in cognitive modeling communities reinvent a lot of the mind. From the perspective of the computer scientist, the people in cognitive modeling communities reinvent a lot of the wheels that AI had already invented. So for many of the people in traditional AI departments, they seem to be all dead. Whereas for many people in the psychology departments, they seem to be not left of the psychology. Still, cognitive science as a new discipline was born with the hope to unite all the disciplines that contribute to an understanding of the mind. That is, philosophy of mind, linguistics, neuroscience, and AI works. And the main premise of these joint cognitive sciences is to understand mind as machine, as an information processing system. Now this motion of machine is a little bit controversial. Here we have this formulation by Leibniz in 1740. He basically said, he mentioned that there is a machine sort of arranged as a ring for thoughts, experience, and perception. And imagine you blow this up, like a giant mill, and you walk in there and you see the mechanism. You only see parts that would be pushing and pulling at each other. And obviously, by pushing and pulling parts, you would never be having anything that would explain how perception and, as we would say today, cognition would be explained. This notion of mind as a machine for lab rats was the clear indication that mind would not be a machine. It must be something beyond a mere mechanism, beyond mere physics. And what we have now is computationalism, the position that the mind is indeed a machine, but it's a computational machine. It's one that is not about pushing parts together, but it's about processing information and all these parts, whether they are moving or whether they are exchanging electricity or whether they're doing anything else, it is only incidental. It's only an implementation of the fact that the mind processes information. And this notion of computationalism is extremely powerful. It also encompasses things like quantum computing and so on, but what's so important about it is the contemporary form of the mechanism of view, it's the current form of mechanism. It's completely agnostic with respect to whether the world is made of matter or whether the world is made of ideas. In any case, we have to explain how minds can process information. And the science of that information processing is the science of computation. So we can distinguish something like a strong universal computational perspective which says the only thing that we can ever know about the universe is that it's strong information and our systemic boundaries, that is, discernible differences. And to do that, if you find regularity in it, it's sufficient that the universe concludes that it does provide us with regularities in this information. Then we have a strong cognitive computation, which would basically say, no matter what we can say about the universe, but what the mind does is basically finding granularities of information, of processing information, so the mind is ontologically an information processing system. And then, if you don't believe in this, you could have a weak computation disposition. No matter what the mind really is and what it does, every theory about the mind that we have that explains how the mind works must necessarily be computationalist. The problem with computationalism is that it's, in some way, it's mechanism, we know that, but it could not be formulated before the definitions that you got in the 30s by Church and Turing, which as philosophy goes is pretty recent and they largely have not calculated into philosophy of mind, philosophy in general and into society in general. Most people don't really understand what computation really is, including many people in computer science even. In those original definitions of computation we had things like the lambda calculus, which is basically a fancy way of doing cut and copy in strings. And then we had the Turing machine, by which I'm going to show the video. You all know the Turing machine. Now, the problem with the Turing machines is that we know that they deliver wrong intuitions. People have this idea that there are tapes and wheels involved, and they will know that these are virtual and theoretical. And if I teach students in philosophy both lambda calculus and the Turing machine, and they make the proof that they are equivalent, and I ask them afterwards, will something like half of them will insist that the lambda calculus can do more than the Turing machine afterwards, because it's doing things in parallel. So, Turing machines do create wrong intuitions and this is really a big problem because they drive a lot of intuition pumps in arguments against AI and cognitive science. And they complete many details of cognition, computation in the wrong ways because they don't properly react to random things or non-reversible computation or limited things that are easy to implement in a Turing machine but are not part of the original definition. The next thing is that minds of course not Turing machines but they can do less than Turing machines. A physical, realizable Turing machine is something that doesn't have an infinite tape and doesn't have infinitely many steps, but it's going to have a very finite tape and is only going to run for a short amount of time compared to infinity. And it might be that all the theoretical paradigms to define computation are equivalent if we have unlimited resources, but they're not equivalent once we start constraining them. If we have another calculus that only allows so and so many steps of resolution and only so and so many characters in the strings, then there's not going to be an exactly equivalent Turing machine that has so and so many steps of computation and so on, or a brain that has so and so many neurons and so many cycles of activation exchange. So the constraint systems of computation are not going to be equivalent. And it's not going to be helpful to think about in terms of Turing computability, it's going to get you on the wrong track. Now, the good thing is that it really doesn't matter so much because in reality when we do computational modeling or cognitive modeling or AI, we are not that much concerned with Turing machines. This is a debate that is basically only important in discussions between philosophers of artificial intelligence. And I think that philosophy of artificial intelligence, sadly, is about as important right now to us as, to bring this old metaphor, as old mythologies to birds, or as, say, philosophy of evolution would be to a molecular biologist. If I go to philosophy of AI conferences, I have this sad impression that I'm like a molecular biologist who sits in on meetings between evolutionists and creationists, which battle it out. It's not very helpful. Okay, the problems of AI are really not about partnerships, they are about personality controls. We don't have a shared personality right now, even among the AGI community, and it's difficult to convince AI to go back to its original goals because they're afraid to lose their funding for good reasons. And they're productive the way they are, unless they change things. Neuroscience is very productive, but it's mainly descriptive and it's below the level of information processing. It's more or less on the level of nervous systems. So it's important for us, but it's not exactly what we want to do. Problems of psychology is mostly a theoretical and experimental, and philosophy right now has no tools for testing its theories because it has not adopted our computational paradigm. So right now we are in a situation that the sciences of cognition basically are not forming a cohesive whole, but there are different sciences that just happen to compete for the same funding. And it seems that this funding race has been mostly won by computational neuroscience, or neuroscience in general, experimental neuroscience, at that point. And we are in a position where we need to shift this a little bit, and AGI is an attempt in a way to reinvent AI and then cognitive science as a science of the mind to build constructionist theories of the mind. So what we need to do, our challenge for HAI is to reintegrate the cognitive sciences on a constructionist paradigm by implementing theories so we can test them. We need to focus on role models, not just isolated properties and narrow AI. We need to focus on all the components that we consider to be necessary to form the mind. And we need to do science instead of engineering. And I think this is a very important point. A very good heuristics would define an engineer from a scientist is their answer to the question, what do you think when you are done? What do you think will happen when you ask a scientist, what do you think when you are done? What do you think will happen when you ask a scientist, what do you think when you are done? What do you think when linguistics will have understood language? What do you think geography is done? When do you think biology is done? I don't think that this is ever done. It's very rare that scientists pick up and say, okay, now we are done, let's close it down in science and do something else. Usually other people do this because they realize it's no longer productive. There's also other reasons why people are not going to stop with non-productive paradigm. But of course science is done as long as it is productive. And an engineer is somebody who makes an estimate of how long it will take him to finish a project. And we are not just finishing a project. We don't have a specification yet. We are in a situation that we have to come up with a specification in the first place, and this is going to be part of our science. We don't have all the regularities yet. We don't know all of the neuroscience and psychology that we need to integrate in this common goal. And we have this impression that we have so many problems, that we have a certain methodology that's going to make a difference. And then we might get a traditional thinking, which is a big trap, and believe that once we follow through with our current project, we are done. And I don't think that we are. We should go away from this narrow engineering paradigm and realize that we need to turn this into a science. That's what I wanted to say. Thank you. Thank you. Thank you. Thank you.", '14.608724594116211')