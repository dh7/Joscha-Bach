('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/6xHVtgwNBcY"> </iframe> </center>', " So, hi everybody. We now come to the final talk of today. I hope you enjoyed our program so far. The first of our speakers is Joshua Bach. I think he doesn't need a lot of introduction. He's an AI expert and a philosopher and he's really a regular at 1E9's conferences and events. And we love to hear him talk. If you know him, you know why. If you don't know him, you will find out in a bit. And today he's not alone on stage. He brought along a very fascinating and inspiring colleague of his, Julia Sander-Mirskaya. She's a research scientist here at Intel in Munich, and she works in neuromorphic computing. She's a research scientist here at Intel in Munich, and she works in neuromorphic computing. That was a hard word to remember. Yes, and I think it's interesting because these are two very different angles on our topic. That makes for a very interesting conversation, and the topic is the third age of artificial intelligence understanding machines that understand. So please welcome on stage Julia Sander-Mierskaya and Joscha Bach. Thank you. Thanks Christian. Yeah, I'm a cognitive scientist. I currently work in a team at Intel Labs that tries to figure out what comes after deep learning. And Julia and me have a shared interest. We are interested in how our minds work, how consciousness works, how we relate to reality, and what this is all about. And we come from very different angles in this. I come from the crystalline world of trying to teach the rocks how to think by etching geometrical patterns into them and teach them logic and then extending this into perception. And you come from the area of thinking muck, right? Of how to build things that are like this biological stuff. That's true. Like following Feynman's idea of understanding through building. In order to understand something. It's a good idea to try to build it. Yeah. And we do have very different minds. I think I'm much more into baroque music and you are more into punk. So... Grand. When I was younger. But baroque is fine. Classics is... Yeah. We do have a lot of convergence and overlap. And the thing that we are looking at is the same. We have the same motivation and trying to... not just building things that have technical applications, even though this is a big part of our work, but we are interested in understanding things. And to us, building things is instrumental to this understanding. So what we're going to talk about tonight, hopefully, is going to be what we are seeing as the third age of artificial intelligence, machines that are increasingly able to understand, and how we can understand how such machines might work. Let's see if this works. It does. AI is basically two things. The big bulk of the field is advancing information processing. And if you would call it advanced information processing, I suspect that most of our colleagues would be fine. It's the pioneer battalion of computer science. It's very productive. And at the other end, AI is something different. It's a very small subfield of AI is concerned with automating the intellect, which is a philosophical project. And this philosophical project is quite old. And it's basically about bridging mathematics and philosophy by mathematizing the mind, by finding a way to express thoughts and perception in mathematics that we can execute on a machine. So it can extend beyond the capabilities of a single human being. And this idea is quite old. For instance, Leibniz had this idea that we can represent concepts in some kind of space of meanings and then perform calculations over them. Or Frege came up with the Begriffskalk√ºl. And Wittgenstein had this at his idea in the last century in the Tractatus, where he describes 100 years ago a way in which he hoped that we could take the natural language of philosophy and formalize it enough to make it meaningful. The difficulty with philosophy is that the languages of philosophy are vague, the natural language of philosophy and formalize it enough to make it meaningful. The difficulty with philosophy is that the languages of philosophy are vague, they're natural languages, so it's hard to say something in them that is strictly true. And on the other hand, we have mathematics where we can define truths formally, but in these formal languages, because they are so simple, it's very hard to say something about the real world. When you say something in mathematics, you typically talk about an extremely simplified, abstract system that you hope is similar enough in some aspects to the real world, that you can use the predictions of that system to apply in the real world. And basically scaling up mathematics so far that you can do philosophy in it, that requires that you have machines that can execute that mathematics because you cannot write down the equations in a sense. And the big difficulty that Wittgenstein struggled with and eventually conceded defeat at the end of his life in the philosophical investigations was to deal with perception, because he didn't see a way to do this in his grammatical languages. But at the same time, his pupils, especially Alan Turing, but also Alan Church and many others, discovered computational languages and they discovered that they have all the same power and that mathematics is actually computation and that we can build machines that perform computation and that we can find laws about this. And so AI was born. And the first age of AI was what I would call classical AI and started with the beginning of the field in roughly 1956. And most of the systems that were built were based on logic, in some sense like Wittgenstein proposed. And they used handcrafted algorithms for problems like chess that would then be solved by that algorithm. And you have narrow solutions that only work in one given domain and cannot be easily transferred to other domains. And then people said this is not really going to scale and we need something else, which was called new AI. And this dominated the 90s. And it was about embodiment and robots and going away from representation and focusing on self-organizing systems and dynamical systems. And this stuff didn't work. Basically this embodimentalism, new AI, novel AI failed. But this didn't stop these developments and these ideas. Eventually, something worked. And what worked is the breakthrough that we had in 2012, deep learning. Deep learning itself, the ideas are much older. Most of the algorithms were discovered in the 60s, 70s, 80s. But scaling them up and finding some crucial incremental ideas, that was something that only happened a few years ago. And now we are in the second age of AI arguably, in an AI where solutions are no longer handcrafted, but where the researchers build algorithms that discover the solution by themselves. So these systems become flexible, You can use the same learning algorithm on a number of different problems and it's going to find solutions, but you still need to train it for these different problems. And now it's very tempting to think that there is going to be a third age of AI. And this third age of AI is going to be systems that will learn how to learn. And it's already beginning, right? We have systems that can solve many tasks now. For instance, we have large language models that were only made as auto-complete algorithms and did statistics over text, completing the text in the future. And it turns out that these things can summarize text, they can do creative writing, they can do sentiment analysis, they can do programming, they can translate between programming languages, they can do an extremely large area of tasks that they were not built for. So these systems are increasingly general. They're not as general as us. They don't understand the same context in which we are, they don't learn in real time, they're struggling to be connected to the world in real time and act on it. And so we are basically now at the doorstep of these systems and what's not clear is whether these systems are basically now at the doorstep of these systems. And what's not clear is whether these systems are going to be a gradual extension of the deep learning algorithms that we have, or whether we will need to write a new stack, stuff that works in real time, stuff that uses different principles. And so at the moment, we see a competition between the scaling hypothesis. We just need more GPUs, more compute, more of the same algorithms with slight tweaks, or do we need something completely different? And at this point, I think most people in the field don't know. It's an open question, which makes it extremely exciting. And also, there's this big question, is this new generation of AI systems, is this going to be just a new generation of machine intelligence, NGMI, or is it going to be general machine intelligence, something that is like us but scaling better? So is it not going to make it, or is it going to make it this next generation? Is it going to get where we are? So a big question is, can machine have minds? And in our culture, that is a very disputed thing. What is agency? Can machines have agency? What is a mind at all? Can in our culture, that is a very disputed thing. What is agency? Can machines have agency? What is a mind at all? Can we understand what a mind is? Can we deal with consciousness and so on? And many of these questions have relatively simple answers. For instance, an agent, I think, is something that is different from a normal controller. The normal controller, for instance, your thermostat, measures temperature, and then makes a decision to turn off the heating on or off based on their temperature. It acts on this present frame of reality. But what happens if you are able to model the future and control future states? As soon as the thermostat is able to integrate expected rewards over the future, the world splits off in trajectories that depend on your decisions and your preferences and you become an agent. So an agent is a set point generator, something out of the world should be, something that you want to regulate for, plus the ability to model the future. If the modeling is general enough, then you should be able to discover yourself and the world and account for how you are and who you are and how you connect to reality. So a mind is that thing, is a coherent model of reality that is reflective and can in real time coarse-grain the universe, make a level at a rough resolution at which it's entangled with the world and basically get in resonance with the universe by modelling it deeply. And consciousness is the real-time control model of attention. It's basically a conductor that makes it coherent, that fixes incoherence in this perceptual model and brings it in tune with reality again. In our culture, we have basically two ways of understanding the mind. We have this idealist perspective that we basically live in a dream, and physical reality is a dream. And physicalism, which says there's only a physically, constantly closed physical universe and everything else must emerge from that. And we see this as opposition. And I think that the answer to this question, what the true solution is that both are in some sense true. What we experience is a dream that is generated in our mind. And this dream is dreamt by a system on a higher plane of existence and this is basically the physical realization of our brain in the world, right? So we live inside of a VR that is generated in the skull of a primate and in this virtual world we are conscious. We are not conscious as physical systems, we are conscious in a dream. Outside of the dream we cannot exist. There is no now, there is no experience, there are no colors, no sounds. All these things are generated in the mind. And I believe that our culture originally understood this before we forgot it. And this story is captured in Genesis 1, right? Genesis 1, you probably know this book. It's the creation of the physical universe by a supernatural being in six days. And I think that's a mistranslation. I think that if you read it properly and if you go to the original text, what you find is that it's a six-stage theory of the development of conscious experience and personal self inside of the mind. And the first step in this thing that is separating a world of ideas from a world of extended things in the mind, that creates a universe in the mind, is the creation of light and darkness, basically of contrast, of intensity and flatness. So, there is a creative spirit that is hovering over the substrate, and everything is tuhuwa buhu and without form and void. And then it learns how to spark light out of the neurons, how to get some contrast, some resonance and separate this, draw this out from the contrast. And it associates this light with the color of the day and the darkness with the color of the night. So the intensity gets associated with brightness and the flatness gets associated with the absence of brightness, with darkness. And once you have that, you can arrange it into extensions. And what you get is first linear extension scales and then you can arrange these scales into spatial regions and create spaces. And you can create 2D and 3D from that. And once you have 2D and 3D, you can create 2D and 3D from that. And once you have 2D and 3D, you can associate the two-dimensional plane with the ground that you're standing on and 3D with the space above it, right? Now you have sky and ground. And then you learn about illumination in the world and the constancy of shapes and you learn about the nature of light sources and and you discover the big light in the sky, and you discover how light changes, and you discover this nature of change and time. And then you discover the plants and the animals, construct them in your mind and give them all their names. And once that is done, you realize that the purpose of that is to control it, to interact with it. And so at this point, the creative spirit, roughly at the age of two and a half or three, creates another spirit in its own image. But it creates this as man and woman, as something that thinks it's a person. At this point, you forget that you are the creator of reality in your own mind, and you become somebody else. You become a first person human being. And you become somebody else. You become a first-person human being. And you notice this in children. They have this big gap in their memories, this childhood amnesia, when they reorganize from a new perspective. And the theory is very speculative. I don't know whether it's literally true and whether Genesis is really the theory about cognitive development. But I find it much more plausible than that our ancestors were deeply confused. So I suspect that our culture actually knew at some point that reality and the sense of reality and of being a mind is the ability to dream, to be some kind of biological machine that dreams about a world that contains it and about itself in that world. in the dreams about a world that contains it, and about itself in that world. All right. So let me try to pick it up where Josje left it. So as Josje mentioned, the register of my talk will be quite different. I think you can see Josje's premise as a background on which I will try to show an example of how this AI of third generation could maybe be actually built, like very concretely, with very concrete examples and principles. So I'll just lay down that example in a very science-y, scientifically technical manner, and then maybe we can see where we meet, where we agree and hopefully also where we maybe disagree a little bit just to make it more interesting. Okay, so I think the future of AI is embodied, situated, multi-modal, adaptive, and neuromorphic. Yosha had this slide number six where he said embodiment, dynamical systems, and emergence didn't work, and I think it's fine. Neuronal networks also didn't work, like once in the 50s and then in the 80s. I still believe that the next AI will be exactly that. Another belief that they have is that it is useful to look and understand intelligence in nature, biological intelligence, and moreover, that we can understand a lot about biological intelligence today, in the 21st century, if we look closely. So what do we learn from intelligence in nature? So first, if we want to try to build it, right, we need some substrates. So we need to start with some representations. And in nature, the representations used seem to be not distributed. Neuronal networks, if you're familiar with those, multiple units, multi-dimensional vectors represent things in the world and not just single symbols. And I'll say a little bit why that's a good idea to do it that way. The second feature that we learn from nature, that nature follows stability, so it creates stable states, but this is a dynamical stability. So it's stability through change. The biological systems, they constantly change, constantly adapt in order to keep certain things stable. Like, you know, not to get too hungry, not to get too unhappy. You have a couple knobs that you try to keep at the set points, and then everything else is in flux. And it's very different from many artificial systems that we build today. So we start with something static, and then we introduce change if needed. In biology, it seems to be the other way around. Change is always there. You create static when it's needed. Predictions and consistency. Yosha made a prediction today behind the stage that I will be talking very quickly. I actually planned to contradict that prediction so that Yosha has a learning signal. I'm not sure I succeeded. But this is something that biological systems seem to do all the time. We live in the future, like on different time scales. We always look one step ahead. Like I plan to do something in the world and then I imagine like what will happen when I when I reach for this bottle. I constantly predict on a millisecond to millisecond and then on different timescales a second ahead, a minute ahead, a year ahead. I constantly need to anticipate what will happen next for two reasons. One is because my sensation, what I perceive now is actually behind now until the signal gets to my retina and then it gets to my to my brain with all the delays in the system. I always a little actually behind. Now, until the signal gets to my retina and then it gets to my brain, with all the delays in the system, I'm always a little bit behind. So I have to look ahead in order to be on time, to be in sync with the reality. And the second reason is learning. So when I anticipate what will happen, I can then observe, did it happen, did it not, and can learn from that. And that allows me to change my system, to be in this dynamic stability, constantly changing, constantly chasing things that are important and that might change in the outside world. And consistency, I'll talk a bit more about that in a second. So our brain tries to keep everything in sync and consistent. Different modalities, what I see and what I hear, I move around, I anticipate what will happen with my visual input when I move around, and I constantly try to keep this balance so that everything is in sync, is consistent for most people, right? Some people are okay with some cognitive dissonances and inconsistencies, but in a very fundamental way, our brain likes to be consistent. And adaptivity. So we are constantly changing. If you're familiar with neuronal network, you know, structures, architectures, in artificial neuronal networks, you have a fixed structure that you have trained, you have learned, and then it solves your task. In biology, it's not the case. In biology, the structure is constantly changing. And what is kept constant is something else. On the other side, they have kind of conclusions, like architectural conclusions from those natural things. The first one is diversity. So biology, architectural diversity, algorithmic diversity. Biology offers us a whole multitude of mechanisms and circuits how to solve any task. You know, in different animals, but even in the same brain, this diversity in the same brain, there's diversity and variability of methods, how a given task can be solved. And this stands in stark contrast with our AI 2.0, when we have found this one silver bullet algorithm that seems to solve so many tasks so nicely. So let's just put all the resources in this one thing and move it forward. Biology prefers a different solution, prefers to have many different mechanisms. Because who knows, maybe this silver bullet is not silver bullet for everything, and we will need other algorithms, and if they all die out, we will have to start from scratch. And I think in kind of sociology of research and in many other processes, it is maybe OK that once in a while you have this large concentration of resources for one thing to really have a big impact. But then it's important that it dissolves in the diversity of methods so that we can then explore other methods if needed. Another thing is interaction. So biological systems, they leave, cognize, learn in interaction with an open environment. So this is interaction almost in thermodynamical sense. We don't live in a closed system where everything is controlled and we can put it all in a vessel. It's important that the system, our artificial intelligence system, can interact with an an open unknown world and can be ready to learn from this world, to adjust to this world, assume that this world will include parts that it doesn't know yet. So this open interaction with the world. That's what we have in biology, that's what we have very little in AI 2.0, but hopefully in 3.0. Then consistency. So consistency is important to keep our different modalities, different motor, sensory, cognitive capabilities and functions in sync. All right. So that's the table of content of my talk. The rest goes in depth in different points. I'll maybe rush through those just to give you a bit more of a gist, That's the table of content of my talk. The rest goes in depth in different points. I'll maybe rush through those just to give you a bit more of a gist. And then we'll try to have a discussion. Is it a plausible plan? OK, so the first, the diversity. Just very quickly, if we look into the biological neuronal systems, I have shown you a couple examples of different circuits that you find in biological brains of different animals. The most fascinating ones are actually insects. They have these fairly simple brains, usually with a handful of neurons, like a bee has a million, like a fly has a couple hundred thousand, but they are interconnected in an intricate, interesting way. So there's one example of a locust, it's a little grasshopper. It has a single neuron that has a connectivity to the sensory surface of this grasshopper to detect looming stimulus. So if it's a little grasshopper, it has a single neuron that has a connectivity to the sensory surface of this grasshopper to detect looming stimulus. So if now if it's the birds approaching the this grasshopper, it immediately will jump away. This neuron detects only the stimulus, nothing else. So you can you know, wave in front of the eyes of this animal, it will ignore it, but the looming stimulus is what this neuron detects. And it has very intricate dendritic tree that is wired up to solve this task. And every grasshopper of this type has this circuit. The last one in that row, it's the fly's head direction circuit. There you can literally see a structure, a physical structure of neurons that forms a little ring on which the neurons are active, forming a little bump that follows the head direction of that fly. So if the fly moves around, there will be like a little compass of activity moving on that ring. Quite fascinating circuit. Also can find it in the head of any fruit fly in your home if you look carefully. Then there's olfactory circuit. So people have kind of decoded some circuits in the smell organs that can learn smells with just one shot learning, and then can recognize the smells in the mixture, and so on and so forth. So if you go to more complex brains, there's a lot of structure there. You know, there is cerebellum architecture that seems to fine tune the movement and make sure all the muscles are coordinated nicely in time. There are basal ganglia circuits that coordinate activation and deactivation of different actions as we move around. Our cortex, so this whole part of the brain that makes us so smart and human, consists of six layers that seems to be interconnected in a very specific and stereotypical way. Seems to do something like predictive coding. So that's why we believe that the brain is always looking in the future a little bit. And tries to see, to check, is the thing happens that I have anticipated. So, I know that kind of supports my point. There's a multitude of algorithms or circuits in the brain that solve different tasks. There's a huge diversity, algorithmic diversity. So what can we learn from biology? We can learn all the specific neuronal circuits for different tasks. So wouldn't it be fascinating to build drones that would be able to fly away, make a map of the environment, come back and communicate this map of the environment like the bees do with just a million neurons. No, drones can, that can avoid obstacles very efficiently. So there are different kinds of circuits. I have listed a couple, sensing, navigation, some motor control circuits, and then people do build them and put them on robots. Usually not getting neither so much money, no fame as people who build systems that can play go or chess. So we can learn a lot from the specific neural circuits for different tasks. But we can also learn some principles, some architectural principles, and I have listed them in the beginning. So you know how things are represented in the brain. All those predictive loops, they seem to be very important and critical, and the learning principles. So these learning principles are, of course, the most interesting part. And I have talked about them. So I will just flash. States and representations. It seems that in the brain, things are not represented by single neurons. So now I want to go left. There's one neuron tells me, go left. No, that's not what happens. There will be 1,000 neurons that will be active with different level of activity. And they, as a group, will represent my command, go left. It's called population coding. It makes for distributed representations that have these nice properties. That's the only thing that matters. They're robust. So if a couple neurons seem to be lazy this day and don't want to fire, that's fine. There are 998 others that can still do the job. You can have very fast response with pretty small, slow biological neurons to your tasks. You have graceful degradation. You have accuracy to resources trade-off. So if you don't care about accuracy, you can spend just a few neurons for this task. If you really care about accuracy, you would recruit more neurons for the task. It can be used with noisy elements, and it's a good substrate for learning. I mentioned the stability, and the stability is really important. So our brains, they work on this relatively fast time scale, right? Every millisecond, the neuron could fire. However, the behavior in which I engage can be very slow. I might take a second to grab this bottle. I planned this talk for the whole 20 minutes, hopefully. And in order for neurons to engage in this longer task, they need to form a stable state. They need to stick to an idea. No, this bottle is a good thing for me to drink. Or I want to move my body in one or another way. So you need to stabilize the states. And there are certain architectures that create these stable states out of dynamical chaos. And I, of course, won't tell you much, but just believe me, there are other architectures, and they're useful. And then we can implement them in technical systems to demonstrate that they're useful. And then we can implement them in technical systems to demonstrate that they are useful if you have these systems that behave in a shaky, dynamical world. And then the question, these stable networks, they're called attractor networks. They create attractor stable states. And we want to compute with those attractors. They are like symbols with which we could compute. And then there are architectures that allow you to compute that. These predictive loops or processing loops, they seem to be pervasive in the brain on different levels. Single neurons seem to anticipate what will happen in its neurons history next. And also big brain regions seem to be interconnected in the loops that anticipate what will happen next. There are some new ideas of these loops. And we can talk a lot about the architectures that you can build that will create the model of the environment and anticipate what will happen next, the future. So this is one point where we converge, that the brain is a controller that is looking into the future, and the mind is a controller that tries to control the future. So I absolutely subscribe to that. And then all these pretty pictures is just about some ideas, a hypothesis, how we could build something like that. And maybe also understand how something like that happens in the brain. All right, so more examples, pretty pictures. How about I just flash them? I think good news for AI 3.0 is that, so one thing that we learn from the brain is that we need a match between the algorithm, the computation, and the hardware that does it. In the brain, you naturally have that. You have all those neurons physically living in the brain, interconnected in a physical way, and they do this neuronal computation. When we simulate them, we use the computer for that. And it works, because computer is a universal, not your machine, it can do any computation, including this neuronal computation. But the question arises, how efficiently can it do it? And it doesn't do it very efficiently. So I don't know if you are aware how much power is being spent on training these large neuronal networks. So there is, I think, a growing understanding in the field that there's a mismatch between this type of architectures, computing architectures that we would like to build, and today's computing hardware that goes all the way back to the 40s, to the structure that Norfolk Neumann has suggested with the memory sitting here and computing CPU, computing units sitting there. And now if you have to compute some massively parallel neuronal system, you have to go back and forth between memory, you know, and back to your computing, burning a lot of energy. But today, there's some diversity in terms of hardware systems emerging. So there are new hardware systems being developed. These are the ones on which my group is working called neuromorphic hardware systems that try to mimic the structure of biological neuronal systems of the brains in hardware. And that gives us the hope that we could run these neuronal architectures that will compute similar to the brain more efficiently in real time so that we can actually build useful systems and control robots. Yeah, just one slide showing off how good this hardware is. So the only thing that you need to notice are these numbers on the axis. So they show how much more efficiently this hardware can compute compares to other hardware and it's 100,000 times more energy efficiently and 1,000 times faster for certain tasks, for certain architectures that are close to those biological ones. A lot of efficiency. Okay. So to conclude, so I think this neuroscience inspired AI is definitely not the only one possible, but there's an interesting example, because I think it's a working example of type of AI that we maybe want to achieve. And then the main conclusions and learnings from there is first the algorithmic diversity. There's a large number of algorithms that we need to develop, need to explore, need to combine, need to use. There are some principles that we can learn from neural computation. So what has happened in physics that we have discovered is laws of nature, there are laws of biological systems and laws of the mind that we can uncover, probably with help of some new math, partially, and we need to link it to some maybe more philosophical or verbal models and understandings or concepts that are out there. There's new type of hardware platforms and software frameworks that support this type of computation, so that's the good news. It doesn't all have to stay in theory only. And so the title of our session was Machines that can understand, understanding machines that can understand. So I think to build machines that can understand, we need to make sure that these machines can interact with an open world. So they are open to new information. They can incorporate new information into their system. They can learn online and on the fly. They can build this emergent and self-consistent representations. And I think the dynamical systems approach, so that one that failed in the 90s, could maybe succeed in 2020. That's kind of our hope and our bet. And an important component to understanding such machines is that they need to be modular and live on multi-level. So it cannot be one big black box, one big black hammer. It needs to contain different solutions to the same problem that can not be active or not active and can interact with each other. And then we can maybe achieve this artificial intelligence that will be embodied because one question that, I would like to ask ourselves, so what kind of artificial intelligence do we need? What problems of the humanity artificial intelligence systems could solve? And I'm not sure that sorting through my pictures or even creating pictures is the most burning problem of the humanity. I can imagine many problems that could be solved by robotic systems that could automate some tasks that are maybe dirty, dull, or dangerous, 3D. And we don't have robotic systems that can do that. We don't have robotic systems that can interact safely with unconstrained dynamical world. And I think to get there, more work needs to be done. And this style of work, neuroscience and biology inspired, could be useful to move in this direction. Yeah, so that would be my sketch of the future. I don't know if we can bring the two ends together. I think that we have a very similar perspective on research that needs to be done. But where we differ is how it needs to be done. I don't think that there is a conspiracy that basically is not giving funding to neuromorphic computing. I suspect neuromorphic computing now has more funding than in the past, because AI itself is bigger, it just has a smaller fraction. And that's because the other stuff is more useful than ever. It's a little bit like this quantum computing. If I ask my kids if they want me to buy a quantum computer next or a gaming laptop, they want the gaming laptop because the quantum computer doesn't run Fortnite. And that makes sense, right? The quantum computer basically doesn't work. It works hypothetically almost for certain problems to some degree maybe. But in practice it's not there yet. It's not over that hill where it becomes competitive or even practical for any application. And the neuromorphic systems seem to be in a similar situation. What I forgot to tell you, all the illustrations that I used in my talk were generated by Dali, by a computational system that is capable of dreaming. When you give it a little bit of text, it's going to make an image that is its dream interpretation of that text. And it has been trained on 800 million images and captions. And it's not been told how this actually works, but it was a contrastive learning. So it gets sometimes correct captions, sometimes the wrong captions, and from this it learns the structure of the space of meanings in these 800 million pictures. It's really brute force, it's something that human being could never do, because we could never incorporate that many pictures into our mind and pay that much attention and so on. On the other hand, it's the only thing that works right now for these machines. Deep learning is dumb and brutish. But the reason why people use it is because it's the only thing that works. And the question is, can we get something else to work? And how much do we need to rewrite to make that happen, right? So maybe a little clarifying question, so it's only things that works to do what? To learn about the world and achieve a certain degree of generality. For instance, if you recreate a neuromorphic chip, you do this based on a neuroscientific understanding that does not result in a working model of how the brain works. Neuroscientists don't know how the brain works. They don't have a single working model of how the brain works. Neuroscientists don't know how the brain works. They don't have a single working theory about how the brain works. It's all conjecture and only very partial. They cannot simulate this. So, it's not an issue that our chips are too slow to simulate neuroscience. The problem is that neuroscientists just don't know. I disagree. I think they maybe don't know about the brain as the whole, but they know a lot about individual systems, like really a lot. Like about cerebellum, we know a ton. About basal ganglia, we know a lot. Hippocampus, we understand all its aspects. Sometimes this knowledge is distributed across different labs. Like in hippocampus, one people will tell you it's about space, other will tell you it's about time, and the third group will tell you it's about episodic memory. But each one of them will have good points, will have models, will have models that you can implement and can put in the robot, and the robot will solve a particular task. I think we just, this ambition to have the full brain there doing whatever humans is doing, that may be too much, but what we can do today is learn different aspects, if particular tasks from the brain, and then build systems that will solve these problems similarly. That's extremely useful. Also, it's extremely useful both for understanding how the brain works and for getting inspired for technology from the brain. So I think we are on a good way there, also on a good way to understanding the brain. I hope there will be more and more conversions between the fields, because that's a little bit of a burden of our academic system that is so disciplinary, right? Someone who does neuroscience will seldomly talk to someone who does cognitive science or behavioral scientists. They talk different languages, they publish in different journals or conferences. It's very difficult for them to talk to each other. Forget the neuromorphic people who deal with electronics and circuits like different planet. But I hope that there will be more and more convergence there through interdisciplinary research organizations and institutions. What worries me in today's AI is that we found this one very simple algorithm, like simple like piece of wood basically, right? Gradient descent. We think that's the only thing that will solve everything. It's neither plausible, nor realistic, nor healthy to believe that. I wish there would be more diversity in algorithms. That's even separate from neuromorphic hardware itself, which of course is just challenging, because coming up with new hardware is so difficult. You need to think about the periphery, you need to think about interfaces, you need to think about programming frameworks, you have to change so much. So it's challenging, and it's fine. Then everyone wants to become the next Intel in this field, so that's why they don't talk to each other that much. They try to keep their secrets and develop their technology with not enough exchange. So that field has maybe their own problems. But the diversity of algorithms to pursue, I think, I wish there would be more of that. And I'm not sure that the big system like this behind Delhi or these GPT-3 models, that they can solve practical problems that people have in industry. When they want some robotic system to recognize a bunch of objects, new objects every week, localize them precisely, and then maybe manipulate them quickly. There are many unsolved problems in this venue that just cannot be addressed just by this one single algorithm. I don't see that there is a religious belief in stochastic gradient descent. And I find that most people that work in deep learning don't hesitate to use arbitrary other algorithms if they work. So they are using semantic networks, a rule-based system, whatever works, and just put it together. Most engineers are very pragmatic. What I mean by not working is that there is, at the moment, no neuromorphic solution that is able to extract meaning from images. And it's not with the humble campus, not with the neocortex, and so on, because in order to be able to do this, you need something like complete system. It doesn't need to be a complete system in a sense that it has arms and legs and a complete motivational system, but it needs to be complete in a sense that it has a working attention system and a working perceptual system and working memory and the ability to structure reality and to discover stability and sparseness so it can form objects and concepts in its mind and manipulate them. And when you look at these pictures that Dali is creating, it's using the same algorithm essentially that is driving GPT-3, the language model. And the big discovery there is that instead of making statistics over everything, and deep learning is just a way to make statistics, you need to make statistics over what you need to make statistics over, so you learn what to pay attention to. Instead of taking everything in, which is way too much, you cannot do equal statistics over all the parts of all the 800 million pictures. You gradually learn in which context you should put resources on what part of the image. And this big advancement, 2017, has driven the current revolution that we see with language models and multimodal models. But it's very simple and efficient. It means you need to show way more data than you should show to a human brain. And for me, the big difficulty is how to overcome that. I'm not necessarily a believer in, you just need more deep learning. I suspect that we need fundamentally different algorithms. What I don't see is why you would want to build hardware first instead of first getting the stuff done in a simulation. I don't think it's a big issue if your simulation takes 10 times more energy, because you have enough energy. And if you were to give me something which the size of the neocortex, which is like a soccer field or so, in terms of space and I would be able to build an FPGA with purely symbolic logic, you wouldn't believe what we could do with that, right? So it's a bit unfair to compare this slow, murky biological system with our computers. Also the energy comparison is a little bit lame. I mean, 18 watts for the human brain, yeah, but four hectares of land to produce these 18 watts as hamburgers, right? It's very expensive to feed the human brain. And it's so much easier just to scale up a few solar cells and fire your learning system. So I suspect that with slight efficiency improvements, it makes more sense to do things on a CPU or GPU until the simulation works. So I don't yet understand why do you focus on neuromorphic first rather than getting the principles of this new type of computation that we are both looking for to work. Okay, I'll answer a couple things. So like three or four points. First, I have a contra example. So you said there's no neuromorphic computer that can process images like the Leecam indeed, but there's also no deep learning system that can land a teeny tiny flapping wings drone. This is something that has been shown with a neuromorphic chip. I can know this really, really tiny drone with a really tiny chip can do this landing maneuver inspired by some insects. So there are some examples of what neuromorphic systems can do even today. And of course, we don't work on producing images as Dali does. So I think it's important to choose the benchmark and comparison field fairly. This is one point. Another point is that it is true that everyone says the main algorithm is stochastic gradient descent, and that's it. And that's not true. For the systems that you actually work, there's much more behind, much more other algorithms, symbolic logic, tree search for the goal system, just conventional software. It also needs to be convenient and easy to use to build all the systems. I wish that would be more apparent from the PR of deep learning. If it would be more open like, hey guys, it's not just the has to gradient descent, it's also X, Y, Z, and that's important for this task. Then probably we would see this diversity. Then I think I personally would have much more problem with this field. If it would be more honest about what is actually behind the curtains, what actually makes these algorithms work, including all the data processing and massaging that you need to do in order to really train the system properly, and the cost function selection. Then I think there also would be much more diversity in this field. I do think that deep learning should be the starting point, that it's important to use all the achievements that have been made in the last 10, 20 years in this field, and then build on top of that. So we cannot throw the baby out with the water as we go to AI 3.0. So I think we have to build on 2.0. The neuromorphic critique, why build hardware before we have completely understood everything? I think when we completely understood everything, it will be late to start thinking of how to build the hardware. So hardware development is a difficult business or research. And people need to explore what is possible, what isn't possible, what actually works, what doesn't, what is efficient, what is not efficient. So this is a research field that just does its research in parallel to us understanding this or that about neuronal systems. And I think it has its merit and its right to live as a research field. And it has its own internal dynamics. Many things in neuroscience are being understood and can be set in stone or right, or sand, or silicon. The more we understand, the more we can build. And I think it's fine, because people need to refine. So people have discovered conventional electronic, the CMOS is maybe not enough, the systems that we build them are too bulky, let's look into new materials. So they discover new materials, they have memoristic properties, so in the future we can model some aspects of this neuroscience models with a single element instead of 30 transistors or 100 transistors with just a single computing element. And it's great. And so the need for this was only discovered because people were building these early systems. And it's a difficult process, I think, to come up with new hardware. So I think it's fine that this hardware is being developed in parallel. And then today we know a lot, so I think all these principles that I tried to sketch in my talk, they are real, and there's a converging evidence that those are important. And I think we are at the point where we could suggest the world no yet another hardware platform. We have CPU, we have GPU, we have FPGA, we'll have another one. Oh, why not? I wish we had 20 other ones. Yeah, so I think it's a fair point. But on the other hand, I'm myself happy to just stay on the theory and algorithmic side and think about this algorithmic basis. I just wish to run my algorithms fast enough to put them in a closed loop with some robotic system. Their CPU or GPU were not enough in the past. So I couldn't run them in real time. In real time in the robotic sense, when computing is done in one millisecond, or maybe four, but not like half a second. That's too slow. An area where we agree is that neural networks are a poor imitation of what the brain is doing. It's basically only a very vague metaphor that synapses are identical to weights, and it's based on the idea that memory is stored in the synapses. But we find, for instance, that the brain is much more flexible than these neural networks. The neural network is basically a chain of a weighted sum of real numbers. And this chain and this architecture is predefined. We give this to, we find an architecture, we search for it, or we define it by hand. And then we gradually change the weights. And we do this by sending error signals through there, and over many, many passes, these error signals converge at the right points, and with the transformer, they converge faster at these points. So it's not a true attentional system in our mind where we pinpoint where something is being computed and then change this in this target. But there is also another difference. Our brain is not made from these sums of weighted numbers. It's made from neurons. And a neuron is a single-celled organism that tries to survive. And it does so by learning when to act. And it acts based on a configuration of its environment, some kind of spatial temporal activation pattern, some kind of incoming wave of activations that comes and that monitors. And once it has figured out which waves it should interact with and modulate and participate in and which it should ignore, it is probably exchanging some of that information with its neighbors. Because what we find in the brain is that these activation patterns that emerge are somewhat fluid. So in the neocortic, sometimes activation patterns can wander. When we look in the next day, we find that they have shifted or rotated a little bit very often. So there seems to be something going on that is mathematically different from the neural networks are doing that would also require a different training algorithm. Stochastic gradient descent comes from the outside, looks at all the neurons and their connections and then changes these connections, like weaving from the outside. But there is nothing outside of the brain that changes it. It's all happening locally. Everything happens by our neurons and other cells directly talking to their neighbors and discussing how to change things. And it's also, the story is much bigger than the neurons. Every cell can in principle act like a neuron. Every cell can process information and exchange multiple message types with its neighbors and learn to approximate functions together with them. So at a larger timecale, it's to be expected that all long lived multicellular organisms basically have brain functionality in their bodies. And what the neurons add on top of this is like a telegraph network that can encode message messages as very short, high intensity bursts, and send them very quickly over long distances in the organism. It's basically, imagine that all these cells are like little beings, little critters, little agents that try to survive and learn how to collaborate, like in a big society, and exchange messages with their neighbors. And then they invent this telegraph network that sends information, like the internet, around in the body of animals. It's very expensive to drive this internet, so you need either to burn oil if you are a civilization, or if you are an organism, you need to eat plants or other animals, right? So it's metabolically expensive to have that. The benefit it gives you is to compete with other things like you at the same level, so you can move your muscles very fast, you can perceive very fast, you can discover meaning very quickly. And we make this all about the neurons. It's really top-down in a similar way as symbolic AI is top-down and starts with objects and words and language rather than bottom-up. And it's interesting to rethink this whole thing from bottom-up. And this is also where we converge. But if this is what really interests you as much as me, I still don't understand why you do neuromorphic computing. I think that somebody needs to do it. I accept that somebody needs to build boring hardware that doesn't work for the next 10 years and that you will discard once we discover the principles of how the brain does information processing. But if you want to figure out how the mind works, why do you waste your time and energy on the hardware? I mean, I would want to join you if you can convince me. But at this point, I'd rather would want you to join me and build simulations first until we get the simulations to work with these new principles. So maybe first to confirm, I think we indeed agree on this picture, that the neurons are much more active entities, and they have their own goals and certain principles indeed. I do think that the Neuromafia hardware that I'm working with now helps to develop the systems or will help very soon when the tools are there. So I still run simulations, so I don't have to imprint every circuit I come up with in the silicon. I have a silicon that works very similar to conventional computer, it's just massively parallel, asynchronous, just more efficient in running these type of models. And the goal is to develop the tools so that I can simulate these models, actually like even more convenient than I would using conventional C++ or Python, PyTorch and so on, that limit me a little bit. So I mean software tools that will limit me in the right way, that will give me the freedom that I need and allow me to do the right thing that stays in this neural plus plus framework. Then it will just run efficiently in real time, so I don't have to wait three days to see the result of my simulation. So I hope that the neuromorphic hardware as a tool, efficiently in real time, so I don't have to wait three days to see the result of my simulation. So I hope that the neuromorphic hardware as a tool, the one that we are using now that is pretty mature, and staying close enough to conventional hardware to be usable, to be usable and to be flexible, so that I can still play around with the models, with architectures, with learning rules, and hopefully be productive, and join you in the search for the model or the architecture of AI3.0. Let me phrase it slightly differently. Given that you want to know how the mind works, and this is the thing that animates you most, I think. Two things. I'm curious how the mind works, for sure, for many different reasons, but I also want to build practical systems that solve some very practical robotic tasks. Yes. So if you had unlimited resources for your quest and no constraints whatsoever, imagine how much would that need to be, how much do you need to realize your most amazing dreams, building the best team whatever around these ideas. How many millions or billions of euros or dollars, the same now, would you need? I didn't do that long-term business planning yet. And what would you do? I would build robotic systems. I would build robotic systems that's a much more elegant, lightweight, agile than what we have today. You look at these call boards that are supposed to work close to humans, and I feel intimidated close to them. Most of them won't fit in my door, in my apartment. So I want something, like the physical system that is lightweight, that is elegant, that is quiet, and that is agile, can do movement. It's 100% aware of what's happening around it. So it knows where to go to solve the task, knows when to stop. So fast perception, faster than we can achieve today with conventional cameras. So I need someone to work on the sensing that is good and fast. Actuation that is flexible and agile, also not quite there yet. Nice design. That framework, that's neural framework that we are talking about, that runs really efficiently, really, really fast. It needs to run fast in order to be safe and robust, so they have time to double-check. Is my decision really correct? Let's simulate it a little bit like the future. Let's see the future a bit further. So I think these three, four components, we're sensing actuation, thinking the brain and the hardware. I don't know, a couple of dozen million might be enough for that. I don't think that a lot of money is needed. Any talent, I would need really, really smart people convinced and coming together as a team. I suspect that intelligence is actually substrate-independent. It should be able to use an arbitrary substrate. It will run more efficiently on some hardware than another. But ultimately, you can probe the hardware and see which algorithm should run best on this if you are a general intelligence. So if I would be able to reverse engineer myself to the degree that I become substrate-independent and can extract the principles of my functioning and then spread myself over all the substrates. I suspect that we have enough robots already. There are enough cars, there are enough things with cameras and even, you know, I could go on a laptop and could talk to you over the screen and observe you via the camera and hypnotize you into carrying me wherever I want to go. I'm not sure if I need more robots, but where I totally agree is that our current algorithms are all not real-time, they're not entangled with the world. What would be a dream for me is to start from scratch and discard the current stack by building systems that watch through a camera how I am waving my hand and tries to extract all the regularities that can see. First order regularities, the neighborhood between pixels that are activated. Second order regularities, the shape of the object that's moving over the screen. Third order regularities, the movement that is behind it and momentum behind it. Then higher order, you get geometry in space, the best interpretations of what is causing this. And then higher order things like, what are the agents that are moving things and what is animating them? And then even higher, what is the universe that I'm in? What are the possible physics in the universe in which it can exist? What are the possible languages and epistemological principles that can discover all these universes? The same thing as happens in our own minds, right? This is what I would want to build. And the current systems are not entangled with the world. But I don't think that it's because they are not robots. I think it's because they're not real time. Both aspects. So I guess we're in agreement that these intelligence algorithms should be substrate independent. However, I think this efficiency that you tried to sweep under the rug matters, right? Efficiency both in terms of energy, because we have energy, but no, it does like, look at this heat wave, right? The climate is changing. So I think we should be careful with energy. Should be faster before everything, should burn the energy faster before it's empty. And then efficiency in terms of time, right? So to be real time. And there I think substrate should burn the energy faster before it's empty. And efficiency in terms of time, right? So to be real time. And there I think substrate matters, right? So our intelligence is not, so this dream that we can upload our intelligence somewhere else, I don't think it will work. So I think the substrate does matter in which intelligence run. Otherwise, I agree that redefining the whole stack is probably what it will take. Yeah. So not easy. Then so that we have enough sensors and enough robots, we have a lot. Robots may be like moving systems, not enough. I'm not sure they're the right ones. So we need other ones. We need them to develop faster. Unfortunately, as a business, robotics is not a very easy endeavor. It's just expensive. You have hardware, and it breaks. It's much more difficult to patch it than some software. So people are much more careful in investing in this field, unfortunately. I think it requires much more resources than all the GDP friends. And it's risky. So people are careful investing in this field. Julia, I think it was an amazing discussion. We are at the top of the hour. I don't know if the organizers will let the audience ask a couple of questions before we conclude. What's your plan, Christian? Sure. Any questions here? That was a fascinating rapid fire. I would say. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. and we build robots, so for course, close with Julia. I think one core argument, you were both in agreement, but then you're arguing about iteration times. And what Julia is basically saying, it will be faster to iterate on the hardware first, because this gives you a speed up that validates, and it iterates so fast that you will be running out of computation time going the classical way. Yeah, yeah, that's true. And we kind of waited long enough for neuromorphic hardware to catch up and to build something that is usable, that is now useful, that is user-friendly, that we can now use and save that time. Yeah, so that's true. Yeah, thank you very much for this evening session. I'm Joachim Schulze from the DZNI in Bonn and I think you know you're coming very soon to our Nature Conference on Hardware AI. From neuroscience to AI and back. Exactly. And I have two questions to Sascha. I think, you know, tell me what your connection between your mind model and the dream and the biological natural hardware is and what plays evolution there as a role. And as a philosopher, I'm pretty sure you have thought about this. And the second part would be, I have a feeling you're contradicting yourself by saying, you know, arguing against putting new hardware out, because you also said that if you want to understand the mind, and we need basically to understand the different parts, it's a... the mind is a combination of many, many functions of the brain. And the brain has so many, many architectures. So, if you really want to have the interplay, that the interplay will be the mind in spatial and temporal, then you actually need these different architectures to put together. It's not even only the speed, it's how do they interact actually in the end in a timely fashion. And probably, that's a philosophical question, maybe I'm not so sure whether we can really simulate that without trying out some principles and then increase the likelihood of simulation. So first of all, the brain has an overall architecture that enables its functionality. And this includes, for instance, motivation and different sensory domains. And there is, in the neocortex itself, which I think is like the pattern matching cortex, about 50 different functions that are represented as a certain type of map that represent embedding spaces that form a certain hierarchy. And discovering this hierarchy goes much faster when you go through evolution and then the individual organism doesn't need to rediscover it. We find that if you disturb it during development, the organism will rediscover most of it, but it takes a long time and it's a huge setback for the individual. For the rest of the organization, for instance, you have the cerebellum. The cerebellum is for automating movements largely that don't require deliberation. So it's not integrated with other things, but if you can entrain your cerebellum with martial arts, you get much better. But if you take out the cerebellum, people will be able to do most of the things that they could do with it. So it's an optimization for certain things. And most of the principles in the brain seem to be in the category of like a few dozen basic ideas that lead to different organizations locally via self-organization. And it probably has to be like this because our genome is relatively short, right? Our entire genome basically fits on a CD-ROM and only a small fraction of that is coding for the brain. And the question of how the brain can be encoded in its mental organization so efficiently is, perhaps, there are some hypotheses. I like the idea by Gary Edelman that what he calls neural Darwinism, that is basically a competitive evolution between different organizational structures in the brain, like different organizations or forms of government in a society. And ultimately this settles into something. So your genome only needs to encode cues to guide that evolution relatively quickly to a good solution. But the system itself is much more flexible than that because given different environmental circumstances it can converge to a different solution, right? But what you need to implement is a kind of evolution. And evolution is for the most part, blind search. It's one where you don't have a gradient, but you don't know which direction you're searching. Once you have a gradient, which means you discover a direction which gets better, you should follow that gradient as much as possible. Then the question is, is the point where it doesn't improve the optimum, or is it just a local optimum that you need to break out of? How does the optimal algorithm look like? And there is probably no general optimal algorithm until you have discovered mathematics from first principle and the general theory of search. And such a theory might exist, but people, for the most part, don't know it. So maybe some mathematicians know it, but it's not common knowledge and it's not part of the scientific core knowledge that you get to learn in university at this point. So this is an open question, how intelligence works, how the intellect works, what the principles of the intellect are. And for me AI is the attempt to figure this out. And the neuromorphic direction and the neuroscientific directions and the purely symbolic and logical and mathematics based and statistical directions are different strains of ideas and I think that probably works best if you don't have a strong belief in the purity of one of these approaches but that you are open to all of them and try to discover what's possible and try to discover what's possible and try to discover possible solutions. We are not there yet, but I think that we are right now in one of the most interesting times ever in science because we seem to be very close to solving this problem of generality. Yeah, sounds good. Maybe a little addition. So there's a different field of study, but I know there are people who try to understand how the individual brain develops, how the brain development unfolds, following these developmental programs encoded in the DNA somehow. So how from a single cell you get the full body, but in particular brain. And people studied experimentally, so try to see how migration of cells happens and how they specialize to form different types of neurons and different networks. And there's more and more understanding in this field as well, and more and more models that what can simulate and maybe eventually we could have something like a DNA out of which then a particular neuronal network solution could unfold with all its structure. So that would be AI40, I guess, maybe 50. structure. So that would be AI40, I guess, maybe 50. With respect to the approaches, I think our supercomputers are already faster than brains. What holds us back are algorithms. And of course it would be great if we have systems that can run with a fraction of these supercomputers in terms of energy and do similar things. And it's extremely useful to work on this and important. But the thing that really holds us back is still that there are basic principles that we don't understand yet. A couple. Yes, okay, I'm so impressed about this discussion and what now I'm questioning myself is where or when we do have, let's say, things which will work with artificial intelligence on a high level in our daily life. And there is this big, big thing called autonomous driving. And now after this discussion, I'm questioning myself, it will never happen. Let's say it takes such a lot of perception and advanced thinking and algorithm that I'm wondering myself whether it will work at all. So maybe very're right. There are some very smart people in the community who would agree with that statement. Like following today's path, it will never work. But there are other paths. So I believe if we alter the structure of the environment a little bit, like we structure the streets already a lot. We could put a couple beacons, a couple sensors, just in the road, and help autonomous cars to see more than us people can see. And then we could make much more reliable system. Because autonomous cars, not only the task is really difficult, and we humans are amazingly good in solving all these perceptual tasks involved, how strict we will judge, how strictly we will judge autonomous cars is also different from how we judge humans. So one accident per year is enough to kill the field for a couple of years, while people do accidents on a daily basis and thousands of those. So the autonomous driver will need to work perfectly to be accepted. And there we could maybe help by not just putting sensors on the car, but also changing the streets a little bit. So that could be a relatively easy solution to that problem. In San Francisco, we sometimes joke that every time Rodney Brooks writes that autonomous cars will never happen, the police stops a sleeping driver in his Tesla on his way home. And so in some sense, autonomous driving is solved except for the edge cases. And there are a lot of edge cases, right? And to understand all the edge cases, you need, eventually, in the limit, to understand the reality that you're in and your place in that reality. And this is very hard to do with pre-trained pattern matching. That requires, I suspect, different approaches. So to me, this is still an open question whether the present approaches will give us good enough driving, which means driving that is better than 90% or 99% of the human drivers. So we will accept this as a solution. Or whether we will need systems that are able to dream about reality deeply. And the current Tesla cars don't do that, right? There is this difficulty when the camera sees a stop sign. It doesn't really know whether the stop sign is painted on the back of a truck or standing at the side of the road. And we humans understand this context and the systems get better and better with these edge cases by having more and more fine-grained training data sets and more engineering work done. But it's a very different way than having a deep context model which is understanding the context that it's operating in. And this is also an issue, for instance, with what we currently discuss as algorithmic fairness and so on. The language models are able to reproduce almost arbitrary context. They just don't know which context they are in. The language model doesn't know whether it's recreating the autobiography of a war criminal or is instructing children in the preschool on how to sing a song, right? It doesn't have that context. It doesn't know which universe it's part of. It doesn't know what its role in that universe is. And this is a point where Julia and me are deeply in agreement, where we think we would want to build systems that are entangled with the world in real time and bootstrap themselves into this interaction with the world. Thank you. That was really a fascinating ending to day one, I think. Thank you very much, Julia and Yosha. Hope to see you all tomorrow. Thanks for coming. And now I think we all need drinks.", '34.118332386016846')