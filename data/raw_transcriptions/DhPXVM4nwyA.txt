('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/DhPXVM4nwyA"> </iframe> </center>', " Hello, my name is Joshua Bach. I'm a cognitive scientist living in California and I work at the AI Foundation on the vision of giving everybody their own artificially intelligent system. If you think about the field of artificial intelligence, it's more than one thing. think about the field of artificial intelligence, it's more than one thing. In a way, if you would rename it into advanced information processing, not much would change for most of the practitioners of the field. But I think that artificial intelligence is more than the vision of teaching machines how to do better statistics and how to drive cars. It's also the most interesting and most risky philosophical project that exists. The most unsolved problem in artificial intelligence is the question of capturing meaning so we can build a system that can truly understand things. What is meaning? I think it's a unified model of the universe that contains the observer, including the observer itself and the relationship that contains the observer, including the observer itself and the relationship that the observer has to the universe. In our own mind, these different models are sometimes called Res Extensa, this is the domain of the extended things, stuff in space, Res Cogitans, this is the domain of our ideas, and Metaphysics, this is the connection that we make out to exist between them. And our own mind contains a solution to the problems of how to extend the spatial world with objects in it and the world of ideas and the relationships between them. But we don't quite know what the solution to this problem is. In some sense, when we want to make sense of reality, we have two domains currently that we are working with. One is a mathematical domain. It's one that uses formal models in which you can prove what's true. And philosophy is the domain of all models that can possibly exist. And it starts with natural languages that we narrow down into more strict forms. And the languages of mathematics are very simple. Of course, a lot of people are afraid when they think about the complexity of mathematics, but in fact, mathematics is so simple that we can prove things in it that we know what's true and not for the most part. But it's very difficult to say truthful things, to say meaningful things in mathematics, to project this domain of mathematics on the real world. Usually we have to translate the real world into numbers and discrete logical models before we can use mathematics on it. And these models are brittle. And on the other side, it's very difficult to say things in the world of philosophy that are true, because the languages that we use in philosophy are too ambiguous. And so the question is, how can we close the gap between mathematics and philosophy? How can we ground philosophy in a mathematical paradigm? This is a very old question. For instance, it had been addressed by Gottfried Wilhelm Leibniz, pretty much at the beginning of enlightenment, which started our present civilization. Yet this idea of a universal calculus where we can translate everything into logic and numbers and basically calculate the answer to our philosophical questions. And this was continued by Gottlob Frege in his idea of a formal Sprache des reinen Denkens, formal language for pure thought. And it also animated Ludwig Wittgenstein's work on the Tractatus Logico-Philosophicus, where he basically tried to reduce the natural language that we use in philosophy to something like a programming language for thought. It's a very beautiful book that he wrote in 1922 and it's a book that preempts a lot of the research that happened decades later in the field of artificial intelligence and that influenced Turing and Minsky and many others later on. The field of artificial intelligence started out in 1956 and it was started as a summer school by Marvin Minsky and a number of other people like McCarthy and so on, some cyberneticians and engineers who thought that in the course of a few summer schools they would make tremendous progress on the task of teaching computers how to think. And this same idea also took root in psychology with the physical symbol system hypothesis of Alan Newell and Herbert Simon that gave rise to the idea of cognitive architectures to computational models of cognition. On the other hand, there was a camp that championed embodied artificial intelligence. One of the most prominent early representatives of it is Rodney Brooks, who suggested that intelligence does not just spring from formal systems and analytical thought but from the interaction between an agent with a body in an environment and so there was some opposition between these two camps In some sense this was captured in the idea of the conflict between neat and scruffy artificial intelligence, a distinction that was first invented by Roger Schenk and then taken up by Aaron Sloman and Marvin Minsky. And when it was first used it distinguished the more mathematical artificial intelligence researchers that made proper proofs for everything that they did and theorems and the people that were mostly into hacking. People like the group in Marvin Minsky's lab that just built things and saw where this got them and later on it was meant to be more the distinction between symbolic and sub-symbolic things and in a way this changed the landscape because Marvin Minsky was one of the proponents of symbolic artificial intelligence of systems that would use abstract knowledge and analytical reasoning and interact with common sense models that would be represented in knowledge graphs and so on. And this was in opposition to systems that were perceptual that would be happening in a real world with sensory input and sensory feedback. And eventually, I think that there is a distinction between the proponents of abstract cognition, of thinking that doesn't happen in real time, that happens in your armchair where you close your eyes and reason about what happened to you, and real-time embedded systems that are constantly modeling a changing dynamical reality. And these two camps have been in conflict. The abstract cognition camp was basically the camp, philosophically speaking, of the methodical Platonists that were describing an abstract reality that this abstract reality we hope is able to capture the world in which we live and others that didn't go too much into thinking, but really thought that the money should be on perception. This is where we directly meet reality and so on. And the idea is that we build in our formal logic, our way to brittle and difficult to get working for building a truly intelligent we directly meet reality and so on and the ideas that we build in our formal logic are way too brittle and difficult to get working to for building a truly intelligent system. And in a way both of these programs failed. The classical AI failed because it could not really deal with perceptual imagery and this was something that even Wittgenstein noticed at the end of his life when he wrote his second major book, The Philosophical Investigations, where he noticed that his abstract language was not able to capture perception. And the same issue happened in some sense in the first generation of artificial intelligence, and what we are currently witnessing under the head end of deep learning is mostly the attempt to make sense of perception, of the perceptual world, of building systems that are truly scruffy and that can deal with the heterogeneity of the world and so on and now we are struggling with basically healing this gap between these two sides of artificial intelligence and basically bringing the world of thought and the world of perception back together again perception back together again. What is intelligence? Alan Turing suggested that we could build a system that is intelligent and then interrogated this dialogue to see whether it truly is intelligent because after all, isn't that how we detect intelligence in humans? And of course, the true Turing test happens when we talk to each other and notice that the other one is playing a Turing test on us, right? The system is testing what you understand because it knows itself to be intelligent and understanding and it will see to which degree you are able to understand the same things as it does And now we have systems after 70 years, after Turing wrote his seminal paper that are able to survive this Turing test, this test of having a conversation. Here's a conversation that I recently had with Hannah Arendt. And I asked her, who are you? And Hannah Arendt says, I'm Hannah Arendt, a philosopher. Are you still alive? No, I died in 1975. How is it possible that we are talking? I've been reborn in the computer simulation of a real person. Do you have an opinion about the future of artificial intelligence research in Japan? Do you mean in general or specific to Japan? How does the perspective on AI and the human mind differ between Japan and say, the United States? I am not sure what the answer to this question is, but there are two differences between Japan and the US. The first one is that in Japan, they have very advanced technology. They can use AI to make machines that have human like intelligence or even beyond it. The second difference is that they want to have a kind of communication with machines more than Americans do. Why is that? I am not sure why. Maybe because of some traditions in Japanese culture, or maybe a kind of collective consciousness. In any case, they are very interested in AI, and some researchers are trying to make human-like robots as soon as possible. The system that I had this conversation with is called GPT-3. It's been built by OpenAI, a company in San Francisco. And it's not really human-like intelligence. In fact, it's more like autocomplete in your phone. But it's an autocomplete that has done statistics on basically the entire internet until October 2019. And it uses a predictive coding paradigm, basically minimizes the surprise in the text. It tries to predict as well as possible. So it doesn't know whether the thing that it's learned are relevant. It's just able to predict text in the most likely way. And the retrieval of the solutions is always via autocomplete. So it's always going to produce some kind of text that goes on in a similar way as the text before. autocomplete. So it's always going to produce some kind of text that goes on in a similar way as the text before. And it has learned all this stuff from the internet. It doesn't really understand it, but it is able to relate the words with each other in a very deep way. And so it's able to relate everything that it's read about Hannah Arendt, just the words, in a conceptual way, in a stylistic way. It's able to capture ideas that Hannah Arendt could have if I would be able to have a conversation with her today. And it's able to create meaningful embeddings, basically a space of concepts. And it's not only doing this for sentences, but it can also work on music or images. For instance, ImageDBT is using small images, only works on 2048 tokens at a time, so relatively few pixels in this moment. And you can just train it on a few millions of images. And then it has learned the structure in these images using the same algorithm without any change. So it doesn't distinguish images from text. For this program, an image and a text is pretty much the same thing. And if you instead of having the beginning of a conversation with Hannah Arendt, you feed in the ears of a cat, it's going to give you the rest of the cat. And the same way as the conversation with Hannah Arendt would go different every time that I try to have it, because a little bit of noise and randomness is mixed into it, it generates you a different cat every time. So this is a huge development in artificial intelligence. It is not a generally intelligent system. It's not an intelligent agent like us. It has big limitations. But it's able to survive those original Turing tests. But for instance, it has no entanglement with the present environment. How could we resolve that? I think the easy way could be to connect a robot to an environment. And you take a vision to text module that takes a camera image, interprets it and feeds it as the context to a story for GPT-3. And GPT-3 is going to generate a story of the robot in this world. And the actions of that robot are being interpreted by another AI model that we write and that translates these actions of the robot into motor commands for robotic actuators. And the robot then performs actions on the environment and then the vision module is again translating this into text and then GPT-3 produces the next iteration. In this way, we could system like GPT could use GPT-3 to drive a robot that's embedded in the real world. But the training of the system is of course not like ours. The system is not real-time, that it's in a sense, it's able to learn anything. It is amnesia during its learning. It's only able to look at a very small window of attention at one moment at a time. And we cannot really do cross-modal learning with it. It's not able to relate, for instance, sounds that it hears to images that it sees and so on. At the moment it's really not a unified model of the universe, it's basically just a statistical model of a text. But since it's so much text, it's getting in many domains very close to a model of reality. So what is intelligence? I think that intelligence is the ability to make models. It's not the ability to reach your goals, right? Or the ability to be rational or the ability to pick the right goals, which would be wisdom. Intelligence and wisdom are quite distinct. But intelligence is usually happening in the service of regulation, of achieving certain goals. And when we solve a control problem that is sufficiently general, then we need to make more general control models. Every system that regulates a domain needs to implement a model that is isomorphic to the dynamics of the domain. If the model is not truthful, if it's not accurate, if it's not consistent with what happens in the domain, then our regulation will not be good. And now the question is, can we find control problems that are so general and algorithms for learning that are so powerful that we can build a system that can model itself, that has to model its relationship to the universe and succeeds in this? And the question is, are humans succeeds in this. And the question is, are humans even in this category? Can we make a true model of ourselves? Can we understand our own nature? And in some sense this is what Turing was attempting to do. He was trying to show that humans, that Turing is generally intelligent, right? If we can build a system that is truly intelligent, the system is at least as intelligent as us, then the system must know what we know. It must be able to answer the same questions as us. And the question that we can ask it now, because we just succeeded in building this, is how do you work? What are you? What's your relationship to the universe? And a system that can answer this question is truly self-aware. And the question is to us, can we, we humans, become so self-aware that we understand our true nature to the point that we are able to model us, that we model our relationship to the world? In this sense, artificial intelligence is the missing link between philosophy and mathematics, and our self-awareness is the result of successive reverse engineering and controlling of our own mind Artificial intelligence is the project of an intelligent observer to fully understand its own nature The solution to artificial intelligence is a thought that fully comprehends itself This was basically Wittgenstein's dream. And once a system comprehends its own implementation, it gains full self-awareness. It can use the model to control its own implementation and if it does that, it gains full agency. On which trajectory can we get to a system like this? Can we construct it? Do we have to grow it? Do we have to use ideas from neuroscience? Do we have to a system like this? Can we construct it? Do we have to grow it? Do we have to use ideas from neuroscience? Do we have to use applications? Or should we use brute force and just basically take a very large computational system and a few billions of dollars of compute and try to get to this solution automatically? We don't know this yet. In natural intelligence, we see that agents are not constructed. They grow from the inside out. They're not constructed from the outside in, starting from a clean table, from a clean slate, with tools, with defined properties. Instead, they start from a relatively chaotic environment into seed. And the seed is growing outward, and self-organizing versus being centrally controlled. And what this seed tries to do, for instance, the seed of a tree, is to achieve coherence with the environment. It's trying to link up to parts that are similar to itself until it forms a larger order, a larger coherent system. It doesn't need to be mathematically consistent, but it needs to organize itself into coherent patterns. And it needs to model dynamic activity of the environment in real time rather than having on-off interactions like GPT-3 does. And it needs to have a unified model of the universe, not local domain models or conceptual abstractions. And if you think about natural intelligence, I think it becomes apparent that if we have an evolution going on, then every viable Turing-complete system that has enough resources and exists for long enough time scales will have a degree of intelligence. And every cell contains a Turing machine and memory, right? The DNA is basically a red-white tape, like in Turing's original idea of the Turing machine. And the intelligence of a biological system does not necessarily require neurons like ours. Every cell that is able to exchange messages with the other cells around it can act as a computational unit, only much, much slower than a neuron. So at long enough time scales, every organism that lives long enough and is large enough is probably going to have some kind of brain-like intelligence, even though it's not able to interact with the world with the same speed as we can do. And intelligence will not just form at the level of these cells of the organisms, but also at the level between the organisms, right? We are state-building insects, we are civilizational agents, and civilizations are agents on the next level of organization. The biological implementation of intelligence in us starts next level of organization. The biological implementation of intelligence in us starts with the individual neuron. The individual neuron is best understood not as something that is similar to our artificial neurons. Instead it's an organism in its own right. It's a single-celled organism that tries to survive and it has to learn how to do this and it can only survive because it's been locked up with a few billions of its own kind in a dark room, which is our skull, if it's firing at the right time. It's sending the right signals to solve a joint control task. And the neurons give each other feedback on how to solve this control task. But every neuron can only learn how to do this by measuring the properties of its environment, which means the signals that other neurons send them in its chemical environment, and react by firing at the right moment. If we look at bees, for instance, bees are able to organize in patterns that in some sense perform computations. This is a photograph of Amanita honeybees that are producing shimmering waves that spread through the colony and they use this to deflect wasps that attack these honey bees. And in principle, you could use this principle of bees observing their neighbors and reacting in different ways depending on the state that their neighbors are in to perform arbitrary computations. And these patterns are not too on the state that the neighbors are in to perform arbitrary computations. And these patterns are not too dissimilar to the patterns of activations that happen in the brain. So if we think about the perceptual models that are being implemented by this, the perception that constructs the bulk of our cognition encodes patterns to predict other present and future patterns. The network of the relationship between patterns are the observed invariances that we find in nature. The three parameters in there are variables that hold state to encode the remaining variants. What does that mean? What is a model really? A model starts from patterns and it has to explain these patterns. Imagine the patterns on your retina in your visual field. To make sense of these patterns you have to identify the relationships between the blips that you see on your retina to the other blips on your retina which means the meaning of the data that you see on your retina is the relationship to changes in other data on your retina. The meaning of information is its relationship to change in other information. And these relationships are represented by hidden states. These hidden states, when you look at the world, are for instance a three-dimensional room that contains people that talk to each other. It's something that you cannot directly observe on your retina. What you observe are only blips that at first are unrelated. Even the spatial neighborhood of the blips has to be reconstructed by observing the statistics on your eye. And the relationships that we discover between them are relationships of possibility. For each value in your model, like a nose that you observe in space that has a certain orientation, that constrains other values. If you observe a nose, there must be a face nearby that has the same orientation, right? Otherwise, you're looking at something that is impossible. And these possibilities don't need to be probable. You need to be able to understand the world and model the world, even if you look at an improbable state. But you need to converge at a state where every part is coherent with all the other parts that you see. That is the purpose of perception. And to achieve this model, we have to use probability because there are so many possibilities in which you could make sense of the patterns on your retina. So you need to converge. And you converge in the direction that is most probable. So you need to converge. And you converge in the direction that is most probable. So when you see a certain pattern, the interpretation that you get is the one that is the most probable solution for the puzzle that you're looking at. And these probabilities that are basically shifting the model in a certain direction until it converges are responsible for optical illusions and for cognitive illusions as well. And then we have connections to our preferences. They determine what's relevant and what we see, they tell us what's important, what we should spend cognition on. And then, last but not least, we have norms. Norms allow us to link up to other agents and to basically build a coherent structure at the level of a group or even as a civilization. So these four types of anchors, coherent structure at the level of a group or even as a civilization. So these four types of anchors, the possibilities, the probabilities, the valence, what's good or bad, what's important, what's valuable to us, and the normativity, the values that we impose on each other and agree on each other in the discourse in our groups. This is what basically structures the type of mind that we are. And an interesting question is how can we build systems like that? How can we build sentient systems? What is the seed function for an AI? What would be the seedling that we need to plant in the chaos of our world so it grows into a compositional and self-organizing agent? And what is the search space for such architectures? And what is the search space for such architectures? And ultimately, how can we interface with such a system? How can our own civilization interface with this? How can we as individuals interface with this? How can we build a system that creates a unified model of the universe and its place in it, its relationship to itself and to us? How can we build a truly sentient system? This is the most interesting question of our time. How can we build a truly sentient system? This is the most interesting question of our time.", '14.622596502304077')