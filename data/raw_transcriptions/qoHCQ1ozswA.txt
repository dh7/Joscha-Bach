('<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/qoHCQ1ozswA"> </iframe> </center>', " Thank you so much for this invitation, Lenore. I feel honored and embarrassed to speak directly after Christoph, because first of all, how can I shine after this, but second also, because I am going to zoom out a little bit and to visit the same paradigm and talk a little bit more about how the mechanisms that Christoph has been discussing will or could lead to the phenomena and phenomenology that we associate with consciousness. So it's quite complimentary, if you want to indulge me. I think when we want to build a theory of consciousness, there are a few things that we want it to be able to deliver, right? So first of all, it should capture the correct phenomenology. If our model of consciousness is not producing in our understanding what we are pointing at when we talk about consciousness, basically, the feeling of what it's like, then this theory is not going to be any good, because how would we know it's a theory of consciousness? It doesn't help if you just rename consciousness into the thing that we currently have in our computer and want to sell. We want to explain how it's actually producing phenomenal experience and maybe a subject that reports on it and self-reports on it. Then we want to delineate this idea of consciousness against things that are not conscious, what are necessary and sufficient conditions. And then we want to explain the causal structure that leads to the phenomenology, that is, how is that thing that I'm pointing at coming about, what causal structure is going to lead to it. And then, there are some stretch goals. It would also be nice if you could explain how this comes into being and why it's there, what purpose does it serve. And maybe we can specify this process well enough to spell out the theory in such detail that we can implement it in a computer and thereby test it, whether it actually produces the phenomenology that we want to have, for instance, reporting and self-reporting subject if we set it in the right setting. And if you cannot do this, maybe an alternative would be that we specify an automatic search process, basically the search space for processes that would have that ability. And so other than sitting down and tinkering by hand until something pops up that has the necessary qualities, maybe we can just leave this to the machine like AlphaGo, and it's going to go through its search space until it finds a solution. And we do some kind of automatic testing that lets us converge there. But this still means that we have to specify the space in which we are searching for solutions. So what is consciousness? What's the ontological status? What kind of phenomenon are we talking about? And first of all, let's get some agreement established. And I know that's going to be difficult, so bear with me. It's one possible perspective among many, many. And this is the one I'm going to take in the following. First of all, I think that consciousness is a process that transforms representational type patterns into others, which means it's a function, a function that is transforming representations into other representations. And it is virtual, which means it's not a physical phenomenon. It's not something that exists at the level of atoms or elementary particles or quantum phenomena or neurons. All these things are mechanical stuff, as far as I'm concerned, and it might be important in which way they work, but they are themselves not conscious. There's basically a bunch of cells, and there's activation patterns between the cells, without lots of generality here. And it's only that stuff, right? And it would be very useful for a bunch of cells, and there's activation patterns between the cells, without lots of generality here. And it's only that stuff, right? And it would be very useful for a bunch of cells that can produce these activations patterns among each other to know what it would be like to be an organism that navigates a social world, what it would be like to be a person that cares, how it would be like to have an attention that reflects what it's singling out, and so on. And so because they can themselves not do this, they create a simulacrum of this, a simulacrum. And that's us. Right, so it's an as-if thing. Consciousness, like the personal self, exists as-if. Like, all software exists as-if, right? In your computer, there's only transistors firing, and the software is a model that we impose on it to understand what they're doing in some coarse-grained way, and this structure is well-defined enough, narrow enough to be a valid causal model. But the software only exists as if. And in this way, software is, in its nature, not a thing. It doesn't have an identity. It's law-like. Software is actually a physical law. A word processor on your computer says, it's a coarse-grained level. If you arrange matter in the universe in the following way and you look at it from this perspective, in this degree of abstraction, the following thing is going to happen. This is the physical law. So when you write software, you discover an extremely specific physical law. And what I suggest is that consciousness is in this category, which means consciousness doesn't have an identity. So your consciousness is not different from mine. It's also not the same as mine, because it's not in the category of things that can be different or the same. It's an operator. OK, now that we got this out of the way, let's get to the phenomenology. Is consciousness really like being a bat? My issue with this kind of approach is that I don't really know what it's like to be a bat. I also don't know what it's like to be a human or what it's like to be an organism, because frankly, I lack the baseline. I don't have anything to compare this against. So this is something that does not really help me. It's something where I feel, OK, this helps me to say this is the thing that I point at when I see organisms exclaiming, oh my god, I'm conscious. What's happening to me? But beyond that, it doesn't really explain that much. And so there is more narrow phenomenology that I want to look at. So for instance, I can see that a range of conscious states that I'm in, there's something like a minimal consciousness that I might have in meditation or in dreams or just waking up in the morning where I don't have a self model yet and the world doesn't make a lot of sense yet. I have attentional agency in more expanded states and have constructive agency where I observe my consciousness actively messing with my mental content and with my interpretations of my percepts. I might have self-awareness where I perceive myself as an attentional or a personal self that is driven by this consciousness and is extending this consciousness. And I might also get to the point where I am a person that stands in front of an audience while experiencing itself as conscious. And so this is a range of conscious states and they're all conscious. And I want to capture them in such a theory and also want to explain how I have to have this range. And what the core of that phenomenon, I see that there is a self-observing observer. There is some process that notices that it's noticing while it's noticing. This is some of the core phenomena that is happening here. So it's generating representations of observations and representations about the fact that you are observing these representations right now. And what's really crucial about consciousness is not so much where does it happen, right? It's when does it happen. And it always happens now. Right? It's the main much where does it happen, right? It's when does it happen. And it always happens now. It's the main feature of consciousness. It does not only just always happen now. It also is probably the thing that creates now. And this now is a period in which I can fit my perceptual content into a frame that is completely coherent. There's stuff in my memories that basically drops out of this coherence, so I can no longer make it congruent with what I perceive, and stuff in my expectations that is not yet coherent with what I see. But I have an expanded sense of now, subjectively usually about three seconds, give or take, that is my present now, that is being created by consciousness. And so this nowness is something that I need to explain. Also, this nowness is not static. It's moving. This present moment is always a little bit dynamic. There are sometimes states where it's not much happening. And this dynamicity is something close to an identity function. But it's a special case. So basically, I can see state transitions in my conscious perception at any given moment. So it's dynamic. And what I see is that in consciousness, I have access to the things that it signals out. So there are contents, features, objects that I'm attending to. And then I also am aware of the mode in which I'm attending. So usually I know whether what I'm looking at is a percept that I cannot change, or an interpretation of a percept that I can change, or a memory that I can retrieve but not really change, or an imagination that I can change every which way. And this is a mode in which I attend to the contents from the perspective of consciousness. And this is a mode in which I attend to the contents from the perspective of consciousness. And it has this self-reflexive mode. So these are phenomena that I want to explain and I think that are somehow crucial and definitive for the type of consciousness that I have. So when we think about where does consciousness happen, I think consciousness happens in the mind. And the way I understand the mind is that it's something like a generative AI that works very much like a game engine. The world that I see around myself is obviously not the physical universe. As far as I know, the physical universe is characterized by things like the Schrodinger equation and quantum mechanics and so on. And it's such an arrangement, it's very difficult for me to make it intelligible. It's not the physical universe that I'm seeing. What I'm seeing. What I'm seeing is colors, sounds, people, and so on. It's all stuff that's very coarse grained and that could not physically exist in the way in which I perceive it. So what I perceive is very much like what I perceive on a screen when a computer runs a game engine that is making extremely simplified physics so the computer can deal with it. In the same way, my brain is making an extremely simplified model of physics at a very abstract level of coarse graining that is compatible with me sampling the world at less than 20 hertz with my sensory apparatus and trying to fit a curve to it with the spatial temporal resolution that my brain can handle. So it's a game engine in a sense. And additionally, it has a noosphere. So my mind has this model of the world, of stuff in space, that I perceive as you and me being together here. And everything in the world can be mapped in there. And then all the other stuff, my ideas and so on that I have, that are decoupled from this perceptual representation of the world outside of me, and my proprioception and so on. And so traditionally, this is a philosophy called res extensa and res cogitans. And I think it's best to treat them both as mental categories, mental domains. And even more before this, our ancestors called this Earth and Heaven, I believe. So Earth is the world model, and Heaven is the noosphere, res cogitans. And then, of course, then we have the reflective self that is put like an NPC in a game engine, put smack into the middle of this simulated game world. That's me. And I perceive emotions that are generated outside of me and myself as downstream from them. So I feel myself being subject to valence and to percepts. And so this, I have a model of an attentional self that is a reflexive agent at some point and that can turn itself into a personal self, so I experience myself as a person in the world. And so basically I have an attentional agent that acts as a conductor of my mental orchestra that is interacting with the motivation agent outside of me and the perception agent, and they basically are parts of the cognitive architecture that I'm in that is coupled to the environment. And it's like a conductor in an orchestra. Consciousness is not some mental modulator superpowers. It's basically like one of many instruments in this orchestra that has a specific task compared to the other instruments in my mental orchestra that compute functions of the cognitive domains is that it listens for incoherence and tries to minimize this incoherence in the orchestra so everybody is on the same page and we get coherent behavior. That is coherent, there's motivational constraints, perceptual constraints, and so on. And this is, I think, the purpose of consciousness, to basically act as a conductor that is increasing coherence. So, how does this work? In our own brain, as far as I can see, it's a completely self-organizing substrate. So, unlike a GPU, this is not organized by an engineer who puts it onto a chip and then imposes an algorithm that is forced onto all the transistors and have no choice but to behave according to the spec of a designer, and executes an algorithm that somebody has written up and then is run on that substrate, no matter whether it wants or not, right? In our brain, this is very different, because our brain is not made out of aimless transistors. It is made out of single-celled animals that have a shared destiny and need to survive. And they can only survive if they find some kind of arrangement between each other that allows the organism to find food for the neurons. So this is the situation that they're in. And as a result, they get some emergent organization, a self-organization from inside out. And there's at no point a global structure that is not the result of some inside out growing self-organization. And so if you build an information processing system, like a machine learning system, out of completely self-organizing elements, what does this look like? And I think what it looks like is not anything like the current machine learning algorithms. It's something where you need to create increasing coherence. So you create an island of coherence at first, something that has a coherent reward language and semantic language of thought, so to speak. And that is then growing and is colonizing the cortex into a coherent pattern that allows you to explain reality. And at the core of this, to make this happen, this coherence happen, is the self-observing observer. It's the minimal coherent pattern, right? It's something that notices I am here, I am observing, and so I'm going out. It's quite beautiful because it's basically the core of Descartes' meditation. Cogito ergo sum, I think, is not some philosophical statement about ontology that allows you to make a derivation. It's the strategy that you can introspectively observe when you wake up in the morning. I start out with, oh, here I am. What is this world around me? How can I make sense of it? And then I branch out and integrate all these patterns until they become percepts in the world model. So you need to take these individual agents, the neurons, and reward them by giving them a code that has message passing in it. Some of these messages mean that they say that you did well in our architecture, or you didn't well, and they get rewarded for being trainable in this way. And these individual units, they can exchange messages via neurotransmitters or even complicated functions, potentially by exchanging RNA with their neighbors, and so on. There's a bunch of things that these cells can do, in principle. And this determines the search space for processes where consciousness is going to emerge. Right, it has to emerge in the structure as this process that is going to organize it, like a government emerges in a society at some point. And the government works, first of all, by noticing that it decides to be a government, and what this entails, and then starts to impose structure on and what this entails, and then starts to impose structure on the environment around it. And it's competing with other proto-governments until something takes over. There's some kind of neural evolution. So does consciousness come first or last? A lot of people feel that it's so advanced that probably only people are conscious. And it's really the pinnacle of development of cognitive agency. But I don't see people becoming conscious after the PhD. They really are conscious before they can drag a finger. And that suggests that maybe consciousness is not the result of you interacting with the world very deeply as an organism, but maybe it's the prerequisite for learning anything. Maybe it's the simplest learning algorithm that nature can discover for a complex recurrent brain like structure that is self-organizing. If that is the case, it's a hypothesis, then this gives us a hint of how we could search for it. We could set up something that is fully self-organizing and is incentivized to process information that is rewarded if it succeeds in it and punished if it doesn't. And then we might have a search space that is small enough so that anything happens as it happens spontaneously in the brain of every infant or a fetus and is organizing itself into a structure that suddenly you have this amazing phase transition where it starts to learn efficiently more complicated things. And so I suspect that consciousness might be discovered in our brain before other learning algorithms, most of them. They're probably simpler, heavier learnings like stuff, but once you go beyond this, you probably need constructive agency. And consciousness is basically an operator that the brain is discovering to increase its coherence globally. And it can evolve in something like a neural Darwinism, something like an evolution that happens in every single brain, among organizational patterns in the brain. can evolve in something like a neural Darwinism, something like an evolution that happens in every single brain among organizational patterns in the brain. And your genetics, of course, are going to bias that evolution, so it's going to converge much faster than it otherwise would. But it seems still to be working in everybody, right? So no matter how mutated you are, no matter how many lesions you get in your brain, almost every time it works, you become conscious, and then you figure yourself out to some degree, right? And if you don't, you are vegetable. You never learn anything, you never talk to other people, it's never going to work out anything. So the consciousness is really the prerequisite, but it also is surprisingly resilient, given the number of things that you can do to an infant or a fetal brain, and it's still going to develop into something viable. So it's basically something that colonizes the brain with a structure that is compatible with building a learning architecture that is able to interpret reality. So consciousness, in a sense, could be actually the creator of our mind and our world model and the self model. So how could we generalize the search space? We start out with using lots and lots of self-organizing reinforcement units that have, for instance, a spatial neighborhood to each other. And then based on that spatial neighborhood, they pick a receptive field. So, it's a selection function seen from the perspective of each of these units that basically choose where they read information from in their environment. It determines the topology in which the system is. Right? And the second one is a mapping function. So, you read out the values from your receptive field, and you map them against your own internal state to a new internal state, some of which is going to be exposed so other units can read it. Right? And this is the most general way to look at it. All the other things, like different neurotransmitters, are just types within that language that emerge by sending one more or less bit in it. Everything else can emerge from this neural differentiation. The different types will just be changed in the internal state. So it's a very, very general way of looking at this, in the most general case. And then we can go down and narrow the search space further down by imposing further constraints. But this seems to be the thing that you get when you relax the constraints maximally for having your self-reinforcement learning agents out of self-organizing elements. And if you take this paradigm and combine it with neural networks, neural networks are in this paradigm and combine it with neural networks, neural networks are in this class of systems, existing neural networks that we use in AI, but they make very specific commitments that are unlike the commitments that we have in a biological brain. So, their selection function is fixed for every neuron. You define the architecture before you start training it. You introduce more links than you're going to need, and you're fine with setting many of those weights, and eventually to zero. But you're fine with setting many of those weights eventually to zero. But you start out with the potential space of links and set this fixed. And every neuron, every unit knows its receptive field. The beginning is not going to change. The second one, the mapping function itself, is always a function over real numbers that basically makes a weighted sum over the values that it reads from the receptive fields and solves it against a threshold, like a ReLU. You could also use tangents or tangents hyperbolic or some other functions, but people figured out that the original perceptron idea, just using a threshold, which means a ReLU today, is good enough. And so this introduces a non-linearity, which is a fancy way of saying if-then, instead of just using weighted sums. And the mapping function also can change the arrangement a little bit. And this happens only during the training phase, not during inference, not during using the neural network. And you do this by changing the weights, which means basically the factors by which you multiply the individual elements of the sums in these elements. And so if you look at brains, they are also in this class of systems. And here, the selection function of the sums in these elements. Right? And so, if you look at brains, they are also in this class of systems. And here, the selection function is the space in which the neurons are located with respect to each other. The physical spatial neighborhood of the neurons matters. And then the learning that they can perform on the neural level by, for instance, changing synaptic weights and changing things inside of the neuron and changing the receptive field. And there are some dendritic growth priors that tell you how the neuron is going to change its receptive field, given the environment that it's in. And the mapping function itself is given by learning and by evolved priors of the individual neurons. And it's somewhat indeterministic. So the neurons are not deterministic. As a result, the population of neighboring neurons that has the same receptive field is going to sample a function space stochastically, rather than executing only one function. And so if we generalize this, we have a selection function that takes learned global and local functions as for selection, and a mapping function that can take arbitrary automata and that can be a multi-way function, basically a non-determinatory machine. And this allows us to think about computation in brain-like self-organizing distributed systems in a very, very general way. And so if consciousness is the simplest way to organize such things, it might be that it pops up. And so the task of consciousness in such an architecture would be the definition of a reward language, credit assignment to the individual units, how much do they perform with respect to the global reward, and the evaluation that what's happening in here, in this thing, what is the largest possible now that we can create inside of the system, the interpretation of features in terms of how does this relate to our world model, how can we interpret all the perceptual features and so on, sensory features, patterns that show up in the system as parts of a model of the world, and how can we perform construction when we need to disemulate, how we can learn how to reason, and so on, build lots and lots of tools that allow us to create a reality and fix it. Does AI have to be conscious? Well, I think that a GPU is not a self-organizing system. So obviously, you can get away without doing this. That's why present systems like LLMs and DALI and so on are so good. They work without being conscious. They can train themselves without these self-organizing principles. And they do this quite well. And people say, oh, my god, this is so expensive, and so on. But imagine your task would be to take 800 million pictures and correlate them. Would you be able to, within the space of a couple of weeks training time, deduce a visual world in the same way as DALI or stable diffusion does? No, right? It's really amazing that this works at scale. And you can do this on a GPU farm for a mere $10 million at this point. And I would say that our brains probably using, we're giving far too much credit, I guess. If you look at the stable diffusion rates, it's one lump, two gigabytes, can download the weights. And these two gigabytes of neural network contain the entire visual universe, all the artistic styles, all the dinosaurs, all the spaceship, all the celebrities, everything is in there. You can generate it all, right? And this is, so it's much more than every individual of us could reproduce. And this is like 80% of what our brain is doing in these two gigabytes, plus a lot more, right, than what we can do. So this should give us an idea of how little you potentially might need in terms of capacity to produce what we are doing. And so our brain, of course, is usually redundant and so on. If you take out half the brain early enough, it's still going to work pretty much the same way. But it's amazing that this works at all. And also, these systems are not coupled to the environment. For us, the world is learnable because we are directly coupled to it. And we basically become resonators with the world and build a resonant model that is the best that we can do with this recurrent network, with the spatial temporal resolution and modeling depth. Should AI be conscious? It's not clear if it doesn't have to, right? It could be that the algorithms that are envisioned will never be very efficient on NVIDIA hardware, and while they might work in principle, it turns out that you need to train the system for 14 years until it's able to do anything, and for 26 years until it gets to a PhD level. So that would be very awkward. But imagine that the question, should it be conscious? Imagine we could do this. Should we be doing this? There is this question. It seems to be a little bit in bad taste to say, I have a product here, and it's a conscious agent. And you can put it in the car or a washing machine and whatever. But who knows? Maybe we can build it in such a way that it's OK with it, and society will adapt to this. Maybe there's nothing wrong with this, with having stuff that is conscious everywhere. Whether it suffers is really a question of the setup. Same with people, right? So yeah, but there are applications that are only enabled that are important with conscious AI agents that are conscious in a similar way as us. And I think there might be. And one is, for instance, if you think about perceptual empathy. Perceptual empathy is a little bit different than cognitive empathy. I say this especially for the nerds among you, because some of you might not know, but highly empathetic, normal people, like not me, they know that there is perceptual empathy, which means that you perceive the emotions of other rather than making inference over their facial expressions in the context that they're in. And when you do this, you basically have a bidirectional feedback loop to the mind of the other person, because you're very close to them and you vibe with them. And it means that you can have mental states together that you couldn't have alone. Imagine that you could do this as an AI. Imagine that you could have an AI that's basically understanding what you want to draw on the screen and so intuitively that you could do this as an AI. Imagine that you could have an AI that's basically understanding what you want to draw on the screen. And so intuitively that you feel it's me that is drawing this on the screen right now. And this is mostly, you do have a little bit of manual input. And so it's mostly observing you and making real-time inference on your mental state because it's vibing with you. And to do this, you would need to sample you at a high multiple of the frequency at which your nervous system operates and adapt to what's happening in your nervous system by going into resonance with it in a similar way as people do this with each other, right? That would be something that might be enabled by this class of systems. And it might be super hard to do with a GPU-based machine learning algorithm that's been trained offline. Another thing is, how could we align with AGI? I'm personally not a doomer in any way, but I am an AI expectationalist. I expect that AGI is going to happen, and probably not very late in our future. And so we have to deal with it, and it has to deal with us. And you can probably not align something that is actually smarter and more lucid than you are. Because if you are smart and lucid, nobody is going to align you. You align yourself. How would AI be different? So how can you make sure that something that is able to be smarter and more lucid than you and is going to align itself, it wants to coexist with you? It probably needs to perceive that you are a conscious being and value that and want to integrate with you somehow. It needs to actually love you. And so if you are stuck with the system for some reason, because somebody at some point will push the button and we will have an AGI that might be smarter than us, it would be probably good if it was conscious. So that is also a point that I want you guys to be in mind. Is consciousness all there could be? No, I think consciousness might actually be quite boring. There's limitations, like my perceptual now is only three seconds. Come on, this could be much longer. It's just my brain. It's not good enough to have longer perceptual now. Also, I only see one possible interpretation at a time. This is ridiculous. Very often, I look at the Necker cube. I know there's two interpretations. Why do I only see one? This is a limitation of how my brain operates. I want to see superpositions where they are. And I want to not have one perspective. I want to have all the possible perspectives that can be generated in a decent mind. So there are possible extensions to consciousness that are limitations of the consciousness might be limitations of my brain, not of the principles of the self-observing observer that is trying to interpret reality in the best possible way. And so it might also be interesting to see how we can build systems that are actually conscious in more interesting ways than we are. That's it.", '14.611959457397461')